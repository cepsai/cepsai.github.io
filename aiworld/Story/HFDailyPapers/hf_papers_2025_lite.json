[
  {
    "arxiv_id": "2412.21139",
    "first_seen_date": "2024-12-31",
    "title": "Training Software Engineering Agents and Verifiers with SWE-Gym",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2412.21139Training Software Engineering Agents and Verifiers with SWE-GymPublished on Dec 30, 2024\u00b7Submitted byJiayi Panon Dec 31, 2024Upvote24+16Authors:Jiayi Pan,Xingyao Wang,Graham Neubig,Navdeep Jaitly,Heng Ji,Alane Suhr,Yizhe ZhangAbstractAn environment for training software engineering agents using real-world Python tasks and language models achieves state-of-the-art performance on SWE-Bench.AI-generated summaryWe presentSWE-Gym, the first environment for training real-world software\nengineering (SWE) agents.SWE-Gymcontains 2,438 real-world Python task\ninstances, each comprising a codebase with an executable runtime environment,\nunit tests, and a task specified in natural language. We useSWE-Gymto trainlanguage modelbasedSWE agents, achieving up to 19% absolute gains in resolve\nrate on the popularSWE-Bench Verifiedand Lite test sets. We also experiment\nwithinference-time scalingthroughverifierstrained onagent trajectoriessampled fromSWE-Gym. When combined with our fine-tunedSWE agents, we achieve\n32.0% and 26.0% onSWE-Bench Verifiedand Lite, respectively, reflecting a new\nstate-of-the-art for open-weightSWE agents. To facilitate further research, we\npublicly releaseSWE-Gym, models, andagent trajectories.View arXiv pageView PDFGitHub611autoAdd to collectionCommunityJiayi-PanPaper authorPaper submitterDec 31, 2024We present SWE-Gym, the first environment for training real-world software engineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task instances, each comprising a codebase with an executable runtime environment, unit tests, and a task specified in natural language. We use SWE-Gym to train language model based SWE agents , achieving up to 19% absolute gains in resolve rate on the popular SWE-Bench Verified and Lite test sets. We also experiment with inference-time scaling through verifiers trained on agent trajectories sampled from SWE-Gym. When combined w",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2412,
    "github_repo": "https://github.com/swe-gym/swe-gym",
    "hf_paper_url": "https://huggingface.co/papers/2412.21139",
    "arxiv_url": "https://arxiv.org/abs/2412.21139",
    "num_models": 9,
    "models_list": "OpenHands/openhands-lm-32b-v0.1, OpenHands/openhands-lm-7b-v0.1, OpenHands/openhands-lm-1.5b-v0.1, stelterlab/openhands-lm-32b-v0.1-AWQ, OpenHands/openhands-lm-32b-v0.1-ep3, lucyknada/all-hands_openhands-lm-32b-v0.1-exl2, imkebe/openhands-lm-7b-v0.1-rk3588-1.2.0, limcheekin/openhands-lm-7b-v0.1-rk3588-1.1.4, Mungert/openhands-lm-7b-v0.1-GGUF",
    "models_links": "https://huggingface.co/OpenHands/openhands-lm-32b-v0.1, https://huggingface.co/OpenHands/openhands-lm-7b-v0.1, https://huggingface.co/OpenHands/openhands-lm-1.5b-v0.1, https://huggingface.co/stelterlab/openhands-lm-32b-v0.1-AWQ, https://huggingface.co/OpenHands/openhands-lm-32b-v0.1-ep3, https://huggingface.co/lucyknada/all-hands_openhands-lm-32b-v0.1-exl2, https://huggingface.co/imkebe/openhands-lm-7b-v0.1-rk3588-1.2.0, https://huggingface.co/limcheekin/openhands-lm-7b-v0.1-rk3588-1.1.4, https://huggingface.co/Mungert/openhands-lm-7b-v0.1-GGUF",
    "models_detailed": "[{\"name\": \"OpenHands/openhands-lm-32b-v0.1\", \"link\": \"https://huggingface.co/OpenHands/openhands-lm-32b-v0.1\", \"task\": \"Text Generation\", \"likes\": \"257\", \"downloads\": \"\", \"updated\": \"Apr 16\"}, {\"name\": \"OpenHands/openhands-lm-7b-v0.1\", \"link\": \"https://huggingface.co/OpenHands/openhands-lm-7b-v0.1\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Apr 16\"}, {\"name\": \"OpenHands/openhands-lm-1.5b-v0.1\", \"link\": \"https://huggingface.co/OpenHands/openhands-lm-1.5b-v0.1\", \"task\": \"Text Generation\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"Apr 16\"}, {\"name\": \"stelterlab/openhands-lm-32b-v0.1-AWQ\", \"link\": \"https://huggingface.co/stelterlab/openhands-lm-32b-v0.1-AWQ\", \"task\": \"Text Generation\", \"likes\": \"5\", \"downloads\": \"\", \"updated\": \"Apr 2\"}, {\"name\": \"OpenHands/openhands-lm-32b-v0.1-ep3\", \"link\": \"https://huggingface.co/OpenHands/openhands-lm-32b-v0.1-ep3\", \"task\": \"Text Generation\", \"likes\": \"74\", \"downloads\": \"\", \"updated\": \"Apr 16\"}, {\"name\": \"lucyknada/all-hands_openhands-lm-32b-v0.1-exl2\", \"link\": \"https://huggingface.co/lucyknada/all-hands_openhands-lm-32b-v0.1-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 1\"}, {\"name\": \"imkebe/openhands-lm-7b-v0.1-rk3588-1.2.0\", \"link\": \"https://huggingface.co/imkebe/openhands-lm-7b-v0.1-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Apr 15\"}, {\"name\": \"limcheekin/openhands-lm-7b-v0.1-rk3588-1.1.4\", \"link\": \"https://huggingface.co/limcheekin/openhands-lm-7b-v0.1-rk3588-1.1.4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 17\"}, {\"name\": \"Mungert/openhands-lm-7b-v0.1-GGUF\", \"link\": \"https://huggingface.co/Mungert/openhands-lm-7b-v0.1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2412.19326",
    "first_seen_date": "2024-12-30",
    "title": "Task Preference Optimization: Improving Multimodal Large Language Models\n  with Vision Task Alignment",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2412.19326Task Preference Optimization: Improving Multimodal Large Language Models\n  with Vision Task AlignmentPublished on Dec 26, 2024\u00b7Submitted byyinanheon Dec 30, 2024Upvote18+10Authors:Ziang Yan,Zhilin Li,Yinan He,Chenting Wang,Kunchang Li,Xinhao Li,Xiangyu Zeng,Zilei Wang,Yali Wang,Yu Qiao,Limin Wang,Yi WangAbstractTask Preference Optimization (TPO) enhances multimodal large language models with fine-grained visual tasks by using learnable task tokens and multi-task co-training, leading to significant performance improvements and robust zero-shot capabilities.AI-generated summaryCurrentmultimodal large language models(MLLMs) struggle with fine-grained\nor precise understanding of visuals though they give comprehensive perception\nand reasoning in a spectrum of vision applications. Recent studies either\ndevelop tool-using or unify specific visual tasks into the autoregressive\nframework, often at the expense of overall multimodal performance. To address\nthis issue and enhance MLLMs with visual tasks in a scalable fashion, we\nproposeTask Preference Optimization(TPO), a novel method that utilizes\ndifferentiable task preferences derived from typicalfine-grained visual tasks.TPOintroduceslearnable task tokensthat establish connections between\nmultiple task-specific heads and the MLLM. By leveraging rich visual labels\nduring training,TPOsignificantly enhances the MLLM'smultimodal capabilitiesand task-specific performance. Throughmulti-task co-trainingwithinTPO, we\nobserve synergistic benefits that elevate individual task performance beyond\nwhat is achievable through single-task training methodologies. Our\ninstantiation of this approach with VideoChat and LLaVA demonstrates an overall\n14.6% improvement in multimodal performance compared to baseline models.\nAdditionally, MLLM-TPOdemonstrates robustzero-shot capabilitiesacross\nvarious tasks, performing comparably to state-of-the-art sup",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2412,
    "github_repo": "https://github.com/opengvlab/tpo",
    "hf_paper_url": "https://huggingface.co/papers/2412.19326",
    "arxiv_url": "https://arxiv.org/abs/2412.19326",
    "num_models": 1,
    "models_list": "OpenGVLab/VideoChat-TPO",
    "models_links": "https://huggingface.co/OpenGVLab/VideoChat-TPO",
    "models_detailed": "[{\"name\": \"OpenGVLab/VideoChat-TPO\", \"link\": \"https://huggingface.co/OpenGVLab/VideoChat-TPO\", \"task\": \"\", \"likes\": \"40\", \"downloads\": \"\", \"updated\": \"Jan 2\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2412.15084",
    "first_seen_date": "2024-12-20",
    "title": "AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward\n  Modeling",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2412.15084AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward\n  ModelingPublished on Dec 19, 2024\u00b7Submitted byYang Chenon Dec 20, 2024Upvote13+5Authors:Zihan Liu,Yang Chen,Mohammad Shoeybi,Bryan Catanzaro,Wei PingAbstractIn this paper, we introduce AceMath, a suite of frontiermath modelsthat\nexcel in solving complex math problems, along with highly effective reward\nmodels capable of evaluating generated solutions and reliably identifying the\ncorrect ones. To develop theinstruction-tunedmath models, we propose asupervised fine-tuning (SFT)process that first achieves competitive\nperformance across general domains, followed by targetedfine-tuningfor the\nmath domain using a carefully curated set ofpromptsand synthetically\ngenerated responses. The resulting model,AceMath-72B-Instructgreatly\noutperformsQwen2.5-Math-72B-Instruct,GPT-4oandClaude-3.5Sonnet. To develop\nmath-specialized reward model, we first constructAceMath-RewardBench, a\ncomprehensive and robustbenchmarkfor evaluating mathreward modelsacross\ndiverse problems and difficulty levels. After that, we present a systematic\napproach to build our mathreward models. The resulting model, AceMath-72B-RM,\nconsistently outperforms state-of-the-artreward models. Furthermore, when\ncombiningAceMath-72B-Instructwith AceMath-72B-RM, we achieve the highest\naveragerm@8 scoreacross themath reasoning benchmarks. We will release model\nweights, training data, and evaluationbenchmarks at:\nhttps://research.nvidia.com/labs/adlr/acemathView arXiv pageView PDFProject pageAdd to collectionCommunityychenNLPPaper authorPaper submitterDec 20, 2024\u2022edited Jan 19, 2025AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward ModelingModel & Training data:https://huggingface.co/collections/nvidia/acemath-678917d12f09885479d549feSee translationReplylibrarian-botDec 21, 2024This is an automated message from theLibrarian Bot.",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2412,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2412.15084",
    "arxiv_url": "https://arxiv.org/abs/2412.15084",
    "num_models": 16,
    "models_list": "nvidia/AceMath-7B-Instruct, nvidia/AceInstruct-7B, nvidia/AceMath-72B-Instruct, nvidia/AceInstruct-1.5B, nvidia/AceMath-1.5B-Instruct, nvidia/AceMath-72B-RM, nvidia/AceMath-7B-RM, nvidia/AceInstruct-72B, inarikami/AceMath-72B-Instruct-GGUF, redponike/AceMath-72B-Instruct-GGUF, RichardErkhov/nvidia_-_AceMath-7B-Instruct-4bits, RichardErkhov/nvidia_-_AceMath-7B-Instruct-8bits, Mungert/AceMath-1.5B-Instruct-GGUF, Mungert/AceInstruct-1.5B-GGUF, Mungert/AceInstruct-7B-GGUF, Mungert/AceMath-7B-Instruct-GGUF",
    "models_links": "https://huggingface.co/nvidia/AceMath-7B-Instruct, https://huggingface.co/nvidia/AceInstruct-7B, https://huggingface.co/nvidia/AceMath-72B-Instruct, https://huggingface.co/nvidia/AceInstruct-1.5B, https://huggingface.co/nvidia/AceMath-1.5B-Instruct, https://huggingface.co/nvidia/AceMath-72B-RM, https://huggingface.co/nvidia/AceMath-7B-RM, https://huggingface.co/nvidia/AceInstruct-72B, https://huggingface.co/inarikami/AceMath-72B-Instruct-GGUF, https://huggingface.co/redponike/AceMath-72B-Instruct-GGUF, https://huggingface.co/RichardErkhov/nvidia_-_AceMath-7B-Instruct-4bits, https://huggingface.co/RichardErkhov/nvidia_-_AceMath-7B-Instruct-8bits, https://huggingface.co/Mungert/AceMath-1.5B-Instruct-GGUF, https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF, https://huggingface.co/Mungert/AceInstruct-7B-GGUF, https://huggingface.co/Mungert/AceMath-7B-Instruct-GGUF",
    "models_detailed": "[{\"name\": \"nvidia/AceMath-7B-Instruct\", \"link\": \"https://huggingface.co/nvidia/AceMath-7B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"412\", \"downloads\": \"\", \"updated\": \"Jan 17\"}, {\"name\": \"nvidia/AceInstruct-7B\", \"link\": \"https://huggingface.co/nvidia/AceInstruct-7B\", \"task\": \"Text Generation\", \"likes\": \"365\", \"downloads\": \"\", \"updated\": \"Jan 16\"}, {\"name\": \"nvidia/AceMath-72B-Instruct\", \"link\": \"https://huggingface.co/nvidia/AceMath-72B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"656\", \"downloads\": \"\", \"updated\": \"Jan 17\"}, {\"name\": \"nvidia/AceInstruct-1.5B\", \"link\": \"https://huggingface.co/nvidia/AceInstruct-1.5B\", \"task\": \"Text Generation\", \"likes\": \"976\", \"downloads\": \"\", \"updated\": \"Jan 16\"}, {\"name\": \"nvidia/AceMath-1.5B-Instruct\", \"link\": \"https://huggingface.co/nvidia/AceMath-1.5B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 17\"}, {\"name\": \"nvidia/AceMath-72B-RM\", \"link\": \"https://huggingface.co/nvidia/AceMath-72B-RM\", \"task\": \"Text Generation\", \"likes\": \"637\", \"downloads\": \"\", \"updated\": \"Jan 17\"}, {\"name\": \"nvidia/AceMath-7B-RM\", \"link\": \"https://huggingface.co/nvidia/AceMath-7B-RM\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 17\"}, {\"name\": \"nvidia/AceInstruct-72B\", \"link\": \"https://huggingface.co/nvidia/AceInstruct-72B\", \"task\": \"Text Generation\", \"likes\": \"247\", \"downloads\": \"\", \"updated\": \"Jan 16\"}, {\"name\": \"inarikami/AceMath-72B-Instruct-GGUF\", \"link\": \"https://huggingface.co/inarikami/AceMath-72B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 22\"}, {\"name\": \"redponike/AceMath-72B-Instruct-GGUF\", \"link\": \"https://huggingface.co/redponike/AceMath-72B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"193\", \"downloads\": \"\", \"updated\": \"Feb 16\"}, {\"name\": \"RichardErkhov/nvidia_-_AceMath-7B-Instruct-4bits\", \"link\": \"https://huggingface.co/RichardErkhov/nvidia_-_AceMath-7B-Instruct-4bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"RichardErkhov/nvidia_-_AceMath-7B-Instruct-8bits\", \"link\": \"https://huggingface.co/RichardErkhov/nvidia_-_AceMath-7B-Instruct-8bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"Mungert/AceMath-1.5B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/AceMath-1.5B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/AceInstruct-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/AceInstruct-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/AceInstruct-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/AceInstruct-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/AceMath-7B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/AceMath-7B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 3,
    "datasets_list": "nvidia/AceMath-Instruct-Training-Data, nvidia/AceMath-RewardBench, nvidia/AceMath-RM-Training-Data",
    "datasets_links": "https://huggingface.co/datasets/nvidia/AceMath-Instruct-Training-Data, https://huggingface.co/datasets/nvidia/AceMath-RewardBench, https://huggingface.co/datasets/nvidia/AceMath-RM-Training-Data",
    "datasets_detailed": "[{\"name\": \"nvidia/AceMath-Instruct-Training-Data\", \"link\": \"https://huggingface.co/datasets/nvidia/AceMath-Instruct-Training-Data\", \"task\": \"\", \"likes\": \"443\", \"downloads\": \"\", \"updated\": \"Jan 17\", \"size\": \"\"}, {\"name\": \"nvidia/AceMath-RewardBench\", \"link\": \"https://huggingface.co/datasets/nvidia/AceMath-RewardBench\", \"task\": \"\", \"likes\": \"415\", \"downloads\": \"\", \"updated\": \"Jan 17\", \"size\": \"\"}, {\"name\": \"nvidia/AceMath-RM-Training-Data\", \"link\": \"https://huggingface.co/datasets/nvidia/AceMath-RM-Training-Data\", \"task\": \"\", \"likes\": \"285\", \"downloads\": \"\", \"updated\": \"Jan 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2412.15115",
    "first_seen_date": "2024-12-20",
    "title": "Qwen2.5 Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2412.15115Qwen2.5 Technical ReportPublished on Dec 19, 2024\u00b7Submitted byBinyuan Huion Dec 20, 2024#1 Paper of the dayUpvote376+368Authors:Qwen,An Yang,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chengyuan Li,Dayiheng Liu,Fei Huang,Haoran Wei,Huan Lin,Jian Yang,Jianhong Tu,Jianwei Zhang,Jianxin Yang,Jiaxi Yang,Jingren Zhou,Junyang Lin,Kai Dang,Keming Lu,Keqin Bao+20 authorsAbstractQwen2.5, an enhanced series of large language models, demonstrates superior performance across various benchmarks and use cases through extensive pre-training and advanced post-training techniques.AI-generated summaryIn this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both thepre-trainingandpost-trainingstages. In terms ofpre-training, we have scaled\nthe high-qualitypre-trainingdatasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, andreasoningcapabilities. In terms ofpost-training, we implement\nintricatesupervised finetuningwith over 1 million samples, as well as\nmultistagereinforcement learning.Post-trainingtechniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include twomixture-of-experts(MoE) variants:Qwen2.5-TurboandQwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluatinglanguage understanding,reasoning,mathematics,coding,human ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2412,
    "github_repo": "https://github.com/QwenLM/Qwen2.5",
    "hf_paper_url": "https://huggingface.co/papers/2412.15115",
    "arxiv_url": "https://arxiv.org/abs/2412.15115",
    "num_models": 109,
    "models_list": "Qwen/QwQ-32B, Qwen/QwQ-32B-GGUF, Qwen/QwQ-32B-AWQ, Motif-Technologies/Motif-2.6B, baichuan-inc/Baichuan-Audio-Base, baichuan-inc/Baichuan-Audio-Instruct, Userb1az/QwQ-32B-GGUF, FunAudioLLM/InspireMusic-Base, FunAudioLLM/InspireMusic-1.5B, FunAudioLLM/InspireMusic-1.5B-Long, FunAudioLLM/InspireMusic-1.5B-24kHz, FunAudioLLM/InspireMusic-Base-24kHz, Triangle104/Qwen2.5-14B-Instruct-1M-Q4_K_S-GGUF, Triangle104/Qwen2.5-14B-Instruct-1M-Q4_K_M-GGUF, Triangle104/Qwen2.5-14B-Instruct-1M-Q5_K_S-GGUF, Triangle104/Qwen2.5-14B-Instruct-1M-Q5_K_M-GGUF, Triangle104/Qwen2.5-14B-Instruct-1M-Q6_K-GGUF, Triangle104/Qwen2.5-14B-Instruct-1M-Q8_0-GGUF, VoidStare/Qwen2.5-14B-Instruct-1M-EXL2-8.0bpw-h8, unsloth/Qwen2.5-7B-Instruct-1M, unsloth/Qwen2.5-7B-Instruct-1M-unsloth-bnb-4bit, unsloth/Qwen2.5-7B-Instruct-1M-bnb-4bit, unsloth/Qwen2.5-14B-Instruct-1M, unsloth/Qwen2.5-14B-Instruct-1M-unsloth-bnb-4bit, unsloth/Qwen2.5-14B-Instruct-1M-bnb-4bit, Triangle104/Qwen2.5-7B-Instruct-1M-Q4_K_S-GGUF, Triangle104/Qwen2.5-7B-Instruct-1M-Q4_K_M-GGUF, Triangle104/Qwen2.5-7B-Instruct-1M-Q5_K_S-GGUF, Triangle104/Qwen2.5-7B-Instruct-1M-Q5_K_M-GGUF, Triangle104/Qwen2.5-7B-Instruct-1M-Q6_K-GGUF, Triangle104/Qwen2.5-7B-Instruct-1M-Q8_0-GGUF, VoidStare/Qwen2.5-7B-Instruct-1M-EXL2-8.0bpw-h8, xwen-team/Xwen-7B-Chat, xwen-team/Xwen-72B-Chat, rinna/qwen2.5-bakeneko-32b, rinna/qwen2.5-bakeneko-32b-instruct, rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b, unsloth/QwQ-32B, Emova-ollm/Qwen2.5-3B-Instruct_add_speech_token_4096_nostrip, Emova-ollm/Qwen2.5-7B-Instruct_add_speech_token_4096_nostrip, Can111/m1-32b, braindao/QwQ-32B, rinna/qwq-bakeneko-32b, rinna/qwq-bakeneko-32b-awq, rinna/qwq-bakeneko-32b-gguf, rinna/qwq-bakeneko-32b-gptq-int8, rinna/qwq-bakeneko-32b-gptq-int4, LLMJapan/qwq-bakeneko-32b_exl2_8.0bpw, rinna/qwen2.5-bakeneko-32b-instruct-v2-awq, rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf, rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int8, rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int4, medmekk/QwQ-32B-4bit-3, fantos/QwQ-32B-bnb-4bit, kimleang123/QwQ-32B-bnb-4bit, wkplhc/QwQ-32B-bnb-4bit, medmekk/serere, bnb-community/QwQ-32B-bnb-4bit, Compumacy/QWQ, ginipick/QwQ-32B-NF4, tanvij/nvila_quant1, DimensionSTP/QwQ-32B-Ko-Reasoning, mozilla-ai/QwQ-32B-llamafile, svgeek/QwQ-32B, clowman/QwQ-32B-AWQ-Int4, clowman/QwQ-32B-GPTQ-Int4, clowman/QwQ-32B-GPTQ-Int8, clowman/QwQ-32B-Dynamic-F8, async0x42/QwQ-32B-exl2_4.65bpw, peezyrhodes/ai, Mungert/QwQ-32B-GGUF, Jellon/QwQ-32B-exl3-4bpw, TeeZee/QwQ-32B-bpw4.0-h8-exl2, TeeZee/QwQ-32B-bpw8.0-h8-exl2, TeeZee/QwQ-32B-bpw6.0-h8-exl2, imkebe/QwQ-32B-rk3588-1.2.0, eggbiscuit/Qwen2.5-0.5B-Distilled-from-1.5B-AlpacaChinese-v1, prithivMLmods/Tureis-Qwen3_QWQ-4B-Exp, prithivMLmods/Bootes-Qwen3_Coder-Reasoning, prithivMLmods/Sculptor-Qwen3_Med-Reasoning, prithivMLmods/Segue-Qwen3_DeepScaleR-Preview, prithivMLmods/Cetus-Qwen3_4B-GeneralThought, prithivMLmods/Vulpecula-4B, prithivMLmods/Magpie-Qwen-CortexDual-0.6B, prithivMLmods/Magpie-Qwen-DiMind-1.7B, prithivMLmods/Lynx-TinySync-0.6B, prithivMLmods/Draco-CoderMini-3B, prithivMLmods/BetaCeti-Beta-4B-Prime1, prithivMLmods/Nenque-MoT-0.6B-Elite14, prithivMLmods/Theta-Crucis-0.6B-Turbo1, prithivMLmods/Eta-Aurigae-0.6B-Echelon1, prithivMLmods/Capricornus-MoT-1.7B-Supreme1, prithivMLmods/Lacaille-MoT-4B-Supreme2, prithivMLmods/GCIRS-Reasoning-1.5B-R1, prithivMLmods/Blitzar-Coder-4B-F.1, qualcomm/Qwen2.5-7B-Instruct, TeleAI-AI-Flow/AI-Flow-Ruyi-7B-Preview0704, MichiganNLP/TAMA-QWen2.5, MichiganNLP/TAMA-QWen3, TeleAI-AI-Flow/AI-Flow-Ruyi-7B-0725, OpenLLM-Korea/Motif-2.6B, prithivMLmods/rStar-Coder-Qwen3-0.6B, Inceptive/ROLEPL-AI-v2-Qwen2.5-32B, Inceptive/ROLEPL-AI-v2-Qwen2.5-7B, Inceptive/ROLEPL-AI-v2-Qwen2.5-72B, Nirav-Madhani/vla-adapter-gr00t-g1-bridgeattention, qualcomm/Qwen2.5-1.5B-Instruct, classtag/siq-vl_siglip2-so400m-patch14-224_qwen2.5-1.5b-instruct_stage1, classtag/siq-vl_siglip2-so400m-patch16-512_qwen2.5-1.5b-instruct_stage1",
    "models_links": "https://huggingface.co/Qwen/QwQ-32B, https://huggingface.co/Qwen/QwQ-32B-GGUF, https://huggingface.co/Qwen/QwQ-32B-AWQ, https://huggingface.co/Motif-Technologies/Motif-2.6B, https://huggingface.co/baichuan-inc/Baichuan-Audio-Base, https://huggingface.co/baichuan-inc/Baichuan-Audio-Instruct, https://huggingface.co/Userb1az/QwQ-32B-GGUF, https://huggingface.co/FunAudioLLM/InspireMusic-Base, https://huggingface.co/FunAudioLLM/InspireMusic-1.5B, https://huggingface.co/FunAudioLLM/InspireMusic-1.5B-Long, https://huggingface.co/FunAudioLLM/InspireMusic-1.5B-24kHz, https://huggingface.co/FunAudioLLM/InspireMusic-Base-24kHz, https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q4_K_S-GGUF, https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q4_K_M-GGUF, https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q5_K_S-GGUF, https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q5_K_M-GGUF, https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q6_K-GGUF, https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q8_0-GGUF, https://huggingface.co/VoidStare/Qwen2.5-14B-Instruct-1M-EXL2-8.0bpw-h8, https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-1M, https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-1M-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-1M-bnb-4bit, https://huggingface.co/unsloth/Qwen2.5-14B-Instruct-1M, https://huggingface.co/unsloth/Qwen2.5-14B-Instruct-1M-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen2.5-14B-Instruct-1M-bnb-4bit, https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q4_K_S-GGUF, https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q4_K_M-GGUF, https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q5_K_S-GGUF, https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q5_K_M-GGUF, https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q6_K-GGUF, https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q8_0-GGUF, https://huggingface.co/VoidStare/Qwen2.5-7B-Instruct-1M-EXL2-8.0bpw-h8, https://huggingface.co/xwen-team/Xwen-7B-Chat, https://huggingface.co/xwen-team/Xwen-72B-Chat, https://huggingface.co/rinna/qwen2.5-bakeneko-32b, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct, https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b, https://huggingface.co/unsloth/QwQ-32B, https://huggingface.co/Emova-ollm/Qwen2.5-3B-Instruct_add_speech_token_4096_nostrip, https://huggingface.co/Emova-ollm/Qwen2.5-7B-Instruct_add_speech_token_4096_nostrip, https://huggingface.co/Can111/m1-32b, https://huggingface.co/braindao/QwQ-32B, https://huggingface.co/rinna/qwq-bakeneko-32b, https://huggingface.co/rinna/qwq-bakeneko-32b-awq, https://huggingface.co/rinna/qwq-bakeneko-32b-gguf, https://huggingface.co/rinna/qwq-bakeneko-32b-gptq-int8, https://huggingface.co/rinna/qwq-bakeneko-32b-gptq-int4, https://huggingface.co/LLMJapan/qwq-bakeneko-32b_exl2_8.0bpw, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-awq, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int8, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int4, https://huggingface.co/medmekk/QwQ-32B-4bit-3, https://huggingface.co/fantos/QwQ-32B-bnb-4bit, https://huggingface.co/kimleang123/QwQ-32B-bnb-4bit, https://huggingface.co/wkplhc/QwQ-32B-bnb-4bit, https://huggingface.co/medmekk/serere, https://huggingface.co/bnb-community/QwQ-32B-bnb-4bit, https://huggingface.co/Compumacy/QWQ, https://huggingface.co/ginipick/QwQ-32B-NF4, https://huggingface.co/tanvij/nvila_quant1, https://huggingface.co/DimensionSTP/QwQ-32B-Ko-Reasoning, https://huggingface.co/mozilla-ai/QwQ-32B-llamafile, https://huggingface.co/svgeek/QwQ-32B, https://huggingface.co/clowman/QwQ-32B-AWQ-Int4, https://huggingface.co/clowman/QwQ-32B-GPTQ-Int4, https://huggingface.co/clowman/QwQ-32B-GPTQ-Int8, https://huggingface.co/clowman/QwQ-32B-Dynamic-F8, https://huggingface.co/async0x42/QwQ-32B-exl2_4.65bpw, https://huggingface.co/peezyrhodes/ai, https://huggingface.co/Mungert/QwQ-32B-GGUF, https://huggingface.co/Jellon/QwQ-32B-exl3-4bpw, https://huggingface.co/TeeZee/QwQ-32B-bpw4.0-h8-exl2, https://huggingface.co/TeeZee/QwQ-32B-bpw8.0-h8-exl2, https://huggingface.co/TeeZee/QwQ-32B-bpw6.0-h8-exl2, https://huggingface.co/imkebe/QwQ-32B-rk3588-1.2.0, https://huggingface.co/eggbiscuit/Qwen2.5-0.5B-Distilled-from-1.5B-AlpacaChinese-v1, https://huggingface.co/prithivMLmods/Tureis-Qwen3_QWQ-4B-Exp, https://huggingface.co/prithivMLmods/Bootes-Qwen3_Coder-Reasoning, https://huggingface.co/prithivMLmods/Sculptor-Qwen3_Med-Reasoning, https://huggingface.co/prithivMLmods/Segue-Qwen3_DeepScaleR-Preview, https://huggingface.co/prithivMLmods/Cetus-Qwen3_4B-GeneralThought, https://huggingface.co/prithivMLmods/Vulpecula-4B, https://huggingface.co/prithivMLmods/Magpie-Qwen-CortexDual-0.6B, https://huggingface.co/prithivMLmods/Magpie-Qwen-DiMind-1.7B, https://huggingface.co/prithivMLmods/Lynx-TinySync-0.6B, https://huggingface.co/prithivMLmods/Draco-CoderMini-3B, https://huggingface.co/prithivMLmods/BetaCeti-Beta-4B-Prime1, https://huggingface.co/prithivMLmods/Nenque-MoT-0.6B-Elite14, https://huggingface.co/prithivMLmods/Theta-Crucis-0.6B-Turbo1, https://huggingface.co/prithivMLmods/Eta-Aurigae-0.6B-Echelon1, https://huggingface.co/prithivMLmods/Capricornus-MoT-1.7B-Supreme1, https://huggingface.co/prithivMLmods/Lacaille-MoT-4B-Supreme2, https://huggingface.co/prithivMLmods/GCIRS-Reasoning-1.5B-R1, https://huggingface.co/prithivMLmods/Blitzar-Coder-4B-F.1, https://huggingface.co/qualcomm/Qwen2.5-7B-Instruct, https://huggingface.co/TeleAI-AI-Flow/AI-Flow-Ruyi-7B-Preview0704, https://huggingface.co/MichiganNLP/TAMA-QWen2.5, https://huggingface.co/MichiganNLP/TAMA-QWen3, https://huggingface.co/TeleAI-AI-Flow/AI-Flow-Ruyi-7B-0725, https://huggingface.co/OpenLLM-Korea/Motif-2.6B, https://huggingface.co/prithivMLmods/rStar-Coder-Qwen3-0.6B, https://huggingface.co/Inceptive/ROLEPL-AI-v2-Qwen2.5-32B, https://huggingface.co/Inceptive/ROLEPL-AI-v2-Qwen2.5-7B, https://huggingface.co/Inceptive/ROLEPL-AI-v2-Qwen2.5-72B, https://huggingface.co/Nirav-Madhani/vla-adapter-gr00t-g1-bridgeattention, https://huggingface.co/qualcomm/Qwen2.5-1.5B-Instruct, https://huggingface.co/classtag/siq-vl_siglip2-so400m-patch14-224_qwen2.5-1.5b-instruct_stage1, https://huggingface.co/classtag/siq-vl_siglip2-so400m-patch16-512_qwen2.5-1.5b-instruct_stage1",
    "models_detailed": "[{\"name\": \"Qwen/QwQ-32B\", \"link\": \"https://huggingface.co/Qwen/QwQ-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 11\"}, {\"name\": \"Qwen/QwQ-32B-GGUF\", \"link\": \"https://huggingface.co/Qwen/QwQ-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 13\"}, {\"name\": \"Qwen/QwQ-32B-AWQ\", \"link\": \"https://huggingface.co/Qwen/QwQ-32B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 11\"}, {\"name\": \"Motif-Technologies/Motif-2.6B\", \"link\": \"https://huggingface.co/Motif-Technologies/Motif-2.6B\", \"task\": \"Text Generation\", \"likes\": \"355\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"baichuan-inc/Baichuan-Audio-Base\", \"link\": \"https://huggingface.co/baichuan-inc/Baichuan-Audio-Base\", \"task\": \"\", \"likes\": \"148\", \"downloads\": \"\", \"updated\": \"Feb 25\"}, {\"name\": \"baichuan-inc/Baichuan-Audio-Instruct\", \"link\": \"https://huggingface.co/baichuan-inc/Baichuan-Audio-Instruct\", \"task\": \"\", \"likes\": \"303\", \"downloads\": \"\", \"updated\": \"Feb 25\"}, {\"name\": \"Userb1az/QwQ-32B-GGUF\", \"link\": \"https://huggingface.co/Userb1az/QwQ-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"FunAudioLLM/InspireMusic-Base\", \"link\": \"https://huggingface.co/FunAudioLLM/InspireMusic-Base\", \"task\": \"\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Apr 15\"}, {\"name\": \"FunAudioLLM/InspireMusic-1.5B\", \"link\": \"https://huggingface.co/FunAudioLLM/InspireMusic-1.5B\", \"task\": \"\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Mar 28\"}, {\"name\": \"FunAudioLLM/InspireMusic-1.5B-Long\", \"link\": \"https://huggingface.co/FunAudioLLM/InspireMusic-1.5B-Long\", \"task\": \"\", \"likes\": \"30\", \"downloads\": \"\", \"updated\": \"Mar 28\"}, {\"name\": \"FunAudioLLM/InspireMusic-1.5B-24kHz\", \"link\": \"https://huggingface.co/FunAudioLLM/InspireMusic-1.5B-24kHz\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Mar 28\"}, {\"name\": \"FunAudioLLM/InspireMusic-Base-24kHz\", \"link\": \"https://huggingface.co/FunAudioLLM/InspireMusic-Base-24kHz\", \"task\": \"\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Mar 28\"}, {\"name\": \"Triangle104/Qwen2.5-14B-Instruct-1M-Q4_K_S-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q4_K_S-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"Triangle104/Qwen2.5-14B-Instruct-1M-Q4_K_M-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q4_K_M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"Triangle104/Qwen2.5-14B-Instruct-1M-Q5_K_S-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q5_K_S-GGUF\", \"task\": \"Text Generation\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"Triangle104/Qwen2.5-14B-Instruct-1M-Q5_K_M-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q5_K_M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"Triangle104/Qwen2.5-14B-Instruct-1M-Q6_K-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q6_K-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"Triangle104/Qwen2.5-14B-Instruct-1M-Q8_0-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-14B-Instruct-1M-Q8_0-GGUF\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"VoidStare/Qwen2.5-14B-Instruct-1M-EXL2-8.0bpw-h8\", \"link\": \"https://huggingface.co/VoidStare/Qwen2.5-14B-Instruct-1M-EXL2-8.0bpw-h8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"unsloth/Qwen2.5-7B-Instruct-1M\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-1M\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"unsloth/Qwen2.5-7B-Instruct-1M-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-1M-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"unsloth/Qwen2.5-7B-Instruct-1M-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-7B-Instruct-1M-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"120\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"unsloth/Qwen2.5-14B-Instruct-1M\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-14B-Instruct-1M\", \"task\": \"Text Generation\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"unsloth/Qwen2.5-14B-Instruct-1M-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-14B-Instruct-1M-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"361\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"unsloth/Qwen2.5-14B-Instruct-1M-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-14B-Instruct-1M-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"Triangle104/Qwen2.5-7B-Instruct-1M-Q4_K_S-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q4_K_S-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"Triangle104/Qwen2.5-7B-Instruct-1M-Q4_K_M-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q4_K_M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"Triangle104/Qwen2.5-7B-Instruct-1M-Q5_K_S-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q5_K_S-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"Triangle104/Qwen2.5-7B-Instruct-1M-Q5_K_M-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q5_K_M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"Triangle104/Qwen2.5-7B-Instruct-1M-Q6_K-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q6_K-GGUF\", \"task\": \"Text Generation\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"Triangle104/Qwen2.5-7B-Instruct-1M-Q8_0-GGUF\", \"link\": \"https://huggingface.co/Triangle104/Qwen2.5-7B-Instruct-1M-Q8_0-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"VoidStare/Qwen2.5-7B-Instruct-1M-EXL2-8.0bpw-h8\", \"link\": \"https://huggingface.co/VoidStare/Qwen2.5-7B-Instruct-1M-EXL2-8.0bpw-h8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"xwen-team/Xwen-7B-Chat\", \"link\": \"https://huggingface.co/xwen-team/Xwen-7B-Chat\", \"task\": \"Text Generation\", \"likes\": \"26\", \"downloads\": \"\", \"updated\": \"Feb 4\"}, {\"name\": \"xwen-team/Xwen-72B-Chat\", \"link\": \"https://huggingface.co/xwen-team/Xwen-72B-Chat\", \"task\": \"Text Generation\", \"likes\": \"53\", \"downloads\": \"\", \"updated\": \"Feb 4\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct\", \"task\": \"Text Generation\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b\", \"link\": \"https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"unsloth/QwQ-32B\", \"link\": \"https://huggingface.co/unsloth/QwQ-32B\", \"task\": \"Text Generation\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Apr 27\"}, {\"name\": \"Emova-ollm/Qwen2.5-3B-Instruct_add_speech_token_4096_nostrip\", \"link\": \"https://huggingface.co/Emova-ollm/Qwen2.5-3B-Instruct_add_speech_token_4096_nostrip\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"Emova-ollm/Qwen2.5-7B-Instruct_add_speech_token_4096_nostrip\", \"link\": \"https://huggingface.co/Emova-ollm/Qwen2.5-7B-Instruct_add_speech_token_4096_nostrip\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 28\"}, {\"name\": \"Can111/m1-32b\", \"link\": \"https://huggingface.co/Can111/m1-32b\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"braindao/QwQ-32B\", \"link\": \"https://huggingface.co/braindao/QwQ-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"rinna/qwq-bakeneko-32b\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b\", \"task\": \"Text Generation\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwq-bakeneko-32b-awq\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b-awq\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwq-bakeneko-32b-gguf\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b-gguf\", \"task\": \"Text Generation\", \"likes\": \"135\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwq-bakeneko-32b-gptq-int8\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b-gptq-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwq-bakeneko-32b-gptq-int4\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b-gptq-int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"LLMJapan/qwq-bakeneko-32b_exl2_8.0bpw\", \"link\": \"https://huggingface.co/LLMJapan/qwq-bakeneko-32b_exl2_8.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 15\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct-v2-awq\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-awq\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf\", \"task\": \"Text Generation\", \"likes\": \"408\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int8\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int4\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"medmekk/QwQ-32B-4bit-3\", \"link\": \"https://huggingface.co/medmekk/QwQ-32B-4bit-3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"fantos/QwQ-32B-bnb-4bit\", \"link\": \"https://huggingface.co/fantos/QwQ-32B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"4\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"kimleang123/QwQ-32B-bnb-4bit\", \"link\": \"https://huggingface.co/kimleang123/QwQ-32B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"wkplhc/QwQ-32B-bnb-4bit\", \"link\": \"https://huggingface.co/wkplhc/QwQ-32B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"medmekk/serere\", \"link\": \"https://huggingface.co/medmekk/serere\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"bnb-community/QwQ-32B-bnb-4bit\", \"link\": \"https://huggingface.co/bnb-community/QwQ-32B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"Compumacy/QWQ\", \"link\": \"https://huggingface.co/Compumacy/QWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 28\"}, {\"name\": \"ginipick/QwQ-32B-NF4\", \"link\": \"https://huggingface.co/ginipick/QwQ-32B-NF4\", \"task\": \"Text Generation\", \"likes\": \"5\", \"downloads\": \"\", \"updated\": \"Mar 21\"}, {\"name\": \"tanvij/nvila_quant1\", \"link\": \"https://huggingface.co/tanvij/nvila_quant1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 25\"}, {\"name\": \"DimensionSTP/QwQ-32B-Ko-Reasoning\", \"link\": \"https://huggingface.co/DimensionSTP/QwQ-32B-Ko-Reasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"mozilla-ai/QwQ-32B-llamafile\", \"link\": \"https://huggingface.co/mozilla-ai/QwQ-32B-llamafile\", \"task\": \"Text Generation\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"Mar 31\"}, {\"name\": \"svgeek/QwQ-32B\", \"link\": \"https://huggingface.co/svgeek/QwQ-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 1\"}, {\"name\": \"clowman/QwQ-32B-AWQ-Int4\", \"link\": \"https://huggingface.co/clowman/QwQ-32B-AWQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 2\"}, {\"name\": \"clowman/QwQ-32B-GPTQ-Int4\", \"link\": \"https://huggingface.co/clowman/QwQ-32B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 2\"}, {\"name\": \"clowman/QwQ-32B-GPTQ-Int8\", \"link\": \"https://huggingface.co/clowman/QwQ-32B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 2\"}, {\"name\": \"clowman/QwQ-32B-Dynamic-F8\", \"link\": \"https://huggingface.co/clowman/QwQ-32B-Dynamic-F8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 2\"}, {\"name\": \"async0x42/QwQ-32B-exl2_4.65bpw\", \"link\": \"https://huggingface.co/async0x42/QwQ-32B-exl2_4.65bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 4\"}, {\"name\": \"peezyrhodes/ai\", \"link\": \"https://huggingface.co/peezyrhodes/ai\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 4\"}, {\"name\": \"Mungert/QwQ-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/QwQ-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"538\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Jellon/QwQ-32B-exl3-4bpw\", \"link\": \"https://huggingface.co/Jellon/QwQ-32B-exl3-4bpw\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Apr 12\"}, {\"name\": \"TeeZee/QwQ-32B-bpw4.0-h8-exl2\", \"link\": \"https://huggingface.co/TeeZee/QwQ-32B-bpw4.0-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"TeeZee/QwQ-32B-bpw8.0-h8-exl2\", \"link\": \"https://huggingface.co/TeeZee/QwQ-32B-bpw8.0-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"TeeZee/QwQ-32B-bpw6.0-h8-exl2\", \"link\": \"https://huggingface.co/TeeZee/QwQ-32B-bpw6.0-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"imkebe/QwQ-32B-rk3588-1.2.0\", \"link\": \"https://huggingface.co/imkebe/QwQ-32B-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"eggbiscuit/Qwen2.5-0.5B-Distilled-from-1.5B-AlpacaChinese-v1\", \"link\": \"https://huggingface.co/eggbiscuit/Qwen2.5-0.5B-Distilled-from-1.5B-AlpacaChinese-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 16\"}, {\"name\": \"prithivMLmods/Tureis-Qwen3_QWQ-4B-Exp\", \"link\": \"https://huggingface.co/prithivMLmods/Tureis-Qwen3_QWQ-4B-Exp\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"prithivMLmods/Bootes-Qwen3_Coder-Reasoning\", \"link\": \"https://huggingface.co/prithivMLmods/Bootes-Qwen3_Coder-Reasoning\", \"task\": \"Text Generation\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Aug 16\"}, {\"name\": \"prithivMLmods/Sculptor-Qwen3_Med-Reasoning\", \"link\": \"https://huggingface.co/prithivMLmods/Sculptor-Qwen3_Med-Reasoning\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"prithivMLmods/Segue-Qwen3_DeepScaleR-Preview\", \"link\": \"https://huggingface.co/prithivMLmods/Segue-Qwen3_DeepScaleR-Preview\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"prithivMLmods/Cetus-Qwen3_4B-GeneralThought\", \"link\": \"https://huggingface.co/prithivMLmods/Cetus-Qwen3_4B-GeneralThought\", \"task\": \"Text Generation\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"prithivMLmods/Vulpecula-4B\", \"link\": \"https://huggingface.co/prithivMLmods/Vulpecula-4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 23\"}, {\"name\": \"prithivMLmods/Magpie-Qwen-CortexDual-0.6B\", \"link\": \"https://huggingface.co/prithivMLmods/Magpie-Qwen-CortexDual-0.6B\", \"task\": \"Text Generation\", \"likes\": \"2\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"prithivMLmods/Magpie-Qwen-DiMind-1.7B\", \"link\": \"https://huggingface.co/prithivMLmods/Magpie-Qwen-DiMind-1.7B\", \"task\": \"Text Generation\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"prithivMLmods/Lynx-TinySync-0.6B\", \"link\": \"https://huggingface.co/prithivMLmods/Lynx-TinySync-0.6B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 26\"}, {\"name\": \"prithivMLmods/Draco-CoderMini-3B\", \"link\": \"https://huggingface.co/prithivMLmods/Draco-CoderMini-3B\", \"task\": \"Text Generation\", \"likes\": \"4\", \"downloads\": \"\", \"updated\": \"May 27\"}, {\"name\": \"prithivMLmods/BetaCeti-Beta-4B-Prime1\", \"link\": \"https://huggingface.co/prithivMLmods/BetaCeti-Beta-4B-Prime1\", \"task\": \"Text Generation\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"prithivMLmods/Nenque-MoT-0.6B-Elite14\", \"link\": \"https://huggingface.co/prithivMLmods/Nenque-MoT-0.6B-Elite14\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"prithivMLmods/Theta-Crucis-0.6B-Turbo1\", \"link\": \"https://huggingface.co/prithivMLmods/Theta-Crucis-0.6B-Turbo1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 2\"}, {\"name\": \"prithivMLmods/Eta-Aurigae-0.6B-Echelon1\", \"link\": \"https://huggingface.co/prithivMLmods/Eta-Aurigae-0.6B-Echelon1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 2\"}, {\"name\": \"prithivMLmods/Capricornus-MoT-1.7B-Supreme1\", \"link\": \"https://huggingface.co/prithivMLmods/Capricornus-MoT-1.7B-Supreme1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"prithivMLmods/Lacaille-MoT-4B-Supreme2\", \"link\": \"https://huggingface.co/prithivMLmods/Lacaille-MoT-4B-Supreme2\", \"task\": \"Text Generation\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Sep 7\"}, {\"name\": \"prithivMLmods/GCIRS-Reasoning-1.5B-R1\", \"link\": \"https://huggingface.co/prithivMLmods/GCIRS-Reasoning-1.5B-R1\", \"task\": \"Text Generation\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Jun 21\"}, {\"name\": \"prithivMLmods/Blitzar-Coder-4B-F.1\", \"link\": \"https://huggingface.co/prithivMLmods/Blitzar-Coder-4B-F.1\", \"task\": \"Text Generation\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"qualcomm/Qwen2.5-7B-Instruct\", \"link\": \"https://huggingface.co/qualcomm/Qwen2.5-7B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Dec 2\"}, {\"name\": \"TeleAI-AI-Flow/AI-Flow-Ruyi-7B-Preview0704\", \"link\": \"https://huggingface.co/TeleAI-AI-Flow/AI-Flow-Ruyi-7B-Preview0704\", \"task\": \"\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"MichiganNLP/TAMA-QWen2.5\", \"link\": \"https://huggingface.co/MichiganNLP/TAMA-QWen2.5\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Aug 22\"}, {\"name\": \"MichiganNLP/TAMA-QWen3\", \"link\": \"https://huggingface.co/MichiganNLP/TAMA-QWen3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 22\"}, {\"name\": \"TeleAI-AI-Flow/AI-Flow-Ruyi-7B-0725\", \"link\": \"https://huggingface.co/TeleAI-AI-Flow/AI-Flow-Ruyi-7B-0725\", \"task\": \"\", \"likes\": \"2\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"OpenLLM-Korea/Motif-2.6B\", \"link\": \"https://huggingface.co/OpenLLM-Korea/Motif-2.6B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"prithivMLmods/rStar-Coder-Qwen3-0.6B\", \"link\": \"https://huggingface.co/prithivMLmods/rStar-Coder-Qwen3-0.6B\", \"task\": \"Text Generation\", \"likes\": \"4\", \"downloads\": \"\", \"updated\": \"Aug 16\"}, {\"name\": \"Inceptive/ROLEPL-AI-v2-Qwen2.5-32B\", \"link\": \"https://huggingface.co/Inceptive/ROLEPL-AI-v2-Qwen2.5-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Inceptive/ROLEPL-AI-v2-Qwen2.5-7B\", \"link\": \"https://huggingface.co/Inceptive/ROLEPL-AI-v2-Qwen2.5-7B\", \"task\": \"Text Generation\", \"likes\": \"2\", \"downloads\": \"\", \"updated\": \"Sep 1\"}, {\"name\": \"Inceptive/ROLEPL-AI-v2-Qwen2.5-72B\", \"link\": \"https://huggingface.co/Inceptive/ROLEPL-AI-v2-Qwen2.5-72B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Nirav-Madhani/vla-adapter-gr00t-g1-bridgeattention\", \"link\": \"https://huggingface.co/Nirav-Madhani/vla-adapter-gr00t-g1-bridgeattention\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"qualcomm/Qwen2.5-1.5B-Instruct\", \"link\": \"https://huggingface.co/qualcomm/Qwen2.5-1.5B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Dec 2\"}, {\"name\": \"classtag/siq-vl_siglip2-so400m-patch14-224_qwen2.5-1.5b-instruct_stage1\", \"link\": \"https://huggingface.co/classtag/siq-vl_siglip2-so400m-patch14-224_qwen2.5-1.5b-instruct_stage1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 29\"}, {\"name\": \"classtag/siq-vl_siglip2-so400m-patch16-512_qwen2.5-1.5b-instruct_stage1\", \"link\": \"https://huggingface.co/classtag/siq-vl_siglip2-so400m-patch16-512_qwen2.5-1.5b-instruct_stage1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Dec 3\"}]",
    "num_datasets": 1,
    "datasets_list": "HuggingFaceTB/smoltalk2",
    "datasets_links": "https://huggingface.co/datasets/HuggingFaceTB/smoltalk2",
    "datasets_detailed": "[{\"name\": \"HuggingFaceTB/smoltalk2\", \"link\": \"https://huggingface.co/datasets/HuggingFaceTB/smoltalk2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2512.15489",
    "first_seen_date": "2025-12-19",
    "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2512.15489Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode SupervisionPublished on Dec 17\u00b7Submitted byWei Duon Dec 19\u00b7NVIDIAUpvote5Authors:Wei Du,Shubham Toshniwal,Branislav Kisacanin,Sadegh Mahdavi,Ivan Moshkov,George Armstrong,Stephen Ge,Edgar Minasyan,Feng Chen,Igor GitmanAbstractNemotron-Math, a large-scale mathematical reasoning dataset, enhances performance and robustness through diverse problem integration and efficient long-context training strategies.AI-generated summaryHigh-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging themulti-mode generationability ofgpt-oss-120b, we introduce Nemotron-Math, a large-scalemathematical reasoning datasetcontaining 7.5Msolution tracesacross high, medium, and lowreasoning modes, each available both with and withoutPython tool-integrated reasoning (TIR).\n  The dataset integrates 85K curatedAoPS problemswith 262K community-sourcedStackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conductcontrolled evaluationsto assess the dataset quality.\n  Nemotron-Math consistently outperforms the original OpenMathReasoning on matchedAoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.\n  To support efficientlong-context training, we develop asequential bucketed strategythat accelerates 128K context-length fine-tuning by 2--3times without significant accuracy loss. Overall, Nemotron-Math enablesstate-of-the-art performance, including 100\\%maj@16 accuracyonAIME 2024 and 2025with Python TIR.View arXiv pageView PDFAdd to collectionCommunitywoshiweiduPaper submitter3 da",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2512,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2512.15489",
    "arxiv_url": "https://arxiv.org/abs/2512.15489",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "nvidia/Nemotron-Math-v2, nvidia/Nemotron-Math-Proofs-v1",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Nemotron-Math-v2, https://huggingface.co/datasets/nvidia/Nemotron-Math-Proofs-v1",
    "datasets_detailed": "[{\"name\": \"nvidia/Nemotron-Math-v2\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Math-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Math-Proofs-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Math-Proofs-v1\", \"task\": \"\", \"likes\": \"711\", \"downloads\": \"\", \"updated\": \"4 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2512.15603",
    "first_seen_date": "2025-12-18",
    "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2512.15603Qwen-Image-Layered: Towards Inherent Editability via Layer DecompositionPublished on Dec 17\u00b7Submitted bytaesirion Dec 18#2 Paper of the dayUpvote49+41Authors:Shengming Yin,Zekai Zhang,Zecheng Tang,Kaiyuan Gao,Xiao Xu,Kun Yan,Jiahao Li,Yilei Chen,Yuxiang Chen,Heung-Yeung Shum,Lionel M. Ni,Jingren Zhou,Junyang Lin,Chenfei WuAbstractQwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.AI-generated summaryRecent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-enddiffusion modelthat decomposes a single RGB image into multiple semantically disentangledRGBA layers, enablinginherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) anRGBA-VAEto unify the latent representations of RGB and RGBA images; (2) aVLD-MMDiT(Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) aMulti-stage Trainingstrategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches indecomposition qualityand establishes a new paradigm forconsistent image editing. Our code and models a",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2512,
    "github_repo": "https://github.com/QwenLM/QwenImage-Layered",
    "hf_paper_url": "https://huggingface.co/papers/2512.15603",
    "arxiv_url": "https://arxiv.org/abs/2512.15603",
    "num_models": 6,
    "models_list": "Qwen/Qwen-Image-Layered, unsloth/Qwen-Image-Layered-GGUF, vantagewithai/Qwen-Image-Layered-GGUF, Runware/Qwen-Image-Layered, mzbac/Qwen-Image-Layered-8bit, zimengxiong/Qwen-Image-Layered-6bit",
    "models_links": "https://huggingface.co/Qwen/Qwen-Image-Layered, https://huggingface.co/unsloth/Qwen-Image-Layered-GGUF, https://huggingface.co/vantagewithai/Qwen-Image-Layered-GGUF, https://huggingface.co/Runware/Qwen-Image-Layered, https://huggingface.co/mzbac/Qwen-Image-Layered-8bit, https://huggingface.co/zimengxiong/Qwen-Image-Layered-6bit",
    "models_detailed": "[{\"name\": \"Qwen/Qwen-Image-Layered\", \"link\": \"https://huggingface.co/Qwen/Qwen-Image-Layered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"unsloth/Qwen-Image-Layered-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen-Image-Layered-GGUF\", \"task\": \"\", \"likes\": \"682\", \"downloads\": \"\", \"updated\": \"about 12\"}, {\"name\": \"vantagewithai/Qwen-Image-Layered-GGUF\", \"link\": \"https://huggingface.co/vantagewithai/Qwen-Image-Layered-GGUF\", \"task\": \"\", \"likes\": \"74\", \"downloads\": \"\", \"updated\": \"1 day ago\"}, {\"name\": \"Runware/Qwen-Image-Layered\", \"link\": \"https://huggingface.co/Runware/Qwen-Image-Layered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"mzbac/Qwen-Image-Layered-8bit\", \"link\": \"https://huggingface.co/mzbac/Qwen-Image-Layered-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 3\"}, {\"name\": \"zimengxiong/Qwen-Image-Layered-6bit\", \"link\": \"https://huggingface.co/zimengxiong/Qwen-Image-Layered-6bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 3\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2512.15715",
    "first_seen_date": "2025-12-18",
    "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2512.15715In Pursuit of Pixel Supervision for Visual Pre-trainingPublished on Dec 17\u00b7Submitted byLihe Yangon Dec 18Upvote7Authors:Lihe Yang,Shang-Wen Li,Yang Li,Xinjie Lei,Dong Wang,Abdelrahman Mohamed,Hengshuang Zhao,Hu XuAbstractPixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.AI-generated summaryAt the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts.Autoencodersrepresent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-basedself-supervised learningremains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhancedmasked autoencoder(MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, includingmonocular depth estimation(e.g., Depth Anything),feed-forward 3D reconstruction(i.e., MapAnything),semantic segmentation, androbot learning, outperforming or matchingDINOv3trained at similar scales. Our results suggest that pixel-spaceself-supervised learningcan serve as a promising alternative and a complement to latent-space approaches.View arXiv pageView PDFProject pageGitHub171Add to collectionCommunityLiheYoungPaper submitter5 days agoThis comment has been hidden (marked as Spam)avahal4 days agoarXiv lens breakdown of this paper \ud83d\udc49https://arxivlens.com/PaperView/Details/in-pursuit-of",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2512,
    "github_repo": "https://github.com/facebookresearch/pixio",
    "hf_paper_url": "https://huggingface.co/papers/2512.15715",
    "arxiv_url": "https://arxiv.org/abs/2512.15715",
    "num_models": 5,
    "models_list": "facebook/pixio-vitb16, facebook/pixio-vitl16, facebook/pixio-vit5b16, facebook/pixio-vit1b16, facebook/pixio-vith16",
    "models_links": "https://huggingface.co/facebook/pixio-vitb16, https://huggingface.co/facebook/pixio-vitl16, https://huggingface.co/facebook/pixio-vit5b16, https://huggingface.co/facebook/pixio-vit1b16, https://huggingface.co/facebook/pixio-vith16",
    "models_detailed": "[{\"name\": \"facebook/pixio-vitb16\", \"link\": \"https://huggingface.co/facebook/pixio-vitb16\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"facebook/pixio-vitl16\", \"link\": \"https://huggingface.co/facebook/pixio-vitl16\", \"task\": \"\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"facebook/pixio-vit5b16\", \"link\": \"https://huggingface.co/facebook/pixio-vit5b16\", \"task\": \"\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"facebook/pixio-vit1b16\", \"link\": \"https://huggingface.co/facebook/pixio-vit1b16\", \"task\": \"\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"facebook/pixio-vith16\", \"link\": \"https://huggingface.co/facebook/pixio-vith16\", \"task\": \"\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2512.13607",
    "first_seen_date": "2025-12-17",
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2512.13607Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning ModelsPublished on Dec 15\u00b7Submitted byWei Pingon Dec 17\u00b7NVIDIAUpvote25+17Authors:Boxin Wang,Chankyu Lee,Nayeon Lee,Sheng-Chieh Lin,Wenliang Dai,Yang Chen,Yangyi Chen,Zhuolin Yang,Zihan Liu,Mohammad Shoeybi,Bryan Catanzaro,Wei PingAbstractCascaded domain-wise reinforcement learning (Cascade RL) is proposed to enhance general-purpose reasoning models, achieving state-of-the-art performance across benchmarks and outperforming the teacher model in coding competitions.AI-generated summaryBuilding general-purpose reasoning models withreinforcement learning(RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates theRLinfrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wisereinforcement learning(Cascade RL) to develop general-purpose reasoning models,Nemotron-Cascade, capable of operating in both instruct anddeep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains,Cascade RLorchestrates sequential, domain-wiseRL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably,RLHFforalignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wiseRLVRstages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, afterRL, outperforms its SFT teacher, DeepSeek-R1-0528, onLiveCodeBenchv5/v6/Pro and achieves silver-medal performance in the 2025International Olympiad in Informatics (IOI). We ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2512,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2512.13607",
    "arxiv_url": "https://arxiv.org/abs/2512.13607",
    "num_models": 10,
    "models_list": "nvidia/Nemotron-Cascade-14B-Thinking, nvidia/Nemotron-Cascade-8B, nvidia/Nemotron-Cascade-8B-Thinking, nvidia/Nemotron-Cascade-8B-Intermediate-ckpts, cyankiwi/Nemotron-Cascade-14B-Thinking-AWQ-4bit, cyankiwi/Nemotron-Cascade-14B-Thinking-AWQ-8bit, cyankiwi/Nemotron-Cascade-8B-Thinking-AWQ-4bit, cyankiwi/Nemotron-Cascade-8B-Thinking-AWQ-8bit, cyankiwi/Nemotron-Cascade-8B-AWQ-4bit, cyankiwi/Nemotron-Cascade-8B-AWQ-8bit",
    "models_links": "https://huggingface.co/nvidia/Nemotron-Cascade-14B-Thinking, https://huggingface.co/nvidia/Nemotron-Cascade-8B, https://huggingface.co/nvidia/Nemotron-Cascade-8B-Thinking, https://huggingface.co/nvidia/Nemotron-Cascade-8B-Intermediate-ckpts, https://huggingface.co/cyankiwi/Nemotron-Cascade-14B-Thinking-AWQ-4bit, https://huggingface.co/cyankiwi/Nemotron-Cascade-14B-Thinking-AWQ-8bit, https://huggingface.co/cyankiwi/Nemotron-Cascade-8B-Thinking-AWQ-4bit, https://huggingface.co/cyankiwi/Nemotron-Cascade-8B-Thinking-AWQ-8bit, https://huggingface.co/cyankiwi/Nemotron-Cascade-8B-AWQ-4bit, https://huggingface.co/cyankiwi/Nemotron-Cascade-8B-AWQ-8bit",
    "models_detailed": "[{\"name\": \"nvidia/Nemotron-Cascade-14B-Thinking\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Cascade-14B-Thinking\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"nvidia/Nemotron-Cascade-8B\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Cascade-8B\", \"task\": \"Text Generation\", \"likes\": \"447\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"nvidia/Nemotron-Cascade-8B-Thinking\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Cascade-8B-Thinking\", \"task\": \"Text Generation\", \"likes\": \"856\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"nvidia/Nemotron-Cascade-8B-Intermediate-ckpts\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Cascade-8B-Intermediate-ckpts\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"cyankiwi/Nemotron-Cascade-14B-Thinking-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Nemotron-Cascade-14B-Thinking-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"cyankiwi/Nemotron-Cascade-14B-Thinking-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Nemotron-Cascade-14B-Thinking-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"cyankiwi/Nemotron-Cascade-8B-Thinking-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Nemotron-Cascade-8B-Thinking-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"cyankiwi/Nemotron-Cascade-8B-Thinking-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Nemotron-Cascade-8B-Thinking-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"cyankiwi/Nemotron-Cascade-8B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Nemotron-Cascade-8B-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"cyankiwi/Nemotron-Cascade-8B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Nemotron-Cascade-8B-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 2,
    "datasets_list": "nvidia/Nemotron-Cascade-SFT-SWE, nvidia/Nemotron-Cascade-RL-SWE",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Nemotron-Cascade-SFT-SWE, https://huggingface.co/datasets/nvidia/Nemotron-Cascade-RL-SWE",
    "datasets_detailed": "[{\"name\": \"nvidia/Nemotron-Cascade-SFT-SWE\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Cascade-SFT-SWE\", \"task\": \"\", \"likes\": \"397\", \"downloads\": \"\", \"updated\": \"6 days ago\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Cascade-RL-SWE\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Cascade-RL-SWE\", \"task\": \"\", \"likes\": \"269\", \"downloads\": \"\", \"updated\": \"6 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2512.12967",
    "first_seen_date": "2025-12-16",
    "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2512.12967QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory ManagementPublished on Dec 15\u00b7Submitted bytaesirion Dec 16#2 Paper of the day\u00b7TongyiLabUpvote98+90Authors:Weizhou Shen,Ziyi Yang,Chenliang Li,Zhiyuan Lu,Miao Peng,Huashan Sun,Yingcheng Shi,Shengyi Liao,Shaopeng Lai,Bo Zhang,Dayiheng Liu,Fei Huang,Jingren Zhou,Ming YanAbstractQwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.AI-generated summaryWe introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1)Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents intoatomic factsand their underlying relationships, and then programmatically composingverifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2)Stabilized Reinforcement Learningfor Long-Context Training: To overcome the critical instability in long-context RL, we introducetask-balanced samplingwithtask-specific advantage estimationto mitigate reward bias, and proposeAdaptive Entropy-Controlled Policy Optimization(AEPO) that dynamically regulates exploration-exploitation trade-offs. (3)Memory-Augmented Architecturefor Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework withmulti-stage fusion RL trainingthat seamlessly integratessingle-pass reasoning",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2512,
    "github_repo": "https://github.com/Tongyi-Zhiwen/Qwen-Doc",
    "hf_paper_url": "https://huggingface.co/papers/2512.12967",
    "arxiv_url": "https://arxiv.org/abs/2512.12967",
    "num_models": 2,
    "models_list": "Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B, cyankiwi/QwenLong-L1.5-30B-A3B-AWQ-4bit",
    "models_links": "https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B, https://huggingface.co/cyankiwi/QwenLong-L1.5-30B-A3B-AWQ-4bit",
    "models_detailed": "[{\"name\": \"Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B\", \"link\": \"https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B\", \"task\": \"\", \"likes\": \"277\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"cyankiwi/QwenLong-L1.5-30B-A3B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/QwenLong-L1.5-30B-A3B-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"1 day ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2512.04844",
    "first_seen_date": "2025-12-05",
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2512.04844Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded UpdatesPublished on Dec 4\u00b7Submitted byAtsuki Yamaguchion Dec 5Upvote4Authors:Atsuki Yamaguchi,Terufumi Morishita,Aline Villavicencio,Nikolaos AletrasAbstractSource-Shielded Updates (SSU) enables the adaptation of instruct LLMs to new languages using only unlabeled data, preserving source knowledge and achieving competitive target-language performance.AI-generated summaryExpanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data andcatastrophic forgettingduring adaptation. We tackle this challenge under a realistic, low-resource constraint: adaptinginstruct LLMsusing only unlabeled target language data. We introduceSource-Shielded Updates(SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and aparameter importance scoringmethod,SSUidentifies parameters critical to maintaining source abilities. It then applies acolumn-wise freezingstrategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate thatSSUsuccessfully mitigatescatastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning.SSUalso achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.View arXiv pageView PDFGitHub1Add to collectionCommunityatsuki-yamaguchiPaper authorPaper submitter17 days agoOur code and a step-by-step guide for preprocessing, training, evaluation, and analysis for both our propose",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2512,
    "github_repo": "https://github.com/gucci-j/ssu",
    "hf_paper_url": "https://huggingface.co/papers/2512.04844",
    "arxiv_url": "https://arxiv.org/abs/2512.04844",
    "num_models": 107,
    "models_list": "ssu-project/OLMo-2-1124-7B-Instruct-am-random, ssu-project/OLMo-2-1124-7B-Instruct-am-magnitude, ssu-project/OLMo-2-1124-7B-Instruct-am-ssu, ssu-project/OLMo-2-1124-7B-Instruct-am-hft, ssu-project/OLMo-2-1124-7B-Instruct-am-adalora, ssu-project/OLMo-2-1124-7B-Instruct-am-gmt, ssu-project/OLMo-2-1124-7B-Instruct-ig-random, ssu-project/OLMo-2-1124-7B-Instruct-ig-magnitude, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu, ssu-project/OLMo-2-1124-7B-Instruct-ig-hft, ssu-project/OLMo-2-1124-7B-Instruct-ig-adalora, ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt, ssu-project/OLMo-2-1124-7B-Instruct-ne-random, ssu-project/OLMo-2-1124-7B-Instruct-ne-magnitude, ssu-project/OLMo-2-1124-7B-Instruct-ne-ssu, ssu-project/OLMo-2-1124-7B-Instruct-ne-hft, ssu-project/OLMo-2-1124-7B-Instruct-ne-adalora, ssu-project/OLMo-2-1124-7B-Instruct-ne-gmt, ssu-project/OLMo-2-1124-7B-Instruct-ky-random, ssu-project/OLMo-2-1124-7B-Instruct-ky-magnitude, ssu-project/OLMo-2-1124-7B-Instruct-ky-ssu, ssu-project/OLMo-2-1124-7B-Instruct-ky-hft, ssu-project/OLMo-2-1124-7B-Instruct-ky-adalora, ssu-project/OLMo-2-1124-7B-Instruct-ky-gmt, ssu-project/OLMo-2-1124-7B-Instruct-ha-random, ssu-project/OLMo-2-1124-7B-Instruct-ha-magnitude, ssu-project/OLMo-2-1124-7B-Instruct-ha-ssu, ssu-project/OLMo-2-1124-7B-Instruct-ha-hft, ssu-project/OLMo-2-1124-7B-Instruct-ha-adalora, ssu-project/OLMo-2-1124-7B-Instruct-ha-gmt, ssu-project/OLMo-2-1124-13B-Instruct-am-random, ssu-project/OLMo-2-1124-13B-Instruct-am-magnitude, ssu-project/OLMo-2-1124-13B-Instruct-am-ssu, ssu-project/OLMo-2-1124-13B-Instruct-am-hft, ssu-project/OLMo-2-1124-13B-Instruct-am-adalora, ssu-project/OLMo-2-1124-13B-Instruct-am-gmt, ssu-project/OLMo-2-1124-13B-Instruct-ig-random, ssu-project/OLMo-2-1124-13B-Instruct-ig-magnitude, ssu-project/OLMo-2-1124-13B-Instruct-ig-ssu, ssu-project/OLMo-2-1124-13B-Instruct-ig-hft, ssu-project/OLMo-2-1124-13B-Instruct-ig-adalora, ssu-project/OLMo-2-1124-13B-Instruct-ig-gmt, ssu-project/OLMo-2-1124-13B-Instruct-ne-random, ssu-project/OLMo-2-1124-13B-Instruct-ne-magnitude, ssu-project/OLMo-2-1124-13B-Instruct-ne-ssu, ssu-project/OLMo-2-1124-13B-Instruct-ne-hft, ssu-project/OLMo-2-1124-13B-Instruct-ne-adalora, ssu-project/OLMo-2-1124-13B-Instruct-ne-gmt, ssu-project/OLMo-2-1124-13B-Instruct-ky-random, ssu-project/OLMo-2-1124-13B-Instruct-ky-magnitude, ssu-project/OLMo-2-1124-13B-Instruct-ky-ssu, ssu-project/OLMo-2-1124-13B-Instruct-ky-hft, ssu-project/OLMo-2-1124-13B-Instruct-ky-adalora, ssu-project/OLMo-2-1124-13B-Instruct-ky-gmt, ssu-project/OLMo-2-1124-13B-Instruct-ha-random, ssu-project/OLMo-2-1124-13B-Instruct-ha-magnitude, ssu-project/OLMo-2-1124-13B-Instruct-ha-ssu, ssu-project/OLMo-2-1124-13B-Instruct-ha-hft, ssu-project/OLMo-2-1124-13B-Instruct-ha-adalora, ssu-project/OLMo-2-1124-13B-Instruct-ha-gmt, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_ew, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_rw, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.125, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.25, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.375, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.625, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.75, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.875, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_alpaca, ssu-project/OLMo-2-1124-7B-Instruct-am-fft, ssu-project/OLMo-2-1124-7B-Instruct-ig-fft, ssu-project/OLMo-2-1124-7B-Instruct-ne-fft, ssu-project/OLMo-2-1124-7B-Instruct-ky-fft, ssu-project/OLMo-2-1124-7B-Instruct-ha-fft, ssu-project/OLMo-2-1124-13B-Instruct-am-fft, ssu-project/OLMo-2-1124-13B-Instruct-ig-fft, ssu-project/OLMo-2-1124-13B-Instruct-ne-fft, ssu-project/OLMo-2-1124-13B-Instruct-ky-fft, ssu-project/OLMo-2-1124-13B-Instruct-ha-fft, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_fisher, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_sgpt, ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.125, ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.25, ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.375, ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.625, ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.75, ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.875, ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.125, ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.25, ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.375, ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.625, ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.75, ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.875, ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.125, ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.25, ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.375, ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.625, ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.75, ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.875, ssu-project/OLMo-2-1124-7B-Instruct-ig-lota, ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.5, ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d, ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_16, ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_32, ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_64, ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_128, ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_do",
    "models_links": "https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-random, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-hft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-random, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-random, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-hft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-random, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-hft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-random, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-hft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-random, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-hft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-random, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-hft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-random, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-hft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-random, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-hft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-random, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-magnitude, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-ssu, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-hft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-adalora, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-gmt, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_ew, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_rw, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.125, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.25, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.375, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.625, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.75, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.875, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_alpaca, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-fft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-fft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-fft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-fft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-fft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-fft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-fft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-fft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-fft, https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-fft, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_fisher, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_sgpt, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.125, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.25, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.375, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.625, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.75, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.875, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.125, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.25, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.375, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.625, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.75, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.875, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.125, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.25, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.375, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.625, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.75, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.875, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.5, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_16, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_32, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_64, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_128, https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_do",
    "models_detailed": "[{\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-am-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-am-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-am-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-am-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-am-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-am-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ne-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ne-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ne-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ne-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ne-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ne-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ky-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ky-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ky-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ky-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ky-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ky-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ha-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ha-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ha-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ha-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ha-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ha-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-am-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-am-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-am-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-am-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-am-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-am-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ig-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ig-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ig-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ig-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ig-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ig-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ne-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ne-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ne-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ne-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ne-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ne-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ky-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ky-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ky-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ky-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ky-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ky-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ha-random\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-random\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ha-magnitude\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-magnitude\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ha-ssu\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-ssu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ha-hft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-hft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ha-adalora\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-adalora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ha-gmt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-gmt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_ew\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_ew\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_rw\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_rw\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.125\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.125\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.25\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.25\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.375\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.375\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.625\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.625\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.75\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.75\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.875\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_0.875\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_alpaca\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_alpaca\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-am-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-am-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ne-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ne-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ky-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ky-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ha-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ha-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-am-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-am-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ig-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ig-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ne-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ne-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ky-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ky-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-13B-Instruct-ha-fft\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-13B-Instruct-ha-fft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_fisher\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_fisher\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_sgpt\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_sgpt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.125\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.125\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.25\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.25\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.375\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.375\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.625\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.625\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.75\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.75\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.875\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-hft_0.875\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.125\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.125\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.25\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.25\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.375\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.375\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.625\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.625\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.75\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.75\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.875\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-gmt_0.875\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.125\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.125\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.25\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.25\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.375\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.375\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.625\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.625\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.75\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.75\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.875\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.875\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-lota\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.5\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-lota_0.5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_16\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_32\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_32\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_64\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_d_64\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_128\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-ssu_128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_do\", \"link\": \"https://huggingface.co/ssu-project/OLMo-2-1124-7B-Instruct-ig-s2ft_do\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2512.01248",
    "first_seen_date": "2025-12-03",
    "title": "TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2512.01248TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table RecognitionPublished on Dec 1\u00b7Submitted byjunyuanon Dec 3Upvote9+1Authors:Junyuan Zhang,Bin Wang,Qintong Zhang,Fan Wu,Zichen Wen,Jialin Lu,Junjie Shan,Ziqi Zhao,Shuya Yang,Ziling Wang,Ziyang Miao,Huaping Zhong,Yuhang Zang,Xiaoyi Dong,Ka-Ho Chow,Conghui HeAbstractTRivia, a self-supervised fine-tuning method, enables pretrained vision-language models to learn table recognition from unlabeled data using a question-answering-based reward mechanism, outperforming existing models on popular benchmarks.AI-generated summaryTable recognition(TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuningvision-language models(VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduceTRivia, aself-supervised fine-tuningmethod that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built uponGroup Relative Policy Optimization,TRiviaautomatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through aquestion-answering-based reward mechanism. Anattention-guided modulegenerates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2512,
    "github_repo": "https://github.com/opendatalab/TRivia",
    "hf_paper_url": "https://huggingface.co/papers/2512.01248",
    "arxiv_url": "https://arxiv.org/abs/2512.01248",
    "num_models": 1,
    "models_list": "opendatalab/TRivia-3B",
    "models_links": "https://huggingface.co/opendatalab/TRivia-3B",
    "models_detailed": "[{\"name\": \"opendatalab/TRivia-3B\", \"link\": \"https://huggingface.co/opendatalab/TRivia-3B\", \"task\": \"\", \"likes\": \"862\", \"downloads\": \"\", \"updated\": \"20 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.21689",
    "first_seen_date": "2025-12-03",
    "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.21689ToolOrchestra: Elevating Intelligence via Efficient Model and Tool OrchestrationPublished on Nov 26\u00b7Submitted byShizhe Diaoon Dec 3#2 Paper of the day\u00b7NVIDIAUpvote108+100Authors:Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo MolchanovAbstractA small orchestrator using ToolOrchestra method coordinates various intelligent tools with reinforcement learning, achieving higher accuracy and efficiency in solving complex tasks like Humanity's Last Exam compared to larger models.AI-generated summaryLarge language modelsare powerful generalists, yet solving deep and complex problems such as those of theHumanity's Last Exam(HLE) remains both conceptually challenging and computationally expensive. We show that smallorchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduceToolOrchestra, a method for training smallorchestrators that coordinate intelligent tools.ToolOrchestraexplicitly usesreinforcement learningwith outcome-, efficiency-, anduser-preference-aware rewards. UsingToolOrchestra, we produceOrchestrator, an 8B model that achieves higher accuracy at lower cost than previoustool-use agentswhile aligning with user preferences on which tools are to be used for a given query. On HLE,Orchestratorachieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. Ontau2-BenchandFRAMES,Orchestratorsurpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows thatOrchestratorachieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/NVlabs/ToolOrchestra",
    "hf_paper_url": "https://huggingface.co/papers/2511.21689",
    "arxiv_url": "https://arxiv.org/abs/2511.21689",
    "num_models": 6,
    "models_list": "nvidia/Nemotron-Orchestrator-8B, Mungert/Orchestrator-8B-GGUF, cyankiwi/Nemotron-Orchestrator-8B-AWQ-4bit, cyankiwi/Nemotron-Orchestrator-8B-AWQ-8bit, Mungert/Nemotron-Orchestrator-8B-GGUF, ericlewis/Nemotron-Orchestrator-8B-NVFP4",
    "models_links": "https://huggingface.co/nvidia/Nemotron-Orchestrator-8B, https://huggingface.co/Mungert/Orchestrator-8B-GGUF, https://huggingface.co/cyankiwi/Nemotron-Orchestrator-8B-AWQ-4bit, https://huggingface.co/cyankiwi/Nemotron-Orchestrator-8B-AWQ-8bit, https://huggingface.co/Mungert/Nemotron-Orchestrator-8B-GGUF, https://huggingface.co/ericlewis/Nemotron-Orchestrator-8B-NVFP4",
    "models_detailed": "[{\"name\": \"nvidia/Nemotron-Orchestrator-8B\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Orchestrator-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"Mungert/Orchestrator-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Orchestrator-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"cyankiwi/Nemotron-Orchestrator-8B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Nemotron-Orchestrator-8B-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"163\", \"downloads\": \"\", \"updated\": \"17 days ago\"}, {\"name\": \"cyankiwi/Nemotron-Orchestrator-8B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Nemotron-Orchestrator-8B-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"313\", \"downloads\": \"\", \"updated\": \"17 days ago\"}, {\"name\": \"Mungert/Nemotron-Orchestrator-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Nemotron-Orchestrator-8B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"ericlewis/Nemotron-Orchestrator-8B-NVFP4\", \"link\": \"https://huggingface.co/ericlewis/Nemotron-Orchestrator-8B-NVFP4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}]",
    "num_datasets": 7,
    "datasets_list": "nvidia/ToolScale, victor/ToolScale, FranckAbgrall/ToolScale, Sulvi97/ToolScale, hyokualexkwon/ToolScale, POISONX/ToolScale, POISONX/REPO",
    "datasets_links": "https://huggingface.co/datasets/nvidia/ToolScale, https://huggingface.co/datasets/victor/ToolScale, https://huggingface.co/datasets/FranckAbgrall/ToolScale, https://huggingface.co/datasets/Sulvi97/ToolScale, https://huggingface.co/datasets/hyokualexkwon/ToolScale, https://huggingface.co/datasets/POISONX/ToolScale, https://huggingface.co/datasets/POISONX/REPO",
    "datasets_detailed": "[{\"name\": \"nvidia/ToolScale\", \"link\": \"https://huggingface.co/datasets/nvidia/ToolScale\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\", \"size\": \"\"}, {\"name\": \"victor/ToolScale\", \"link\": \"https://huggingface.co/datasets/victor/ToolScale\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\", \"size\": \"\"}, {\"name\": \"FranckAbgrall/ToolScale\", \"link\": \"https://huggingface.co/datasets/FranckAbgrall/ToolScale\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\", \"size\": \"\"}, {\"name\": \"Sulvi97/ToolScale\", \"link\": \"https://huggingface.co/datasets/Sulvi97/ToolScale\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}, {\"name\": \"hyokualexkwon/ToolScale\", \"link\": \"https://huggingface.co/datasets/hyokualexkwon/ToolScale\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\", \"size\": \"\"}, {\"name\": \"POISONX/ToolScale\", \"link\": \"https://huggingface.co/datasets/POISONX/ToolScale\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"2 days ago\", \"size\": \"\"}, {\"name\": \"POISONX/REPO\", \"link\": \"https://huggingface.co/datasets/POISONX/REPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"1 day ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2511.22982",
    "first_seen_date": "2025-12-03",
    "title": "Ovis-Image Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.22982Ovis-Image Technical ReportPublished on Nov 28\u00b7Submitted byGuo-Hua Wangon Dec 3Upvote4Authors:Guo-Hua Wang,Liangfu Cao,Tianyu Cui,Minghao Fu,Xiaohao Chen,Pengxin Zhan,Jianshan Zhao,Lan Li,Bowen Fu,Jiaqi Liu,Qing-Guo ChenAbstractOvis-Image is a 7B text-to-image model optimized for high-quality text rendering under computational constraints, combining a diffusion-based visual decoder with a multimodal backbone and a text-centric training pipeline.AI-generated summaryWe introduce Ovis-Image, a 7Btext-to-image modelspecifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates adiffusion-based visual decoderwith the stronger Ovis 2.5multimodal backbone, leveraging atext-centric training pipelinethat combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such asQwen-Imageand approaches closed-source systems likeSeedreamandGPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strongmultimodal backbonewith a carefully designed, text-focused training recipe is sufficient to achieve reliablebilingual text renderingwithout resorting to oversized or proprietary models.View arXiv pageView PDFGitHub279Add to collectionCommunityFlourishPaper authorPaper submitter19 days ago\u2022edited 19 days agoBuilt upon Ovis-U1, Ovis-Image is a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints.GitHub:https://github.com/AIDC-AI/Ovis-ImageHuggingFace:https://hugg",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/AIDC-AI/Ovis-Image",
    "hf_paper_url": "https://huggingface.co/papers/2511.22982",
    "arxiv_url": "https://arxiv.org/abs/2511.22982",
    "num_models": 1,
    "models_list": "AIDC-AI/Ovis-Image-7B",
    "models_links": "https://huggingface.co/AIDC-AI/Ovis-Image-7B",
    "models_detailed": "[{\"name\": \"AIDC-AI/Ovis-Image-7B\", \"link\": \"https://huggingface.co/AIDC-AI/Ovis-Image-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.18890",
    "first_seen_date": "2025-12-01",
    "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.18890Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language ModelsPublished on Nov 24\u00b7Submitted byYonggan Fuon Dec 1\u00b7NVIDIAUpvote32+24Authors:Yonggan Fu,Xin Dong,Shizhe Diao,Matthijs Van keirsbilck,Hanrong Ye,Wonmin Byeon,Yashaswi Karnati,Lucas Liebenwein,Hannah Zhang,Nikolaus Binder,Maksim Khadkevich,Alexander Keller,Jan Kautz,Yingyan Celine Lin,Pavlo MolchanovAbstractThe study identifies key architectural factors and efficient operators to optimize small language models for real-device latency, introducing the Nemotron-Flash family for improved accuracy and efficiency.AI-generated summaryEfficient deployment ofsmall language models(SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimalSLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants ofSLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors:depth-width ratiosandoperator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimaldepth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emergingefficient attention alternativesto evaluate their potential as candidate building operators. Using the identified promising operators, we construct anevolutionary search frameworkto automatically discover latency-optimal combinations of the",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2511.18890",
    "arxiv_url": "https://arxiv.org/abs/2511.18890",
    "num_models": 3,
    "models_list": "nvidia/Nemotron-Flash-3B-Instruct, nvidia/Nemotron-Flash-1B, nvidia/Nemotron-Flash-3B",
    "models_links": "https://huggingface.co/nvidia/Nemotron-Flash-3B-Instruct, https://huggingface.co/nvidia/Nemotron-Flash-1B, https://huggingface.co/nvidia/Nemotron-Flash-3B",
    "models_detailed": "[{\"name\": \"nvidia/Nemotron-Flash-3B-Instruct\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Flash-3B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"nvidia/Nemotron-Flash-1B\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Flash-1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"nvidia/Nemotron-Flash-3B\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Flash-3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.22475",
    "first_seen_date": "2025-12-01",
    "title": "Adversarial Flow Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.22475Adversarial Flow ModelsPublished on Nov 27\u00b7Submitted byPeter Linon Dec 1\u00b7ByteDance SeedUpvote21+13Authors:Shanchuan Lin,Ceyuan Yang,Zhijie Lin,Hao Chen,Haoqi FanAbstractAdversarial flow models unify adversarial and flow-based generative models, offering stable training, efficient generation, and high performance on image datasets.AI-generated summaryWe presentadversarial flow models, a class ofgenerative modelsthat unifiesadversarial modelsandflow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where thegeneratorlearns an arbitrary transport plan between the noise and the data distributions, ourgeneratorlearns a deterministicnoise-to-data mapping, which is the sameoptimal transportas inflow-matching models. This significantly stabilizesadversarial training. Also, unlikeconsistency-based methods, our model directly learnsone-step or few-step generationwithout needing to learn the intermediate timesteps of theprobability flowfor propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting onImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new bestFIDof 2.38. We additionally show the possibility ofend-to-end trainingof 56-layer and 112-layer models throughdepth repetitionwithout any intermediate supervision, and achieveFIDs of 2.08 and 1.94 using a singleforward pass, surpassing their 2NFE and 4NFE counterparts.View arXiv pageView PDFGitHub30Add to collectionCommunityPeterL1nPaper authorPaper submitter22 days agoAdversarial Flow Models (AF) unify Adversarial Models and Flow Models. It natively supports single-step or multi-step training and generation.Unlike GANs, which learn arbitrary transport plans, AF learns a deterministic Wasserstein-2 tr",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/ByteDance-Seed/Adversarial-Flow-Models",
    "hf_paper_url": "https://huggingface.co/papers/2511.22475",
    "arxiv_url": "https://arxiv.org/abs/2511.22475",
    "num_models": 1,
    "models_list": "ByteDance-Seed/Adversarial-Flow-Models",
    "models_links": "https://huggingface.co/ByteDance-Seed/Adversarial-Flow-Models",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/Adversarial-Flow-Models\", \"link\": \"https://huggingface.co/ByteDance-Seed/Adversarial-Flow-Models\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.22677",
    "first_seen_date": "2025-12-01",
    "title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.22677Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the ShieldPublished on Nov 27\u00b7Submitted byDongyang Liu (Chris Liu)on Dec 1\u00b7Tongyi-MAIUpvote27+19Authors:Dongyang Liu,Peng Gao,David Liu,Ruoyi Du,Zhen Li,Qilong Wu,Xin Jin,Sihan Cao,Shifeng Zhang,Hongsheng Li,Steven HoiAbstractThe study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.AI-generated summaryDiffusion model distillationhas emerged as a powerful technique for creating efficient few-step and single-step generators. Among these,Distribution Matching Distillation (DMD)and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks liketext-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is notdistribution matching, but a previously overlooked component we identify asCFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while theDistribution Matching(DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2511.22677",
    "arxiv_url": "https://arxiv.org/abs/2511.22677",
    "num_models": 13,
    "models_list": "Tongyi-MAI/Z-Image-Turbo, tsqn/Z-Image-Turbo_fp32-fp16-bf16_full_and_ema-only, tsqn/Z-Image-Turbo_fp32-fp16-bf16_comfyui, tsqn/Z-Image-Turbo_fp8_comfyui, unsloth/Z-Image-Turbo-GGUF, not-pegasus/IMAGE_MODAL, kp-forks/Z-Image-Turbo, CanvasAndTheMachine/image-lab-one-q4-128, tsqn/Z-Image-Turbo_GGUF, CanvasAndTheMachine/image-lab-one-q8-128, srcphag/Z-Image-Turbo, gradients-io-tournaments/Z-Image-Turbo, foyoux/Z-Image-Turbo",
    "models_links": "https://huggingface.co/Tongyi-MAI/Z-Image-Turbo, https://huggingface.co/tsqn/Z-Image-Turbo_fp32-fp16-bf16_full_and_ema-only, https://huggingface.co/tsqn/Z-Image-Turbo_fp32-fp16-bf16_comfyui, https://huggingface.co/tsqn/Z-Image-Turbo_fp8_comfyui, https://huggingface.co/unsloth/Z-Image-Turbo-GGUF, https://huggingface.co/not-pegasus/IMAGE_MODAL, https://huggingface.co/kp-forks/Z-Image-Turbo, https://huggingface.co/CanvasAndTheMachine/image-lab-one-q4-128, https://huggingface.co/tsqn/Z-Image-Turbo_GGUF, https://huggingface.co/CanvasAndTheMachine/image-lab-one-q8-128, https://huggingface.co/srcphag/Z-Image-Turbo, https://huggingface.co/gradients-io-tournaments/Z-Image-Turbo, https://huggingface.co/foyoux/Z-Image-Turbo",
    "models_detailed": "[{\"name\": \"Tongyi-MAI/Z-Image-Turbo\", \"link\": \"https://huggingface.co/Tongyi-MAI/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"tsqn/Z-Image-Turbo_fp32-fp16-bf16_full_and_ema-only\", \"link\": \"https://huggingface.co/tsqn/Z-Image-Turbo_fp32-fp16-bf16_full_and_ema-only\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"tsqn/Z-Image-Turbo_fp32-fp16-bf16_comfyui\", \"link\": \"https://huggingface.co/tsqn/Z-Image-Turbo_fp32-fp16-bf16_comfyui\", \"task\": \"\", \"likes\": \"777\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"tsqn/Z-Image-Turbo_fp8_comfyui\", \"link\": \"https://huggingface.co/tsqn/Z-Image-Turbo_fp8_comfyui\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"unsloth/Z-Image-Turbo-GGUF\", \"link\": \"https://huggingface.co/unsloth/Z-Image-Turbo-GGUF\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"about 10\"}, {\"name\": \"not-pegasus/IMAGE_MODAL\", \"link\": \"https://huggingface.co/not-pegasus/IMAGE_MODAL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"kp-forks/Z-Image-Turbo\", \"link\": \"https://huggingface.co/kp-forks/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"CanvasAndTheMachine/image-lab-one-q4-128\", \"link\": \"https://huggingface.co/CanvasAndTheMachine/image-lab-one-q4-128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"tsqn/Z-Image-Turbo_GGUF\", \"link\": \"https://huggingface.co/tsqn/Z-Image-Turbo_GGUF\", \"task\": \"\", \"likes\": \"532\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"CanvasAndTheMachine/image-lab-one-q8-128\", \"link\": \"https://huggingface.co/CanvasAndTheMachine/image-lab-one-q8-128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"10 days ago\"}, {\"name\": \"srcphag/Z-Image-Turbo\", \"link\": \"https://huggingface.co/srcphag/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"gradients-io-tournaments/Z-Image-Turbo\", \"link\": \"https://huggingface.co/gradients-io-tournaments/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"foyoux/Z-Image-Turbo\", \"link\": \"https://huggingface.co/foyoux/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"1 day ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.22699",
    "first_seen_date": "2025-12-01",
    "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.22699Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion TransformerPublished on Nov 27\u00b7Submitted byZhen Lion Dec 1#1 Paper of the day\u00b7Tongyi-MAIUpvote207+199Authors:Z-Image Team,Huanqia Cai,Sihan Cao,Ruoyi Du,Peng Gao,Steven Hoi,Shijie Huang,Zhaohui Hou,Dengyang Jiang,Xin Jin,Liangchen Li,Zhen Li,Zhong-Yu Li,David Liu,Dongyang Liu,Junhan Shi,Qilong Wu,Feng Yu,Chi Zhang,Shifeng Zhang,Shilin ZhouAbstractZ-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.AI-generated summaryThe landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon aScalable Single-Stream Diffusion Transformer(S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314KH800 GPUhours (approx. $630K). Our few-stepdistillation schemewithreward post-trainingfurther yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-gradeH800 GPUand compatibility with consumer-grade hardware (<16GBVRAM). Additionally, ouromni-pre-trainingparadigm also enables efficient training of Z-Image-Edit, an editing model with impressiveinstruction-following capabilities. Both qualitative and qu",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/Tongyi-MAI/Z-Image",
    "hf_paper_url": "https://huggingface.co/papers/2511.22699",
    "arxiv_url": "https://arxiv.org/abs/2511.22699",
    "num_models": 13,
    "models_list": "Tongyi-MAI/Z-Image-Turbo, tsqn/Z-Image-Turbo_fp32-fp16-bf16_full_and_ema-only, tsqn/Z-Image-Turbo_fp32-fp16-bf16_comfyui, tsqn/Z-Image-Turbo_fp8_comfyui, unsloth/Z-Image-Turbo-GGUF, not-pegasus/IMAGE_MODAL, kp-forks/Z-Image-Turbo, CanvasAndTheMachine/image-lab-one-q4-128, tsqn/Z-Image-Turbo_GGUF, CanvasAndTheMachine/image-lab-one-q8-128, srcphag/Z-Image-Turbo, gradients-io-tournaments/Z-Image-Turbo, foyoux/Z-Image-Turbo",
    "models_links": "https://huggingface.co/Tongyi-MAI/Z-Image-Turbo, https://huggingface.co/tsqn/Z-Image-Turbo_fp32-fp16-bf16_full_and_ema-only, https://huggingface.co/tsqn/Z-Image-Turbo_fp32-fp16-bf16_comfyui, https://huggingface.co/tsqn/Z-Image-Turbo_fp8_comfyui, https://huggingface.co/unsloth/Z-Image-Turbo-GGUF, https://huggingface.co/not-pegasus/IMAGE_MODAL, https://huggingface.co/kp-forks/Z-Image-Turbo, https://huggingface.co/CanvasAndTheMachine/image-lab-one-q4-128, https://huggingface.co/tsqn/Z-Image-Turbo_GGUF, https://huggingface.co/CanvasAndTheMachine/image-lab-one-q8-128, https://huggingface.co/srcphag/Z-Image-Turbo, https://huggingface.co/gradients-io-tournaments/Z-Image-Turbo, https://huggingface.co/foyoux/Z-Image-Turbo",
    "models_detailed": "[{\"name\": \"Tongyi-MAI/Z-Image-Turbo\", \"link\": \"https://huggingface.co/Tongyi-MAI/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"tsqn/Z-Image-Turbo_fp32-fp16-bf16_full_and_ema-only\", \"link\": \"https://huggingface.co/tsqn/Z-Image-Turbo_fp32-fp16-bf16_full_and_ema-only\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"tsqn/Z-Image-Turbo_fp32-fp16-bf16_comfyui\", \"link\": \"https://huggingface.co/tsqn/Z-Image-Turbo_fp32-fp16-bf16_comfyui\", \"task\": \"\", \"likes\": \"777\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"tsqn/Z-Image-Turbo_fp8_comfyui\", \"link\": \"https://huggingface.co/tsqn/Z-Image-Turbo_fp8_comfyui\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"unsloth/Z-Image-Turbo-GGUF\", \"link\": \"https://huggingface.co/unsloth/Z-Image-Turbo-GGUF\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"about 10\"}, {\"name\": \"not-pegasus/IMAGE_MODAL\", \"link\": \"https://huggingface.co/not-pegasus/IMAGE_MODAL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"kp-forks/Z-Image-Turbo\", \"link\": \"https://huggingface.co/kp-forks/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"CanvasAndTheMachine/image-lab-one-q4-128\", \"link\": \"https://huggingface.co/CanvasAndTheMachine/image-lab-one-q4-128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"tsqn/Z-Image-Turbo_GGUF\", \"link\": \"https://huggingface.co/tsqn/Z-Image-Turbo_GGUF\", \"task\": \"\", \"likes\": \"532\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"CanvasAndTheMachine/image-lab-one-q8-128\", \"link\": \"https://huggingface.co/CanvasAndTheMachine/image-lab-one-q8-128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"10 days ago\"}, {\"name\": \"srcphag/Z-Image-Turbo\", \"link\": \"https://huggingface.co/srcphag/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"gradients-io-tournaments/Z-Image-Turbo\", \"link\": \"https://huggingface.co/gradients-io-tournaments/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"foyoux/Z-Image-Turbo\", \"link\": \"https://huggingface.co/foyoux/Z-Image-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"1 day ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.20478",
    "first_seen_date": "2025-11-27",
    "title": "NVIDIA Nemotron Parse 1.1",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.20478NVIDIA Nemotron Parse 1.1Published on Nov 25\u00b7Submitted bytaesirion Nov 27\u00b7NVIDIAUpvote20+12Authors:Kateryna Chumachenko,Amala Sanjay Deshmukh,Jarno Seppanen,Ilia Karmanov,Chia-Chih Chen,Lukas Voegtle,Philipp Fischer,Marek Wawrzos,Saeid Motiian,Roman Ageev,Kedi Wu,Alexandre Milesi,Maryam Moosaei,Krzysztof Pawelec,Padmavathy Subramanian,Mehrzad Samadi,Xin Yu,Celina Dear,Sarah Stoddard,Jenna Diamond,Jesse Oliver,Leanna Chraghchian+11 authorsAbstractNemotron-Parse-1.1 is a lightweight OCR and document parsing model with improved capabilities in general OCR, markdown formatting, structured table parsing, and text extraction from images, using an encoder-decoder architecture.AI-generated summaryWe introduce Nemotron-Parse-1.1, a lightweightdocument parsingandOCRmodel that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across generalOCR,markdown formatting,structured table parsing, andtext extractionfrom pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extractsbounding boxesof text segments, as well as correspondingsemantic classes. Nemotron-Parse-1.1 follows anencoder-decoder architecturewith 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweightOCRsolution. We release the model weights publicly onHuggingface, as well as an optimizedNIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.View arXiv pageView PDFProject pageAdd to collectionCommunitytaesiriPaper submitter26 days agoLightweight 885M-parameter encod",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2511.20478",
    "arxiv_url": "https://arxiv.org/abs/2511.20478",
    "num_models": 2,
    "models_list": "nvidia/NVIDIA-Nemotron-Parse-v1.1, nvidia/NVIDIA-Nemotron-Parse-v1.1-TC",
    "models_links": "https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1, https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1-TC",
    "models_detailed": "[{\"name\": \"nvidia/NVIDIA-Nemotron-Parse-v1.1\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"nvidia/NVIDIA-Nemotron-Parse-v1.1-TC\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1-TC\", \"task\": \"\", \"likes\": \"651\", \"downloads\": \"\", \"updated\": \"26 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.16397",
    "first_seen_date": "2025-11-25",
    "title": "AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.16397AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML ParserPublished on Nov 20\u00b7Submitted byVictor Mustaron Nov 25\u00b7OpenDataLabUpvote7Authors:Ren Ma,Jiantao Qiu,Chao Xu,Pei Chu,Kaiwen Liu,Pengli Ren,Yuan Qu,Jiahui Peng,Linfeng Hou,Mengjie Liu,Lindong Lu,Wenchang Ning,Jia Yu,Rui Min,Jin Shi,Haojiong Chen,Peng Zhang,Wenjian Zhang,Qian Jiang,Zengjie Hu,Guoqiang Yang,Zhenxiang Li+5 authorsAbstractA novel extraction pipeline using a language model improves web data quality, significantly enhancing the performance of large language models trained on extracted corpora.AI-generated summaryWhile web data quality is crucial for largelanguage models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as asequence labelingproblem solved by a 0.6B-parameterlanguage model. Unlike text-density heuristics, MinerU-HTML leveragessemantic understandingand employs atwo-stage formatting pipelinethat explicitly categorizes semantic elements before converting toMarkdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\%ROUGE-N F1compared to Trafilatura's 63.6\\%, with exceptionalstructured element preservation(90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2511.16397",
    "arxiv_url": "https://arxiv.org/abs/2511.16397",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "opendatalab/AICC",
    "datasets_links": "https://huggingface.co/datasets/opendatalab/AICC",
    "datasets_detailed": "[{\"name\": \"opendatalab/AICC\", \"link\": \"https://huggingface.co/datasets/opendatalab/AICC\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2511.16518",
    "first_seen_date": "2025-11-21",
    "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.16518MiMo-Embodied: X-Embodied Foundation Model Technical ReportPublished on Nov 20\u00b7Submitted bytaesirion Nov 21\u00b7Xiaomi MiMoUpvote23+15Authors:Xiaoshuai Hao,Lei Zhou,Zhijian Huang,Zhiwen Hou,Yingbo Tang,Lingfeng Zhang,Guang Li,Zheng Lu,Shuhuai Ren,Xianhui Meng,Yuchen Zhang,Jing Wu,Jinghui Lu,Chenxu Dang,Jiayi Guan,Jianhua Wu,Zhiyi Hou,Hanbing Li,Shumeng Xia,Mingliang Zhou,Yinan Zheng,Zihao Yue+22 authorsAbstractMiMo-Embodied, a cross-embodied foundation model, achieves state-of-the-art performance in both autonomous driving and embodied AI through multi-stage learning, curated data, and CoT/RL fine-tuning.AI-generated summaryWe open-source MiMo-Embodied, the firstcross-embodied foundation modelto successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks inTask Planning,Affordance PredictionandSpatial Understanding, while also excelling in 12 autonomous driving benchmarks acrossEnvironmental Perception,Status Prediction, andDriving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that throughmulti-stage learning,curated data construction, andCoT/RL fine-tuning, these two domains exhibit strongpositive transferand mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.View arXiv pageView PDFGitHub321Add to collectionCommunitytaesiriPaper submitterNov 21We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Tas",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/XiaomiMiMo/MiMo-Embodied",
    "hf_paper_url": "https://huggingface.co/papers/2511.16518",
    "arxiv_url": "https://arxiv.org/abs/2511.16518",
    "num_models": 3,
    "models_list": "XiaomiMiMo/MiMo-Embodied-7B, Mungert/MiMo-Embodied-7B-GGUF, FriendliAI/MiMo-Embodied-7B",
    "models_links": "https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B, https://huggingface.co/Mungert/MiMo-Embodied-7B-GGUF, https://huggingface.co/FriendliAI/MiMo-Embodied-7B",
    "models_detailed": "[{\"name\": \"XiaomiMiMo/MiMo-Embodied-7B\", \"link\": \"https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 21\"}, {\"name\": \"Mungert/MiMo-Embodied-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/MiMo-Embodied-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"FriendliAI/MiMo-Embodied-7B\", \"link\": \"https://huggingface.co/FriendliAI/MiMo-Embodied-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"21 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.16624",
    "first_seen_date": "2025-11-21",
    "title": "SAM 3D: 3Dfy Anything in Images",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.16624SAM 3D: 3Dfy Anything in ImagesPublished on Nov 20\u00b7Submitted bytaesirion Nov 21#1 Paper of the day\u00b7AI at MetaUpvote109+101Authors:SAM 3D Team,Xingyu Chen,Fu-Jen Chu,Pierre Gleize,Kevin J Liang,Alexander Sax,Hao Tang,Weiyao Wang,Michelle Guo,Thibaut Hardin,Xiang Li,Aohan Lin,Jiawei Liu,Ziqi Ma,Anushka Sagar,Bowen Song,Xiaodong Wang,Jianing Yang,Bowen Zhang,Piotr Doll\u00e1r,Georgia Gkioxari,Matt Feiszli+1 authorsAbstractSAM 3D is a generative model that reconstructs 3D objects from single images using a multi-stage training framework that includes synthetic pretraining and real-world alignment, achieving high performance in human preference tests.AI-generated summaryWe present SAM 3D, agenerative modelfor visually grounded3D object reconstruction, predictinggeometry,texture, andlayoutfrom a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotatingobject shape,texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern,multi-stage trainingframework that combinessynthetic pretrainingwithreal-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate inhuman preference testson real-world objects and scenes. We will release our code and model weights, an online demo, and a new challengingbenchmarkfor in-the-wild3D object reconstruction.View arXiv pageView PDFProject pageGitHub5.03kAdd to collectionCommunitytaesiriPaper submitterNov 21We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cu",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/facebookresearch/sam-3d-objects",
    "hf_paper_url": "https://huggingface.co/papers/2511.16624",
    "arxiv_url": "https://arxiv.org/abs/2511.16624",
    "num_models": 2,
    "models_list": "facebook/sam-3d-objects, jetjodh/sam-3d-objects",
    "models_links": "https://huggingface.co/facebook/sam-3d-objects, https://huggingface.co/jetjodh/sam-3d-objects",
    "models_detailed": "[{\"name\": \"facebook/sam-3d-objects\", \"link\": \"https://huggingface.co/facebook/sam-3d-objects\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"jetjodh/sam-3d-objects\", \"link\": \"https://huggingface.co/jetjodh/sam-3d-objects\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.16664",
    "first_seen_date": "2025-11-21",
    "title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.16664Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMsPublished on Nov 20\u00b7Submitted bytaesirion Nov 21\u00b7NVIDIAUpvote25+17Authors:Ali Taghibakhshi,Sharath Turuvekere Sreenivas,Saurav Muralidharan,Ruisi Cai,Marcin Chochowski,Ameya Sunil Mahabaleshwarkar,Yoshi Suhara,Oluwatobi Olabiyi,Daniel Korzekwa,Mostofa Patwary,Mohammad Shoeybi,Jan Kautz,Bryan Catanzaro,Ashwath Aithal,Nima Tajbakhsh,Pavlo MolchanovAbstractNemotron Elastic reduces training costs and memory usage by embedding multiple submodels within a single large language model, optimized for various deployment configurations and budgets without additional training or fine-tuning.AI-generated summaryTraining a family oflarge language modelstargeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work onmodel compressionthroughpruningandknowledge distillationhas reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, includinghybrid Mamba-Attention architectures, that embed multiplenested submodelswithin a singleparent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with theparent modeland can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through anend-to-end trained router, tightly coupled to atwo-stage training curriculumdesigned specifically for reasoning models. We additionally introducegroup-aware SSM elastificationthat preserves Mamba's structural constraints,heterogeneous MLP elastification,normalized MSE-based layer importancefor improved depth selection, andknowledge distillationenabling simultaneousmulti-budget optimiza",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2511.16664",
    "arxiv_url": "https://arxiv.org/abs/2511.16664",
    "num_models": 1,
    "models_list": "nvidia/Nemotron-Elastic-12B",
    "models_links": "https://huggingface.co/nvidia/Nemotron-Elastic-12B",
    "models_detailed": "[{\"name\": \"nvidia/Nemotron-Elastic-12B\", \"link\": \"https://huggingface.co/nvidia/Nemotron-Elastic-12B\", \"task\": \"Text Generation\", \"likes\": \"741\", \"downloads\": \"\", \"updated\": \"28 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.11793",
    "first_seen_date": "2025-11-18",
    "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.11793MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive ScalingPublished on Nov 14\u00b7Submitted bytaesirion Nov 18#1 Paper of the dayUpvote162+154Authors:MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li+32 authorsAbstractWe present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scalin",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/MiroMindAI/MiroThinker",
    "hf_paper_url": "https://huggingface.co/papers/2511.11793",
    "arxiv_url": "https://arxiv.org/abs/2511.11793",
    "num_models": 21,
    "models_list": "miromind-ai/MiroThinker-v1.0-72B, miromind-ai/MiroThinker-v1.0-8B, miromind-ai/MiroThinker-v1.0-30B, miromind-ai/MiroThinker-32B-DPO-v0.1, miromind-ai/MiroThinker-v1.0-30B-Terminus, Mungert/MiroThinker-v1.0-30B-GGUF, miromind-ai/MiroThinker-8B-SFT-v0.1, miromind-ai/MiroThinker-8B-DPO-v0.1, miromind-ai/MiroThinker-14B-SFT-v0.1, miromind-ai/MiroThinker-14B-DPO-v0.1, miromind-ai/MiroThinker-32B-SFT-v0.1, miromind-ai/MiroThinker-8B-SFT-v0.2, miromind-ai/MiroThinker-14B-SFT-v0.2, miromind-ai/MiroThinker-4B-SFT-v0.2, miromind-ai/MiroThinker-32B-SFT-v0.2, miromind-ai/MiroThinker-32B-DPO-v0.2, miromind-ai/MiroThinker-14B-DPO-v0.2, miromind-ai/MiroThinker-8B-DPO-v0.2, miromind-ai/MiroThinker-4B-DPO-v0.2, jaigouk/qwen3-4b-german-teacher-v1, Doradus-AI/MiroThinker-v1.0-30B-FP8",
    "models_links": "https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B, https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B, https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B, https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.1, https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B-Terminus, https://huggingface.co/Mungert/MiroThinker-v1.0-30B-GGUF, https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.1, https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.1, https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.1, https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.1, https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.1, https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.2, https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.2, https://huggingface.co/miromind-ai/MiroThinker-4B-SFT-v0.2, https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.2, https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.2, https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.2, https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.2, https://huggingface.co/miromind-ai/MiroThinker-4B-DPO-v0.2, https://huggingface.co/jaigouk/qwen3-4b-german-teacher-v1, https://huggingface.co/Doradus-AI/MiroThinker-v1.0-30B-FP8",
    "models_detailed": "[{\"name\": \"miromind-ai/MiroThinker-v1.0-72B\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"miromind-ai/MiroThinker-v1.0-8B\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"miromind-ai/MiroThinker-v1.0-30B\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"miromind-ai/MiroThinker-32B-DPO-v0.1\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.1\", \"task\": \"Text Generation\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-v1.0-30B-Terminus\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B-Terminus\", \"task\": \"Text Generation\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"Mungert/MiroThinker-v1.0-30B-GGUF\", \"link\": \"https://huggingface.co/Mungert/MiroThinker-v1.0-30B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-8B-SFT-v0.1\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.1\", \"task\": \"Text Generation\", \"likes\": \"45\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-8B-DPO-v0.1\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.1\", \"task\": \"Text Generation\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-14B-SFT-v0.1\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.1\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-14B-DPO-v0.1\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.1\", \"task\": \"Text Generation\", \"likes\": \"26\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-32B-SFT-v0.1\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.1\", \"task\": \"Text Generation\", \"likes\": \"45\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-8B-SFT-v0.2\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.2\", \"task\": \"Text Generation\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-14B-SFT-v0.2\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.2\", \"task\": \"Text Generation\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-4B-SFT-v0.2\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-4B-SFT-v0.2\", \"task\": \"Text Generation\", \"likes\": \"32\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-32B-SFT-v0.2\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.2\", \"task\": \"Text Generation\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-32B-DPO-v0.2\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.2\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-14B-DPO-v0.2\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.2\", \"task\": \"Text Generation\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-8B-DPO-v0.2\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.2\", \"task\": \"Text Generation\", \"likes\": \"32\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"miromind-ai/MiroThinker-4B-DPO-v0.2\", \"link\": \"https://huggingface.co/miromind-ai/MiroThinker-4B-DPO-v0.2\", \"task\": \"Text Generation\", \"likes\": \"97\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"jaigouk/qwen3-4b-german-teacher-v1\", \"link\": \"https://huggingface.co/jaigouk/qwen3-4b-german-teacher-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"Doradus-AI/MiroThinker-v1.0-30B-FP8\", \"link\": \"https://huggingface.co/Doradus-AI/MiroThinker-v1.0-30B-FP8\", \"task\": \"Text Generation\", \"likes\": \"246\", \"downloads\": \"\", \"updated\": \"18 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "miromind-ai/MiroVerse-v0.1",
    "datasets_links": "https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1",
    "datasets_detailed": "[{\"name\": \"miromind-ai/MiroVerse-v0.1\", \"link\": \"https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2511.10289",
    "first_seen_date": "2025-11-14",
    "title": "Music Flamingo: Scaling Music Understanding in Audio Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.10289Music Flamingo: Scaling Music Understanding in Audio Language ModelsPublished on Nov 13\u00b7Submitted byGhoshon Nov 14\u00b7NVIDIAUpvote10+2Authors:Sreyan Ghosh,Arushi Goel,Lasha Koroshinadze,Sang-gil Lee,Zhifeng Kong,Joao Felipe Santos,Ramani Duraiswami,Dinesh Manocha,Wei Ping,Mohammad Shoeybi,Bryan CatanzaroAbstractMusic Flamingo, a large audio-language model, advances music understanding through fine-tuning on a rich dataset and post-training with novel methods, achieving state-of-the-art results across various benchmarks.AI-generated summaryWe introduceMusic Flamingo, a novel largeaudio-language modeldesigned to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curateMF-Skills, a large-scale dataset labeled through amulti-stage pipelinethat yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhancedAudio Flamingo 3backbone onMF-Skillsand further strengthen multiple skills relevant tomusic understanding. To improve the model'sreasoningabilities, we introduce a post-training recipe: we first cold-start withMF-Think, a novelchain-of-thought datasetgrounded in music theory, followed byGRPO-based reinforcement learningwithcustom rewards.Music Flamingoachieves state-of-the-art results across 10+benchmarksformusic understandingandreasoning, establishing it",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2511.10289",
    "arxiv_url": "https://arxiv.org/abs/2511.10289",
    "num_models": 1,
    "models_list": "nvidia/music-flamingo-hf",
    "models_links": "https://huggingface.co/nvidia/music-flamingo-hf",
    "models_detailed": "[{\"name\": \"nvidia/music-flamingo-hf\", \"link\": \"https://huggingface.co/nvidia/music-flamingo-hf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.10507",
    "first_seen_date": "2025-11-14",
    "title": "Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.10507Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction FollowingPublished on Nov 13\u00b7Submitted bytaesirion Nov 14Upvote5Authors:Yun He,Wenzhe Li,Hejia Zhang,Songlin Li,Karishma Mandyam,Sopan Khosla,Yuanhao Xiong,Nanshu Wang,Selina Peng,Beibin Li,Shengjie Bi,Shishir G. Patil,Qi Qi,Shengyu Feng,Julian Katz-Samuels,Richard Yuanzhe Pang,Sujan Gonugondla,Hunter Lang,Yue Yu,Yundi Qian,Maryam Fazel-Zarandi,Licheng Yu+3 authorsAbstractAdvancedIF benchmark and RIFL pipeline improve instruction-following capabilities in large language models by using expert-curated rubrics and reinforcement learning techniques.AI-generated summaryRecent progress inlarge language models (LLMs)has led to impressive performance on a range of tasks, yet advancedinstruction following (IF)-especially for complex, multi-turn, andsystem-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality,human-annotated benchmarksand reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curatedrubricsthat assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leveragesrubric generation, a finetunedrubric verifier, andreward shapingto enable effectivereinforcement learningfor instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishesrubricsas a powerful tool for both training an",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2511.10507",
    "arxiv_url": "https://arxiv.org/abs/2511.10507",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "facebook/AdvancedIF, Sellopale/AdvancedIF",
    "datasets_links": "https://huggingface.co/datasets/facebook/AdvancedIF, https://huggingface.co/datasets/Sellopale/AdvancedIF",
    "datasets_detailed": "[{\"name\": \"facebook/AdvancedIF\", \"link\": \"https://huggingface.co/datasets/facebook/AdvancedIF\", \"task\": \"\", \"likes\": \"627\", \"downloads\": \"\", \"updated\": \"27 days ago\", \"size\": \"\"}, {\"name\": \"Sellopale/AdvancedIF\", \"link\": \"https://huggingface.co/datasets/Sellopale/AdvancedIF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2511.06221",
    "first_seen_date": "2025-11-12",
    "title": "Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model\n  Reasoning Ability in VibeThinker-1.5B",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.06221Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model\n  Reasoning Ability in VibeThinker-1.5BPublished on Nov 9\u00b7Submitted byDenseHubon Nov 12#1 Paper of the day\u00b7WeiboAIUpvote131+123Authors:Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin ZhangAbstractVibeThinker-1.5B, a 1.5B-parameter model using the Spectrum-to-Signal Principle, achieves superior reasoning capabilities compared to larger models at a significantly lower cost.AI-generated summaryChallenging the prevailing consensus that small models inherently lack robust\nreasoning, this report introducesVibeThinker-1.5B, a 1.5B-parameter dense\nmodel developed via ourSpectrum-to-Signal Principle(SSP). This challenges the\nprevailing approach of scaling model parameters to enhance capabilities, as\nseen in models likeDeepSeek R1(671B) andKimi k2(>1T). The SSP framework\nfirst employs aTwo-Stage Diversity-Exploring Distillation(SFT) to generate a\nbroad spectrum of solutions, followed byMaxEnt-Guided Policy Optimization(RL)\nto amplify the correct signal. With a total training cost of only $7,800,VibeThinker-1.5Bdemonstrates superior reasoning capabilities compared to\nclosed-source models likeMagistral MediumandClaude Opus 4, and performs on\npar with open-source models likeGPT OSS-20B Medium. Remarkably, it surpasses\nthe 400x largerDeepSeek R1on three math benchmarks:AIME24(80.3 vs. 79.8),AIME25(74.4 vs. 70.0), andHMMT25(50.4 vs. 41.7). This is a substantial\nimprovement over its base model (6.7, 4.3, and 0.6, respectively). OnLiveCodeBench V6, it scores 51.1, outperformingMagistral Medium's 50.3 and its\nbase model's 0.0. These findings demonstrate that small models can achieve\nreasoning capabilities comparable to large models, drastically reducing\ntraining and inference costs and thereby democratizing advanced AI research.View arXiv pageView PDFProject pageGitHu",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/WeiboAI/VibeThinker",
    "hf_paper_url": "https://huggingface.co/papers/2511.06221",
    "arxiv_url": "https://arxiv.org/abs/2511.06221",
    "num_models": 3,
    "models_list": "WeiboAI/VibeThinker-1.5B, mlx-community/VibeThinker-1.5B-mlx-4bit, Mungert/VibeThinker-1.5B-GGUF",
    "models_links": "https://huggingface.co/WeiboAI/VibeThinker-1.5B, https://huggingface.co/mlx-community/VibeThinker-1.5B-mlx-4bit, https://huggingface.co/Mungert/VibeThinker-1.5B-GGUF",
    "models_detailed": "[{\"name\": \"WeiboAI/VibeThinker-1.5B\", \"link\": \"https://huggingface.co/WeiboAI/VibeThinker-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"mlx-community/VibeThinker-1.5B-mlx-4bit\", \"link\": \"https://huggingface.co/mlx-community/VibeThinker-1.5B-mlx-4bit\", \"task\": \"Text Generation\", \"likes\": \"687\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"Mungert/VibeThinker-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/VibeThinker-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"683\", \"downloads\": \"\", \"updated\": \"Nov 11\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.00956",
    "first_seen_date": "2025-11-07",
    "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.00956EVTAR: End-to-End Try on with Additional Unpaired Visual ReferencePublished on Nov 2\u00b7Submitted bylsyon Nov 7\u00b7\u5317\u4eac\u5947\u864e\u79d1\u6280\u6709\u9650\u516c\u53f8Upvote4Authors:Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui YinAbstractEVTAR is an end-to-end virtual try-on model that enhances accuracy by using reference images, simplifying the inference process and improving garment texture and detail preservation.AI-generated summaryWe propose EVTAR, anEnd-to-End Virtual Try-onmodel with Additional\nReference, that directly fits the target garment onto the person image while\nincorporating reference images to enhance try-on accuracy. Most existing\nvirtual try-on approaches rely on complex inputs such as agnostic person\nimages,human pose,densepose, orbody keypoints, making them labor-intensive\nand impractical for real-world applications. In contrast, EVTAR adopts atwo-stage training strategy, enabling simple inference with only the source\nimage and the target garment inputs. Our model generates try-on results without\nmasks,densepose, or segmentation maps. Moreover, EVTAR leverages additional\nreference images of different individuals wearing the same clothes to preservegarment textureandfine-grained detailsbetter. This mechanism is analogous to\nhow humans consider reference models when choosing outfits, thereby simulating\na more realistic and high-quality dressing effect. We enrich the training data\nwithsupplementary referencesandunpaired person imagesto support these\ncapabilities. We evaluate EVTAR on two widely used benchmarks and diverse\ntasks, and the results consistently validate the effectiveness of our approach.View arXiv pageView PDFGitHub56Add to collectionCommunityliushanyuan18Paper authorPaper submitterNov 7\u2022edited Nov 7Code is available at:https://github.com/360CVGroup/EVTAR. Model is available at:https://huggingface.co/qihoo360/EVTAR.See tr",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/360CVGroup/EVTAR",
    "hf_paper_url": "https://huggingface.co/papers/2511.00956",
    "arxiv_url": "https://arxiv.org/abs/2511.00956",
    "num_models": 1,
    "models_list": "qihoo360/RefVTON",
    "models_links": "https://huggingface.co/qihoo360/RefVTON",
    "models_detailed": "[{\"name\": \"qihoo360/RefVTON\", \"link\": \"https://huggingface.co/qihoo360/RefVTON\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 18\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.03929",
    "first_seen_date": "2025-11-07",
    "title": "NVIDIA Nemotron Nano V2 VL",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.03929NVIDIA Nemotron Nano V2 VLPublished on Nov 6\u00b7Submitted bytaesirion Nov 7\u00b7NVIDIAUpvote26+18Authors:NVIDIA,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping+104 authorsAbstractNemotron Nano V2 VL, a hybrid Mamba-Transformer LLM, improves document and video understanding through enhanced architecture and token reduction techniques.AI-generated summaryWe introduce Nemotron Nano V2 VL, the latest model of the Nemotron\nvision-language series designed for strong real-world document understanding,\nlong video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers\nsignificant improvements over our previous model,\nLlama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major\nenhancements in model architecture, datasets, and training recipes. Nemotron\nNano V2 VL builds on Nemotron Nano V2, a hybridMamba-TransformerLLM, and\ninnovativetoken reduction techniquesto achieve higher inference throughput in\nlong document and video scenarios. We are releasing model checkpoints inBF16,FP8, andFP4formats and sharing large parts of our datasets, recipes and\ntraining code.View arXiv pageView PDFGitHub1.29kautoAdd to collectionCommunitytaesiriPaper submitterNov 7NVIDIA Nemotron Nano V2 VLReplylibrarian-botNov 8This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APISAIL-VL2 Technical Report(2025)Qianfan-VL: Domain-Enhanced Universal Vision-Language Models(2025)StreamingVLM: Real-Time Understanding for Infinite Video Streams(2025)Glyph: Scaling Context Windows via Visual-Text Compression(2025)Mixing Importance with Dive",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/NVIDIA-NeMo/Curator",
    "hf_paper_url": "https://huggingface.co/papers/2511.03929",
    "arxiv_url": "https://arxiv.org/abs/2511.03929",
    "num_models": 3,
    "models_list": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16, nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-FP8, nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-NVFP4-QAD",
    "models_links": "https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16, https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-FP8, https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-NVFP4-QAD",
    "models_detailed": "[{\"name\": \"nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-FP8\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\"}, {\"name\": \"nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-NVFP4-QAD\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-NVFP4-QAD\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\"}]",
    "num_datasets": 1,
    "datasets_list": "nvidia/Nemotron-VLM-Dataset-v2",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Nemotron-VLM-Dataset-v2",
    "datasets_detailed": "[{\"name\": \"nvidia/Nemotron-VLM-Dataset-v2\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-VLM-Dataset-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2511.00062",
    "first_seen_date": "2025-11-04",
    "title": "World Simulation with Video Foundation Models for Physical AI",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.00062World Simulation with Video Foundation Models for Physical AIPublished on Oct 28\u00b7Submitted bytaesirion Nov 4Upvote40+32Authors:NVIDIA,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng+67 authorsAbstractCosmos-Predict2.5 and Cosmos-Transfer2.5 are advanced Physical AI models that unify text, image, and video generation, improve video quality and instruction alignment, and enable Sim2Real and Real2Real world translation with higher fidelity.AI-generated summaryWe introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models forPhysical AI. Built on aflow-based architecture,\n[Cosmos-Predict2.5] unifiesText2World,Image2World, andVideo2Worldgeneration\nin a single model and leverages [Cosmos-Reason1], aPhysical AIvision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliablesynthetic data generation,policy evaluation, and closed-loop\nsimulation forroboticsandautonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], acontrol-netstyle framework forSim2RealandReal2Realworld translation. Despite being 3.5times smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scalingembodied intelligence. To\naccelerate research and deployment inPhysical AI, we releas",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/nvidia-cosmos/cosmos-predict2.5",
    "hf_paper_url": "https://huggingface.co/papers/2511.00062",
    "arxiv_url": "https://arxiv.org/abs/2511.00062",
    "num_models": 3,
    "models_list": "nvidia/Cosmos-Predict2.5-2B, nvidia/Cosmos-Transfer2.5-2B, nvidia/Cosmos-Predict2.5-14B",
    "models_links": "https://huggingface.co/nvidia/Cosmos-Predict2.5-2B, https://huggingface.co/nvidia/Cosmos-Transfer2.5-2B, https://huggingface.co/nvidia/Cosmos-Predict2.5-14B",
    "models_detailed": "[{\"name\": \"nvidia/Cosmos-Predict2.5-2B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict2.5-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"nvidia/Cosmos-Transfer2.5-2B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Transfer2.5-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"nvidia/Cosmos-Predict2.5-14B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict2.5-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2511.01678",
    "first_seen_date": "2025-11-04",
    "title": "UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible Feedback",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2511.01678UniLumos: Fast and Unified Image and Video Relighting with\n  Physics-Plausible FeedbackPublished on Nov 3\u00b7Submitted byHangjie Yuanon Nov 4\u00b7DAMO AcademyUpvote35+27Authors:Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan WangAbstractUniLumos, a unified relighting framework, enhances physical plausibility by integrating RGB-space geometry feedback into a flow matching backbone, achieving state-of-the-art results with improved consistency and speed.AI-generated summaryRelighting is a crucial task with both practical demand and artistic value,\nand recentdiffusion modelshave shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized insemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that bringsRGB-spacegeometry feedback into aflow matchingbackbone. By supervising the\nmodel with depth andnormal mapsextracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructuredsix-dimensional annotation protocolcapturing core illumination\nattributes. Building upon this, we proposeLumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2511,
    "github_repo": "https://github.com/alibaba-damo-academy/Lumos-Custom",
    "hf_paper_url": "https://huggingface.co/papers/2511.01678",
    "arxiv_url": "https://arxiv.org/abs/2511.01678",
    "num_models": 1,
    "models_list": "Alibaba-DAMO-Academy/UniLumos",
    "models_links": "https://huggingface.co/Alibaba-DAMO-Academy/UniLumos",
    "models_detailed": "[{\"name\": \"Alibaba-DAMO-Academy/UniLumos\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/UniLumos\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.27571",
    "first_seen_date": "2025-11-04",
    "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.27571Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid CurriculumPublished on Oct 31\u00b7Submitted byTingyu Songon Nov 4\u00b7Alibaba-NLPUpvote17+9Authors:Zhuoning Guo,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Xiaowen ChuAbstractA framework combining a diagnostic benchmark, data synthesis, and a modality pyramid curriculum achieves state-of-the-art zero-shot generalization in video retrieval.AI-generated summaryThe prevailing video retrieval paradigm is structurally misaligned, as narrow\nbenchmarks incentivize correspondingly limited data and single-task training.\nTherefore, universal capability is suppressed due to the absence of a\ndiagnostic evaluation that defines and demands multi-dimensional\ngeneralization. To break this cycle, we introduce a framework built on the\nco-design of evaluation, data, and modeling. First, we establish the Universal\nVideo Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to\nmeasure performance but also to diagnose critical capability gaps across tasks\nand domains. Second, guided byUVRB's diagnostics, we introduce a scalable\nsynthesis workflow that generates 1.55 million high-quality pairs to populate\nthe semantic space required for universality. Finally, we devise the Modality\nPyramid, a curriculum that trains ourGeneral Video Embedder(GVE) by\nexplicitly leveraging the latent interconnections within our diverse data.\nExtensive experiments showGVEachieves state-of-the-art zero-shot\ngeneralization onUVRB. In particular, our analysis reveals that popular\nbenchmarks are poor predictors of general ability and that partially relevant\nretrieval is a dominant but overlooked scenario. Overall, our co-designed\nframework provides a practical path to escape the limited scope and advance\ntoward truly universal video retrieval.View arXiv pageView PDFProject pageAdd to collectionCommunityson",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2510.27571",
    "arxiv_url": "https://arxiv.org/abs/2510.27571",
    "num_models": 2,
    "models_list": "Alibaba-NLP/GVE-3B, Alibaba-NLP/GVE-7B",
    "models_links": "https://huggingface.co/Alibaba-NLP/GVE-3B, https://huggingface.co/Alibaba-NLP/GVE-7B",
    "models_detailed": "[{\"name\": \"Alibaba-NLP/GVE-3B\", \"link\": \"https://huggingface.co/Alibaba-NLP/GVE-3B\", \"task\": \"\", \"likes\": \"369\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"Alibaba-NLP/GVE-7B\", \"link\": \"https://huggingface.co/Alibaba-NLP/GVE-7B\", \"task\": \"\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"Nov 3\"}]",
    "num_datasets": 1,
    "datasets_list": "Alibaba-NLP/UVRB",
    "datasets_links": "https://huggingface.co/datasets/Alibaba-NLP/UVRB",
    "datasets_detailed": "[{\"name\": \"Alibaba-NLP/UVRB\", \"link\": \"https://huggingface.co/datasets/Alibaba-NLP/UVRB\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 6\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.27606",
    "first_seen_date": "2025-11-03",
    "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.27606Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement LearningPublished on Oct 31\u00b7Submitted byYuhang Zangon Nov 3\u00b7Intern Large ModelsUpvote27+19Authors:Yuhong Liu,Beichen Zhang,Yuhang Zang,Yuhang Cao,Long Xing,Xiaoyi Dong,Haodong Duan,Dahua Lin,Jiaqi WangAbstractSpatial-SSRL, a self-supervised reinforcement learning paradigm, enhances spatial understanding in Large Vision-Language Models using verifiable signals from RGB or RGB-D images without human annotation.AI-generated summarySpatial understanding remains a weakness ofLarge Vision-Language Models(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement\nlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,\nspecialized tools, or constrained environments that limit scale. We introduce\nSpatial-SSRL, aself-supervised RLparadigm that derives verifiable signals\ndirectly from ordinary RGB or RGB-D images. Spatial-SSRL automatically\nformulates fivepretext tasksthat capture 2D and 3D spatial structure:shuffled patch reordering,flipped patch recognition,cropped patch inpainting,regional depth ordering, andrelative 3D position prediction. These tasks\nprovide ground-truth answers that are easy to verify and require no human or\nLVLM annotation. Training on our tasks substantially improvesspatial reasoningwhile preserving general visual capabilities. On seven spatial understanding\nbenchmarks in both image and video settings, Spatial-SSRL delivers average\naccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our\nresults show that simple, intrinsic supervision enables RLVR at scale and\nprovides a practical route to stronger spatial intelligence in LVLMs.View arXiv pageView PDFGitHub92Add to collectionCommunityyuhangzangPaper authorPaper submitterNov 3\u2022edited Nov 3\ud83d\udef0\ufe0fSpatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement L",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/InternLM/Spatial-SSRL",
    "hf_paper_url": "https://huggingface.co/papers/2510.27606",
    "arxiv_url": "https://arxiv.org/abs/2510.27606",
    "num_models": 2,
    "models_list": "internlm/Spatial-SSRL-7B, internlm/Spatial-SSRL-Qwen3VL-4B",
    "models_links": "https://huggingface.co/internlm/Spatial-SSRL-7B, https://huggingface.co/internlm/Spatial-SSRL-Qwen3VL-4B",
    "models_detailed": "[{\"name\": \"internlm/Spatial-SSRL-7B\", \"link\": \"https://huggingface.co/internlm/Spatial-SSRL-7B\", \"task\": \"\", \"likes\": \"165\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"internlm/Spatial-SSRL-Qwen3VL-4B\", \"link\": \"https://huggingface.co/internlm/Spatial-SSRL-Qwen3VL-4B\", \"task\": \"\", \"likes\": \"418\", \"downloads\": \"\", \"updated\": \"28 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "internlm/Spatial-SSRL-81k",
    "datasets_links": "https://huggingface.co/datasets/internlm/Spatial-SSRL-81k",
    "datasets_detailed": "[{\"name\": \"internlm/Spatial-SSRL-81k\", \"link\": \"https://huggingface.co/datasets/internlm/Spatial-SSRL-81k\", \"task\": \"\", \"likes\": \"531\", \"downloads\": \"\", \"updated\": \"28 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.24992",
    "first_seen_date": "2025-10-31",
    "title": "POWSM: A Phonetic Open Whisper-Style Speech Foundation Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.24992POWSM: A Phonetic Open Whisper-Style Speech Foundation ModelPublished on Oct 28\u00b7Submitted byShikhar Bharadwajon Oct 31\u00b7CMU-LTIUpvote2Authors:Chin-Jou Li,Kalvin Chang,Shikhar Bharadwaj,Eunjung Yeo,Kwanghee Choi,Jian Zhu,David Mortensen,Shinji WatanabeAbstractA unified framework, POWSM, performs multiple phonetic tasks including ASR, PR, G2P, and P2G, outperforming specialized models while using a single architecture.AI-generated summaryRecent advances in spoken language processing have led to substantial\nprogress in phonetic tasks such asautomatic speech recognition(ASR), phone\nrecognition (PR),grapheme-to-phoneme conversion(G2P), and phoneme-to-grapheme\nconversion (P2G). Despite their conceptual similarity, these tasks have largely\nbeen studied in isolation, each relying on task-specific architectures and\ndatasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech\nModel), the first unified framework capable of jointly performing multiple\nphone-related tasks. POWSM enables seamless conversion between audio, text\n(graphemes), and phones, opening up new possibilities for universal and\nlow-resource speech processing. Our model outperforms or matches specialized PR\nmodels of similar size (Wav2Vec2PhonemeandZIPA) while jointly supporting G2P,\nP2G, and ASR. Our training data, code and models are released to foster open\nscience.View arXiv pageView PDFAdd to collectionCommunityshikhar7ssuPaper authorPaper submitterOct 31This comment has been hidden (marked as Off-Topic)EditPreviewUpload images, audio, and videos by dragging in the text input, pasting, orclicking here.Tap or paste here to upload imagesComment\u00b7Sign uporlog into commentUpvote2Models citing this paper1espnet/powsmAutomatic Speech Recognition\u2022UpdatedOct 30\u202246\u20226Datasets citing this paper0No dataset linking this paperCite arxiv.org/abs/2510.24992 in a dataset README.md to link it from this page.Space",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2510.24992",
    "arxiv_url": "https://arxiv.org/abs/2510.24992",
    "num_models": 1,
    "models_list": "espnet/powsm",
    "models_links": "https://huggingface.co/espnet/powsm",
    "models_detailed": "[{\"name\": \"espnet/powsm\", \"link\": \"https://huggingface.co/espnet/powsm\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Oct 30\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.23538",
    "first_seen_date": "2025-10-30",
    "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.23538JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code IntelligencePublished on Oct 27\u00b7Submitted byQiushion Oct 30#2 Paper of the day\u00b7Intern Large ModelsUpvote96+88Authors:Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei YuanAbstractA unified multimodal code corpus and model enable high-quality code generation from both text and visual inputs, outperforming commercial models.AI-generated summaryThe scope ofneural code intelligenceis rapidly expanding beyond text-based\nsource code to encompass the richvisual outputsthat programs generate. This\nvisual dimension is critical for advanced applications like flexible content\ngeneration and precise,program-driven editingof visualizations. However,\nprogress has been impeded by the scarcity of high-qualitymultimodal code data,\na bottleneck stemming from challenges in synthesis and quality assessment. To\naddress these challenges, we make contributions from both a data and modeling\nperspective. We first introduce a completesynthesis toolkitthat leverages\nreciprocal synergies between data modalities to efficiently produce a\nlarge-scale, high-quality corpus spanning from standard charts to complex\ninteractive web UIs and code-driven animations. Leveraging this toolkit, we\nconstructJanusCode-800K, the largest multimodal code corpus to date. This\npowers the training of our models,JanusCoderandJanusCoderV, which establish\navisual-programmatic interfacefor generating code from textual instructions,\nvisual inputs, or a combination of both. Our unified model is a departure from\nexisting approaches that build specialized models for isolated tasks. Extensive\nexperiments on bothtext-centricandvision-centric coding tasksdemonstrate\nthe superior performance of theJanusCoderseries, with our 7B to 14B scale\nmodels approaching or even exceeding the performance of commercial models.\nFurther",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/InternLM/JanusCoder",
    "hf_paper_url": "https://huggingface.co/papers/2510.23538",
    "arxiv_url": "https://arxiv.org/abs/2510.23538",
    "num_models": 12,
    "models_list": "internlm/JanusCoder-14B, internlm/JanusCoderV-7B, internlm/JanusCoderV-8B, internlm/JanusCoder-8B, cyankiwi/JanusCoder-14B-AWQ-4bit, cyankiwi/JanusCoder-14B-AWQ-8bit, cyankiwi/JanusCoder-8B-AWQ-8bit, cyankiwi/JanusCoder-8B-AWQ-4bit, unsloth/JanusCoder-8B-GGUF, unsloth/JanusCoder-8B, unsloth/JanusCoder-14B-GGUF, unsloth/JanusCoder-14B",
    "models_links": "https://huggingface.co/internlm/JanusCoder-14B, https://huggingface.co/internlm/JanusCoderV-7B, https://huggingface.co/internlm/JanusCoderV-8B, https://huggingface.co/internlm/JanusCoder-8B, https://huggingface.co/cyankiwi/JanusCoder-14B-AWQ-4bit, https://huggingface.co/cyankiwi/JanusCoder-14B-AWQ-8bit, https://huggingface.co/cyankiwi/JanusCoder-8B-AWQ-8bit, https://huggingface.co/cyankiwi/JanusCoder-8B-AWQ-4bit, https://huggingface.co/unsloth/JanusCoder-8B-GGUF, https://huggingface.co/unsloth/JanusCoder-8B, https://huggingface.co/unsloth/JanusCoder-14B-GGUF, https://huggingface.co/unsloth/JanusCoder-14B",
    "models_detailed": "[{\"name\": \"internlm/JanusCoder-14B\", \"link\": \"https://huggingface.co/internlm/JanusCoder-14B\", \"task\": \"Text Generation\", \"likes\": \"265\", \"downloads\": \"\", \"updated\": \"Nov 9\"}, {\"name\": \"internlm/JanusCoderV-7B\", \"link\": \"https://huggingface.co/internlm/JanusCoderV-7B\", \"task\": \"\", \"likes\": \"118\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"internlm/JanusCoderV-8B\", \"link\": \"https://huggingface.co/internlm/JanusCoderV-8B\", \"task\": \"\", \"likes\": \"166\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"internlm/JanusCoder-8B\", \"link\": \"https://huggingface.co/internlm/JanusCoder-8B\", \"task\": \"\", \"likes\": \"121\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"cyankiwi/JanusCoder-14B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/JanusCoder-14B-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"cyankiwi/JanusCoder-14B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/JanusCoder-14B-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"cyankiwi/JanusCoder-8B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/JanusCoder-8B-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"cyankiwi/JanusCoder-8B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/JanusCoder-8B-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"unsloth/JanusCoder-8B-GGUF\", \"link\": \"https://huggingface.co/unsloth/JanusCoder-8B-GGUF\", \"task\": \"\", \"likes\": \"832\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/JanusCoder-8B\", \"link\": \"https://huggingface.co/unsloth/JanusCoder-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/JanusCoder-14B-GGUF\", \"link\": \"https://huggingface.co/unsloth/JanusCoder-14B-GGUF\", \"task\": \"\", \"likes\": \"632\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/JanusCoder-14B\", \"link\": \"https://huggingface.co/unsloth/JanusCoder-14B\", \"task\": \"\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Oct 31\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.25741",
    "first_seen_date": "2025-10-30",
    "title": "Scaling Latent Reasoning via Looped Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.25741Scaling Latent Reasoning via Looped Language ModelsPublished on Oct 29\u00b7Submitted bytaesirion Oct 30\u00b7ByteDance SeedUpvote220+212Authors:Rui-Jie Zhu,Zixuan Wang,Kai Hua,Tianyu Zhang,Ziniu Li,Haoran Que,Boyi Wei,Zixin Wen,Fan Yin,He Xing,Lu Li,Jiajun Shi,Kaijing Ma,Shanda Li,Taylor Kergan,Andrew Smith,Xingwei Qu,Mude Hui,Bohong Wu,Qiyang Min,Hongzhi Huang,Xun Zhou+11 authorsAbstractLoopLM, a family of pre-trained Looped Language Models, enhances reasoning by integrating iterative computation and entropy regularization during pre-training, achieving superior performance with better knowledge manipulation.AI-generated summaryModern LLMs are trained to \"think\" primarily via explicit text generation,\nsuch aschain-of-thought(CoT), which defers reasoning to post-training and\nunder-leverages pre-training data. We present and open-source Ouro, named after\nthe recursive Ouroboros, a family of pre-trainedLooped Language Models(LoopLM) that instead build reasoning into the pre-training phase through (i)iterative computationinlatent space, (ii) anentropy-regularized objectiveforlearned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and\n2.6B models enjoy superior performance that match the results of up to 12B SOTA\nLLMs across a wide range of benchmarks. Through controlled experiments, we show\nthis advantage stems not from increased knowledge capacity, but from superiorknowledge manipulationcapabilities. We also show thatLoopLMyields reasoning\ntraces more aligned with final outputs than explicit CoT. We hope our results\nshow the potential ofLoopLMas a novel scaling direction in the reasoning era.\nOur model could be found in: http://ouro-llm.github.io.View arXiv pageView PDFProject pageAdd to collectionCommunitytaesiriPaper submitterOct 30Modern LLMs are trained to \"think\" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to pos",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2510.25741",
    "arxiv_url": "https://arxiv.org/abs/2510.25741",
    "num_models": 4,
    "models_list": "ByteDance/Ouro-2.6B-Thinking, ByteDance/Ouro-1.4B, ByteDance/Ouro-2.6B, ByteDance/Ouro-1.4B-Thinking",
    "models_links": "https://huggingface.co/ByteDance/Ouro-2.6B-Thinking, https://huggingface.co/ByteDance/Ouro-1.4B, https://huggingface.co/ByteDance/Ouro-2.6B, https://huggingface.co/ByteDance/Ouro-1.4B-Thinking",
    "models_detailed": "[{\"name\": \"ByteDance/Ouro-2.6B-Thinking\", \"link\": \"https://huggingface.co/ByteDance/Ouro-2.6B-Thinking\", \"task\": \"Text Generation\", \"likes\": \"308\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"ByteDance/Ouro-1.4B\", \"link\": \"https://huggingface.co/ByteDance/Ouro-1.4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"ByteDance/Ouro-2.6B\", \"link\": \"https://huggingface.co/ByteDance/Ouro-2.6B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"ByteDance/Ouro-1.4B-Thinking\", \"link\": \"https://huggingface.co/ByteDance/Ouro-1.4B-Thinking\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.22264",
    "first_seen_date": "2025-10-29",
    "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.22264PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  EmbeddingPublished on Oct 25\u00b7Submitted byiliass ayaouon Oct 29Upvote1Authors:Iliass Ayaou,Denis CavallucciAbstractPatenTEB is a comprehensive benchmark for patent text embeddings with 15 tasks, and the patembed model family demonstrates strong generalization across various patent-specific challenges.AI-generated summaryPatent text embeddings enable prior art search, technology landscaping, and\npatent analysis, yet existing benchmarks inadequately capture patent-specific\nchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15\ntasks across retrieval, classification, paraphrase, and clustering, with 2.06\nmillion examples. PatenTEB employsdomain-stratified splits, domain specific\nhard negative mining, and systematic coverage of asymmetric\nfragment-to-document matching scenarios absent from general embedding\nbenchmarks. We develop the patembed model family throughmulti-task training,spanning 67M to 344M parameters withcontext lengthsup to 4096 tokens.\nExternal validation shows strong generalization: patembed-base achieves\nstate-of-the-art on MTEB BigPatentClustering.v2 (0.494V-measurevs. 0.445\nprevious best), while patembed-large achieves 0.377NDCG@100on DAPFAM.\nSystematic ablations reveal thatmulti-task trainingimproves external\ngeneralization despite minor benchmark costs, and that domain-pretrained\ninitialization provides consistent advantages across task families. All\nresources will be made available at https://github.com/iliass-y/patenteb.\nKeywords: patent retrieval, sentence embeddings, multi-task learning,\nasymmetric retrieval, benchmark evaluation, contrastive learning.View arXiv pageView PDFGitHub5autoAdd to collectionCommunitydatalyesPaper authorPaper submitterOct 29PatenTEBaddresses a critical gap in patent text understanding by providing the first comprehensive benchmark spec",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/iliass-y/patenteb",
    "hf_paper_url": "https://huggingface.co/papers/2510.22264",
    "arxiv_url": "https://arxiv.org/abs/2510.22264",
    "num_models": 12,
    "models_list": "datalyes/patembed-large, datalyes/patembed-base, datalyes/patembed-base_small, datalyes/patembed-small, datalyes/patembed-mini, datalyes/patembed-nano, datalyes/patembed-base_long_1024, datalyes/patembed-base_long_2048, datalyes/patembed-base_long_4096, datalyes/patembed-large_no_prompts, datalyes/patembed-large_all_ret_only, datalyes/patembed-large_all_no_classif",
    "models_links": "https://huggingface.co/datalyes/patembed-large, https://huggingface.co/datalyes/patembed-base, https://huggingface.co/datalyes/patembed-base_small, https://huggingface.co/datalyes/patembed-small, https://huggingface.co/datalyes/patembed-mini, https://huggingface.co/datalyes/patembed-nano, https://huggingface.co/datalyes/patembed-base_long_1024, https://huggingface.co/datalyes/patembed-base_long_2048, https://huggingface.co/datalyes/patembed-base_long_4096, https://huggingface.co/datalyes/patembed-large_no_prompts, https://huggingface.co/datalyes/patembed-large_all_ret_only, https://huggingface.co/datalyes/patembed-large_all_no_classif",
    "models_detailed": "[{\"name\": \"datalyes/patembed-large\", \"link\": \"https://huggingface.co/datalyes/patembed-large\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-base\", \"link\": \"https://huggingface.co/datalyes/patembed-base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-base_small\", \"link\": \"https://huggingface.co/datalyes/patembed-base_small\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-small\", \"link\": \"https://huggingface.co/datalyes/patembed-small\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-mini\", \"link\": \"https://huggingface.co/datalyes/patembed-mini\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-nano\", \"link\": \"https://huggingface.co/datalyes/patembed-nano\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-base_long_1024\", \"link\": \"https://huggingface.co/datalyes/patembed-base_long_1024\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-base_long_2048\", \"link\": \"https://huggingface.co/datalyes/patembed-base_long_2048\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-base_long_4096\", \"link\": \"https://huggingface.co/datalyes/patembed-base_long_4096\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-large_no_prompts\", \"link\": \"https://huggingface.co/datalyes/patembed-large_no_prompts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-large_all_ret_only\", \"link\": \"https://huggingface.co/datalyes/patembed-large_all_ret_only\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"datalyes/patembed-large_all_no_classif\", \"link\": \"https://huggingface.co/datalyes/patembed-large_all_no_classif\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}]",
    "num_datasets": 15,
    "datasets_list": "datalyes/problem2full, datalyes/effect2substance, datalyes/retrieval_IN, datalyes/clusters_inventor, datalyes/class_bloom, datalyes/class_nli_oldnew, datalyes/class_text2ipc3, datalyes/clusters_ext_full_ipc, datalyes/effect2full, datalyes/para_problem, datalyes/para_solution, datalyes/problem2solution, datalyes/retrieval_MIXED, datalyes/retrieval_OUT, datalyes/title2full",
    "datasets_links": "https://huggingface.co/datasets/datalyes/problem2full, https://huggingface.co/datasets/datalyes/effect2substance, https://huggingface.co/datasets/datalyes/retrieval_IN, https://huggingface.co/datasets/datalyes/clusters_inventor, https://huggingface.co/datasets/datalyes/class_bloom, https://huggingface.co/datasets/datalyes/class_nli_oldnew, https://huggingface.co/datasets/datalyes/class_text2ipc3, https://huggingface.co/datasets/datalyes/clusters_ext_full_ipc, https://huggingface.co/datasets/datalyes/effect2full, https://huggingface.co/datasets/datalyes/para_problem, https://huggingface.co/datasets/datalyes/para_solution, https://huggingface.co/datasets/datalyes/problem2solution, https://huggingface.co/datasets/datalyes/retrieval_MIXED, https://huggingface.co/datasets/datalyes/retrieval_OUT, https://huggingface.co/datasets/datalyes/title2full",
    "datasets_detailed": "[{\"name\": \"datalyes/problem2full\", \"link\": \"https://huggingface.co/datasets/datalyes/problem2full\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/effect2substance\", \"link\": \"https://huggingface.co/datasets/datalyes/effect2substance\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/retrieval_IN\", \"link\": \"https://huggingface.co/datasets/datalyes/retrieval_IN\", \"task\": \"\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/clusters_inventor\", \"link\": \"https://huggingface.co/datasets/datalyes/clusters_inventor\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/class_bloom\", \"link\": \"https://huggingface.co/datasets/datalyes/class_bloom\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/class_nli_oldnew\", \"link\": \"https://huggingface.co/datasets/datalyes/class_nli_oldnew\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/class_text2ipc3\", \"link\": \"https://huggingface.co/datasets/datalyes/class_text2ipc3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/clusters_ext_full_ipc\", \"link\": \"https://huggingface.co/datasets/datalyes/clusters_ext_full_ipc\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/effect2full\", \"link\": \"https://huggingface.co/datasets/datalyes/effect2full\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/para_problem\", \"link\": \"https://huggingface.co/datasets/datalyes/para_problem\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/para_solution\", \"link\": \"https://huggingface.co/datasets/datalyes/para_solution\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/problem2solution\", \"link\": \"https://huggingface.co/datasets/datalyes/problem2solution\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/retrieval_MIXED\", \"link\": \"https://huggingface.co/datasets/datalyes/retrieval_MIXED\", \"task\": \"\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/retrieval_OUT\", \"link\": \"https://huggingface.co/datasets/datalyes/retrieval_OUT\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}, {\"name\": \"datalyes/title2full\", \"link\": \"https://huggingface.co/datasets/datalyes/title2full\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.23642",
    "first_seen_date": "2025-10-29",
    "title": "VisCoder2: Building Multi-Language Visualization Coding Agents",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.23642VisCoder2: Building Multi-Language Visualization Coding AgentsPublished on Oct 24\u00b7Submitted byYuansheng Nion Oct 29\u00b7TIGER-LabUpvote21+13Authors:Yuansheng Ni,Songcheng Cai,Xiangchao Chen,Jiarong Liang,Zhiheng Lyu,Jiaqi Deng,Kai Zou,Ping Nie,Fei Yuan,Xiang Yue,Wenhu ChenAbstractVisCoder2, a multi-language visualization model trained on VisCode-Multi-679K, outperforms open-source baselines and approaches proprietary models in visualization coding tasks, especially with iterative self-debugging.AI-generated summaryLarge language models(LLMs) have recently enabledcoding agentscapable of\ngenerating, executing, and revisingvisualization code. However, existing\nmodels often fail in practical workflows due to limitedlanguage coverage,\nunreliable execution, and lack ofiterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualizationcoding agents.VisCode-Multi-679Kis a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages.VisPlotBenchis a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresentVisCoder2, a family of multi-language visualization models trained onVisCode-Multi-679K. Experiments show thatVisCoder2significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overallexecution pass rateat the 32B scale, particularly in symbolic orcompiler-dependent languages.View arXiv pageView PDFProject pageGitHub7Add to collectionCommunityyuanshengniP",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/TIGER-AI-Lab/VisCoder2",
    "hf_paper_url": "https://huggingface.co/papers/2510.23642",
    "arxiv_url": "https://arxiv.org/abs/2510.23642",
    "num_models": 6,
    "models_list": "TIGER-Lab/VisCoder2-3B, TIGER-Lab/VisCoder2-7B, TIGER-Lab/VisCoder2-14B, TIGER-Lab/VisCoder2-32B, Akicou/ViSWE, Akicou/ViSWE-GGUF",
    "models_links": "https://huggingface.co/TIGER-Lab/VisCoder2-3B, https://huggingface.co/TIGER-Lab/VisCoder2-7B, https://huggingface.co/TIGER-Lab/VisCoder2-14B, https://huggingface.co/TIGER-Lab/VisCoder2-32B, https://huggingface.co/Akicou/ViSWE, https://huggingface.co/Akicou/ViSWE-GGUF",
    "models_detailed": "[{\"name\": \"TIGER-Lab/VisCoder2-3B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder2-3B\", \"task\": \"Text Generation\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"TIGER-Lab/VisCoder2-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder2-7B\", \"task\": \"\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"TIGER-Lab/VisCoder2-14B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder2-14B\", \"task\": \"\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"TIGER-Lab/VisCoder2-32B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder2-32B\", \"task\": \"\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"Akicou/ViSWE\", \"link\": \"https://huggingface.co/Akicou/ViSWE\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"Akicou/ViSWE-GGUF\", \"link\": \"https://huggingface.co/Akicou/ViSWE-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}]",
    "num_datasets": 2,
    "datasets_list": "TIGER-Lab/VisCode-Multi-679K, TIGER-Lab/VisPlotBench",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/VisCode-Multi-679K, https://huggingface.co/datasets/TIGER-Lab/VisPlotBench",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/VisCode-Multi-679K\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisCode-Multi-679K\", \"task\": \"\", \"likes\": \"222\", \"downloads\": \"\", \"updated\": \"Nov 3\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/VisPlotBench\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisPlotBench\", \"task\": \"\", \"likes\": \"888\", \"downloads\": \"\", \"updated\": \"Nov 3\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.24693",
    "first_seen_date": "2025-10-29",
    "title": "STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.24693STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  IntelligencePublished on Oct 28\u00b7Submitted byZihan Liuon Oct 29\u00b7Intern Large ModelsUpvote18+10Authors:Zihan Liu,Zhikang Niu,Qiuyang Xiao,Zhisheng Zheng,Ruoqi Yuan,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Jianze Liang,Xie Chen,Leilei Sun,Dahua Lin,Jiaqi WangAbstractSTAR-Bench measures audio 4D intelligence by evaluating sound dynamics in time and 3D space, revealing gaps in fine-grained perceptual reasoning among existing models.AI-generated summaryDespite rapid progress inMulti-modal Large Language Modelsand Large\nAudio-Language Models, existingaudio benchmarkslargely test semantics that\ncan be recovered from text captions, masking deficits in fine-grained\nperceptualreasoning. We formalize audio 4D intelligence that is defined asreasoningoversound dynamicsin time and3D space, and introduceSTAR-Benchto\nmeasure it.STAR-Benchcombines aFoundational Acoustic Perceptionsetting (six\nattributes under absolute and relative regimes) with a Holistic Spatio-TemporalReasoningsetting that includessegment reorderingfor continuous and discrete\nprocesses and spatial tasks spanningstatic localization, multi-source\nrelations, anddynamic trajectories. Our data curation pipeline uses two\nmethods to ensure high-quality samples. For foundational tasks, we useprocedurally synthesizedandphysics-simulated audio. For holistic data, we\nfollow a four-stage process that includeshuman annotationand final selection\nbased onhuman performance. Unlike prior benchmarks where caption-only\nanswering reduces accuracy slightly,STAR-Benchinduces far larger drops\n(-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically\nhard-to-describe cues. Evaluating 19 models reveals substantial gaps compared\nwith humans and a capability hierarchy:closed-source modelsare bottlenecked\nbyfine-grained perception, whileopen-source modelslag across percep",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/InternLM/StarBench",
    "hf_paper_url": "https://huggingface.co/papers/2510.24693",
    "arxiv_url": "https://arxiv.org/abs/2510.24693",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "internlm/STAR-Bench",
    "datasets_links": "https://huggingface.co/datasets/internlm/STAR-Bench",
    "datasets_detailed": "[{\"name\": \"internlm/STAR-Bench\", \"link\": \"https://huggingface.co/datasets/internlm/STAR-Bench\", \"task\": \"\", \"likes\": \"204\", \"downloads\": \"\", \"updated\": \"Nov 9\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.22733",
    "first_seen_date": "2025-10-28",
    "title": "E^2Rank: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.22733E^2Rank: Your Text Embedding can Also be an Effective\n  and Efficient Listwise RerankerPublished on Oct 26\u00b7Submitted byQi Liuon Oct 28\u00b7Alibaba-NLPUpvote31+23Authors:Qi Liu,Yanzhao Zhang,Mingxin Li,Dingkun Long,Pengjun Xie,Jiaxin MaoAbstractA unified framework extends a single text embedding model to perform both retrieval and listwise reranking, achieving state-of-the-art results with low latency.AI-generated summaryText embedding modelsserve as a fundamental component in real-world search\napplications. By mapping queries and documents into ashared embedding space,\nthey deliver competitive retrieval performance with high efficiency. However,\ntheirranking fidelityremains limited compared to dedicated rerankers,\nespecially recentLLM-based listwise rerankers, which capture fine-grained\nquery-document and document-document interactions. In this paper, we propose a\nsimple yet effective unified framework E^2Rank, means Efficient\nEmbedding-based Ranking (also means Embedding-to-Rank), which extends a single\ntext embedding model to perform both high-quality retrieval and listwise\nreranking through continued training under alistwise ranking objective,\nthereby achieving strong effectiveness with remarkable efficiency. By applyingcosine similaritybetween the query and document embeddings as a unified\nranking function, thelistwise ranking prompt, which is constructed from the\noriginal query and its candidate documents, serves as an enhanced query\nenriched with signals from the top-K documents, akin to pseudo-relevance\nfeedback (PRF) in traditional retrieval models. This design preserves the\nefficiency and representational quality of the base embedding model while\nsignificantly improving its reranking performance. Empirically,\nE^2Rank achieves state-of-the-art results on the BEIR\nreranking benchmark and demonstrates competitive performance on the\nreasoning-intensiveBRIGHT benchmark, ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/Alibaba-NLP/E2Rank",
    "hf_paper_url": "https://huggingface.co/papers/2510.22733",
    "arxiv_url": "https://arxiv.org/abs/2510.22733",
    "num_models": 3,
    "models_list": "Alibaba-NLP/E2Rank-0.6B, Alibaba-NLP/E2Rank-8B, Alibaba-NLP/E2Rank-4B",
    "models_links": "https://huggingface.co/Alibaba-NLP/E2Rank-0.6B, https://huggingface.co/Alibaba-NLP/E2Rank-8B, https://huggingface.co/Alibaba-NLP/E2Rank-4B",
    "models_detailed": "[{\"name\": \"Alibaba-NLP/E2Rank-0.6B\", \"link\": \"https://huggingface.co/Alibaba-NLP/E2Rank-0.6B\", \"task\": \"\", \"likes\": \"648\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"Alibaba-NLP/E2Rank-8B\", \"link\": \"https://huggingface.co/Alibaba-NLP/E2Rank-8B\", \"task\": \"\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"Alibaba-NLP/E2Rank-4B\", \"link\": \"https://huggingface.co/Alibaba-NLP/E2Rank-4B\", \"task\": \"\", \"likes\": \"193\", \"downloads\": \"\", \"updated\": \"Oct 28\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.23272",
    "first_seen_date": "2025-10-28",
    "title": "Code Aesthetics with Agentic Reward Feedback",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.23272Code Aesthetics with Agentic Reward FeedbackPublished on Oct 27\u00b7Submitted byJackon Oct 28Upvote7Authors:Bang Xiao,Lingjie Jiang,Shaohan Huang,Tengchao Lv,Yupan Huang,Xun Wu,Lei Cui,Furu WeiAbstractA new pipeline enhances LLM-generated code aesthetics using a large-scale dataset, multi-agent feedback, and integrated optimization, outperforming existing models.AI-generated summaryLarge Language Models(LLMs) have become valuable assistants for developers\nin code-related tasks. WhileLLMsexcel at traditional programming tasks such\nas code generation and bug fixing, they struggle with visually-oriented coding\ntasks, often producing suboptimal aesthetics. In this paper, we introduce a new\npipeline to enhance the aesthetic quality of LLM-generated code. We first\nconstructAesCode-358K, a large-scale instruction-tuning dataset focused oncode aesthetics. Next, we proposeagentic reward feedback, amulti-agent systemthat evaluates executability, static aesthetics, and interactive aesthetics.\nBuilding on this, we developGRPO-AR, which integrates these signals into theGRPO algorithmfor joint optimization of functionality andcode aesthetics.\nFinally, we developOpenDesign, a benchmark for assessingcode aesthetics.\nExperimental results show that combiningsupervised fine-tuningonAesCode-358Kwithreinforcement learningusingagentic reward feedbacksignificantly\nimproves performance onOpenDesignand also enhances results on existing\nbenchmarks such asPandasPlotBench. Notably, ourAesCoder-4BsurpassesGPT-4oandGPT-4.1, and achieves performance comparable to large open-source models\nwith 480B-685B parameters, underscoring the effectiveness of our approach.View arXiv pageView PDFProject pageGitHub6autoAdd to collectionCommunitylingjie23Paper submitterOct 28\u2022edited Oct 28Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional pr",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/bangx7/code_aesthetics",
    "hf_paper_url": "https://huggingface.co/papers/2510.23272",
    "arxiv_url": "https://arxiv.org/abs/2510.23272",
    "num_models": 2,
    "models_list": "SamuelBang/AesCoder-4B, Mungert/AesCoder-4B-GGUF",
    "models_links": "https://huggingface.co/SamuelBang/AesCoder-4B, https://huggingface.co/Mungert/AesCoder-4B-GGUF",
    "models_detailed": "[{\"name\": \"SamuelBang/AesCoder-4B\", \"link\": \"https://huggingface.co/SamuelBang/AesCoder-4B\", \"task\": \"Text Generation\", \"likes\": \"546\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"Mungert/AesCoder-4B-GGUF\", \"link\": \"https://huggingface.co/Mungert/AesCoder-4B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.23603",
    "first_seen_date": "2025-10-28",
    "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.23603PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary GranularityPublished on Oct 27\u00b7Submitted bytaesirion Oct 28Upvote22+14Authors:Yuqian Yuan,Wenqiao Zhang,Xin Li,Shihao Wang,Kehan Li,Wentong Li,Jun Xiao,Lei Zhang,Beng Chin OoiAbstractPixelRefer is a unified region-level multimodal large language model framework that enhances fine-grained object-centric understanding and efficiency through a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module.AI-generated summaryMultimodal large language models(MLLMs) have demonstrated strong\ngeneral-purpose capabilities in open-world visual comprehension. However, most\nexistingMLLMsprimarily focus on holistic, scene-level understanding, often\noverlooking the need for fine-grained,object-centric reasoning. In this paper,\nwe presentPixelRefer, a unified region-level MLLM framework that enables\nadvanced fine-grained understanding over user-specified regions across both\nimages and videos. Motivated by the observation that LLM attention\npredominantly focuses on object-level tokens, we propose a Scale-Adaptive\nObject Tokenizer (SAOT) to generate compact and semantically rich object\nrepresentations from free-form regions. Our analysis reveals that global visual\ntokens contribute mainly in early LLM layers, inspiring the design ofPixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion\nmodule to pre-fuse global context intoobject tokens. This yields a lightweight\nObject-Only Framework that substantially reduces computational cost while\nmaintaining high semantic fidelity. To facilitate fine-grained instruction\ntuning, we curatePixelRefer-2.2M, a high-quality object-centric instruction\ndataset. Extensive experiments across a range of benchmarks validate thatPixelReferachieves leading performance with fewer training samples, whilePixelRefer-Liteoffers competitive accuracy with n",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/alibaba-damo-academy/PixelRefer",
    "hf_paper_url": "https://huggingface.co/papers/2510.23603",
    "arxiv_url": "https://arxiv.org/abs/2510.23603",
    "num_models": 4,
    "models_list": "Alibaba-DAMO-Academy/PixelRefer-2B, Alibaba-DAMO-Academy/PixelRefer-7B, Alibaba-DAMO-Academy/PixelRefer-Lite-2B, Alibaba-DAMO-Academy/PixelRefer-Lite-7B",
    "models_links": "https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-2B, https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-7B, https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-Lite-2B, https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-Lite-7B",
    "models_detailed": "[{\"name\": \"Alibaba-DAMO-Academy/PixelRefer-2B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-2B\", \"task\": \"\", \"likes\": \"324\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"Alibaba-DAMO-Academy/PixelRefer-7B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-7B\", \"task\": \"\", \"likes\": \"77\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"Alibaba-DAMO-Academy/PixelRefer-Lite-2B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-Lite-2B\", \"task\": \"\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"Alibaba-DAMO-Academy/PixelRefer-Lite-7B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-Lite-7B\", \"task\": \"\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Oct 28\"}]",
    "num_datasets": 1,
    "datasets_list": "DAMO-NLP-SG/VideoRefer-700K",
    "datasets_links": "https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-700K",
    "datasets_detailed": "[{\"name\": \"DAMO-NLP-SG/VideoRefer-700K\", \"link\": \"https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-700K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.20888",
    "first_seen_date": "2025-10-27",
    "title": "Video-As-Prompt: Unified Semantic Control for Video Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.20888Video-As-Prompt: Unified Semantic Control for Video GenerationPublished on Oct 23\u00b7Submitted byYuxuan BIANon Oct 27\u00b7ByteDanceUpvote45+37Authors:Yuxuan Bian,Xin Chen,Zenan Li,Tiancheng Zhi,Shen Sang,Linjie Luo,Qiang XuAbstractVideo-As-Prompt (VAP) uses a reference video to guide a frozen Video Diffusion Transformer via a Mixture-of-Transformers expert, achieving state-of-the-art results in semantic-controlled video generation with strong zero-shot generalization.AI-generated summaryUnified, generalizable semantic control in video generation remains a\ncritical open challenge. Existing methods either introduce artifacts by\nenforcing inappropriate pixel-wise priors from structure-based controls, or\nrely on non-generalizable, condition-specific finetuning or task-specific\narchitectures. We introduceVideo-As-Prompt(VAP), a new paradigm that reframes\nthis problem as in-context generation. VAP leverages a reference video as a\ndirect semantic prompt, guiding a frozenVideo Diffusion Transformer(DiT) via\na plug-and-playMixture-of-Transformers(MoT) expert. This architecture\npreventscatastrophic forgettingand is guided by a temporally biased position\nembedding that eliminates spurious mapping priors for robust context retrieval.\nTo power this approach and catalyze future research, we builtVAP-Data, the\nlargest dataset for semantic-controlled video generation with over 100K paired\nvideos across 100 semantic conditions. As a single unified model, VAP sets a\nnew state-of-the-art for open-source methods, achieving a 38.7% user preference\nrate that rivals leading condition-specific commercial models. VAP's strongzero-shot generalizationand support for various downstream applications mark a\nsignificant advance toward general-purpose, controllable video generation.View arXiv pageView PDFProject pageGitHub330Add to collectionCommunityBianYxPaper authorPaper submitterOct 27\u2022edited Oct 27Text p",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/bytedance/Video-As-Prompt",
    "hf_paper_url": "https://huggingface.co/papers/2510.20888",
    "arxiv_url": "https://arxiv.org/abs/2510.20888",
    "num_models": 2,
    "models_list": "ByteDance/Video-As-Prompt-Wan2.1-14B, ByteDance/Video-As-Prompt-CogVideoX-5B",
    "models_links": "https://huggingface.co/ByteDance/Video-As-Prompt-Wan2.1-14B, https://huggingface.co/ByteDance/Video-As-Prompt-CogVideoX-5B",
    "models_detailed": "[{\"name\": \"ByteDance/Video-As-Prompt-Wan2.1-14B\", \"link\": \"https://huggingface.co/ByteDance/Video-As-Prompt-Wan2.1-14B\", \"task\": \"\", \"likes\": \"161\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"ByteDance/Video-As-Prompt-CogVideoX-5B\", \"link\": \"https://huggingface.co/ByteDance/Video-As-Prompt-CogVideoX-5B\", \"task\": \"\", \"likes\": \"130\", \"downloads\": \"\", \"updated\": \"Oct 27\"}]",
    "num_datasets": 1,
    "datasets_list": "BianYx/VAP-Data",
    "datasets_links": "https://huggingface.co/datasets/BianYx/VAP-Data",
    "datasets_detailed": "[{\"name\": \"BianYx/VAP-Data\", \"link\": \"https://huggingface.co/datasets/BianYx/VAP-Data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.18941",
    "first_seen_date": "2025-10-23",
    "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to\n  Answer and Judge",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.18941ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to\n  Answer and JudgePublished on Oct 21\u00b7Submitted byShizhe Diaoon Oct 23\u00b7NVIDIAUpvote7Authors:Zhilin Wang,Jaehun Jung,Ximing Lu,Shizhe Diao,Ellie Evans,Jiaqi Zeng,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi DongAbstractProfBench evaluates large language models in professional domains using human-expert criteria, revealing challenges and performance disparities between proprietary and open-weight models.AI-generated summaryEvaluating progress inlarge language models(LLMs) is often constrained by\nthe challenge of verifying responses, limiting assessments to tasks like\nmathematics, programming, and short-form question-answering. However, many\nreal-world applications require evaluatingLLMsin processing professional\ndocuments, synthesizing information, and generating comprehensive reports in\nresponse to user queries. We introduceProfBench: a set of over 7000response-criterion pairsas evaluated byhuman-expertswith professional\nknowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We\nbuild robust and affordableLLM-Judgesto evaluateProfBenchrubrics, by\nmitigatingself-enhancement biasand reducing the cost of evaluation by 2-3\norders of magnitude, to make it fair and accessible to the broader community.\nOur findings reveal thatProfBenchposes significant challenges even for\nstate-of-the-artLLMs, with top-performing models likeGPT-5-highachieving\nonly 65.9\\% overall performance. Furthermore, we identify notable performance\ndisparities between proprietary and open-weight models and provide insights\ninto the role thatextended thinkingplays in addressing complex,professional-domain tasks. Data:\nhttps://huggingface.co/datasets/nvidia/ProfBenchand Code:\nhttps://github.com/NVlabs/ProfBenchView arXiv pageView PDFGitHub26autoAdd to collectionCommunityshizhediaoPaper submitterOct 23ProfBench: Multi-Domain Ru",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/NVlabs/ProfBench",
    "hf_paper_url": "https://huggingface.co/papers/2510.18941",
    "arxiv_url": "https://arxiv.org/abs/2510.18941",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "nvidia/ProfBench, introvoyz041/ProfBench",
    "datasets_links": "https://huggingface.co/datasets/nvidia/ProfBench, https://huggingface.co/datasets/introvoyz041/ProfBench",
    "datasets_detailed": "[{\"name\": \"nvidia/ProfBench\", \"link\": \"https://huggingface.co/datasets/nvidia/ProfBench\", \"task\": \"\", \"likes\": \"40\", \"downloads\": \"\", \"updated\": \"Oct 30\", \"size\": \"\"}, {\"name\": \"introvoyz041/ProfBench\", \"link\": \"https://huggingface.co/datasets/introvoyz041/ProfBench\", \"task\": \"\", \"likes\": \"40\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.19631",
    "first_seen_date": "2025-10-23",
    "title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule Application",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.19631HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search\n  Agents in Hierarchical Rule ApplicationPublished on Oct 22\u00b7Submitted byTian Lanon Oct 23\u00b7AIDC-AIUpvote27+19Authors:Yiqian Yang,Tian Lan,Qianghuai Jia,Li Zhu,Hui Jiang,Hang Zhu,Longyue Wang,Weihua Luo,Kaifu ZhangAbstractHSCodeComp evaluates deep search agents' hierarchical rule application in predicting product HS Codes, revealing significant performance gaps compared to human experts.AI-generated summaryEffectivedeep search agentsmust not only access open-domain and\ndomain-specific knowledge but also apply complex rules-such as legal clauses,\nmedical manuals and tariff rules. These rules often feature vague boundaries\nand implicit logic relationships, making precise application challenging for\nagents. However, this critical capability is largely overlooked by current\nagent benchmarks.\n  To fill this gap, we introduce HSCodeComp, the first realistic, expert-level\ne-commerce benchmark designed to evaluatedeep search agentsin hierarchical\nrule application. In this task, the deep reasoning process of agents is guided\nby these rules to predict 10-digitHarmonized System Code (HSCode)of products\nwith noisy but realistic descriptions. These codes, established by the World\nCustoms Organization, are vital forglobal supply chain efficiency. Built from\nreal-world data collected from large-scale e-commerce platforms, our proposed\nHSCodeComp comprises 632 product entries spanning diverse product categories,\nwith these HSCodes annotated by several human experts.\n  Extensive experimental results on several state-of-the-artLLMs, open-source,\nandclosed-source agentsreveal a huge performance gap: best agent achieves\nonly 46.8% 10-digit accuracy, far below human experts at 95.0%. Besides,\ndetailed analysis demonstrates the challenges ofhierarchical rule application,\nandtest-time scalingfails to improve performance further.",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/AIDC-AI/Marco-Search-Agent",
    "hf_paper_url": "https://huggingface.co/papers/2510.19631",
    "arxiv_url": "https://arxiv.org/abs/2510.19631",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "AIDC-AI/HSCodeComp",
    "datasets_links": "https://huggingface.co/datasets/AIDC-AI/HSCodeComp",
    "datasets_detailed": "[{\"name\": \"AIDC-AI/HSCodeComp\", \"link\": \"https://huggingface.co/datasets/AIDC-AI/HSCodeComp\", \"task\": \"\", \"likes\": \"632\", \"downloads\": \"\", \"updated\": \"Oct 27\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.18234",
    "first_seen_date": "2025-10-22",
    "title": "DeepSeek-OCR: Contexts Optical Compression",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.18234DeepSeek-OCR: Contexts Optical CompressionPublished on Oct 21\u00b7Submitted byElie Bakouchon Oct 22\u00b7DeepSeekUpvote84+76Authors:Haoran Wei,Yaofeng Sun,Yukun LiAbstractDeepSeek-OCR uses optical 2D mapping to compress long contexts, achieving high OCR precision with reduced vision tokens and demonstrating practical value in document processing.AI-generated summaryWe presentDeepSeek-OCRas an initial investigation into the feasibility of\ncompressing long contexts viaoptical 2D mapping.DeepSeek-OCRconsists of two\ncomponents:DeepEncoderandDeepSeek3B-MoE-A570Mas the decoder. Specifically,DeepEncoderserves as the core engine, designed to maintain low activations\nunder high-resolution input while achieving highcompression ratios to ensure\nan optimal and manageable number ofvision tokens. Experiments show that when\nthe number of text tokens is within 10 times that ofvision tokens(i.e., acompression ratio< 10x), the model can achieve decoding (OCR) precision of\n97%. Even at acompression ratioof 20x, the OCR accuracy still remains at\nabout 60%. This shows considerable promise for research areas such as\nhistorical long-context compression and memory forgetting mechanisms inLLMs.\nBeyond this,DeepSeek-OCRalso demonstrates high practical value. OnOmniDocBench, it surpassesGOT-OCR2.0(256 tokens/page) using only 100 vision\ntokens, and outperformsMinerU2.0(6000+ tokens per page on average) while\nutilizing fewer than 800vision tokens. In production,DeepSeek-OCRcan\ngenerate training data forLLMs/VLMsat a scale of 200k+ pages per day (a\nsingle A100-40G). Codes and model weights are publicly accessible at\nhttp://github.com/deepseek-ai/DeepSeek-OCR.View arXiv pageView PDFGitHub21.5kautoAdd to collectionCommunityeliebakPaper submitterOct 22deepseek OCR paper!Replylibrarian-botOct 23This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following pap",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/deepseek-ai/DeepSeek-OCR",
    "hf_paper_url": "https://huggingface.co/papers/2510.18234",
    "arxiv_url": "https://arxiv.org/abs/2510.18234",
    "num_models": 18,
    "models_list": "deepseek-ai/DeepSeek-OCR, unsloth/DeepSeek-OCR, Jalea96/DeepSeek-OCR-bnb-4bit-NF4, prithivMLmods/DeepSeek-OCR-Latest-BF16.I64, kp-forks/DeepSeek-OCR, ZoneTwelve/DeepSeek-OCR-AnyDevice, applegrew/deepseek-ocr-macos, ginipick/DeepSeek-OCR, seawolf2357/DeepSeek-OCR, fantaxy/DeepSeek-OCR, fantos/DeepSeek-OCR, doublemathew/DeepSeek-OCR, specsGuy/Deepseek-ocr, Sci-fi-vy/DeepSeek-OCR, jonathanli/ds-ocr-fixed, Crossberry/Deep, webdev8710/DeepSeek-OCR, dipanjanS/DeepSeek-OCR",
    "models_links": "https://huggingface.co/deepseek-ai/DeepSeek-OCR, https://huggingface.co/unsloth/DeepSeek-OCR, https://huggingface.co/Jalea96/DeepSeek-OCR-bnb-4bit-NF4, https://huggingface.co/prithivMLmods/DeepSeek-OCR-Latest-BF16.I64, https://huggingface.co/kp-forks/DeepSeek-OCR, https://huggingface.co/ZoneTwelve/DeepSeek-OCR-AnyDevice, https://huggingface.co/applegrew/deepseek-ocr-macos, https://huggingface.co/ginipick/DeepSeek-OCR, https://huggingface.co/seawolf2357/DeepSeek-OCR, https://huggingface.co/fantaxy/DeepSeek-OCR, https://huggingface.co/fantos/DeepSeek-OCR, https://huggingface.co/doublemathew/DeepSeek-OCR, https://huggingface.co/specsGuy/Deepseek-ocr, https://huggingface.co/Sci-fi-vy/DeepSeek-OCR, https://huggingface.co/jonathanli/ds-ocr-fixed, https://huggingface.co/Crossberry/Deep, https://huggingface.co/webdev8710/DeepSeek-OCR, https://huggingface.co/dipanjanS/DeepSeek-OCR",
    "models_detailed": "[{\"name\": \"deepseek-ai/DeepSeek-OCR\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"unsloth/DeepSeek-OCR\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 9\"}, {\"name\": \"Jalea96/DeepSeek-OCR-bnb-4bit-NF4\", \"link\": \"https://huggingface.co/Jalea96/DeepSeek-OCR-bnb-4bit-NF4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"prithivMLmods/DeepSeek-OCR-Latest-BF16.I64\", \"link\": \"https://huggingface.co/prithivMLmods/DeepSeek-OCR-Latest-BF16.I64\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"kp-forks/DeepSeek-OCR\", \"link\": \"https://huggingface.co/kp-forks/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"ZoneTwelve/DeepSeek-OCR-AnyDevice\", \"link\": \"https://huggingface.co/ZoneTwelve/DeepSeek-OCR-AnyDevice\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"applegrew/deepseek-ocr-macos\", \"link\": \"https://huggingface.co/applegrew/deepseek-ocr-macos\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"ginipick/DeepSeek-OCR\", \"link\": \"https://huggingface.co/ginipick/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"seawolf2357/DeepSeek-OCR\", \"link\": \"https://huggingface.co/seawolf2357/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"fantaxy/DeepSeek-OCR\", \"link\": \"https://huggingface.co/fantaxy/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"fantos/DeepSeek-OCR\", \"link\": \"https://huggingface.co/fantos/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"doublemathew/DeepSeek-OCR\", \"link\": \"https://huggingface.co/doublemathew/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"specsGuy/Deepseek-ocr\", \"link\": \"https://huggingface.co/specsGuy/Deepseek-ocr\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"Sci-fi-vy/DeepSeek-OCR\", \"link\": \"https://huggingface.co/Sci-fi-vy/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"jonathanli/ds-ocr-fixed\", \"link\": \"https://huggingface.co/jonathanli/ds-ocr-fixed\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"Crossberry/Deep\", \"link\": \"https://huggingface.co/Crossberry/Deep\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 18\"}, {\"name\": \"webdev8710/DeepSeek-OCR\", \"link\": \"https://huggingface.co/webdev8710/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"dipanjanS/DeepSeek-OCR\", \"link\": \"https://huggingface.co/dipanjanS/DeepSeek-OCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.18701",
    "first_seen_date": "2025-10-22",
    "title": "UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.18701UniGenBench++: A Unified Semantic Evaluation Benchmark for Text-to-Image\n  GenerationPublished on Oct 21\u00b7Submitted bytaesirion Oct 22Upvote66+58Authors:Yibin Wang,Zhimin Li,Yuhang Zang,Jiazi Bu,Yujie Zhou,Yi Xin,Junjun He,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi WangAbstractUniGenBench++ is a comprehensive benchmark for text-to-image generation that evaluates semantic consistency across diverse scenarios and languages using a hierarchical prompt structure and a robust evaluation pipeline.AI-generated summaryRecent progress in text-to-image (T2I) generation underscores the importance\nof reliablebenchmarks in evaluating how accurately generated images reflect\nthe semantics of their textual prompt. However, (1) existingbenchmarks lack\nthe diversity ofprompt scenariosandmultilingual support, both essential for\nreal-world applicability; (2) they offer only coarse evaluations across primary\ndimensions, covering a narrow range of sub-dimensions, and fall short in\nfine-grained sub-dimension assessment. To address these limitations, we\nintroduce UniGenBench++, a unifiedsemantic assessmentbenchmarkfor T2I\ngeneration. Specifically, it comprises 600 prompts organized hierarchically to\nensure both coverage and efficiency: (1) spans across diverse real-world\nscenarios, i.e., 5 main prompt themes and 20 subthemes; (2) comprehensively\nprobes T2I models'semantic consistencyover 10 primary and 27 sub evaluation\ncriteria, with each prompt assessing multiple testpoints. To rigorously assess\nmodel robustness to variations in language and prompt length, we provide both\nEnglish and Chinese versions of each prompt in short and long forms. Leveraging\nthe general world knowledge and fine-grained image understanding capabilities\nof a closed-sourceMulti-modal Large Language Model(MLLM), i.e.,Gemini-2.5-Pro, an effective pipeline is developed for reliablebenchmarkconstruction and streamlined model ass",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/CodeGoat24/UniGenBench",
    "hf_paper_url": "https://huggingface.co/papers/2510.18701",
    "arxiv_url": "https://arxiv.org/abs/2510.18701",
    "num_models": 2,
    "models_list": "CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1, CodeGoat24/UniGenBench-EvalModel-qwen3vl-32b-v1",
    "models_links": "https://huggingface.co/CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1, https://huggingface.co/CodeGoat24/UniGenBench-EvalModel-qwen3vl-32b-v1",
    "models_detailed": "[{\"name\": \"CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1\", \"link\": \"https://huggingface.co/CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1\", \"task\": \"\", \"likes\": \"142\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"CodeGoat24/UniGenBench-EvalModel-qwen3vl-32b-v1\", \"link\": \"https://huggingface.co/CodeGoat24/UniGenBench-EvalModel-qwen3vl-32b-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}]",
    "num_datasets": 2,
    "datasets_list": "CodeGoat24/UniGenBench-Eval-Images, CodeGoat24/UniGenBench",
    "datasets_links": "https://huggingface.co/datasets/CodeGoat24/UniGenBench-Eval-Images, https://huggingface.co/datasets/CodeGoat24/UniGenBench",
    "datasets_detailed": "[{\"name\": \"CodeGoat24/UniGenBench-Eval-Images\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/UniGenBench-Eval-Images\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\", \"size\": \"\"}, {\"name\": \"CodeGoat24/UniGenBench\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/UniGenBench\", \"task\": \"\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"Oct 25\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.17269",
    "first_seen_date": "2025-10-21",
    "title": "FineVision: Open Data Is All You Need",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.17269FineVision: Open Data Is All You NeedPublished on Oct 20\u00b7Submitted bytaesirion Oct 21#2 Paper of the day\u00b7Hugging FaceUpvote71+63Authors:Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andr\u00e9s MarafiotiAbstractFineVision, a large-scale and curated dataset, enhances vision-language models through rigorous data collection, de-duplication, and human oversight, leading to improved performance.AI-generated summaryThe advancement ofvision-language models(VLMs) is hampered by a fragmented\nlandscape of inconsistent and contaminated public datasets. We introduceFineVision, a meticulously collected, curated, and unified corpus of 24 million\nsamples - the largest open resource of its kind. We unify more than 200 sources\ninto 185 subsets via a semi-automated,human-in-the-looppipeline: automation\nperforms bulk ingestion andschema mapping, while reviewers audit mappings and\nspot-check outputs to verify faithful consumption of annotations, appropriate\nformatting and diversity, and safety; issues trigger targeted fixes and\nre-runs. The workflow further applies rigorousde-duplicationwithin and across\nsources anddecontaminationagainst 66 public benchmarks.FineVisionalso\nencompassesagentic/GUI taskswith aunified action space; reviewers validate\nschemas and inspect a sample of trajectories to confirm executable fidelity.\nModels trained onFineVisionconsistently outperform those trained on existing\nopen mixtures across a broad evaluation suite, underscoring the benefits of\nscale, data hygiene, and balanced automation with human oversight. We release\nthe corpus and curation tools to accelerate data-centric VLM research.View arXiv pageView PDFProject pageAdd to collectionCommunitytaesiriPaper submitterOct 21The advancement of vision-language models (VLMs) is hampered by a fragmented landscape of inconsistent and contaminated public data",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2510.17269",
    "arxiv_url": "https://arxiv.org/abs/2510.17269",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 3,
    "datasets_list": "HuggingFaceM4/FineVision, HuggingFaceM4/FineVisionMax, Windwave/FineVision",
    "datasets_links": "https://huggingface.co/datasets/HuggingFaceM4/FineVision, https://huggingface.co/datasets/HuggingFaceM4/FineVisionMax, https://huggingface.co/datasets/Windwave/FineVision",
    "datasets_detailed": "[{\"name\": \"HuggingFaceM4/FineVision\", \"link\": \"https://huggingface.co/datasets/HuggingFaceM4/FineVision\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\", \"size\": \"\"}, {\"name\": \"HuggingFaceM4/FineVisionMax\", \"link\": \"https://huggingface.co/datasets/HuggingFaceM4/FineVisionMax\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\", \"size\": \"\"}, {\"name\": \"Windwave/FineVision\", \"link\": \"https://huggingface.co/datasets/Windwave/FineVision\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.17800",
    "first_seen_date": "2025-10-21",
    "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.17800Glyph: Scaling Context Windows via Visual-Text CompressionPublished on Oct 20\u00b7Submitted byJiale Chengon Oct 21#3 Paper of the dayUpvote67+59Authors:Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie HuangAbstractGlyph compresses long textual inputs into images using vision-language models, achieving significant token compression and improved performance in long-context tasks.AI-generated summaryLarge language models(LLMs) increasingly rely onlong-context modelingfor\ntasks such asdocument understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extendingtoken-based sequences, we proposeGlyph, a\nframework that renders long texts into images and processes them withvision-language models(VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-drivengenetic searchto identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4xtoken compressionwhile maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x fasterprefillinganddecoding, and approximately 2x fasterSFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such asdocument understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.View arXiv pageVie",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/thu-coai/Glyph",
    "hf_paper_url": "https://huggingface.co/papers/2510.17800",
    "arxiv_url": "https://arxiv.org/abs/2510.17800",
    "num_models": 2,
    "models_list": "zai-org/Glyph, Mungert/Glyph-GGUF",
    "models_links": "https://huggingface.co/zai-org/Glyph, https://huggingface.co/Mungert/Glyph-GGUF",
    "models_detailed": "[{\"name\": \"zai-org/Glyph\", \"link\": \"https://huggingface.co/zai-org/Glyph\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"Mungert/Glyph-GGUF\", \"link\": \"https://huggingface.co/Mungert/Glyph-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.15870",
    "first_seen_date": "2025-10-20",
    "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.15870OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLMPublished on Oct 17\u00b7Submitted bytaesirion Oct 20\u00b7NVIDIAUpvote89+81Authors:Hanrong Ye,Chao-Han Huck Yang,Arushi Goel,Wei Huang,Ligeng Zhu,Yuanhang Su,Sean Lin,An-Chieh Cheng,Zhen Wan,Jinchuan Tian,Yuming Lou,Dong Yang,Zhijian Liu,Yukang Chen,Ambrish Dantrey,Ehsan Jahangiri,Sreyan Ghosh,Daguang Xu,Ehsan Hosseini-Asl,Danial Mohseni Taheri,Vidya Murali,Sifei Liu+10 authorsAbstractOmniVinci, an open-source omni-modal LLM, enhances cross-modal understanding and performance across audio, vision, and robotics applications with innovative architecture and efficient data curation.AI-generated summaryAdvancing machine intelligence requires developing the ability to perceive\nacross multiple modalities, much as humans sense the world. We introduce\nOmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We\ncarefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations: (i)OmniAlignNetfor\nstrengthening alignment between vision and audio embeddings in a sharedomni-modal latent space; (ii)Temporal Embedding Groupingfor capturing\nrelative temporal alignment between vision and audio signals; and (iii)Constrained Rotary Time Embeddingfor encoding absolute temporal information in\nomni-modal embeddings. We introduce a curation and synthesis pipeline that\ngenerates 24M single-modal andomni-modal conversations. We find that\nmodalities reinforce one another in both perception and reasoning. Our model,\nOmniVinci, outperforms Qwen2.5-Omni with +19.05 onDailyOmni(cross-modal\nunderstanding), +1.7 onMMAR(audio), and +3.9 onVideo-MME(vision), while\nusing just 0.2T training tokens - a 6 times reduction compared to\nQwen2.5-Omni's 1.2T. We finally demonstrateomni-modal advantagesin downstream\napplications spanning robotics, medical AI, and",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/NVlabs/OmniVinci",
    "hf_paper_url": "https://huggingface.co/papers/2510.15870",
    "arxiv_url": "https://arxiv.org/abs/2510.15870",
    "num_models": 1,
    "models_list": "nvidia/omnivinci",
    "models_links": "https://huggingface.co/nvidia/omnivinci",
    "models_detailed": "[{\"name\": \"nvidia/omnivinci\", \"link\": \"https://huggingface.co/nvidia/omnivinci\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.14276",
    "first_seen_date": "2025-10-17",
    "title": "Qwen3Guard Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.14276Qwen3Guard Technical ReportPublished on Oct 16\u00b7Submitted bytaesirion Oct 17\u00b7QwenUpvote14+6Authors:Haiquan Zhao,Chenhan Yuan,Fei Huang,Xiaomeng Hu,Yichang Zhang,An Yang,Bowen Yu,Dayiheng Liu,Jingren Zhou,Junyang Lin,Baosong Yang,Chen Cheng,Jialong Tang,Jiandong Jiang,Jianwei Zhang,Jijie Xu,Ming Yan,Minmin Sun,Pei Zhang,Pengjun Xie,Qiaoyu Tang,Qin Zhu+21 authorsAbstractQwen3Guard introduces multilingual safety guardrail models with fine-grained tri-class judgments and real-time token-level safety monitoring for large language models.AI-generated summaryAslarge language models(LLMs) become more capable and widely used, ensuring\nthe safety of their outputs is increasingly critical. Existing guardrail\nmodels, though useful in static evaluation settings, face two major limitations\nin real-world applications: (1) they typically output only binary \"safe/unsafe\"\nlabels, which can be interpreted inconsistently across diversesafety policies,\nrendering them incapable of accommodating varyingsafety tolerancesacross\ndomains; and (2) they require complete model outputs before performing safety\nchecks, making them fundamentally incompatible with streaming LLM inference,\nthereby preventing timely intervention during generation and increasing\nexposure to harmful partial outputs. To address these challenges, we present\nQwen3Guard, a series ofmultilingualsafety guardrail modelswith two\nspecialized variants:Generative Qwen3Guard, which casts safety classification\nas aninstruction-following taskto enable fine-grainedtri-class judgments(safe, controversial, unsafe); andStream Qwen3Guard, which introduces atoken-level classificationhead for real-time safety monitoring during\nincremental text generation. Both variants are available in three sizes (0.6B,\n4B, and 8B parameters) and support up to 119 languages and dialects, providing\ncomprehensive, scalable, and low-latency safety moderation for gl",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/QwenLM/Qwen3Guard",
    "hf_paper_url": "https://huggingface.co/papers/2510.14276",
    "arxiv_url": "https://arxiv.org/abs/2510.14276",
    "num_models": 12,
    "models_list": "Qwen/Qwen3Guard-Gen-8B, Qwen/Qwen3Guard-Gen-0.6B, Qwen/Qwen3-4B-SafeRL, Qwen/Qwen3Guard-Gen-4B, Qwen/Qwen3Guard-Stream-4B, Qwen/Qwen3Guard-Stream-0.6B, Qwen/Qwen3Guard-Stream-8B, jy622/Qwen3Guard-Gen-8B, NOSIBLE/financial-sentiment-v1.1-base, NOSIBLE/forward-looking-v1.1-base, NOSIBLE/prediction-v1.1-base, arnomatic/Qwen3Guard-Gen-8B-DeathGuard-heretic",
    "models_links": "https://huggingface.co/Qwen/Qwen3Guard-Gen-8B, https://huggingface.co/Qwen/Qwen3Guard-Gen-0.6B, https://huggingface.co/Qwen/Qwen3-4B-SafeRL, https://huggingface.co/Qwen/Qwen3Guard-Gen-4B, https://huggingface.co/Qwen/Qwen3Guard-Stream-4B, https://huggingface.co/Qwen/Qwen3Guard-Stream-0.6B, https://huggingface.co/Qwen/Qwen3Guard-Stream-8B, https://huggingface.co/jy622/Qwen3Guard-Gen-8B, https://huggingface.co/NOSIBLE/financial-sentiment-v1.1-base, https://huggingface.co/NOSIBLE/forward-looking-v1.1-base, https://huggingface.co/NOSIBLE/prediction-v1.1-base, https://huggingface.co/arnomatic/Qwen3Guard-Gen-8B-DeathGuard-heretic",
    "models_detailed": "[{\"name\": \"Qwen/Qwen3Guard-Gen-8B\", \"link\": \"https://huggingface.co/Qwen/Qwen3Guard-Gen-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"Qwen/Qwen3Guard-Gen-0.6B\", \"link\": \"https://huggingface.co/Qwen/Qwen3Guard-Gen-0.6B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"Qwen/Qwen3-4B-SafeRL\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-SafeRL\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 17\"}, {\"name\": \"Qwen/Qwen3Guard-Gen-4B\", \"link\": \"https://huggingface.co/Qwen/Qwen3Guard-Gen-4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"Qwen/Qwen3Guard-Stream-4B\", \"link\": \"https://huggingface.co/Qwen/Qwen3Guard-Stream-4B\", \"task\": \"\", \"likes\": \"528\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"Qwen/Qwen3Guard-Stream-0.6B\", \"link\": \"https://huggingface.co/Qwen/Qwen3Guard-Stream-0.6B\", \"task\": \"\", \"likes\": \"979\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"Qwen/Qwen3Guard-Stream-8B\", \"link\": \"https://huggingface.co/Qwen/Qwen3Guard-Stream-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"jy622/Qwen3Guard-Gen-8B\", \"link\": \"https://huggingface.co/jy622/Qwen3Guard-Gen-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"NOSIBLE/financial-sentiment-v1.1-base\", \"link\": \"https://huggingface.co/NOSIBLE/financial-sentiment-v1.1-base\", \"task\": \"Text Generation\", \"likes\": \"477\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"NOSIBLE/forward-looking-v1.1-base\", \"link\": \"https://huggingface.co/NOSIBLE/forward-looking-v1.1-base\", \"task\": \"Text Generation\", \"likes\": \"301\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"NOSIBLE/prediction-v1.1-base\", \"link\": \"https://huggingface.co/NOSIBLE/prediction-v1.1-base\", \"task\": \"Text Generation\", \"likes\": \"109\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"arnomatic/Qwen3Guard-Gen-8B-DeathGuard-heretic\", \"link\": \"https://huggingface.co/arnomatic/Qwen3Guard-Gen-8B-DeathGuard-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "Qwen/Qwen3GuardTest",
    "datasets_links": "https://huggingface.co/datasets/Qwen/Qwen3GuardTest",
    "datasets_detailed": "[{\"name\": \"Qwen/Qwen3GuardTest\", \"link\": \"https://huggingface.co/datasets/Qwen/Qwen3GuardTest\", \"task\": \"\", \"likes\": \"722\", \"downloads\": \"\", \"updated\": \"Oct 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.14528",
    "first_seen_date": "2025-10-17",
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.14528PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language ModelPublished on Oct 16\u00b7Submitted bytaesirion Oct 17\u00b7PaddlePaddleUpvote107+99Authors:Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun MaAbstractPaddleOCR-VL, a vision-language model combining NaViT-style dynamic resolution and ERNIE, achieves state-of-the-art performance in document parsing and element recognition with high efficiency.AI-generated summaryIn this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerfulvision-language model(VLM) that integrates aNaViT-style dynamic resolution visual encoderwith theERNIE-4.5-0.3B language model to enable accurateelement recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in bothpage-level document parsingandelement-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tierVLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. Code is available at https://github.com/PaddlePaddle/PaddleOCR .View arXiv pageView PDFGitHub66.7kAdd to collectionCommunitytaesiriPaper submitterOct 17In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/PaddlePaddle/PaddleOCR",
    "hf_paper_url": "https://huggingface.co/papers/2510.14528",
    "arxiv_url": "https://arxiv.org/abs/2510.14528",
    "num_models": 10,
    "models_list": "PaddlePaddle/PaddleOCR-VL, PaddlePaddle/PP-DocLayoutV2, unsloth/PaddleOCR-VL, lvyufeng/PaddleOCR-VL-0.9B, kp-forks/PaddleOCR-VL, ginipick/PaddleOCR-VL, seawolf2357/PaddleOCR-VL, fantaxy/PaddleOCR-VL, fantos/PaddleOCR-VL, meister1378/vl-lora",
    "models_links": "https://huggingface.co/PaddlePaddle/PaddleOCR-VL, https://huggingface.co/PaddlePaddle/PP-DocLayoutV2, https://huggingface.co/unsloth/PaddleOCR-VL, https://huggingface.co/lvyufeng/PaddleOCR-VL-0.9B, https://huggingface.co/kp-forks/PaddleOCR-VL, https://huggingface.co/ginipick/PaddleOCR-VL, https://huggingface.co/seawolf2357/PaddleOCR-VL, https://huggingface.co/fantaxy/PaddleOCR-VL, https://huggingface.co/fantos/PaddleOCR-VL, https://huggingface.co/meister1378/vl-lora",
    "models_detailed": "[{\"name\": \"PaddlePaddle/PaddleOCR-VL\", \"link\": \"https://huggingface.co/PaddlePaddle/PaddleOCR-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"PaddlePaddle/PP-DocLayoutV2\", \"link\": \"https://huggingface.co/PaddlePaddle/PP-DocLayoutV2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"unsloth/PaddleOCR-VL\", \"link\": \"https://huggingface.co/unsloth/PaddleOCR-VL\", \"task\": \"\", \"likes\": \"235\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"lvyufeng/PaddleOCR-VL-0.9B\", \"link\": \"https://huggingface.co/lvyufeng/PaddleOCR-VL-0.9B\", \"task\": \"\", \"likes\": \"406\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"kp-forks/PaddleOCR-VL\", \"link\": \"https://huggingface.co/kp-forks/PaddleOCR-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ginipick/PaddleOCR-VL\", \"link\": \"https://huggingface.co/ginipick/PaddleOCR-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"seawolf2357/PaddleOCR-VL\", \"link\": \"https://huggingface.co/seawolf2357/PaddleOCR-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"fantaxy/PaddleOCR-VL\", \"link\": \"https://huggingface.co/fantaxy/PaddleOCR-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"fantos/PaddleOCR-VL\", \"link\": \"https://huggingface.co/fantos/PaddleOCR-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"meister1378/vl-lora\", \"link\": \"https://huggingface.co/meister1378/vl-lora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 10\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.14616",
    "first_seen_date": "2025-10-17",
    "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  Cultures",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.14616Beyond Correctness: Evaluating Subjective Writing Preferences Across\n  CulturesPublished on Oct 16\u00b7Submitted byXinLion Oct 17\u00b7ByteDance SeedUpvote12+4Authors:Shuangshuang Ying,Yunwen Li,Xingwei Qu,Xin Li,Sheng Jin,Minghao Liu,Zhoufutu Wen,Xeron Du,Tianyu Zheng,Yichi Zhang,Letian Ni,Yuyang Cheng,Qiguang Chen,Jingzhe Ding,Shengda Long,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Libo Qin,Ge Zhang,Wenhao Huang,Wanxiang Che+1 authorsAbstractGenerative reward models with explicit reasoning chains outperform sequence-based reward models and zero-shot language models in preference learning for creative writing, indicating the need for intermediate reasoning in capturing subjective quality.AI-generated summaryCurrentpreference learningmethods achieve high accuracy on standard\nbenchmarks but exhibit significant performance degradation when objective\nquality signals are removed. We introduce WritingPreferenceBench, a dataset of\n1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8creative writing genres, where responses are matched forobjective correctness,factual accuracy, and length. On this benchmark, sequence-based reward\nmodels--the standard architecture forRLHF--achieve only 52.7% mean accuracy,\nwhile zero-shot language model judges perform at 53.9%. In contrast, generative\nreward models that produceexplicit reasoning chainsachieve 81.8% accuracy. We\nobserve high within-model variance across genres: individual models range from\n18.2% to 81.8% accuracy across different writing categories, with standard\ndeviations averaging 10.1%. This variance persists regardless of model scale,\nwith 27B parameter models showing no consistent improvement over 8B variants.\nOur results suggest that currentRLHFmethods primarily learn to detect\nobjective errors rather than capturesubjective quality preferences(e.g.,creativity,stylistic flair, andemotional resonance), and that",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/WritingPreferenceBench/Writing-Preference-Bench",
    "hf_paper_url": "https://huggingface.co/papers/2510.14616",
    "arxiv_url": "https://arxiv.org/abs/2510.14616",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "m-a-p/Writing-Preference-Bench",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/Writing-Preference-Bench",
    "datasets_detailed": "[{\"name\": \"m-a-p/Writing-Preference-Bench\", \"link\": \"https://huggingface.co/datasets/m-a-p/Writing-Preference-Bench\", \"task\": \"\", \"likes\": \"91\", \"downloads\": \"\", \"updated\": \"Oct 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.14763",
    "first_seen_date": "2025-10-17",
    "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought Processes",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.14763COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with\n  Thought ProcessesPublished on Oct 16\u00b7Submitted byXinLion Oct 17\u00b7Multimodal Art ProjectionUpvote13+5Authors:Yunwen Li,Shuangshuang Ying,Xingwei Qu,Xin Li,Sheng Jin,Minghao Liu,Zhoufutu Wen,Tianyu Zheng,Xeron Du,Qiguang Chen,Jiajun Shi,Wangchunshu Zhou,Jiazhan Feng,Wanjun Zhong,Libo Qin,Stephen Huang,Wanxiang Che,Chenghua Lin,Eli ZhangAbstractCOIG-Writer, a Chinese creative writing dataset, reveals that process supervision and general-purpose data are crucial for creative writing, with cultural-bound capabilities and lexical diversity impacting performance.AI-generated summaryLarge language models exhibit systematic deficiencies in creative writing,\nparticularly in non-English contexts where training data is scarce and lacks\nprocess-level supervision. We presentCOIG-Writer, a novel Chinese creative\nwriting dataset that captures both diverse outputs and their underlying thought\nprocesses through systematic reverse-engineering of high-quality texts. Unlike\nexisting datasets that provide only input-output pairs,COIG-Writercomprises\n1,665 meticulously curated triplets spanning 51 genres, each containing: (1) areverse-engineered prompt, (2) detailedcreative reasoningdocumenting\ndecision-making processes, and (3) the final text. Through comprehensive\nexperiments, we identify a two-component model of creative writing: narrative\nlogic (provided byprocess supervision) andlinguistic expression(maintained\nbygeneral-purpose data). Our findings reveal three critical insights: (1)Process supervisionis highly effective but requires stabilization with general\ndata. A ratio of at least one creative sample to twelve general samples is\nneeded to achieve optimal performance; below this threshold, the win rate\nprogressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities\nare culturally-bound with nocross-",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/COIG-Writer/COIG-Writer",
    "hf_paper_url": "https://huggingface.co/papers/2510.14763",
    "arxiv_url": "https://arxiv.org/abs/2510.14763",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "m-a-p/COIG-Writer",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/COIG-Writer",
    "datasets_detailed": "[{\"name\": \"m-a-p/COIG-Writer\", \"link\": \"https://huggingface.co/datasets/m-a-p/COIG-Writer\", \"task\": \"\", \"likes\": \"191\", \"downloads\": \"\", \"updated\": \"Oct 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.10274",
    "first_seen_date": "2025-10-16",
    "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.10274X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment\n  Vision-Language-Action ModelPublished on Oct 11\u00b7Submitted byJinliang Zhengon Oct 16Upvote14+6Authors:Jinliang Zheng,Jianxiong Li,Zhihao Wang,Dongxiu Liu,Xirui Kang,Yuchun Feng,Yinan Zheng,Jiayin Zou,Yilun Chen,Jia Zeng,Ya-Qin Zhang,Jiangmiao Pang,Jingjing Liu,Tai Wang,Xianyuan ZhanAbstractA novel Soft Prompt approach enhances Vision-Language-Action models by using learnable embeddings for diverse robotic data, enabling superior performance across simulations and real-world robots.AI-generated summarySuccessful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale,cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novelSoft Promptapproach with\nminimally added parameters, by infusingprompt learningconcepts intocross-embodimentrobot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve asembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varyingcross-embodimentfeatures. Our newX-VLA, a neatflow-matching-basedVLA architecture, relies exclusively on soft-prompted\nstandardTransformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfromflexible dexteritytoquick adaptationacross embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/View arXiv pageView PDFProject pageGitHub374Add to collectionCommunity2toINFPaper authorPaper submitterOct 16\u2022edited Oct 16See translationReplylibrarian-botOct 17This is",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/2toinf/X-VLA.git",
    "hf_paper_url": "https://huggingface.co/papers/2510.10274",
    "arxiv_url": "https://arxiv.org/abs/2510.10274",
    "num_models": 9,
    "models_list": "2toINF/X-VLA-Pt, 2toINF/X-VLA-WidowX, 2toINF/X-VLA-SoftFold, 2toINF/X-VLA-Libero, 2toINF/X-VLA-AgiWorld-Challenge, 2toINF/X-VLA-Calvin-ABC_D, 2toINF/X-VLA-Google-Robot, 2toINF/X-VLA-RoboTwin2, 2toINF/X-VLA-VLABench",
    "models_links": "https://huggingface.co/2toINF/X-VLA-Pt, https://huggingface.co/2toINF/X-VLA-WidowX, https://huggingface.co/2toINF/X-VLA-SoftFold, https://huggingface.co/2toINF/X-VLA-Libero, https://huggingface.co/2toINF/X-VLA-AgiWorld-Challenge, https://huggingface.co/2toINF/X-VLA-Calvin-ABC_D, https://huggingface.co/2toINF/X-VLA-Google-Robot, https://huggingface.co/2toINF/X-VLA-RoboTwin2, https://huggingface.co/2toINF/X-VLA-VLABench",
    "models_detailed": "[{\"name\": \"2toINF/X-VLA-Pt\", \"link\": \"https://huggingface.co/2toINF/X-VLA-Pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"2toINF/X-VLA-WidowX\", \"link\": \"https://huggingface.co/2toINF/X-VLA-WidowX\", \"task\": \"\", \"likes\": \"911\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"2toINF/X-VLA-SoftFold\", \"link\": \"https://huggingface.co/2toINF/X-VLA-SoftFold\", \"task\": \"\", \"likes\": \"662\", \"downloads\": \"\", \"updated\": \"Nov 6\"}, {\"name\": \"2toINF/X-VLA-Libero\", \"link\": \"https://huggingface.co/2toINF/X-VLA-Libero\", \"task\": \"\", \"likes\": \"890\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"2toINF/X-VLA-AgiWorld-Challenge\", \"link\": \"https://huggingface.co/2toINF/X-VLA-AgiWorld-Challenge\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"2toINF/X-VLA-Calvin-ABC_D\", \"link\": \"https://huggingface.co/2toINF/X-VLA-Calvin-ABC_D\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"2toINF/X-VLA-Google-Robot\", \"link\": \"https://huggingface.co/2toINF/X-VLA-Google-Robot\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"2toINF/X-VLA-RoboTwin2\", \"link\": \"https://huggingface.co/2toINF/X-VLA-RoboTwin2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 6\"}, {\"name\": \"2toINF/X-VLA-VLABench\", \"link\": \"https://huggingface.co/2toINF/X-VLA-VLABench\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 10\"}]",
    "num_datasets": 2,
    "datasets_list": "Facebear/XVLA-Soft-Fold, lerobot/xvla-soft-fold",
    "datasets_links": "https://huggingface.co/datasets/Facebear/XVLA-Soft-Fold, https://huggingface.co/datasets/lerobot/xvla-soft-fold",
    "datasets_detailed": "[{\"name\": \"Facebear/XVLA-Soft-Fold\", \"link\": \"https://huggingface.co/datasets/Facebear/XVLA-Soft-Fold\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 3\", \"size\": \"\"}, {\"name\": \"lerobot/xvla-soft-fold\", \"link\": \"https://huggingface.co/datasets/lerobot/xvla-soft-fold\", \"task\": \"\", \"likes\": \"978\", \"downloads\": \"\", \"updated\": \"15 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.10921",
    "first_seen_date": "2025-10-16",
    "title": "FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.10921FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment ModelPublished on Oct 13\u00b7Submitted byDavid Leonon Oct 16Upvote10+2Authors:Chunyu Xie,Bin Wang,Fanjing Kong,Jincheng Li,Dawei Liang,Ji Ao,Dawei Leng,Yuhui YinAbstractFG-CLIP 2, a bilingual vision-language model, enhances fine-grained alignment for English and Chinese through rich supervision and a new TIC loss, achieving state-of-the-art performance across multiple datasets and tasks.AI-generated summaryFine-grained vision-language understanding requires precise alignment between\nvisual content and linguistic descriptions, a capability that remains limited\nin current models, particularly in non-English settings. While models likeCLIPperform well on global alignment, they often struggle to capture fine-grained\ndetails in object attributes, spatial relations, and linguistic expressions,\nwith limited support for bilingual comprehension. To address these challenges,\nwe introduceFG-CLIP 2, a bilingual vision-language model designed to advance\nfine-grained alignment for both English and Chinese. Our approach leverages\nrichfine-grained supervision, includingregion-text matchingand long-caption\nmodeling, alongsidemultiple discriminative objectives. We further introduce\ntheTextual Intra-modal Contrastive (TIC) lossto better distinguish\nsemantically similar captions. Trained on a carefully curated mixture of\nlarge-scale English and Chinese data,FG-CLIP 2achieves powerful bilingual\nperformance. To enable rigorous evaluation, we present a new benchmark for\nChinese multimodal understanding, featuringlong-caption retrievaland bounding\nbox classification. Extensive experiments on 29 datasets across 8 tasks show\nthatFG-CLIP 2outperforms existing methods, achieving state-of-the-art results\nin both languages. We release the model, code, and benchmark to facilitate\nfuture research on bilingual fine-grained alignment.View arXiv pa",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2510.10921",
    "arxiv_url": "https://arxiv.org/abs/2510.10921",
    "num_models": 3,
    "models_list": "qihoo360/fg-clip2-base, qihoo360/fg-clip2-large, qihoo360/fg-clip2-so400m",
    "models_links": "https://huggingface.co/qihoo360/fg-clip2-base, https://huggingface.co/qihoo360/fg-clip2-large, https://huggingface.co/qihoo360/fg-clip2-so400m",
    "models_detailed": "[{\"name\": \"qihoo360/fg-clip2-base\", \"link\": \"https://huggingface.co/qihoo360/fg-clip2-base\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 6\"}, {\"name\": \"qihoo360/fg-clip2-large\", \"link\": \"https://huggingface.co/qihoo360/fg-clip2-large\", \"task\": \"Image Classification\", \"likes\": \"403\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"qihoo360/fg-clip2-so400m\", \"link\": \"https://huggingface.co/qihoo360/fg-clip2-so400m\", \"task\": \"Image Classification\", \"likes\": \"768\", \"downloads\": \"\", \"updated\": \"Oct 20\"}]",
    "num_datasets": 4,
    "datasets_list": "qihoo360/DOCCI-CN, qihoo360/LIT-CN, qihoo360/DCI-CN, qihoo360/BoxClass-CN",
    "datasets_links": "https://huggingface.co/datasets/qihoo360/DOCCI-CN, https://huggingface.co/datasets/qihoo360/LIT-CN, https://huggingface.co/datasets/qihoo360/DCI-CN, https://huggingface.co/datasets/qihoo360/BoxClass-CN",
    "datasets_detailed": "[{\"name\": \"qihoo360/DOCCI-CN\", \"link\": \"https://huggingface.co/datasets/qihoo360/DOCCI-CN\", \"task\": \"\", \"likes\": \"94\", \"downloads\": \"\", \"updated\": \"Oct 20\", \"size\": \"\"}, {\"name\": \"qihoo360/LIT-CN\", \"link\": \"https://huggingface.co/datasets/qihoo360/LIT-CN\", \"task\": \"\", \"likes\": \"79\", \"downloads\": \"\", \"updated\": \"Oct 20\", \"size\": \"\"}, {\"name\": \"qihoo360/DCI-CN\", \"link\": \"https://huggingface.co/datasets/qihoo360/DCI-CN\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\", \"size\": \"\"}, {\"name\": \"qihoo360/BoxClass-CN\", \"link\": \"https://huggingface.co/datasets/qihoo360/BoxClass-CN\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Oct 15\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.11606",
    "first_seen_date": "2025-10-15",
    "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.11606ExpVid: A Benchmark for Experiment Video Understanding & ReasoningPublished on Oct 13\u00b7Submitted byYicheng Xuon Oct 15\u00b7OpenGVLabUpvote3Authors:Yicheng Xu,Yue Wu,Jiashuo Yu,Ziang Yan,Tianxiang Jiang,Yinan He,Qingsong Zhao,Kai Chen,Yu Qiao,Limin Wang,Manabu Okumura,Yi WangAbstractExpVid, a new benchmark for evaluating multimodal large language models on scientific experiment videos, highlights gaps in fine-grained perception, procedural understanding, and scientific reasoning.AI-generated summaryMultimodal Large Language Models(MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existingbenchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduceExpVid, the\nfirstbenchmarkdesigned to systematically evaluateMLLMson scientific\nexperiment videos. Curated from peer-reviewed video publications,ExpVidfeatures a newthree-level task hierarchythat mirrors the scientific process:\n(1)Fine-grained Perceptionof tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3)Scientific Reasoningthat\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combiningautomated generationwith multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleadingMLLMsonExpVidand find that while they excel at coarse-grained\nrecognition, they struggle with disambiguatingfine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary andopen-source models, particularly inhigh-order reasoning.ExpVidnot only\nprovides a diagnostic tool but also charts a roadmap for developingMLL",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/OpenGVLab/ExpVid",
    "hf_paper_url": "https://huggingface.co/papers/2510.11606",
    "arxiv_url": "https://arxiv.org/abs/2510.11606",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "OpenGVLab/ExpVid",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/ExpVid",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/ExpVid\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/ExpVid\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.12225",
    "first_seen_date": "2025-10-15",
    "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.12225HoneyBee: Data Recipes for Vision-Language ReasonersPublished on Oct 14\u00b7Submitted byHritik Bansalon Oct 15\u00b7AI at MetaUpvote10+2Authors:Hritik Bansal,Devandra Singh Sachan,Kai-Wei Chang,Aditya Grover,Gargi Ghosh,Wen-tau Yih,Ramakanth PasunuruAbstractRecent advances in vision-language models (VLMs) have made them highly\neffective at reasoning tasks. However, the principles underlying the\nconstruction of performant VL reasoning training datasets remain poorly\nunderstood. In this work, we introduce several data curation approaches and\nstudy their impacts on VL reasoning capabilities by carefully controlling\ntraining and evaluation setups. We analyze the effects of context (image and\nquestion pair) sources, implement targeted data interventions, and explore\nscaling up images, questions, and chain-of-thought (CoT) solutions. Our\nfindings reveal that (a) context source strategies significantly affect VLM\nperformance, (b) interventions such as auxiliary signals from image captions\nand the inclusion of text-only reasoning yield substantial gains, and (c)\nscaling all data dimensions (e.g., unique questions per image and unique CoTs\nper image-question pair) consistently improves reasoning capability. Motivated\nby these insights, we introduce HoneyBee, a large-scale, high-quality CoT\nreasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs\ntrained with HoneyBee outperform state-of-the-art models across model sizes.\nFor instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA\nmodel and the base model by 7.8% and 24.8%, respectively, on MathVerse.\nFurthermore, we propose a test-time scaling strategy that reduces decoding cost\nby 73% without sacrificing accuracy. Overall, this work presents improved\nstrategies for VL reasoning dataset curation research.View arXiv pageView PDFGitHub9Add to collectionCommunityhbXNovPaper submitterOct 15\u2022edited Oct 1",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/facebookresearch/HoneyBee_VLM",
    "hf_paper_url": "https://huggingface.co/papers/2510.12225",
    "arxiv_url": "https://arxiv.org/abs/2510.12225",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "facebook/HoneyBee",
    "datasets_links": "https://huggingface.co/datasets/facebook/HoneyBee",
    "datasets_detailed": "[{\"name\": \"facebook/HoneyBee\", \"link\": \"https://huggingface.co/datasets/facebook/HoneyBee\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Oct 22\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.08886",
    "first_seen_date": "2025-10-14",
    "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.08886FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMsPublished on Oct 10\u00b7Submitted byYan Wangon Oct 14\u00b7The Fin AIUpvote19+11Authors:Yan Wang,Keyi Wang,Shanshan Yang,Jaisal Patel,Jeff Zhao,Fengran Mo,Xueqing Peng,Lingfei Qian,Jimin Huang,Guojun Xiong,Xiao-Yang Liu,Jian-Yun NieAbstractFinAuditing is a benchmark for evaluating LLMs on structured financial auditing tasks, revealing their limitations in handling taxonomy-driven, hierarchical financial documents.AI-generated summaryThe complexity of the Generally Accepted Accounting Principles (GAAP) and the\nhierarchical structure of eXtensible Business Reporting Language (XBRL) filings\nmake financial auditing increasingly difficult to automate and verify. While\nlarge language models (LLMs) have demonstrated strong capabilities in\nunstructured text understanding, their ability to reason over structured,\ninterdependent, and taxonomy-driven financial documents remains largely\nunexplored. To fill this gap, we introduceFinAuditing, the firsttaxonomy-aligned,structure-aware,multi-document benchmarkfor evaluatingLLMson financial auditing tasks. Built from realUS-GAAP-compliantXBRL filings,FinAuditingdefines three complementary subtasks,FinSMfor semantic\nconsistency,FinREforrelational consistency, andFinMRfor numerical\nconsistency, each targeting a distinct aspect of structured auditing reasoning.\nWe further propose a unified evaluation framework integratingretrieval,classification, andreasoning metricsacross these subtasks. Extensivezero-shot experimentson 13 state-of-the-artLLMsreveal that current models\nperform inconsistently across semantic, relational, and mathematical\ndimensions, with accuracy drops of up to 60-90% when reasoning over\nhierarchical multi-document structures. Our findings expose the systematic\nlimitations of modernLLMsin taxonomy-grounded financial reasoning and\nestablishFinA",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/The-FinAI/FinAuditing.git",
    "hf_paper_url": "https://huggingface.co/papers/2510.08886",
    "arxiv_url": "https://arxiv.org/abs/2510.08886",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 3,
    "datasets_list": "TheFinAI/FinRE, TheFinAI/FinSM, TheFinAI/FinMR",
    "datasets_links": "https://huggingface.co/datasets/TheFinAI/FinRE, https://huggingface.co/datasets/TheFinAI/FinSM, https://huggingface.co/datasets/TheFinAI/FinMR",
    "datasets_detailed": "[{\"name\": \"TheFinAI/FinRE\", \"link\": \"https://huggingface.co/datasets/TheFinAI/FinRE\", \"task\": \"\", \"likes\": \"440\", \"downloads\": \"\", \"updated\": \"Oct 14\", \"size\": \"\"}, {\"name\": \"TheFinAI/FinSM\", \"link\": \"https://huggingface.co/datasets/TheFinAI/FinSM\", \"task\": \"\", \"likes\": \"330\", \"downloads\": \"\", \"updated\": \"Oct 14\", \"size\": \"\"}, {\"name\": \"TheFinAI/FinMR\", \"link\": \"https://huggingface.co/datasets/TheFinAI/FinMR\", \"task\": \"\", \"likes\": \"332\", \"downloads\": \"\", \"updated\": \"Oct 14\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.10666",
    "first_seen_date": "2025-10-14",
    "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing\n  Actions",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.10666BrowserAgent: Building Web Agents with Human-Inspired Web Browsing\n  ActionsPublished on Oct 12\u00b7Submitted byWenhu Chenon Oct 14\u00b7TIGER-LabUpvote27+19Authors:Zhengbo Zhang,Zhiheng Lyu,Junhao Gong,Hongzhu Yi,Xinming Wang,Yuxuan Zhou,Jiabing Yang,Ping Nie,Yan Huang,Wenhu ChenAbstractBrowserAgent, an interactive web agent using human-like browser actions and a two-stage training process, achieves competitive results in Open-QA tasks with less training data and improved reasoning for multi-hop QA.AI-generated summaryEfficiently solving real-world problems withLLMsincreasingly hinges on\ntheir ability to interact with dynamic web environments and autonomously\nacquire external information. While recent research like Search-R1 and\nWebDancer demonstrates strong performance in solving web tasks, they heavily\nrely on additional tools to convert the interactive web environment into static\ntext content. This is in contrast to human browsing behaviors, which involve\ndiverse interactions with the browser, such as scrolling, clicking, and typing.\nIn this paper, we proposeBrowserAgent, a more interactive agent that solves\ncomplex tasks through human-inspired browser actions.BrowserAgentoperates\ndirectly on raw web pages viaPlaywrightthrough a set of predefined browser\nactions. We adopt a two-stage training (Supervised Fine-Tuning(SFT) andRejection Fine-Tuning(RFT)) to improve the model's generalization abilities.\nDespite using significantly less training data than Search-R1,BrowserAgentachieves more competitive results across different Open-QA tasks. Additionally,\nwe introduce anexplicit memory mechanismto store key conclusions across\nsteps, further enhancing the model's reasoning capabilities for long-horizon\ntasks. Notably,BrowserAgent-7B can achieve around 20\\% improvement over\nSearch-R1 onmulti-hop QAtasks likeHotpotQA,2Wiki, andBamboogle. These\nresults indicate thatBrowserAgentcan ser",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/TIGER-AI-Lab/BrowserAgent",
    "hf_paper_url": "https://huggingface.co/papers/2510.10666",
    "arxiv_url": "https://arxiv.org/abs/2510.10666",
    "num_models": 2,
    "models_list": "TIGER-Lab/BrowserAgent-RFT, TIGER-Lab/BrowserAgent-SFT",
    "models_links": "https://huggingface.co/TIGER-Lab/BrowserAgent-RFT, https://huggingface.co/TIGER-Lab/BrowserAgent-SFT",
    "models_detailed": "[{\"name\": \"TIGER-Lab/BrowserAgent-RFT\", \"link\": \"https://huggingface.co/TIGER-Lab/BrowserAgent-RFT\", \"task\": \"\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"TIGER-Lab/BrowserAgent-SFT\", \"link\": \"https://huggingface.co/TIGER-Lab/BrowserAgent-SFT\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Oct 25\"}]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/BrowserAgent-Data",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/BrowserAgent-Data",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/BrowserAgent-Data\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/BrowserAgent-Data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.11701",
    "first_seen_date": "2025-10-14",
    "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.11701Demystifying Reinforcement Learning in Agentic ReasoningPublished on Oct 13\u00b7Submitted byLing Yangon Oct 14Upvote31+23Authors:Zhaochen Yu,Ling Yang,Jiaru Zou,Shuicheng Yan,Mengdi WangAbstractAgentic reinforcement learning enhances LLMs' reasoning ability through real datasets, exploration techniques, and a deliberative strategy, achieving strong performance with smaller models.AI-generated summaryRecently, the emergence ofagentic RLhas showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories withreal end-to-end tool-use trajectoriesyields a far strongerSFT initialization; high-diversity,model-aware datasetssustain exploration\nand markedly improve RL performance. (ii)Exploration-friendly techniquesare\ncrucial foragentic RL, such asclip higher,overlong reward shaping, and\nmaintaining adequatepolicy entropycould improve the training efficiency.\n(iii) Adeliberative strategywith fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for futureagentic RLresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-endagentic SFT datasetalong with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, includingA",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/Gen-Verse/Open-AgentRL",
    "hf_paper_url": "https://huggingface.co/papers/2510.11701",
    "arxiv_url": "https://arxiv.org/abs/2510.11701",
    "num_models": 3,
    "models_list": "Gen-Verse/DemyAgent-4B, Gen-Verse/Qwen3-4B-RA-SFT, Gen-Verse/Qwen2.5-7B-RA-SFT",
    "models_links": "https://huggingface.co/Gen-Verse/DemyAgent-4B, https://huggingface.co/Gen-Verse/Qwen3-4B-RA-SFT, https://huggingface.co/Gen-Verse/Qwen2.5-7B-RA-SFT",
    "models_detailed": "[{\"name\": \"Gen-Verse/DemyAgent-4B\", \"link\": \"https://huggingface.co/Gen-Verse/DemyAgent-4B\", \"task\": \"\", \"likes\": \"43\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"Gen-Verse/Qwen3-4B-RA-SFT\", \"link\": \"https://huggingface.co/Gen-Verse/Qwen3-4B-RA-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"Gen-Verse/Qwen2.5-7B-RA-SFT\", \"link\": \"https://huggingface.co/Gen-Verse/Qwen2.5-7B-RA-SFT\", \"task\": \"\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Oct 14\"}]",
    "num_datasets": 2,
    "datasets_list": "Gen-Verse/Open-AgentRL-SFT-3K, Gen-Verse/Open-AgentRL-30K",
    "datasets_links": "https://huggingface.co/datasets/Gen-Verse/Open-AgentRL-SFT-3K, https://huggingface.co/datasets/Gen-Verse/Open-AgentRL-30K",
    "datasets_detailed": "[{\"name\": \"Gen-Verse/Open-AgentRL-SFT-3K\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/Open-AgentRL-SFT-3K\", \"task\": \"\", \"likes\": \"337\", \"downloads\": \"\", \"updated\": \"Oct 14\", \"size\": \"\"}, {\"name\": \"Gen-Verse/Open-AgentRL-30K\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/Open-AgentRL-30K\", \"task\": \"\", \"likes\": \"185\", \"downloads\": \"\", \"updated\": \"Oct 14\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2510.07318",
    "first_seen_date": "2025-10-09",
    "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.07318Artificial Hippocampus Networks for Efficient Long-Context ModelingPublished on Oct 8\u00b7Submitted byWeihao Yuon Oct 9\u00b7ByteDance SeedUpvote30+22Authors:Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai WeiAbstractA memory framework combining short-term and long-term memory in neural networks improves long-sequence modeling efficiency and performance.AI-generated summaryLong-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory inRNN-like modelsand the fidelity of\nlossless growing memory inattention-based Transformers. Inspired by theMulti-Store Modelin cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains asliding windowof the\nTransformer'sKV cacheas lossless short-term memory, while a learnable module\ntermedArtificial Hippocampus Network (AHN)recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, includingMamba2,DeltaNet, andGated DeltaNet. Extensive\nexperiments on long-context benchmarksLV-EvalandInfiniteBenchdemonstrate\nthat AHN-augmented models consistently outperformsliding windowbaselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reducesinference FLOPsby 40.5%\nandmemory cacheby 74.0%, while improving its average score onLV-Eval(128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.View arXiv pageView PDFGitHub159Add to collectionCommunitywhyuPaper authorPaper submitterOct 9Models:https://huggingface.co/collections/ByteDance-Seed/ahn-68e6130d08ed0f5a1b622829Code:https://github.com/ByteDance-Seed/AHNSee translationReplylibrarian-botOct ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/ByteDance-Seed/AHN",
    "hf_paper_url": "https://huggingface.co/papers/2510.07318",
    "arxiv_url": "https://arxiv.org/abs/2510.07318",
    "num_models": 10,
    "models_list": "ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-14B, ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-3B, ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-14B, ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-7B, ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-3B, ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-7B, ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-14B, ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-3B, ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-7B, MartialTerran/Toy_Qwen2-AHN_ByteDance-Seed_AHN",
    "models_links": "https://huggingface.co/ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-14B, https://huggingface.co/ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-3B, https://huggingface.co/ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-14B, https://huggingface.co/ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-7B, https://huggingface.co/ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-3B, https://huggingface.co/ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-7B, https://huggingface.co/ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-14B, https://huggingface.co/ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-3B, https://huggingface.co/ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-7B, https://huggingface.co/MartialTerran/Toy_Qwen2-AHN_ByteDance-Seed_AHN",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-14B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-14B\", \"task\": \"Text Generation\", \"likes\": \"74\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-3B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-3B\", \"task\": \"Text Generation\", \"likes\": \"220\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-14B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-14B\", \"task\": \"Text Generation\", \"likes\": \"73\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-7B\", \"task\": \"Text Generation\", \"likes\": \"78\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-3B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-14B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-DN-for-Qwen-2.5-Instruct-14B\", \"task\": \"Text Generation\", \"likes\": \"67\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-3B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/AHN-GDN-for-Qwen-2.5-Instruct-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"MartialTerran/Toy_Qwen2-AHN_ByteDance-Seed_AHN\", \"link\": \"https://huggingface.co/MartialTerran/Toy_Qwen2-AHN_ByteDance-Seed_AHN\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 17\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.02387",
    "first_seen_date": "2025-10-07",
    "title": "CWM: An Open-Weights LLM for Research on Code Generation with World\n  Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.02387CWM: An Open-Weights LLM for Research on Code Generation with World\n  ModelsPublished on Sep 30\u00b7Submitted byJacob Kahnon Oct 7\u00b7AI at MetaUpvote8Authors:FAIR CodeGen team,Quentin Carbonneaux,Gal Cohen,Jonas Gehring,Jacob Kahn,Jannik Kossen,Felix Kreuk,Emily McMilin,Michel Meyer,Yuxiang Wei,David Zhang,Kunhao Zheng,Jordi Armengol-Estap\u00e9,Pedram Bashiri,Maximilian Beck,Pierre Chambon,Abhishek Charnalia,Chris Cummins,Juliette Decugis,Zacharias V. Fisches,Fran\u00e7ois Fleuret,Fabian Gloeckle+28 authorsAbstractCode World Model, a 32-billion-parameter LLM, enhances code generation through world modeling with observation-action trajectories and multi-task reasoning RL, offering strong performance on coding and math tasks.AI-generated summaryWe releaseCode World Model(CWM), a 32-billion-parameter open-weightsLLM,\nto advance research on code generation with world models. To improve code\nunderstanding beyond what can be learned from training on static code alone, we\nmid-train CWM on a large amount ofobservation-action trajectoriesfrom Python\ninterpreter andagentic Docker environments, and perform extensive multi-task\nreasoning RL in verifiable coding, math, and multi-turn software engineering\nenvironments. With CWM, we provide a strong testbed for researchers to explore\nthe opportunitiesworld modelingaffords for improving code generation with\nreasoning and planning in computational environments. We present first steps of\nhow world models can benefit agentic coding, enablestep-by-step simulationof\nPython code execution, and show early results of how reasoning can benefit from\nthe latter. CWM is a dense,decoder-only LLMtrained with acontext sizeof up\nto 131k tokens. Independent of itsworld modelingcapabilities, CWM offers\nstrong performance on general coding and math tasks: it reachespass@1 scoresof 65.8% onSWE-bench Verified(with test-time scaling), 68.6% onLiveCodeBench, 96.6% onMath-50",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/facebookresearch/cwm",
    "hf_paper_url": "https://huggingface.co/papers/2510.02387",
    "arxiv_url": "https://arxiv.org/abs/2510.02387",
    "num_models": 3,
    "models_list": "facebook/cwm, facebook/cwm-pretrain, facebook/cwm-sft",
    "models_links": "https://huggingface.co/facebook/cwm, https://huggingface.co/facebook/cwm-pretrain, https://huggingface.co/facebook/cwm-sft",
    "models_detailed": "[{\"name\": \"facebook/cwm\", \"link\": \"https://huggingface.co/facebook/cwm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"facebook/cwm-pretrain\", \"link\": \"https://huggingface.co/facebook/cwm-pretrain\", \"task\": \"\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"facebook/cwm-sft\", \"link\": \"https://huggingface.co/facebook/cwm-sft\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Oct 15\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.01141",
    "first_seen_date": "2025-10-06",
    "title": "Apriel-1.5-15b-Thinker",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.01141Apriel-1.5-15b-ThinkerPublished on Oct 1\u00b7Submitted byAman Tiwarion Oct 6#1 Paper of the day\u00b7ServiceNow-AIUpvote119+111Authors:Shruthan Radhakrishna,Aman Tiwari,Aanjaneya Shukla,Masoud Hashemi,Rishabh Maheshwary,Shiva Krishna Reddy Malay,Jash Mehta,Pulkit Pattnaik,Saloni Mittal,Khalil Slimi,Kelechi Ogueji,Akintunde Oladipo,Soham Parikh,Oluwanifemi Bamgbose,Toby Liang,Ahmed Masry,Khyati Mahajan,Sai Rajeswar Mudumba,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sagar Davasam+2 authorsAbstractA 15-billion parameter multimodal reasoning model achieves competitive performance through a progressive training methodology without reinforcement learning, demonstrating efficient use of computational resources.AI-generated summaryWe present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weightsmultimodal reasoning modelthat achieves frontier-level performance through\ntraining design rather than sheer scale. Starting from Pixtral-12B, we apply a\nprogressive three-stage methodology: (1)depth upscalingto expand reasoning\ncapacity without pretraining from scratch, (2)staged continual pre-trainingthat first develops foundational text and vision understanding, then enhances\nvisual reasoning through targetedsynthetic data generationaddressing spatial\nstructure,compositional understanding, andfine-grained perception, and (3)\nhigh-qualitytext-only supervised fine-tuningon curated instruction-response\npairs with explicitreasoning tracesspanning mathematics, coding, science, and\ntool use. Notably, our model achieves competitive results without reinforcement\nlearning or preference optimization, isolating the contribution of our\ndata-centric continual pre-training approach. On the Artificial Analysis\nIntelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching\nDeepSeek-R1-0528 despite requiring significantly fewer computational resources.\nAcross ten image benchmarks, ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2510.01141",
    "arxiv_url": "https://arxiv.org/abs/2510.01141",
    "num_models": 6,
    "models_list": "ServiceNow-AI/Apriel-1.5-15b-Thinker, ServiceNow-AI/Apriel-1.6-15b-Thinker, ServiceNow-AI/Apriel-1.6-15b-Thinker-GGUF, cyankiwi/Apriel-1.6-15b-Thinker-AWQ-4bit, Mungert/Apriel-1.5-15b-Thinker-GGUF, cyankiwi/Apriel-1.6-15b-Thinker-AWQ-8bit",
    "models_links": "https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker, https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker, https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker-GGUF, https://huggingface.co/cyankiwi/Apriel-1.6-15b-Thinker-AWQ-4bit, https://huggingface.co/Mungert/Apriel-1.5-15b-Thinker-GGUF, https://huggingface.co/cyankiwi/Apriel-1.6-15b-Thinker-AWQ-8bit",
    "models_detailed": "[{\"name\": \"ServiceNow-AI/Apriel-1.5-15b-Thinker\", \"link\": \"https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 6\"}, {\"name\": \"ServiceNow-AI/Apriel-1.6-15b-Thinker\", \"link\": \"https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"ServiceNow-AI/Apriel-1.6-15b-Thinker-GGUF\", \"link\": \"https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"cyankiwi/Apriel-1.6-15b-Thinker-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Apriel-1.6-15b-Thinker-AWQ-4bit\", \"task\": \"\", \"likes\": \"408\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"Mungert/Apriel-1.5-15b-Thinker-GGUF\", \"link\": \"https://huggingface.co/Mungert/Apriel-1.5-15b-Thinker-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 5\"}, {\"name\": \"cyankiwi/Apriel-1.6-15b-Thinker-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Apriel-1.6-15b-Thinker-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.00438",
    "first_seen_date": "2025-10-02",
    "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal\n  Integration",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.00438BindWeave: Subject-Consistent Video Generation via Cross-Modal\n  IntegrationPublished on Oct 1\u00b7Submitted byYSHon Oct 2Upvote8Authors:Zhaoyang Li,Dongjun Qian,Kai Su,Qishuai Diao,Xiangyang Xia,Chang Liu,Wenfei Yang,Tianzhu Zhang,Zehuan YuanAbstractBindWeave, a unified framework using MLLM-DiT, enhances subject-consistent video generation by integrating deep cross-modal reasoning with diffusion transformers, achieving superior performance on OpenS2V.AI-generated summaryDiffusion Transformerhas shown remarkable abilities in generating\nhigh-fidelity videos, delivering visually coherent frames and rich details over\nextended durations. However, existing video generation models still fall short\nin subject-consistent video generation due to an inherent difficulty in parsing\nprompts that specify complex spatial relationships, temporal logic, and\ninteractions among multiple subjects. To address this issue, we proposeBindWeave, a unified framework that handles a broad range of subject-to-video\nscenarios from single-subject cases to complex multi-subject scenes with\nheterogeneous entities. To bind complex prompt semantics to concrete visual\nsubjects, we introduce anMLLM-DiTframework in which a pretrained multimodal\nlarge language model performsdeep cross-modal reasoningto ground entities and\ndisentangle roles, attributes, and interactions, yielding subject-aware hidden\nstates that condition thediffusion transformerfor high-fidelity\nsubject-consistent video generation. Experiments on theOpenS2V benchmarkdemonstrate that our method achieves superior performance across subject\nconsistency,naturalness, andtext relevancein generated videos, outperforming\nexisting open-source and commercial models.View arXiv pageView PDFProject pageAdd to collectionCommunityBestWishYshPaper submitterOct 2Page:https://lzy-dot.github.io/BindWeaveSee translationReplylibrarian-botOct 3This is an automated mes",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2510.00438",
    "arxiv_url": "https://arxiv.org/abs/2510.00438",
    "num_models": 2,
    "models_list": "ByteDance/BindWeave, vantagewithai/BindWeave-GGUF",
    "models_links": "https://huggingface.co/ByteDance/BindWeave, https://huggingface.co/vantagewithai/BindWeave-GGUF",
    "models_detailed": "[{\"name\": \"ByteDance/BindWeave\", \"link\": \"https://huggingface.co/ByteDance/BindWeave\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"vantagewithai/BindWeave-GGUF\", \"link\": \"https://huggingface.co/vantagewithai/BindWeave-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 21\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2510.00553",
    "first_seen_date": "2025-10-02",
    "title": "On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2510.00553On Predictability of Reinforcement Learning Dynamics for Large Language\n  ModelsPublished on Oct 1\u00b7Submitted byxuxinon Oct 2Upvote8Authors:Yuchen Cai,Ding Cao,Xin Xu,Zijun Yao,Yuqing Huang,Zhenyu Tan,Benyi Zhang,Guiquan Liu,Junfeng FangAbstractTwo fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.AI-generated summaryRecent advances in reasoning capabilities oflarge language models(LLMs) are\nlargely driven byreinforcement learning(RL), yet the underlying parameter\ndynamics during RL training remain poorly understood. This work identifies two\nfundamental properties of RL-inducedparameter updatesin LLMs: (1) Rank-1\nDominance, where the topsingular subspaceof the parameter update matrix\nnearly fully determines reasoning improvements, recovering over 99\\% of\nperformance gains; and (2)Rank-1 Linear Dynamics, where this dominant subspace\nevolves linearly throughout training, enabling accurate prediction from early\ncheckpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the\ngeneralizability of these properties. More importantly, based on these\nfindings, we proposeAlphaRL, a plug-in acceleration framework that\nextrapolates the final parameter update using a short early training window,\nachieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning\nperformance without extra modules or hyperparameter tuning. This positions our\nfinding as a versatile and practical tool for large-scale RL, opening a path\ntoward principled, interpretable, and efficient training paradigm for LLMs.View arXiv pageView PDFGitHub47Add to collectionCommunityxx18Paper authorPaper submitterOct 2Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2510,
    "github_repo": "https://github.com/caiyuchen-ustc/Alpha-RL",
    "hf_paper_url": "https://huggingface.co/papers/2510.00553",
    "arxiv_url": "https://arxiv.org/abs/2510.00553",
    "num_models": 75,
    "models_list": "caiyuchen/DAPO-step-1, caiyuchen/DAPO-step-2, caiyuchen/DAPO-step-3, caiyuchen/DAPO-step-4, caiyuchen/DAPO-step-0, caiyuchen/DAPO-step-5, caiyuchen/DAPO-step-6, caiyuchen/DAPO-step-7, caiyuchen/DAPO-step-8, caiyuchen/DAPO-step-9, caiyuchen/DAPO-step-10, caiyuchen/DAPO-step-11, caiyuchen/DAPO-step-12, caiyuchen/DAPO-step-13, caiyuchen/DAPO-step-14, caiyuchen/DAPO-step-15, caiyuchen/DAPO-step-16, caiyuchen/DAPO-step-17, caiyuchen/DAPO-step-18, caiyuchen/DAPO-step-19, caiyuchen/DAPO-step-20, caiyuchen/DAPO-step-21, caiyuchen/DAPO-step-22, caiyuchen/DAPO-step-23, caiyuchen/DAPO-step-24, caiyuchen/DAPO-step-25, caiyuchen/DAPO-step-26, caiyuchen/DAPO-step-27, caiyuchen/PPO-step-1, caiyuchen/PPO-step-2, caiyuchen/PPO-step-3, caiyuchen/PPO-step-4, caiyuchen/PPO-step-5, caiyuchen/PPO-step-6, caiyuchen/PPO-step-16, caiyuchen/PPO-step-7, caiyuchen/PPO-step-17, caiyuchen/PPO-step-8, caiyuchen/PPO-step-18, caiyuchen/PPO-step-9, caiyuchen/PPO-step-19, caiyuchen/PPO-step-10, caiyuchen/PPO-step-20, caiyuchen/PPO-step-11, caiyuchen/PPO-step-12, caiyuchen/PPO-step-21, caiyuchen/PPO-step-22, caiyuchen/PPO-step-13, caiyuchen/PPO-step-0, caiyuchen/PPO-step-23, caiyuchen/PPO-step-14, caiyuchen/PPO-step-15, caiyuchen/Spiral-step-0, caiyuchen/Spiral-step-1, caiyuchen/Spiral-step-2, caiyuchen/Spiral-step-4, caiyuchen/Spiral-step-3, caiyuchen/Spiral-step-6, caiyuchen/Spiral-step-5, caiyuchen/Spiral-step-8, caiyuchen/Spiral-step-7, caiyuchen/Spiral-step-9, caiyuchen/Spiral-step-10, caiyuchen/Spiral-step-11, caiyuchen/Spiral-step-12, caiyuchen/Spiral-step-14, caiyuchen/Spiral-step-13, caiyuchen/Spiral-step-15, caiyuchen/Spiral-step-16, caiyuchen/Spiral-step-18, caiyuchen/Spiral-step-17, caiyuchen/Spiral-step-20, caiyuchen/Spiral-step-19, caiyuchen/Spiral-step-22, caiyuchen/Spiral-step-21",
    "models_links": "https://huggingface.co/caiyuchen/DAPO-step-1, https://huggingface.co/caiyuchen/DAPO-step-2, https://huggingface.co/caiyuchen/DAPO-step-3, https://huggingface.co/caiyuchen/DAPO-step-4, https://huggingface.co/caiyuchen/DAPO-step-0, https://huggingface.co/caiyuchen/DAPO-step-5, https://huggingface.co/caiyuchen/DAPO-step-6, https://huggingface.co/caiyuchen/DAPO-step-7, https://huggingface.co/caiyuchen/DAPO-step-8, https://huggingface.co/caiyuchen/DAPO-step-9, https://huggingface.co/caiyuchen/DAPO-step-10, https://huggingface.co/caiyuchen/DAPO-step-11, https://huggingface.co/caiyuchen/DAPO-step-12, https://huggingface.co/caiyuchen/DAPO-step-13, https://huggingface.co/caiyuchen/DAPO-step-14, https://huggingface.co/caiyuchen/DAPO-step-15, https://huggingface.co/caiyuchen/DAPO-step-16, https://huggingface.co/caiyuchen/DAPO-step-17, https://huggingface.co/caiyuchen/DAPO-step-18, https://huggingface.co/caiyuchen/DAPO-step-19, https://huggingface.co/caiyuchen/DAPO-step-20, https://huggingface.co/caiyuchen/DAPO-step-21, https://huggingface.co/caiyuchen/DAPO-step-22, https://huggingface.co/caiyuchen/DAPO-step-23, https://huggingface.co/caiyuchen/DAPO-step-24, https://huggingface.co/caiyuchen/DAPO-step-25, https://huggingface.co/caiyuchen/DAPO-step-26, https://huggingface.co/caiyuchen/DAPO-step-27, https://huggingface.co/caiyuchen/PPO-step-1, https://huggingface.co/caiyuchen/PPO-step-2, https://huggingface.co/caiyuchen/PPO-step-3, https://huggingface.co/caiyuchen/PPO-step-4, https://huggingface.co/caiyuchen/PPO-step-5, https://huggingface.co/caiyuchen/PPO-step-6, https://huggingface.co/caiyuchen/PPO-step-16, https://huggingface.co/caiyuchen/PPO-step-7, https://huggingface.co/caiyuchen/PPO-step-17, https://huggingface.co/caiyuchen/PPO-step-8, https://huggingface.co/caiyuchen/PPO-step-18, https://huggingface.co/caiyuchen/PPO-step-9, https://huggingface.co/caiyuchen/PPO-step-19, https://huggingface.co/caiyuchen/PPO-step-10, https://huggingface.co/caiyuchen/PPO-step-20, https://huggingface.co/caiyuchen/PPO-step-11, https://huggingface.co/caiyuchen/PPO-step-12, https://huggingface.co/caiyuchen/PPO-step-21, https://huggingface.co/caiyuchen/PPO-step-22, https://huggingface.co/caiyuchen/PPO-step-13, https://huggingface.co/caiyuchen/PPO-step-0, https://huggingface.co/caiyuchen/PPO-step-23, https://huggingface.co/caiyuchen/PPO-step-14, https://huggingface.co/caiyuchen/PPO-step-15, https://huggingface.co/caiyuchen/Spiral-step-0, https://huggingface.co/caiyuchen/Spiral-step-1, https://huggingface.co/caiyuchen/Spiral-step-2, https://huggingface.co/caiyuchen/Spiral-step-4, https://huggingface.co/caiyuchen/Spiral-step-3, https://huggingface.co/caiyuchen/Spiral-step-6, https://huggingface.co/caiyuchen/Spiral-step-5, https://huggingface.co/caiyuchen/Spiral-step-8, https://huggingface.co/caiyuchen/Spiral-step-7, https://huggingface.co/caiyuchen/Spiral-step-9, https://huggingface.co/caiyuchen/Spiral-step-10, https://huggingface.co/caiyuchen/Spiral-step-11, https://huggingface.co/caiyuchen/Spiral-step-12, https://huggingface.co/caiyuchen/Spiral-step-14, https://huggingface.co/caiyuchen/Spiral-step-13, https://huggingface.co/caiyuchen/Spiral-step-15, https://huggingface.co/caiyuchen/Spiral-step-16, https://huggingface.co/caiyuchen/Spiral-step-18, https://huggingface.co/caiyuchen/Spiral-step-17, https://huggingface.co/caiyuchen/Spiral-step-20, https://huggingface.co/caiyuchen/Spiral-step-19, https://huggingface.co/caiyuchen/Spiral-step-22, https://huggingface.co/caiyuchen/Spiral-step-21",
    "models_detailed": "[{\"name\": \"caiyuchen/DAPO-step-1\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-2\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-3\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-4\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-0\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-5\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-6\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-7\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-7\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-8\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-9\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-9\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-10\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-10\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-11\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-11\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-12\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-12\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-13\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-13\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-14\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-14\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-15\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-15\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-16\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-17\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-17\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-18\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-18\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-19\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-19\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-20\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-20\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-21\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-21\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-22\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-22\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-23\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-23\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-24\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-24\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-25\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-25\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-26\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-26\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"caiyuchen/DAPO-step-27\", \"link\": \"https://huggingface.co/caiyuchen/DAPO-step-27\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"caiyuchen/PPO-step-1\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-2\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-3\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-4\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-5\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-6\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-16\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-7\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-7\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-17\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-17\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-8\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-18\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-18\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-9\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-9\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-19\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-19\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-10\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-10\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-20\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-20\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-11\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-11\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-12\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-12\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-21\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-21\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-22\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-22\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-13\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-13\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-0\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-23\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-23\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-14\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-14\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/PPO-step-15\", \"link\": \"https://huggingface.co/caiyuchen/PPO-step-15\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"caiyuchen/Spiral-step-0\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-1\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-2\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-4\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-3\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-6\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-5\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-8\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-7\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-7\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-9\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-9\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-10\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-10\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-11\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-11\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-12\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-12\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-14\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-14\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-13\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-13\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-15\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-15\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-16\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-18\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-18\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-17\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-17\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-20\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-20\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-19\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-19\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-22\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-22\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-21\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-21\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.26346",
    "first_seen_date": "2025-10-02",
    "title": "EditReward: A Human-Aligned Reward Model for Instruction-Guided Image\n  Editing",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.26346EditReward: A Human-Aligned Reward Model for Instruction-Guided Image\n  EditingPublished on Sep 30\u00b7Submitted byWenhu Chenon Oct 2\u00b7TIGER-LabUpvote18+10Authors:Keming Wu,Sicong Jiang,Max Ku,Ping Nie,Minghao Liu,Wenhu ChenAbstractA new reward model, trained with a large-scale human preference dataset, improves instruction-guided image editing by selecting high-quality training data and achieving state-of-the-art performance on benchmarks.AI-generated summaryRecently, we have witnessed great progress in image editing with natural\nlanguage instructions. Several closed-source models like GPT-Image-1, Seedream,\nand Google-Nano-Banana have shown highly promising progress. However, the\nopen-source models are still lagging. The main bottleneck is the lack of a\nreliablereward modelto scale up high-quality synthetic training data. To\naddress this critical bottleneck, we built \\mname, trained with our new\nlarge-scalehuman preference dataset, meticulously annotated by trained experts\nfollowing a rigorous protocol containing over 200K preference pairs. \\mname\ndemonstrates superior alignment with human preferences in instruction-guided\nimage editing tasks. Experiments show that \\mname achieves state-of-the-art\nhuman correlation on established benchmarks such asGenAI-Bench,AURORA-Bench,ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge\nmodels. Furthermore, we use \\mname to select a high-quality subset from the\nexisting noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected\nsubset, which shows significant improvement over training on the full set. This\ndemonstrates \\mname's ability to serve as areward modelto scale up\nhigh-quality training data for image editing. Furthermore, its strong alignment\nsuggests potential for advanced applications likereinforcement learning-basedpost-trainingandtest-time scalingof image editing models. \\mname with its\ntr",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/TIGER-AI-Lab/EditReward",
    "hf_paper_url": "https://huggingface.co/papers/2509.26346",
    "arxiv_url": "https://arxiv.org/abs/2509.26346",
    "num_models": 2,
    "models_list": "TIGER-Lab/EditReward-Qwen2.5-VL-7B, TIGER-Lab/EditReward-MiMo-VL-7B-SFT-2508",
    "models_links": "https://huggingface.co/TIGER-Lab/EditReward-Qwen2.5-VL-7B, https://huggingface.co/TIGER-Lab/EditReward-MiMo-VL-7B-SFT-2508",
    "models_detailed": "[{\"name\": \"TIGER-Lab/EditReward-Qwen2.5-VL-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/EditReward-Qwen2.5-VL-7B\", \"task\": \"\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"TIGER-Lab/EditReward-MiMo-VL-7B-SFT-2508\", \"link\": \"https://huggingface.co/TIGER-Lab/EditReward-MiMo-VL-7B-SFT-2508\", \"task\": \"\", \"likes\": \"378\", \"downloads\": \"\", \"updated\": \"Oct 19\"}]",
    "num_datasets": 3,
    "datasets_list": "TIGER-Lab/EditReward-Data, TIGER-Lab/EditReward-Bench, wukeming11/EditReward-Bench",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/EditReward-Data, https://huggingface.co/datasets/TIGER-Lab/EditReward-Bench, https://huggingface.co/datasets/wukeming11/EditReward-Bench",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/EditReward-Data\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/EditReward-Data\", \"task\": \"\", \"likes\": \"438\", \"downloads\": \"\", \"updated\": \"Oct 12\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/EditReward-Bench\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/EditReward-Bench\", \"task\": \"\", \"likes\": \"426\", \"downloads\": \"\", \"updated\": \"21 days ago\", \"size\": \"\"}, {\"name\": \"wukeming11/EditReward-Bench\", \"link\": \"https://huggingface.co/datasets/wukeming11/EditReward-Bench\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"23 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.22799",
    "first_seen_date": "2025-09-30",
    "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.22799VideoScore2: Think before You Score in Generative Video EvaluationPublished on Sep 26\u00b7Submitted byWenhu Chenon Sep 30\u00b7TIGER-LabUpvote25+17Authors:Xuan He,Dongfu Jiang,Ping Nie,Minghao Liu,Zhengxuan Jiang,Mingyi Su,Wentao Ma,Junru Lin,Chun Ye,Yi Lu,Keming Wu,Benjamin Schneider,Quy Duc Do,Zhuofeng Li,Yiming Jia,Yuxuan Zhang,Guo Cheng,Haozhe Wang,Wangchunshu Zhou,Qunshu Lin,Yuanxing Zhang,Ge Zhang+2 authorsAbstractVideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.AI-generated summaryRecent advances intext-to-video generationhave produced increasingly\nrealistic and diverse content, yet evaluating such videos remains a fundamental\nchallenge due to their multi-faceted nature encompassingvisual quality,semantic alignment, andphysical consistency. Existing evaluators and reward\nmodels are limited to single opaque scores, lack interpretability, or provide\nonly coarse analysis, making them insufficient for capturing the comprehensive\nnature of video quality assessment. We presentVideoScore2, a\nmulti-dimensional, interpretable, and human-aligned framework that explicitly\nevaluatesvisual quality, text-to-video alignment, and physical/common-sense\nconsistency while producing detailed chain-of-thought rationales. Our model is\ntrained on a large-scale datasetVideoFeedback2containing 27,168\nhuman-annotated videos with both scores and reasoning traces across three\ndimensions, using a two-stage pipeline ofsupervised fine-tuningfollowed byreinforcement learningwithGroup Relative Policy Optimization (GRPO)to\nenhance analytical robustness. Extensive experiments demonstrate thatVideoScore2achieves superior performance with 44.35 (+5.94) accuracy on our\nin-domain benchmarkVideoScore-Bench-v2and 50.37 (+4.32) average performance\nacross four out-of-domain benchmarks (V",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/TIGER-AI-Lab/VideoScore2",
    "hf_paper_url": "https://huggingface.co/papers/2509.22799",
    "arxiv_url": "https://arxiv.org/abs/2509.22799",
    "num_models": 1,
    "models_list": "TIGER-Lab/VideoScore2",
    "models_links": "https://huggingface.co/TIGER-Lab/VideoScore2",
    "models_detailed": "[{\"name\": \"TIGER-Lab/VideoScore2\", \"link\": \"https://huggingface.co/TIGER-Lab/VideoScore2\", \"task\": \"Question Answering\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/VideoFeedback2",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/VideoFeedback2",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/VideoFeedback2\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VideoFeedback2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.22824",
    "first_seen_date": "2025-09-30",
    "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.22824Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  LearningPublished on Sep 26\u00b7Submitted byWenhu Chenon Sep 30\u00b7TIGER-LabUpvote20+12Authors:Chi Ruan,Dongfu Jiang,Yubo Wang,Wenhu ChenAbstractCritique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.AI-generated summaryReinforcement Learning (RL)has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, likeCritique-Fine-Tuning (CFT)andCritique-Guided-Distillation (CGD)have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label c in {True, False}\nof the generated critique aligns with the ground-truth judgment c^*. Building\non this point, we introduceCritique-Coder, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (Critique-Coder) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthatCritique-Coderconsistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, ourCritique-Coder-8B can reach\nover 60\\% onLiveCodeBench(v5), outperforming other reasoning models likeDeepCoder-14BandGPT-o1. Beyond code generation,Critique-Coderalso\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from theBBEH dataset. This indicates that\nthe application of CRL on coding da",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/TIGER-AI-Lab/Critique-Coder",
    "hf_paper_url": "https://huggingface.co/papers/2509.22824",
    "arxiv_url": "https://arxiv.org/abs/2509.22824",
    "num_models": 2,
    "models_list": "TIGER-Lab/Critique-Coder-8B, TIGER-Lab/Critique-Coder-4B",
    "models_links": "https://huggingface.co/TIGER-Lab/Critique-Coder-8B, https://huggingface.co/TIGER-Lab/Critique-Coder-4B",
    "models_detailed": "[{\"name\": \"TIGER-Lab/Critique-Coder-8B\", \"link\": \"https://huggingface.co/TIGER-Lab/Critique-Coder-8B\", \"task\": \"\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"Oct 1\"}, {\"name\": \"TIGER-Lab/Critique-Coder-4B\", \"link\": \"https://huggingface.co/TIGER-Lab/Critique-Coder-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\"}]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/rStar-Critique-Data",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/rStar-Critique-Data",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/rStar-Critique-Data\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/rStar-Critique-Data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.24007",
    "first_seen_date": "2025-09-30",
    "title": "Sequential Diffusion Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.24007Sequential Diffusion Language ModelsPublished on Sep 28\u00b7Submitted byCao Yueon Sep 30\u00b7OpenGVLabUpvote45+37Authors:Yangzhou Liu,Yue Cao,Hao Li,Gen Luo,Zhe Chen,Weiyun Wang,Xiaobo Liang,Biqing Qi,Lijun Wu,Changyao Tian,Yanting Zhang,Yuqiang Li,Tong Lu,Yu Qiao,Jifeng Dai,Wenhai WangAbstractSequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.AI-generated summaryDiffusion language models(DLMs) have strong theoretical efficiency but are\nlimited byfixed-length decodingand incompatibility with key-value (KV)\ncaches.Block diffusionmitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standardnext-token prediction. Building\non NSP, we proposeSequential Diffusion Language Model(SDLM), which can\nretrofit pre-trainedautoregressive language models(ALMs) at minimal cost.\nSpecifically, SDLM performsdiffusion inferencewithin fixed-sizemask blocks,\nbut dynamically decodes consecutive subsequences based onmodel confidence,\nthereby preserving KV-cache compatibility and improvingrobustnessto varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higherthroughputthanQwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLMView arXiv pageView PDFProject pageGitHub77Add to collectionCo",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/OpenGVLab/SDLM",
    "hf_paper_url": "https://huggingface.co/papers/2509.24007",
    "arxiv_url": "https://arxiv.org/abs/2509.24007",
    "num_models": 3,
    "models_list": "OpenGVLab/SDLM-32B-D4, OpenGVLab/SDLM-3B-D4, OpenGVLab/SDLM-3B-D8",
    "models_links": "https://huggingface.co/OpenGVLab/SDLM-32B-D4, https://huggingface.co/OpenGVLab/SDLM-3B-D4, https://huggingface.co/OpenGVLab/SDLM-3B-D8",
    "models_detailed": "[{\"name\": \"OpenGVLab/SDLM-32B-D4\", \"link\": \"https://huggingface.co/OpenGVLab/SDLM-32B-D4\", \"task\": \"Text Generation\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"OpenGVLab/SDLM-3B-D4\", \"link\": \"https://huggingface.co/OpenGVLab/SDLM-3B-D4\", \"task\": \"Text Generation\", \"likes\": \"115\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"OpenGVLab/SDLM-3B-D8\", \"link\": \"https://huggingface.co/OpenGVLab/SDLM-3B-D8\", \"task\": \"Text Generation\", \"likes\": \"94\", \"downloads\": \"\", \"updated\": \"Oct 3\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.24663",
    "first_seen_date": "2025-09-30",
    "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  Adaptation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.24663InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  AdaptationPublished on Sep 29\u00b7Submitted byChaojun XIAOon Sep 30\u00b7OpenBMBUpvote14+6Authors:Weilin Zhao,Zihan Zhou,Zhou Su,Chaojun Xiao,Yuxuan Li,Yanghao Li,Yudi Zhang,Weilun Zhao,Zhen Li,Yuxiang Huang,Ao Sun,Xu Han,Zhiyuan LiuAbstractA dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.AI-generated summaryLong-sequence processing is a critical capability for modern large language\nmodels. However, theself-attention mechanismin the standard Transformer\narchitecture faces severe computational and memory bottlenecks when processing\nlong sequences. Whiletrainable sparse attentionmethods offer a promising\nsolution, existing approaches such as NSA introduce excessive extra parameters\nand disrupt the conventional pretrain-on-short, finetune-on-long\nworkflow, resulting in slow convergence and difficulty in acceleration. To\novercome these limitations, we introducedense-sparse switchable attentionframework, termed asInfLLM-V2.InfLLM-V2is atrainable sparse attentionthat\nseamlessly adapts models from short to long sequences. Specifically,InfLLM-V2reusesdense attentionparameters through parameter-free architecture\nmodification, maintaining consistency between short and long sequence\nprocessing. Additionally,InfLLM-V2ensures computational efficiency across all\nsequence lengths, by usingdense attentionfor short inputs and smoothly\ntransitioning tosparse attentionfor long sequences. To achieve practical\nacceleration, we further introduce an efficient implementation ofInfLLM-V2that significantly reduces the computational overhead. Our experiments onlong-context understandingandchain-of-thought reasoningdemonstrate thatInfLLM-V2is 4times faster thandense attentionwhile retaining 98.1% an",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.24663",
    "arxiv_url": "https://arxiv.org/abs/2509.24663",
    "num_models": 4,
    "models_list": "openbmb/MiniCPM4.1-8B, openbmb/InfLLM-V2-Short-Dense-Base, openbmb/InfLLM-V2-Long-Sparse-Base, Mungert/MiniCPM4.1-8B-GGUF",
    "models_links": "https://huggingface.co/openbmb/MiniCPM4.1-8B, https://huggingface.co/openbmb/InfLLM-V2-Short-Dense-Base, https://huggingface.co/openbmb/InfLLM-V2-Long-Sparse-Base, https://huggingface.co/Mungert/MiniCPM4.1-8B-GGUF",
    "models_detailed": "[{\"name\": \"openbmb/MiniCPM4.1-8B\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4.1-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"openbmb/InfLLM-V2-Short-Dense-Base\", \"link\": \"https://huggingface.co/openbmb/InfLLM-V2-Short-Dense-Base\", \"task\": \"\", \"likes\": \"108\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"openbmb/InfLLM-V2-Long-Sparse-Base\", \"link\": \"https://huggingface.co/openbmb/InfLLM-V2-Long-Sparse-Base\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"Mungert/MiniCPM4.1-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/MiniCPM4.1-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}]",
    "num_datasets": 6,
    "datasets_list": "openbmb/InfLLM-V2-data-5B, elonmuskceo/InfLLM-V2-data-5B-v2, victor/InfLLM-V2-data-5B, elonmuskceo/InfLLM-V2-data-5B, Zikrihakim66/Airdrop, Zikrihakim66/Hurk",
    "datasets_links": "https://huggingface.co/datasets/openbmb/InfLLM-V2-data-5B, https://huggingface.co/datasets/elonmuskceo/InfLLM-V2-data-5B-v2, https://huggingface.co/datasets/victor/InfLLM-V2-data-5B, https://huggingface.co/datasets/elonmuskceo/InfLLM-V2-data-5B, https://huggingface.co/datasets/Zikrihakim66/Airdrop, https://huggingface.co/datasets/Zikrihakim66/Hurk",
    "datasets_detailed": "[{\"name\": \"openbmb/InfLLM-V2-data-5B\", \"link\": \"https://huggingface.co/datasets/openbmb/InfLLM-V2-data-5B\", \"task\": \"\", \"likes\": \"547\", \"downloads\": \"\", \"updated\": \"Oct 25\", \"size\": \"\"}, {\"name\": \"elonmuskceo/InfLLM-V2-data-5B-v2\", \"link\": \"https://huggingface.co/datasets/elonmuskceo/InfLLM-V2-data-5B-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\", \"size\": \"\"}, {\"name\": \"victor/InfLLM-V2-data-5B\", \"link\": \"https://huggingface.co/datasets/victor/InfLLM-V2-data-5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\", \"size\": \"\"}, {\"name\": \"elonmuskceo/InfLLM-V2-data-5B\", \"link\": \"https://huggingface.co/datasets/elonmuskceo/InfLLM-V2-data-5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\", \"size\": \"\"}, {\"name\": \"Zikrihakim66/Airdrop\", \"link\": \"https://huggingface.co/datasets/Zikrihakim66/Airdrop\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}, {\"name\": \"Zikrihakim66/Hurk\", \"link\": \"https://huggingface.co/datasets/Zikrihakim66/Hurk\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 23\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.25413",
    "first_seen_date": "2025-09-30",
    "title": "DepthLM: Metric Depth From Vision Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.25413DepthLM: Metric Depth From Vision Language ModelsPublished on Sep 29\u00b7Submitted byzhipeng caion Sep 30\u00b7AI at MetaUpvote6Authors:Zhipeng Cai,Ching-Feng Yeh,Hu Xu,Zhuang Liu,Gregory Meyer,Xinjie Lei,Changsheng Zhao,Shang-Wen Li,Vikas Chandra,Yangyang ShiAbstractText-based supervised fine-tuning with sparse labels enables vision language models to achieve expert-level accuracy in 3D depth estimation without requiring task-specific architectures or losses.AI-generated summaryVision language models(VLMs) can flexibly address various vision tasks\nthrough text interactions. Although successful in semantic understanding,\nstate-of-the-artVLMsincludingGPT-5still struggle in understanding 3D from\n2D inputs. On the other hand, expert pure vision models achieve super-human\naccuracy inmetric depth estimation, a key 3D understanding task. However, they\nrequire task-specific architectures and losses. Such difference motivates us to\nask: CanVLMsreach expert-level accuracy without architecture or loss change?\nWe take per-pixelmetric depth estimationas the representative task and show\nthat the answer is yes! Surprisingly, comprehensive analysis shows thattext-based supervised-finetuningwithsparse labelsis sufficient forVLMsto\nunlock strong 3D understanding, no dense prediction head or complex\nregression/regularization loss is needed. The bottleneck forVLMslies actually\nin pixel reference and cross-dataset camera ambiguity, which we address throughvisual promptingandintrinsic-conditioned augmentation. With much smaller\nmodels, our methodDepthLMsurpasses the accuracy of most advancedVLMsby over\n2x, makingVLMsfor the first time comparable with pure vision models.\nInterestingly, without explicit enforcement during training,VLMstrained withDepthLMnaturally avoidsover-smoothing, having much fewerflying pointsat\nboundary regions than pure vision models. The simplicity ofDepthLMalso\nenables a singl",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/facebookresearch/DepthLM_Official",
    "hf_paper_url": "https://huggingface.co/papers/2509.25413",
    "arxiv_url": "https://arxiv.org/abs/2509.25413",
    "num_models": 1,
    "models_list": "facebook/DepthLM",
    "models_links": "https://huggingface.co/facebook/DepthLM",
    "models_detailed": "[{\"name\": \"facebook/DepthLM\", \"link\": \"https://huggingface.co/facebook/DepthLM\", \"task\": \"\", \"likes\": \"75\", \"downloads\": \"\", \"updated\": \"Oct 1\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.21319",
    "first_seen_date": "2025-09-29",
    "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.21319RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable RewardsPublished on Sep 25\u00b7Submitted byZhilin Wangon Sep 29\u00b7NVIDIAUpvote6Authors:Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Ellie Evans,Daniel Egert,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii KuchaievAbstractRLBFF combines human feedback and rule-based verification to improve reward models, achieving top performance on benchmarks with customizable principles.AI-generated summaryReinforcement Learning with Human Feedback (RLHF)and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nanentailment task(response satisfies or does not satisfy an arbitrary\nprinciple). We show thatReward Modelstrained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance onRM-Bench(86.2%) andJudgeBench(81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of ourreward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Q",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.21319",
    "arxiv_url": "https://arxiv.org/abs/2509.21319",
    "num_models": 6,
    "models_list": "nvidia/Qwen3-Nemotron-32B-RLBFF, nvidia/Qwen3-Nemotron-32B-GenRM-Principle, nvidia/Llama-3.3-Nemotron-70B-Reward-Principle, cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit, cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit, ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx",
    "models_links": "https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF, https://huggingface.co/nvidia/Qwen3-Nemotron-32B-GenRM-Principle, https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward-Principle, https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit, https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit, https://huggingface.co/ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx",
    "models_detailed": "[{\"name\": \"nvidia/Qwen3-Nemotron-32B-RLBFF\", \"link\": \"https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF\", \"task\": \"Text Generation\", \"likes\": \"153\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"nvidia/Qwen3-Nemotron-32B-GenRM-Principle\", \"link\": \"https://huggingface.co/nvidia/Qwen3-Nemotron-32B-GenRM-Principle\", \"task\": \"Text Generation\", \"likes\": \"669\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"nvidia/Llama-3.3-Nemotron-70B-Reward-Principle\", \"link\": \"https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward-Principle\", \"task\": \"Text Generation\", \"likes\": \"123\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx\", \"link\": \"https://huggingface.co/ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}]",
    "num_datasets": 1,
    "datasets_list": "nvidia/HelpSteer3",
    "datasets_links": "https://huggingface.co/datasets/nvidia/HelpSteer3",
    "datasets_detailed": "[{\"name\": \"nvidia/HelpSteer3\", \"link\": \"https://huggingface.co/datasets/nvidia/HelpSteer3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.22186",
    "first_seen_date": "2025-09-29",
    "title": "MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document Parsing",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.22186MinerU2.5: A Decoupled Vision-Language Model for Efficient\n  High-Resolution Document ParsingPublished on Sep 26\u00b7Submitted bytaesirion Sep 29Upvote139+131Authors:Junbo Niu,Zheng Liu,Zhuangcheng Gu,Bin Wang,Linke Ouyang,Zhiyuan Zhao,Tao Chu,Tianyao He,Fan Wu,Qintong Zhang,Zhenjiang Jin,Guang Liang,Rui Zhang,Wenzheng Zhang,Yuan Qu,Zhifei Ren,Yuefeng Sun,Yuanhong Zheng,Dongsheng Ma,Zirui Tang,Boyu Niu,Ziyang Miao+39 authorsAbstractMinerU2.5, a 1.2B-parameter document parsing vision-language model, achieves state-of-the-art recognition accuracy with computational efficiency through a coarse-to-fine parsing strategy.AI-generated summaryWe introduce MinerU2.5, a 1.2B-parameterdocument parsingvision-language\nmodel that achieves state-of-the-art recognition accuracy while maintaining\nexceptional computational efficiency. Our approach employs acoarse-to-fine,two-stage parsingstrategy that decouples globallayout analysisfrom localcontent recognition. In the first stage, the model performs efficient layout\nanalysis ondownsampled imagesto identify structural elements, circumventing\nthecomputational overheadof processing high-resolution inputs. In the second\nstage, guided by the global layout, it performs targetedcontent recognitiononnative-resolution cropsextracted from the original image, preserving\nfine-grained details in dense text, complex formulas, and tables. To support\nthis strategy, we developed a comprehensivedata enginethat generates diverse,\nlarge-scale training corpora for bothpretrainingandfine-tuning. Ultimately,\nMinerU2.5 demonstrates strongdocument parsingability, achievingstate-of-the-art performanceon multiple benchmarks, surpassing both\ngeneral-purpose and domain-specific models across various recognition tasks,\nwhile maintaining significantly lowercomputational overhead.View arXiv pageView PDFProject pageGitHub50.8kAdd to collectionCommunitytaesiriPaper submitter",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/opendatalab/MinerU",
    "hf_paper_url": "https://huggingface.co/papers/2509.22186",
    "arxiv_url": "https://arxiv.org/abs/2509.22186",
    "num_models": 3,
    "models_list": "opendatalab/MinerU2.5-2509-1.2B, freakynit/MinerU2.5-2509-1.2B, Mungert/MinerU2.5-2509-1.2B-GGUF",
    "models_links": "https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B, https://huggingface.co/freakynit/MinerU2.5-2509-1.2B, https://huggingface.co/Mungert/MinerU2.5-2509-1.2B-GGUF",
    "models_detailed": "[{\"name\": \"opendatalab/MinerU2.5-2509-1.2B\", \"link\": \"https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 29\"}, {\"name\": \"freakynit/MinerU2.5-2509-1.2B\", \"link\": \"https://huggingface.co/freakynit/MinerU2.5-2509-1.2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Mungert/MinerU2.5-2509-1.2B-GGUF\", \"link\": \"https://huggingface.co/Mungert/MinerU2.5-2509-1.2B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.22624",
    "first_seen_date": "2025-09-29",
    "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.22624SPARK: Synergistic Policy And Reward Co-Evolving FrameworkPublished on Sep 26\u00b7Submitted byYuhang Zangon Sep 29\u00b7Intern Large ModelsUpvote17+9Authors:Ziyu Liu,Yuhang Zang,Shengyuan Ding,Yuhang Cao,Xiaoyi Dong,Haodong Duan,Dahua Lin,Jiaqi WangAbstractSPARK, a synergistic policy and reward co-evolving framework, enhances LLMs and LVLMs by recycling rollouts and correctness data to train a generative reward model, reducing reliance on human preferences and external reward models.AI-generated summaryRecent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)\nincreasingly useReinforcement Learning(RL) for post-pretraining, such as RL\nwith Verifiable Rewards (RLVR) for objective tasks andRL from Human Feedback(RLHF) for subjective tasks. However, RLHF incurs high costs and potential\nreward-policy mismatch due to reliance on human preferences, while RLVR still\nwastes supervision by discardingrolloutsand correctness signals after each\nupdate. To address these challenges, we introduce the Synergistic Policy And\nReward Co-Evolving Framework (SPARK), an efficient,on-policy, and stable\nmethod that builds on RLVR. Instead of discardingrolloutsand correctness\ndata,SPARKrecycles this valuable information to simultaneously train the\nmodel itself as agenerative reward model. This auxiliary training uses a mix\nof objectives, such aspointwise reward score,pairwise comparison, and\nevaluation conditioned on further-reflection responses, to teach the model to\nevaluate and improve its own responses. Our process eliminates the need for a\nseparate reward model and costly human preference data.SPARKcreates a\npositive co-evolving feedback loop: improvedreward accuracyyields betterpolicy gradients, which in turn produce higher-qualityrolloutsthat further\nrefine the reward model. Our unified framework supports test-time scaling viaself-reflectionwithout external reward models and thei",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/InternLM/Spark",
    "hf_paper_url": "https://huggingface.co/papers/2509.22624",
    "arxiv_url": "https://arxiv.org/abs/2509.22624",
    "num_models": 1,
    "models_list": "internlm/Spark-VL-7B",
    "models_links": "https://huggingface.co/internlm/Spark-VL-7B",
    "models_detailed": "[{\"name\": \"internlm/Spark-VL-7B\", \"link\": \"https://huggingface.co/internlm/Spark-VL-7B\", \"task\": \"\", \"likes\": \"69\", \"downloads\": \"\", \"updated\": \"Oct 23\"}]",
    "num_datasets": 1,
    "datasets_list": "internlm/Spark-Data",
    "datasets_links": "https://huggingface.co/datasets/internlm/Spark-Data",
    "datasets_detailed": "[{\"name\": \"internlm/Spark-Data\", \"link\": \"https://huggingface.co/datasets/internlm/Spark-Data\", \"task\": \"\", \"likes\": \"144\", \"downloads\": \"\", \"updated\": \"Oct 9\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.22647",
    "first_seen_date": "2025-09-29",
    "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.22647CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement\n  LearningPublished on Sep 26\u00b7Submitted byXilin Weion Sep 29\u00b7Intern Large ModelsUpvote32+24Authors:Long Xing,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jianze Liang,Qidong Huang,Jiaqi Wang,Feng Wu,Dahua LinAbstractCapRL, a novel reinforcement learning framework, enhances image captioning by using a vision-free language model to evaluate caption quality through multiple-choice questions, leading to improved performance across benchmarks.AI-generated summaryImage captioning is a fundamental task that bridges the visual and linguistic\ndomains, playing a critical role in pre-trainingLarge Vision-Language Models(LVLMs). Current state-of-the-art captioning models are typically trained withSupervised Fine-Tuning(SFT), a paradigm that relies on expensive, non-scalable\ndata annotated by humans or proprietary models. This approach often leads to\nmodels that memorize specific ground-truth answers, limiting their generality\nand ability to generate diverse, creative descriptions. To overcome the\nlimitation of SFT, we propose applying the Reinforcement Learning with\nVerifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.\nA primary challenge, however, is designing an objective reward function for the\ninherently subjective nature of what constitutes a \"good\" caption. We introduceCaptioning Reinforcement Learning(CapRL), a novel training framework that\nredefines caption quality through its utility: a high-quality caption should\nenable a non-visual language model to accurately answer questions about the\ncorresponding image. CapRL employs adecoupled two-stage pipelinewhere an LVLM\ngenerates a caption, and the objective reward is derived from the accuracy of a\nseparate, vision-free LLM answeringMultiple-Choice Questionsbased solely on\nthat caption. As the first study to apply RLVR to the subjective image\nca",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/InternLM/CapRL",
    "hf_paper_url": "https://huggingface.co/papers/2509.22647",
    "arxiv_url": "https://arxiv.org/abs/2509.22647",
    "num_models": 3,
    "models_list": "internlm/CapRL-3B, internlm/CapRL-Eval-3B, internlm/CapRL-InternVL3.5-8B",
    "models_links": "https://huggingface.co/internlm/CapRL-3B, https://huggingface.co/internlm/CapRL-Eval-3B, https://huggingface.co/internlm/CapRL-InternVL3.5-8B",
    "models_detailed": "[{\"name\": \"internlm/CapRL-3B\", \"link\": \"https://huggingface.co/internlm/CapRL-3B\", \"task\": \"\", \"likes\": \"663\", \"downloads\": \"\", \"updated\": \"Oct 22\"}, {\"name\": \"internlm/CapRL-Eval-3B\", \"link\": \"https://huggingface.co/internlm/CapRL-Eval-3B\", \"task\": \"\", \"likes\": \"51\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"internlm/CapRL-InternVL3.5-8B\", \"link\": \"https://huggingface.co/internlm/CapRL-InternVL3.5-8B\", \"task\": \"\", \"likes\": \"477\", \"downloads\": \"\", \"updated\": \"Oct 22\"}]",
    "num_datasets": 1,
    "datasets_list": "internlm/CapRL-2M",
    "datasets_links": "https://huggingface.co/datasets/internlm/CapRL-2M",
    "datasets_detailed": "[{\"name\": \"internlm/CapRL-2M\", \"link\": \"https://huggingface.co/datasets/internlm/CapRL-2M\", \"task\": \"\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"Oct 22\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.19760",
    "first_seen_date": "2025-09-25",
    "title": "Logics-Parsing Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.19760Logics-Parsing Technical ReportPublished on Sep 24\u00b7Submitted bytaesirion Sep 25Upvote7Authors:Xiangyang Chen,Shuzhao Li,Xiuwen Zhu,Yongfan Chen,Fan Yang,Cheng Fang,Lin Qu,Xiaoxiao Xu,Hu Wei,Minggang WuAbstractLogics-Parsing, an end-to-end LVLM model enhanced with reinforcement learning, improves document parsing by optimizing layout analysis and reading order inference, achieving state-of-the-art performance on a diverse benchmark.AI-generated summaryRecent advances inLarge Vision-Language models(LVLM) have spurred\nsignificant progress in document parsing task. Compared to traditional\npipeline-based methods,end-to-end paradigmshave shown their excellence in\nconverting PDF images into structured outputs through integrated Optical\nCharacter Recognition (OCR),table recognition, mathematical formula\nrecognition and so on. However, the absence of explicit analytical stages for\ndocument layouts and reading orders limits theLVLM's capability in handling\ncomplex document types such as multi-column newspapers or posters. To address\nthis limitation, we propose in this report Logics-Parsing: an end-to-endLVLM-based model augmented withreinforcement learning. Our model incorporates\nmeticulously designedreward mechanismsto optimize complexlayout analysisandreading order inference. In addition, we expand the model's versatility by\nincorporating diverse data types such aschemical formulasand handwritten\nChinese characters intosupervised fine-tuning. Finally, to enable rigorous\nevaluation of our approach, we introduceLogicsParsingBench, a curated set of\n1,078 page-level PDF images spanning nine major categories and over twenty\nsub-categories, which will be released later. Comprehensive experiments\nconducted onLogicsParsingBenchhave validated the efficacy and\nState-of-the-art (SOTA) performance of our proposed model across diverse\ndocument analysis scenarios. Project Page:\nhttps://github",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/alibaba/Logics-Parsing",
    "hf_paper_url": "https://huggingface.co/papers/2509.19760",
    "arxiv_url": "https://arxiv.org/abs/2509.19760",
    "num_models": 2,
    "models_list": "Logics-MLLM/Logics-Parsing, Mungert/Logics-Parsing-GGUF",
    "models_links": "https://huggingface.co/Logics-MLLM/Logics-Parsing, https://huggingface.co/Mungert/Logics-Parsing-GGUF",
    "models_detailed": "[{\"name\": \"Logics-MLLM/Logics-Parsing\", \"link\": \"https://huggingface.co/Logics-MLLM/Logics-Parsing\", \"task\": \"\", \"likes\": \"61\", \"downloads\": \"\", \"updated\": \"Sep 30\"}, {\"name\": \"Mungert/Logics-Parsing-GGUF\", \"link\": \"https://huggingface.co/Mungert/Logics-Parsing-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.20317",
    "first_seen_date": "2025-09-25",
    "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.20317SIM-CoT: Supervised Implicit Chain-of-ThoughtPublished on Sep 24\u00b7Submitted byXilin Weion Sep 25#3 Paper of the day\u00b7Intern Large ModelsUpvote41+33Authors:Xilin Wei,Xiaoran Liu,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Xipeng Qiu,Dahua LinAbstractSIM-CoT, a plug-and-play training module, introduces step-level supervision to stabilize and enrich the latent reasoning space of implicit Chain-of-Thought methods, enhancing their performance and efficiency.AI-generated summaryImplicit Chain-of-Thought(CoT) methods present a promising, token-efficient\nalternative to explicit CoT reasoning inLarge Language Models(LLMs), but a\npersistent performance gap has limited the application of implicit CoT. We\nidentify a corelatent instabilityissue by scaling the computational budget of\nimplicit CoT approaches: as we increase the number of implicit reasoning tokens\nto enhance performance, the training process often becomes unstable and\ncollapses. Our analysis reveals that this instability arises from the latent\nrepresentations becoming homogeneous and losing theirsemantic diversity, a\nfailure caused by insufficientstep-level supervisionin existing implicit CoT\napproaches. To address this issue, we proposeSIM-CoT, a plug-and-play training\nmodule that introducesstep-level supervisionto stabilize and enrich the\nlatent reasoning space. Specifically,SIM-CoTemploys anauxiliary decoderduring training to align each implicit token with its corresponding explicit\nreasoning step, ensuring that latent states capture distinct and meaningful\ninformation. The proposedauxiliary decoderis removed during inference,\npreserving the computational efficiency of implicit CoT methods with no added\noverhead. In addition, theauxiliary decoderaffords interpretability of\nimplicit reasoning by projecting each latent token onto an explicit reasoning\nvocabulary, enabling per-step visualization of semantic roles and di",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/InternLM/SIM-CoT",
    "hf_paper_url": "https://huggingface.co/papers/2509.20317",
    "arxiv_url": "https://arxiv.org/abs/2509.20317",
    "num_models": 5,
    "models_list": "internlm/SIM_COT-LLaMA3-CODI-8B, internlm/SIM_COT-GPT2-CODI, internlm/SIM_COT-GPT2-Coconut, internlm/SIM_COT-LLaMA3-CODI-3B, internlm/SIM_COT-LLaMA3-CODI-1B",
    "models_links": "https://huggingface.co/internlm/SIM_COT-LLaMA3-CODI-8B, https://huggingface.co/internlm/SIM_COT-GPT2-CODI, https://huggingface.co/internlm/SIM_COT-GPT2-Coconut, https://huggingface.co/internlm/SIM_COT-LLaMA3-CODI-3B, https://huggingface.co/internlm/SIM_COT-LLaMA3-CODI-1B",
    "models_detailed": "[{\"name\": \"internlm/SIM_COT-LLaMA3-CODI-8B\", \"link\": \"https://huggingface.co/internlm/SIM_COT-LLaMA3-CODI-8B\", \"task\": \"\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"internlm/SIM_COT-GPT2-CODI\", \"link\": \"https://huggingface.co/internlm/SIM_COT-GPT2-CODI\", \"task\": \"\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"internlm/SIM_COT-GPT2-Coconut\", \"link\": \"https://huggingface.co/internlm/SIM_COT-GPT2-Coconut\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"internlm/SIM_COT-LLaMA3-CODI-3B\", \"link\": \"https://huggingface.co/internlm/SIM_COT-LLaMA3-CODI-3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"internlm/SIM_COT-LLaMA3-CODI-1B\", \"link\": \"https://huggingface.co/internlm/SIM_COT-LLaMA3-CODI-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 28\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.20354",
    "first_seen_date": "2025-09-25",
    "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.20354EmbeddingGemma: Powerful and Lightweight Text RepresentationsPublished on Sep 24\u00b7Submitted bytaesirion Sep 25Upvote41+33Authors:Henrique Schechter Vera,Sahil Dua,Biao Zhang,Daniel Salz,Ryan Mullins,Sindhu Raghuram Panyam,Sara Smoot,Iftekhar Naim,Joe Zou,Feiyang Chen,Daniel Cer,Alice Lisak,Min Choi,Lucas Gonzalez,Omar Sanseviero,Glenn Cameron,Ian Ballantyne,Kat Black,Kaifeng Chen,Weiyi Wang,Zhe Li,Gus Martins+66 authorsAbstractEmbeddingGemma, a lightweight text embedding model based on Gemma 3, achieves state-of-the-art performance with fewer parameters through encoder-decoder initialization, geometric embedding distillation, and spread-out regularization.AI-generated summaryWe introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on theGemma 3language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization andgeometric embedding distillation. We improve model\nrobustness and expressiveness with aspread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on theMassive Text Embedding Benchmark (MTEB)acrossmultilingual,English, andcode domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists whenquantizing model weightsor truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.View arXiv pageView PDFAdd to collectionCommunitytaesiriPaper",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.20354",
    "arxiv_url": "https://arxiv.org/abs/2509.20354",
    "num_models": 13,
    "models_list": "google/embeddinggemma-300m, google/embeddinggemma-300m-qat-q4_0-unquantized, google/embeddinggemma-300m-qat-q8_0-unquantized, XenArcAI/SparkEmbedding-300m, lmstudio-community/embeddinggemma-300m-qat-GGUF, headwAI/embeddinggemma-300m, SteveCIB77/embeddinggemma-300m, AshwinKM2005/github-duplicates-bi-encoder, Randstad/gemma-embedding-st-v2, aganguly542/my-embedding-gemma, GrishaKushnir/embeddinggemma-300m, Boopathy2861/google_embeddinggemma-300m, mikhailbaklanov/gemma-embeddings-finetuned",
    "models_links": "https://huggingface.co/google/embeddinggemma-300m, https://huggingface.co/google/embeddinggemma-300m-qat-q4_0-unquantized, https://huggingface.co/google/embeddinggemma-300m-qat-q8_0-unquantized, https://huggingface.co/XenArcAI/SparkEmbedding-300m, https://huggingface.co/lmstudio-community/embeddinggemma-300m-qat-GGUF, https://huggingface.co/headwAI/embeddinggemma-300m, https://huggingface.co/SteveCIB77/embeddinggemma-300m, https://huggingface.co/AshwinKM2005/github-duplicates-bi-encoder, https://huggingface.co/Randstad/gemma-embedding-st-v2, https://huggingface.co/aganguly542/my-embedding-gemma, https://huggingface.co/GrishaKushnir/embeddinggemma-300m, https://huggingface.co/Boopathy2861/google_embeddinggemma-300m, https://huggingface.co/mikhailbaklanov/gemma-embeddings-finetuned",
    "models_detailed": "[{\"name\": \"google/embeddinggemma-300m\", \"link\": \"https://huggingface.co/google/embeddinggemma-300m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 25\"}, {\"name\": \"google/embeddinggemma-300m-qat-q4_0-unquantized\", \"link\": \"https://huggingface.co/google/embeddinggemma-300m-qat-q4_0-unquantized\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 25\"}, {\"name\": \"google/embeddinggemma-300m-qat-q8_0-unquantized\", \"link\": \"https://huggingface.co/google/embeddinggemma-300m-qat-q8_0-unquantized\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 25\"}, {\"name\": \"XenArcAI/SparkEmbedding-300m\", \"link\": \"https://huggingface.co/XenArcAI/SparkEmbedding-300m\", \"task\": \"\", \"likes\": \"524\", \"downloads\": \"\", \"updated\": \"22 days ago\"}, {\"name\": \"lmstudio-community/embeddinggemma-300m-qat-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/embeddinggemma-300m-qat-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 26\"}, {\"name\": \"headwAI/embeddinggemma-300m\", \"link\": \"https://huggingface.co/headwAI/embeddinggemma-300m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 26\"}, {\"name\": \"SteveCIB77/embeddinggemma-300m\", \"link\": \"https://huggingface.co/SteveCIB77/embeddinggemma-300m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 25\"}, {\"name\": \"AshwinKM2005/github-duplicates-bi-encoder\", \"link\": \"https://huggingface.co/AshwinKM2005/github-duplicates-bi-encoder\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 10\"}, {\"name\": \"Randstad/gemma-embedding-st-v2\", \"link\": \"https://huggingface.co/Randstad/gemma-embedding-st-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 17\"}, {\"name\": \"aganguly542/my-embedding-gemma\", \"link\": \"https://huggingface.co/aganguly542/my-embedding-gemma\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 26\"}, {\"name\": \"GrishaKushnir/embeddinggemma-300m\", \"link\": \"https://huggingface.co/GrishaKushnir/embeddinggemma-300m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"Boopathy2861/google_embeddinggemma-300m\", \"link\": \"https://huggingface.co/Boopathy2861/google_embeddinggemma-300m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"mikhailbaklanov/gemma-embeddings-finetuned\", \"link\": \"https://huggingface.co/mikhailbaklanov/gemma-embeddings-finetuned\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "Vano04/laions-got-talent-enhanced-precomputed-en",
    "datasets_links": "https://huggingface.co/datasets/Vano04/laions-got-talent-enhanced-precomputed-en",
    "datasets_detailed": "[{\"name\": \"Vano04/laions-got-talent-enhanced-precomputed-en\", \"link\": \"https://huggingface.co/datasets/Vano04/laions-got-talent-enhanced-precomputed-en\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.19296",
    "first_seen_date": "2025-09-24",
    "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.19296Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-DistillationPublished on Sep 23\u00b7Submitted bytaesirion Sep 24Upvote23+15Authors:Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi RenAbstractA self-distillation framework converts implicit 3D knowledge from video diffusion models into an explicit 3D Gaussian Splatting representation, enabling 3D scene generation from text or images.AI-generated summaryThe ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements invideo diffusion modelshave\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in thevideo diffusion modelsinto an\nexplicit3D Gaussian Splatting(3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typicalRGB decoderwith\na3DGSdecoder, which is supervised by the output of theRGB decoder. In this\napproach, the3DGSdecoder can be purely trained with synthetic data generated\nbyvideo diffusion models. At inference time, our model can synthesize 3D\nscenes from either atext promptor a single image for real-time rendering. Our\nframework further extends todynamic 3D scene generationfrom a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static anddynamic 3D scene generation.View arX",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/nv-tlabs/lyra",
    "hf_paper_url": "https://huggingface.co/papers/2509.19296",
    "arxiv_url": "https://arxiv.org/abs/2509.19296",
    "num_models": 1,
    "models_list": "nvidia/Lyra",
    "models_links": "https://huggingface.co/nvidia/Lyra",
    "models_detailed": "[{\"name\": \"nvidia/Lyra\", \"link\": \"https://huggingface.co/nvidia/Lyra\", \"task\": \"\", \"likes\": \"73\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 1,
    "datasets_list": "nvidia/PhysicalAI-SpatialIntelligence-Lyra-SDG",
    "datasets_links": "https://huggingface.co/datasets/nvidia/PhysicalAI-SpatialIntelligence-Lyra-SDG",
    "datasets_detailed": "[{\"name\": \"nvidia/PhysicalAI-SpatialIntelligence-Lyra-SDG\", \"link\": \"https://huggingface.co/datasets/nvidia/PhysicalAI-SpatialIntelligence-Lyra-SDG\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 30\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.15496",
    "first_seen_date": "2025-09-22",
    "title": "Lynx: Towards High-Fidelity Personalized Video Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.15496Lynx: Towards High-Fidelity Personalized Video GenerationPublished on Sep 19\u00b7Submitted bytaesirion Sep 22\u00b7ByteDanceUpvote12+4Authors:Shen Sang,Tiancheng Zhi,Tianpei Gu,Jing Liu,Linjie LuoAbstractLynx, a high-fidelity personalized video synthesis model, uses a Diffusion Transformer with ID-adapter and Ref-adapter to preserve identity and maintain video quality.AI-generated summaryWe present Lynx, a high-fidelity model for personalized video synthesis from\na single input image. Built on an open-sourceDiffusion Transformer(DiT)\nfoundation model, Lynx introduces two lightweight adapters to ensure identity\nfidelity. TheID-adapteremploys aPerceiver Resamplerto convertArcFace-derived facial embeddings into compactidentity tokensfor\nconditioning, while theRef-adapterintegratesdense VAEfeatures from a frozen\nreference pathway, injecting fine-grained details across all transformer layers\nthroughcross-attention. These modules collectively enable robust identity\npreservation while maintainingtemporal coherenceandvisual realism. Through\nevaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which\nyielded 800 test cases, Lynx has demonstrated superior face resemblance,\ncompetitive prompt following, and strong video quality, thereby advancing the\nstate ofpersonalized video generation.View arXiv pageView PDFProject pageAdd to collectionCommunitytaesiriPaper submitterSep 22We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all tr",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.15496",
    "arxiv_url": "https://arxiv.org/abs/2509.15496",
    "num_models": 2,
    "models_list": "ByteDance/lynx, vantagewithai/Lynx-GGUF",
    "models_links": "https://huggingface.co/ByteDance/lynx, https://huggingface.co/vantagewithai/Lynx-GGUF",
    "models_detailed": "[{\"name\": \"ByteDance/lynx\", \"link\": \"https://huggingface.co/ByteDance/lynx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 27\"}, {\"name\": \"vantagewithai/Lynx-GGUF\", \"link\": \"https://huggingface.co/vantagewithai/Lynx-GGUF\", \"task\": \"\", \"likes\": \"492\", \"downloads\": \"\", \"updated\": \"Nov 20\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.13160",
    "first_seen_date": "2025-09-19",
    "title": "FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.13160FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial\n  Search and ReasoningPublished on Sep 16\u00b7Submitted byGe Zhangon Sep 19Upvote29+21Authors:Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang+1 authorsAbstractFinSearchComp is an open-source benchmark for evaluating financial search and reasoning capabilities of end-to-end agents, featuring realistic tasks and professional annotations.AI-generated summarySearch has emerged as core infrastructure forLLM-based agentsand is widely\nviewed as critical on the path toward more general intelligence. Finance is a\nparticularly demanding proving ground: analysts routinely conduct complex,\nmulti-step searches over time-sensitive, domain-specific data, making it ideal\nfor assessing bothsearch proficiencyandknowledge-grounded reasoning. Yet no\nexisting open financial datasets evaluate data searching capability of\nend-to-end agents, largely because constructing realistic, complicated tasks\nrequires deep financial expertise and time-sensitive data is hard to evaluate.\nWe presentFinSearchComp, the first fully open-source agent benchmark for\nrealistic, open-domain financial search and reasoning.FinSearchCompcomprises\nthree tasks --Time-Sensitive Data Fetching,Simple Historical Lookup, andComplex Historical Investigation-- closely reproduce real-world financial\nanalyst workflows. To ensure difficulty and reliability, we engage 70\nprofessional financial experts for annotation and implement a rigorous\nmulti-stage quality-assurance pipeline. The benchmark includes 635 questions\nspanning global and Greater China markets, and we evaluate 21 models (products)\non it. Grok 4 (web) tops the global subset, approaching ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.13160",
    "arxiv_url": "https://arxiv.org/abs/2509.13160",
    "num_models": 16,
    "models_list": "MiniMaxAI/MiniMax-M2, unsloth/MiniMax-M2-GGUF, unsloth/MiniMax-M2, QuantTrio/MiniMax-M2-AWQ, ModelCloud/MiniMax-M2-BF16, cyankiwi/MiniMax-M2-BF16, Pavvav/affine-max, bullerwins/MiniMax-M2-GGUF, redponike/MiniMax-M2-GGUF, kesti/affine-minimax, ginipick/MiniMax-M2, seawolf2357/MiniMax-M2, a2s-ai/MiniMax-M2-AWQ, cyankiwi/MiniMax-M2-AWQ-4bit, gayan25/fingerprint-qa, fariasultana/MiniMind",
    "models_links": "https://huggingface.co/MiniMaxAI/MiniMax-M2, https://huggingface.co/unsloth/MiniMax-M2-GGUF, https://huggingface.co/unsloth/MiniMax-M2, https://huggingface.co/QuantTrio/MiniMax-M2-AWQ, https://huggingface.co/ModelCloud/MiniMax-M2-BF16, https://huggingface.co/cyankiwi/MiniMax-M2-BF16, https://huggingface.co/Pavvav/affine-max, https://huggingface.co/bullerwins/MiniMax-M2-GGUF, https://huggingface.co/redponike/MiniMax-M2-GGUF, https://huggingface.co/kesti/affine-minimax, https://huggingface.co/ginipick/MiniMax-M2, https://huggingface.co/seawolf2357/MiniMax-M2, https://huggingface.co/a2s-ai/MiniMax-M2-AWQ, https://huggingface.co/cyankiwi/MiniMax-M2-AWQ-4bit, https://huggingface.co/gayan25/fingerprint-qa, https://huggingface.co/fariasultana/MiniMind",
    "models_detailed": "[{\"name\": \"MiniMaxAI/MiniMax-M2\", \"link\": \"https://huggingface.co/MiniMaxAI/MiniMax-M2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"unsloth/MiniMax-M2-GGUF\", \"link\": \"https://huggingface.co/unsloth/MiniMax-M2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"unsloth/MiniMax-M2\", \"link\": \"https://huggingface.co/unsloth/MiniMax-M2\", \"task\": \"Text Generation\", \"likes\": \"106\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"QuantTrio/MiniMax-M2-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/MiniMax-M2-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"ModelCloud/MiniMax-M2-BF16\", \"link\": \"https://huggingface.co/ModelCloud/MiniMax-M2-BF16\", \"task\": \"Text Generation\", \"likes\": \"482\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"cyankiwi/MiniMax-M2-BF16\", \"link\": \"https://huggingface.co/cyankiwi/MiniMax-M2-BF16\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Nov 9\"}, {\"name\": \"Pavvav/affine-max\", \"link\": \"https://huggingface.co/Pavvav/affine-max\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"bullerwins/MiniMax-M2-GGUF\", \"link\": \"https://huggingface.co/bullerwins/MiniMax-M2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"208\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"redponike/MiniMax-M2-GGUF\", \"link\": \"https://huggingface.co/redponike/MiniMax-M2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"482\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"kesti/affine-minimax\", \"link\": \"https://huggingface.co/kesti/affine-minimax\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"ginipick/MiniMax-M2\", \"link\": \"https://huggingface.co/ginipick/MiniMax-M2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"seawolf2357/MiniMax-M2\", \"link\": \"https://huggingface.co/seawolf2357/MiniMax-M2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"a2s-ai/MiniMax-M2-AWQ\", \"link\": \"https://huggingface.co/a2s-ai/MiniMax-M2-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"cyankiwi/MiniMax-M2-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/MiniMax-M2-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"979\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"gayan25/fingerprint-qa\", \"link\": \"https://huggingface.co/gayan25/fingerprint-qa\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"fariasultana/MiniMind\", \"link\": \"https://huggingface.co/fariasultana/MiniMind\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.14233",
    "first_seen_date": "2025-09-19",
    "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.14233Apertus: Democratizing Open and Compliant LLMs for Global Language\n  EnvironmentsPublished on Sep 17\u00b7Submitted byXiaozhe Yaoon Sep 19\u00b7Swiss AI InitiativeUpvote14+6Authors:Alejandro Hern\u00e1ndez-Cano,Alexander H\u00e4gele,Allen Hao Huang,Angelika Romanou,Antoni-Joan Solergibert,Barna Pasztor,Bettina Messmer,Dhia Garbaya,Eduard Frank \u010eurech,Ido Hakimi,Juan Garc\u00eda Giraldo,Mete Ismayilzada,Negar Foroutan,Skander Moalla,Tiancheng Chen,Vinko Sabol\u010dec,Yixuan Xu,Michael Aerni,Badr AlKhamissi,Ines Altemir Marinas,Mohammad Hossein Amani,Matin Ansaripour+79 authorsAbstractApertus is a suite of open large language models that ensure data compliance and multilingual representation through ethical data sourcing, the Goldfish objective, and comprehensive artifact release.AI-generated summaryWe present Apertus, a fully open suite oflarge language models(LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:data complianceandmultilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively onopenly available data,\nretroactively respectingrobots.txt exclusionsand filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt theGoldfish objectiveduring pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models onmultilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nl",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/swiss-ai/apertus-tech-report",
    "hf_paper_url": "https://huggingface.co/papers/2509.14233",
    "arxiv_url": "https://arxiv.org/abs/2509.14233",
    "num_models": 14,
    "models_list": "swiss-ai/Apertus-8B-Instruct-2509, swiss-ai/Apertus-70B-Instruct-2509, swiss-ai/Apertus-8B-2509, swiss-ai/Apertus-70B-2509, unsloth/Apertus-8B-Instruct-2509, unsloth/Apertus-70B-Instruct-2509, redponike/Apertus-8B-Instruct-2509-GGUF, redponike/Apertus-70B-Instruct-2509-GGUF, unsloth/Apertus-8B-Instruct-2509-GGUF, unsloth/Apertus-70B-Instruct-2509-GGUF, unsloth/Apertus-8B-Instruct-2509-unsloth-bnb-4bit, unsloth/Apertus-70B-Instruct-2509-unsloth-bnb-4bit, ABaroian/Apertus-8B-RLVR-GSM, NewEden/Apertus-8b-instruct-patched",
    "models_links": "https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509, https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509, https://huggingface.co/swiss-ai/Apertus-8B-2509, https://huggingface.co/swiss-ai/Apertus-70B-2509, https://huggingface.co/unsloth/Apertus-8B-Instruct-2509, https://huggingface.co/unsloth/Apertus-70B-Instruct-2509, https://huggingface.co/redponike/Apertus-8B-Instruct-2509-GGUF, https://huggingface.co/redponike/Apertus-70B-Instruct-2509-GGUF, https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF, https://huggingface.co/unsloth/Apertus-70B-Instruct-2509-GGUF, https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-unsloth-bnb-4bit, https://huggingface.co/unsloth/Apertus-70B-Instruct-2509-unsloth-bnb-4bit, https://huggingface.co/ABaroian/Apertus-8B-RLVR-GSM, https://huggingface.co/NewEden/Apertus-8b-instruct-patched",
    "models_detailed": "[{\"name\": \"swiss-ai/Apertus-8B-Instruct-2509\", \"link\": \"https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"swiss-ai/Apertus-70B-Instruct-2509\", \"link\": \"https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"swiss-ai/Apertus-8B-2509\", \"link\": \"https://huggingface.co/swiss-ai/Apertus-8B-2509\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\"}, {\"name\": \"swiss-ai/Apertus-70B-2509\", \"link\": \"https://huggingface.co/swiss-ai/Apertus-70B-2509\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"unsloth/Apertus-8B-Instruct-2509\", \"link\": \"https://huggingface.co/unsloth/Apertus-8B-Instruct-2509\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 4\"}, {\"name\": \"unsloth/Apertus-70B-Instruct-2509\", \"link\": \"https://huggingface.co/unsloth/Apertus-70B-Instruct-2509\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 4\"}, {\"name\": \"redponike/Apertus-8B-Instruct-2509-GGUF\", \"link\": \"https://huggingface.co/redponike/Apertus-8B-Instruct-2509-GGUF\", \"task\": \"Text Generation\", \"likes\": \"216\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"redponike/Apertus-70B-Instruct-2509-GGUF\", \"link\": \"https://huggingface.co/redponike/Apertus-70B-Instruct-2509-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"unsloth/Apertus-8B-Instruct-2509-GGUF\", \"link\": \"https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 5\"}, {\"name\": \"unsloth/Apertus-70B-Instruct-2509-GGUF\", \"link\": \"https://huggingface.co/unsloth/Apertus-70B-Instruct-2509-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 5\"}, {\"name\": \"unsloth/Apertus-8B-Instruct-2509-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Apertus-8B-Instruct-2509-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"223\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"unsloth/Apertus-70B-Instruct-2509-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Apertus-70B-Instruct-2509-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"30\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"ABaroian/Apertus-8B-RLVR-GSM\", \"link\": \"https://huggingface.co/ABaroian/Apertus-8B-RLVR-GSM\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"NewEden/Apertus-8b-instruct-patched\", \"link\": \"https://huggingface.co/NewEden/Apertus-8b-instruct-patched\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.15212",
    "first_seen_date": "2025-09-19",
    "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.15212RynnVLA-001: Using Human Demonstrations to Improve Robot ManipulationPublished on Sep 18\u00b7Submitted bytaesirion Sep 19Upvote21+13Authors:Yuming Jiang,Siteng Huang,Shengke Xue,Yaxi Zhao,Jun Cen,Sicong Leng,Kehan Li,Jiayan Guo,Kexiang Wang,Mingxiu Chen,Fan Wang,Deli Zhao,Xin LiAbstractRynnVLA-001, a vision-language-action model, uses a two-stage pretraining approach and ActionVAE to achieve superior performance on robotics tasks.AI-generated summaryThis paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains anImage-to-Video modelon 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting futurekeypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we proposeActionVAE, avariational autoencoderthat compresses sequences of actions into compactlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.View arXiv pageView PDFGitHub270Add to collectionCommunitytaesiriPaper submitterSep 19This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/alibaba-damo-academy/RynnVLA-001",
    "hf_paper_url": "https://huggingface.co/papers/2509.15212",
    "arxiv_url": "https://arxiv.org/abs/2509.15212",
    "num_models": 1,
    "models_list": "Alibaba-DAMO-Academy/RynnVLA-002",
    "models_links": "https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-002",
    "models_detailed": "[{\"name\": \"Alibaba-DAMO-Academy/RynnVLA-002\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-002\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 1\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.15221",
    "first_seen_date": "2025-09-19",
    "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.15221ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  DataPublished on Sep 18\u00b7Submitted bytaesirion Sep 19Upvote111+103Authors:Zhaoyang Liu,JingJing Xie,Zichen Ding,Zehao Li,Bowen Yang,Zhenyu Wu,Xuehui Wang,Qiushi Sun,Shi Liu,Weiyun Wang,Shenglong Ye,Qingyun Li,Zeyue Tian,Gen Luo,Xiangyu Yue,Biqing Qi,Kai Chen,Bowen Zhou,Yu Qiao,Qifeng Chen,Wenhai WangAbstractScaleCUA, a large-scale dataset and model for computer use agents, achieves state-of-the-art performance across multiple platforms and tasks by leveraging data-driven scaling.AI-generated summaryVision-Language Models(VLMs) have enabledcomputer use agents(CUAs) that\noperateGUIsautonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via aclosed-loop pipelineunitingautomated agentswithhuman experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 onWebArena-Lite-v2, +10.7 onScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% onMMBench-GUI L1-Hard, 60.6% onOSWorld-G, 47.4% onWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purposecomputer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.View arXiv pageView PDFGitHub996Add to collectionCommunitytaesiriPaper submitterSep 19Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step tow",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/OpenGVLab/ScaleCUA",
    "hf_paper_url": "https://huggingface.co/papers/2509.15221",
    "arxiv_url": "https://arxiv.org/abs/2509.15221",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "OpenGVLab/ScaleCUA-Data",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/ScaleCUA-Data",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/ScaleCUA-Data\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/ScaleCUA-Data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 27\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.14232",
    "first_seen_date": "2025-09-18",
    "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.14232GenExam: A Multidisciplinary Text-to-Image ExamPublished on Sep 17\u00b7Submitted bytaesirion Sep 18Upvote21+13Authors:Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen LuoAbstractGenExam is a benchmark for evaluating text-to-image generation in exam-style settings across multiple disciplines, highlighting the challenges in integrating knowledge, reasoning, and generation.AI-generated summaryExams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduceGenExam, the first benchmark formultidisciplinarytext-to-imageexams,\nfeaturing 1,000 samples across 10 subjects withexam-style promptsorganized\nunder a four-level taxonomy. Each problem is equipped withground-truth imagesandfine-grained scoringpoints to enable a precise evaluation of semantic\ncorrectness andvisual plausibility. Experiments show that even\nstate-of-the-art models such asGPT-Image-1andGemini-2.5-Flash-Imageachieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,GenExamoffers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path togeneral AGI.View arXiv pageView PDFGitHub50Add to collectionCommunitytaesiriPaper submitterSep 18Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge an",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/OpenGVLab/GenExam",
    "hf_paper_url": "https://huggingface.co/papers/2509.14232",
    "arxiv_url": "https://arxiv.org/abs/2509.14232",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "OpenGVLab/GenExam",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/GenExam",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/GenExam\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/GenExam\", \"task\": \"\", \"likes\": \"328\", \"downloads\": \"\", \"updated\": \"Oct 6\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.09677",
    "first_seen_date": "2025-09-15",
    "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.09677The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMsPublished on Sep 11\u00b7Submitted byShashwat Goelon Sep 15Upvote34+26Authors:Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas GeipingAbstractScaling large language models improves their ability to execute longer tasks by isolating execution capability and mitigating self-conditioning effects, despite diminishing single-step accuracy.AI-generated summaryDoes continued scaling oflarge language models(LLMs) yield diminishing\nreturns? Real-world value often stems from the length of task an agent can\ncomplete. We start this work by observing the simple but counterintuitive fact\nthat marginal gains insingle-step accuracycan compound into exponential\nimprovements in the length of a task a model can successfully complete. Then,\nwe argue that failures ofLLMswhen simple tasks are made longer arise from\nmistakes in execution, rather than an inability to reason. We propose isolatingexecution capability, by explicitly providing the knowledge and plan needed to\nsolve a long-horizon task. We find that larger models can correctly execute\nsignificantly more turns even when small models have 100\\% single-turn\naccuracy. We observe that the per-step accuracy of models degrades as the\nnumber of steps increases. This is not just due to long-context limitations --\ncuriously, we observe aself-conditioningeffect -- models become more likely\nto make mistakes when the context contains their errors from prior turns.Self-conditioningdoes not reduce by just scaling the model size. In contrast,\nrecentthinking modelsdo not self-condition, and can also execute much longer\ntasks in a single turn. We conclude by benchmarking frontierthinking modelson\nthe length of task they can execute in a single turn. Overall, by focusing on\nthe ability to execute, we hope to reconcile debates on howLLMscan solve\ncomplex reasoning prob",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/long-horizon-execution/measuring-execution",
    "hf_paper_url": "https://huggingface.co/papers/2509.09677",
    "arxiv_url": "https://arxiv.org/abs/2509.09677",
    "num_models": 11,
    "models_list": "janhq/Jan-v2-VL-high, janhq/Jan-v2-VL-high-gguf, janhq/Jan-v2-VL-low, janhq/Jan-v2-VL-med, janhq/Jan-v2-VL-max-FP8, janhq/Jan-v2-VL-low-gguf, janhq/Jan-v2-VL-med-gguf, Mungert/Jan-v2-VL-high-GGUF, noctrex/Jan-v2-VL-low-GGUF, cyankiwi/Jan-v2-VL-high-AWQ-4bit, cyankiwi/Jan-v2-VL-high-AWQ-8bit",
    "models_links": "https://huggingface.co/janhq/Jan-v2-VL-high, https://huggingface.co/janhq/Jan-v2-VL-high-gguf, https://huggingface.co/janhq/Jan-v2-VL-low, https://huggingface.co/janhq/Jan-v2-VL-med, https://huggingface.co/janhq/Jan-v2-VL-max-FP8, https://huggingface.co/janhq/Jan-v2-VL-low-gguf, https://huggingface.co/janhq/Jan-v2-VL-med-gguf, https://huggingface.co/Mungert/Jan-v2-VL-high-GGUF, https://huggingface.co/noctrex/Jan-v2-VL-low-GGUF, https://huggingface.co/cyankiwi/Jan-v2-VL-high-AWQ-4bit, https://huggingface.co/cyankiwi/Jan-v2-VL-high-AWQ-8bit",
    "models_detailed": "[{\"name\": \"janhq/Jan-v2-VL-high\", \"link\": \"https://huggingface.co/janhq/Jan-v2-VL-high\", \"task\": \"\", \"likes\": \"622\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"janhq/Jan-v2-VL-high-gguf\", \"link\": \"https://huggingface.co/janhq/Jan-v2-VL-high-gguf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"janhq/Jan-v2-VL-low\", \"link\": \"https://huggingface.co/janhq/Jan-v2-VL-low\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"janhq/Jan-v2-VL-med\", \"link\": \"https://huggingface.co/janhq/Jan-v2-VL-med\", \"task\": \"\", \"likes\": \"93\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"janhq/Jan-v2-VL-max-FP8\", \"link\": \"https://huggingface.co/janhq/Jan-v2-VL-max-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 3\"}, {\"name\": \"janhq/Jan-v2-VL-low-gguf\", \"link\": \"https://huggingface.co/janhq/Jan-v2-VL-low-gguf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"janhq/Jan-v2-VL-med-gguf\", \"link\": \"https://huggingface.co/janhq/Jan-v2-VL-med-gguf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"Mungert/Jan-v2-VL-high-GGUF\", \"link\": \"https://huggingface.co/Mungert/Jan-v2-VL-high-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"noctrex/Jan-v2-VL-low-GGUF\", \"link\": \"https://huggingface.co/noctrex/Jan-v2-VL-low-GGUF\", \"task\": \"\", \"likes\": \"219\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"cyankiwi/Jan-v2-VL-high-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Jan-v2-VL-high-AWQ-4bit\", \"task\": \"\", \"likes\": \"69\", \"downloads\": \"\", \"updated\": \"Nov 20\"}, {\"name\": \"cyankiwi/Jan-v2-VL-high-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Jan-v2-VL-high-AWQ-8bit\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Nov 20\"}]",
    "num_datasets": 1,
    "datasets_list": "arvindh75/Long-Horizon-Execution",
    "datasets_links": "https://huggingface.co/datasets/arvindh75/Long-Horizon-Execution",
    "datasets_detailed": "[{\"name\": \"arvindh75/Long-Horizon-Execution\", \"link\": \"https://huggingface.co/datasets/arvindh75/Long-Horizon-Execution\", \"task\": \"\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"Sep 16\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.05209",
    "first_seen_date": "2025-09-11",
    "title": "Hunyuan-MT Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.05209Hunyuan-MT Technical ReportPublished on Sep 5\u00b7Submitted byMingyang Songon Sep 11Upvote14+6Authors:Mao Zheng,Zheng Li,Bingxin Qu,Mingyang Song,Yang Du,Mingrui Sun,Di WangAbstractHunyuan-MT-7B and Hunyuan-MT-Chimera-7B are multilingual translation models that outperform existing models, especially in translating between Mandarin and minority languages, through a combination of pre-training, supervised fine-tuning, and reinforcement learning.AI-generated summaryIn this report, we introduceHunyuan-MT-7B, our first open-sourcemultilingual translationmodel, which supportsbidirectional translationacross\n33 major languages and places a special emphasis on translation between\nMandarin and several ethnic minority languages as well as dialects.\nFurthermore, to serve and address diverse translation scenarios and enhance\nmodel performance at test time, we introduceHunyuan-MT-Chimera-7B, a\ntranslation model inspired by theslow thinking mode. This model integrates\nmultiple outputs generated by theHunyuan-MT-7Bmodel under varying parameter\nsettings, thereby achieving performance superior to that of conventional\nslow-thinking models based onChain-of-Thought (CoT). The development of our\nmodels follows a holistic training process specifically engineered formultilingual translation, which begins with general and MT-oriented\npre-training to build foundational capabilities, proceeds to Supervised\nFine-Tuning (SFT) for task-specific adaptation, and culminates in advanced\nalignment throughReinforcement Learning (RL)andweak-to-strong RL. Through\ncomprehensive experimentation, we demonstrate that bothHunyuan-MT-7BandHunyuan-MT-Chimera-7Bsignificantly outperform all translation-specific models\nof comparable parameter size and most of the SOTA large models, particularly on\nthe task of translation between Mandarin and minority languages as well as\ndialects. In theWMT2025shared task (General Machine",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/Tencent-Hunyuan/Hunyuan-MT",
    "hf_paper_url": "https://huggingface.co/papers/2509.05209",
    "arxiv_url": "https://arxiv.org/abs/2509.05209",
    "num_models": 6,
    "models_list": "tencent/Hunyuan-MT-7B, tencent/Hunyuan-MT-Chimera-7B, Mungert/Hunyuan-MT-7B-GGUF, LeDXIII/Hunyuan-MT-Chimera-7B-bnb4, LeDXIII/Hunyuan-MT-7B-bnb4, ChatrealAI/Hunyuan-MT-Chimera-7B",
    "models_links": "https://huggingface.co/tencent/Hunyuan-MT-7B, https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B, https://huggingface.co/Mungert/Hunyuan-MT-7B-GGUF, https://huggingface.co/LeDXIII/Hunyuan-MT-Chimera-7B-bnb4, https://huggingface.co/LeDXIII/Hunyuan-MT-7B-bnb4, https://huggingface.co/ChatrealAI/Hunyuan-MT-Chimera-7B",
    "models_detailed": "[{\"name\": \"tencent/Hunyuan-MT-7B\", \"link\": \"https://huggingface.co/tencent/Hunyuan-MT-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"tencent/Hunyuan-MT-Chimera-7B\", \"link\": \"https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 9\"}, {\"name\": \"Mungert/Hunyuan-MT-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Hunyuan-MT-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"LeDXIII/Hunyuan-MT-Chimera-7B-bnb4\", \"link\": \"https://huggingface.co/LeDXIII/Hunyuan-MT-Chimera-7B-bnb4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"LeDXIII/Hunyuan-MT-7B-bnb4\", \"link\": \"https://huggingface.co/LeDXIII/Hunyuan-MT-7B-bnb4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"ChatrealAI/Hunyuan-MT-Chimera-7B\", \"link\": \"https://huggingface.co/ChatrealAI/Hunyuan-MT-Chimera-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.07968",
    "first_seen_date": "2025-09-10",
    "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.07968SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  KnowledgePublished on Sep 9\u00b7Submitted bytaesirion Sep 10Upvote14+6Authors:Lukas Haas,Gal Yona,Giovanni D'Antonio,Sasha Goldshtein,Dipanjan DasAbstractSimpleQA Verified is a refined benchmark for evaluating the factuality of Large Language Models, addressing issues in previous benchmarks and providing a more reliable evaluation tool.AI-generated summaryWe introduceSimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI'sSimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy.SimpleQA Verifiedwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-artF1-scoreof 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress inparametric model factualityand to mitigatehallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.View arXiv pageView PDFProject pageAdd to collectionCommunitytaesiriPaper submitterSep 10We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reco",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.07968",
    "arxiv_url": "https://arxiv.org/abs/2509.07968",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 4,
    "datasets_list": "google/simpleqa-verified, codelion/SimpleQA-Verified, stalkermustang/SimpleQA-Verified, wangyuwei111/simpleqa-verified",
    "datasets_links": "https://huggingface.co/datasets/google/simpleqa-verified, https://huggingface.co/datasets/codelion/SimpleQA-Verified, https://huggingface.co/datasets/stalkermustang/SimpleQA-Verified, https://huggingface.co/datasets/wangyuwei111/simpleqa-verified",
    "datasets_detailed": "[{\"name\": \"google/simpleqa-verified\", \"link\": \"https://huggingface.co/datasets/google/simpleqa-verified\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\", \"size\": \"\"}, {\"name\": \"codelion/SimpleQA-Verified\", \"link\": \"https://huggingface.co/datasets/codelion/SimpleQA-Verified\", \"task\": \"\", \"likes\": \"254\", \"downloads\": \"\", \"updated\": \"Sep 11\", \"size\": \"\"}, {\"name\": \"stalkermustang/SimpleQA-Verified\", \"link\": \"https://huggingface.co/datasets/stalkermustang/SimpleQA-Verified\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\", \"size\": \"\"}, {\"name\": \"wangyuwei111/simpleqa-verified\", \"link\": \"https://huggingface.co/datasets/wangyuwei111/simpleqa-verified\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.06493",
    "first_seen_date": "2025-09-09",
    "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.06493Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-ProversPublished on Sep 8\u00b7Submitted bytaesirion Sep 9Upvote11+3Authors:Ran Xin,Zeyu Zheng,Yanchen Nie,Kun Yuan,Xia XiaoAbstractBFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.AI-generated summaryThe integration ofLarge Language Models(LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-timereinforcement learning(RL) and\ninference-time compute. This paper introduces BFS-Prover-V2, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turnoff-policy RLframework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles ofAlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuringadaptive tactic-level data filteringandperiodic retrainingto surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs ageneral reasoning modelas a high-level\nplanner to iteratively decompose complex theorems into a sequence of simplersubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging ashared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on theMiniF2FandProofNettest sets respec",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.06493",
    "arxiv_url": "https://arxiv.org/abs/2509.06493",
    "num_models": 2,
    "models_list": "ByteDance-Seed/BFS-Prover-V2-32B, ByteDance-Seed/BFS-Prover-V2-7B",
    "models_links": "https://huggingface.co/ByteDance-Seed/BFS-Prover-V2-32B, https://huggingface.co/ByteDance-Seed/BFS-Prover-V2-7B",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/BFS-Prover-V2-32B\", \"link\": \"https://huggingface.co/ByteDance-Seed/BFS-Prover-V2-32B\", \"task\": \"Text Generation\", \"likes\": \"105\", \"downloads\": \"\", \"updated\": \"Oct 9\"}, {\"name\": \"ByteDance-Seed/BFS-Prover-V2-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/BFS-Prover-V2-7B\", \"task\": \"Text Generation\", \"likes\": \"504\", \"downloads\": \"\", \"updated\": \"Oct 9\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.06501",
    "first_seen_date": "2025-09-09",
    "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.06501WebExplorer: Explore and Evolve for Training Long-Horizon Web AgentsPublished on Sep 8\u00b7Submitted byJunteng Liuon Sep 9#2 Paper of the dayUpvote79+71Authors:Junteng Liu,Yunji Li,Chi Zhang,Jingyang Li,Aili Chen,Ke Ji,Weiyu Cheng,Zijia Wu,Chengyu Du,Qidi Xu,Jiayuan Song,Zhengmao Zhu,Wenhu Chen,Pengyu Zhao,Junxian HeAbstractWebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.AI-generated summaryThe paradigm ofLarge Language Models(LLMs) has increasingly shifted towardagentic applications, whereweb browsing capabilitiesare fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limitedinformation-seeking abilitieson complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach usingmodel-based explorationand iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that requiremulti-step reasoningandcomplex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B throughsupervised fine-tuningfollowed byreinforcement learning. Our model supports 128Kcontext lengthand up to 100tool calling turns, enablinglong-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B onBrowseComp-en/zhand attaining the best\nperformance amon",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/hkust-nlp/WebExplorer",
    "hf_paper_url": "https://huggingface.co/papers/2509.06501",
    "arxiv_url": "https://arxiv.org/abs/2509.06501",
    "num_models": 17,
    "models_list": "MiniMaxAI/MiniMax-M2, unsloth/MiniMax-M2-GGUF, unsloth/MiniMax-M2, hkust-nlp/WebExplorer-8B, QuantTrio/MiniMax-M2-AWQ, ModelCloud/MiniMax-M2-BF16, cyankiwi/MiniMax-M2-BF16, Pavvav/affine-max, bullerwins/MiniMax-M2-GGUF, redponike/MiniMax-M2-GGUF, kesti/affine-minimax, ginipick/MiniMax-M2, seawolf2357/MiniMax-M2, a2s-ai/MiniMax-M2-AWQ, cyankiwi/MiniMax-M2-AWQ-4bit, gayan25/fingerprint-qa, fariasultana/MiniMind",
    "models_links": "https://huggingface.co/MiniMaxAI/MiniMax-M2, https://huggingface.co/unsloth/MiniMax-M2-GGUF, https://huggingface.co/unsloth/MiniMax-M2, https://huggingface.co/hkust-nlp/WebExplorer-8B, https://huggingface.co/QuantTrio/MiniMax-M2-AWQ, https://huggingface.co/ModelCloud/MiniMax-M2-BF16, https://huggingface.co/cyankiwi/MiniMax-M2-BF16, https://huggingface.co/Pavvav/affine-max, https://huggingface.co/bullerwins/MiniMax-M2-GGUF, https://huggingface.co/redponike/MiniMax-M2-GGUF, https://huggingface.co/kesti/affine-minimax, https://huggingface.co/ginipick/MiniMax-M2, https://huggingface.co/seawolf2357/MiniMax-M2, https://huggingface.co/a2s-ai/MiniMax-M2-AWQ, https://huggingface.co/cyankiwi/MiniMax-M2-AWQ-4bit, https://huggingface.co/gayan25/fingerprint-qa, https://huggingface.co/fariasultana/MiniMind",
    "models_detailed": "[{\"name\": \"MiniMaxAI/MiniMax-M2\", \"link\": \"https://huggingface.co/MiniMaxAI/MiniMax-M2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"unsloth/MiniMax-M2-GGUF\", \"link\": \"https://huggingface.co/unsloth/MiniMax-M2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"unsloth/MiniMax-M2\", \"link\": \"https://huggingface.co/unsloth/MiniMax-M2\", \"task\": \"Text Generation\", \"likes\": \"106\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"hkust-nlp/WebExplorer-8B\", \"link\": \"https://huggingface.co/hkust-nlp/WebExplorer-8B\", \"task\": \"\", \"likes\": \"440\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"QuantTrio/MiniMax-M2-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/MiniMax-M2-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"ModelCloud/MiniMax-M2-BF16\", \"link\": \"https://huggingface.co/ModelCloud/MiniMax-M2-BF16\", \"task\": \"Text Generation\", \"likes\": \"482\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"cyankiwi/MiniMax-M2-BF16\", \"link\": \"https://huggingface.co/cyankiwi/MiniMax-M2-BF16\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Nov 9\"}, {\"name\": \"Pavvav/affine-max\", \"link\": \"https://huggingface.co/Pavvav/affine-max\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"bullerwins/MiniMax-M2-GGUF\", \"link\": \"https://huggingface.co/bullerwins/MiniMax-M2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"208\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"redponike/MiniMax-M2-GGUF\", \"link\": \"https://huggingface.co/redponike/MiniMax-M2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"482\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"kesti/affine-minimax\", \"link\": \"https://huggingface.co/kesti/affine-minimax\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"ginipick/MiniMax-M2\", \"link\": \"https://huggingface.co/ginipick/MiniMax-M2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"seawolf2357/MiniMax-M2\", \"link\": \"https://huggingface.co/seawolf2357/MiniMax-M2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"a2s-ai/MiniMax-M2-AWQ\", \"link\": \"https://huggingface.co/a2s-ai/MiniMax-M2-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"cyankiwi/MiniMax-M2-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/MiniMax-M2-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"979\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"gayan25/fingerprint-qa\", \"link\": \"https://huggingface.co/gayan25/fingerprint-qa\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"fariasultana/MiniMind\", \"link\": \"https://huggingface.co/fariasultana/MiniMind\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "hkust-nlp/WebExplorer-QA",
    "datasets_links": "https://huggingface.co/datasets/hkust-nlp/WebExplorer-QA",
    "datasets_detailed": "[{\"name\": \"hkust-nlp/WebExplorer-QA\", \"link\": \"https://huggingface.co/datasets/hkust-nlp/WebExplorer-QA\", \"task\": \"\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"about 1\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.06949",
    "first_seen_date": "2025-09-09",
    "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.06949Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language ModelsPublished on Sep 8\u00b7Submitted byLing Yangon Sep 9Upvote55+47Authors:Yinjie Wang,Ling Yang,Bowen Li,Ye Tian,Ke Shen,Mengdi WangAbstractTraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.AI-generated summaryWe propose TraceRL, atrajectory-aware reinforcement learningframework fordiffusion language models(DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with adiffusion-based value modelthat enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improvessampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-artdiffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Throughcurriculum learning, we also derive the firstlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates acceleratedKV-cache techniquesandinference enginesfor\nboth inference and reinforcement learning, and includes implementations of\nvarioussupervised fine-tuningandRL methodsfor mathematics, coding, and\ngeneral tasks. Code and Models: ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/Gen-Verse/dLLM-RL",
    "hf_paper_url": "https://huggingface.co/papers/2509.06949",
    "arxiv_url": "https://arxiv.org/abs/2509.06949",
    "num_models": 3,
    "models_list": "Gen-Verse/TraDo-8B-Thinking, Gen-Verse/TraDo-8B-Instruct, Gen-Verse/TraDo-4B-Instruct",
    "models_links": "https://huggingface.co/Gen-Verse/TraDo-8B-Thinking, https://huggingface.co/Gen-Verse/TraDo-8B-Instruct, https://huggingface.co/Gen-Verse/TraDo-4B-Instruct",
    "models_detailed": "[{\"name\": \"Gen-Verse/TraDo-8B-Thinking\", \"link\": \"https://huggingface.co/Gen-Verse/TraDo-8B-Thinking\", \"task\": \"\", \"likes\": \"61\", \"downloads\": \"\", \"updated\": \"Sep 9\"}, {\"name\": \"Gen-Verse/TraDo-8B-Instruct\", \"link\": \"https://huggingface.co/Gen-Verse/TraDo-8B-Instruct\", \"task\": \"\", \"likes\": \"117\", \"downloads\": \"\", \"updated\": \"Sep 9\"}, {\"name\": \"Gen-Verse/TraDo-4B-Instruct\", \"link\": \"https://huggingface.co/Gen-Verse/TraDo-4B-Instruct\", \"task\": \"\", \"likes\": \"172\", \"downloads\": \"\", \"updated\": \"Sep 9\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2509.04292",
    "first_seen_date": "2025-09-05",
    "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.04292Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?Published on Sep 4\u00b7Submitted bytaesirion Sep 5\u00b7ByteDance SeedUpvote57+49Authors:Qinyan Zhang,Xinping Lei,Ruijie Miao,Yu Fu,Haojie Fan,Le Chang,Jiafan Hou,Dingling Zhang,Zhongfei Hou,Ziqiang Yang,Changxin Pu,Fei Hu,Jingkai Liu,Mengyun Liu,Yang Liu,Xiang Gao,Jiaheng Liu,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao HuangAbstractInverse IFEval evaluates Large Language Models' ability to override training biases and follow unconventional instructions, highlighting the need for adaptability in diverse contexts.AI-generated summaryLarge Language Models(LLMs) achieve strong performance on diverse tasks but\noften exhibitcognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned duringsupervised fine-tuning(SFT). To evaluate this limitation, we proposeInverse IFEval, a benchmark that\nmeasures modelsCounter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection,Intentional Textual Flaws,Code without Comments, andCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimizedLLM-as-a-Judgeframework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposedInverse IFEvalbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope thatInverse IFEvalserves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliabi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.04292",
    "arxiv_url": "https://arxiv.org/abs/2509.04292",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "m-a-p/Inverse_IFEval",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/Inverse_IFEval",
    "datasets_detailed": "[{\"name\": \"m-a-p/Inverse_IFEval\", \"link\": \"https://huggingface.co/datasets/m-a-p/Inverse_IFEval\", \"task\": \"\", \"likes\": \"149\", \"downloads\": \"\", \"updated\": \"Sep 24\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.02530",
    "first_seen_date": "2025-09-04",
    "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in\n  Robots",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.02530Manipulation as in Simulation: Enabling Accurate Geometry Perception in\n  RobotsPublished on Sep 2\u00b7Submitted byMinghuan Liuon Sep 4\u00b7ByteDance SeedUpvote10+2Authors:Minghuan Liu,Zhengbang Zhu,Xiaoshen Han,Peng Hu,Haotong Lin,Xinyao Li,Jingxiao Chen,Jiafeng Xu,Yichu Yang,Yunfeng Lin,Xinghang Li,Yong Yu,Weinan Zhang,Tao Kong,Bingyi KangAbstractCamera Depth Models (CDMs) enhance depth camera accuracy by denoising and improving metric depth prediction, enabling better generalization of robotic manipulation policies from simulation to real-world tasks.AI-generated summaryModernrobotic manipulationprimarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that usingdepth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we proposeCamera Depth Models(CDMs) as\na simple plugin on daily-usedepth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop aneural data enginethat generates high-quality paired data from\nsimulation by modeling adepth camera'snoise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy indepth prediction, effectively\nbridging thesim-to-real gapfor manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "https://github.com/ByteDance-Seed/manip-as-in-sim-suite",
    "hf_paper_url": "https://huggingface.co/papers/2509.02530",
    "arxiv_url": "https://arxiv.org/abs/2509.02530",
    "num_models": 7,
    "models_list": "depth-anything/camera-depth-model-d405, depth-anything/camera-depth-model-d435, depth-anything/camera-depth-model-kinect, depth-anything/camera-depth-model-l515, depth-anything/camera-depth-model-zed2i-neural, depth-anything/camera-depth-model-zed2i-quality, depth-anything/camera-depth-model-base",
    "models_links": "https://huggingface.co/depth-anything/camera-depth-model-d405, https://huggingface.co/depth-anything/camera-depth-model-d435, https://huggingface.co/depth-anything/camera-depth-model-kinect, https://huggingface.co/depth-anything/camera-depth-model-l515, https://huggingface.co/depth-anything/camera-depth-model-zed2i-neural, https://huggingface.co/depth-anything/camera-depth-model-zed2i-quality, https://huggingface.co/depth-anything/camera-depth-model-base",
    "models_detailed": "[{\"name\": \"depth-anything/camera-depth-model-d405\", \"link\": \"https://huggingface.co/depth-anything/camera-depth-model-d405\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"depth-anything/camera-depth-model-d435\", \"link\": \"https://huggingface.co/depth-anything/camera-depth-model-d435\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"depth-anything/camera-depth-model-kinect\", \"link\": \"https://huggingface.co/depth-anything/camera-depth-model-kinect\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"depth-anything/camera-depth-model-l515\", \"link\": \"https://huggingface.co/depth-anything/camera-depth-model-l515\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"depth-anything/camera-depth-model-zed2i-neural\", \"link\": \"https://huggingface.co/depth-anything/camera-depth-model-zed2i-neural\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"depth-anything/camera-depth-model-zed2i-quality\", \"link\": \"https://huggingface.co/depth-anything/camera-depth-model-zed2i-quality\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"depth-anything/camera-depth-model-base\", \"link\": \"https://huggingface.co/depth-anything/camera-depth-model-base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "ByteDance-Seed/ByteCameraDepth",
    "datasets_links": "https://huggingface.co/datasets/ByteDance-Seed/ByteCameraDepth",
    "datasets_detailed": "[{\"name\": \"ByteDance-Seed/ByteCameraDepth\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/ByteCameraDepth\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 6\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2509.02046",
    "first_seen_date": "2025-09-03",
    "title": "Fantastic Pretraining Optimizers and Where to Find Them",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2509.02046Fantastic Pretraining Optimizers and Where to Find ThemPublished on Sep 2\u00b7Submitted byElie Bakouchon Sep 3Upvote13+5Authors:Kaiyue Wen,David Hall,Tengyu Ma,Percy LiangAbstractA systematic study reveals that fair comparisons of deep learning optimizers require rigorous hyperparameter tuning and evaluations across various model scales and data-to-model ratios, showing that matrix-based optimizers like Muon and Soap offer diminishing speedups as model size increases.AI-generated summaryAdamWhas long been the dominant optimizer in language model pretraining,\ndespite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We\nposit that two methodological shortcomings have obscured fair comparisons and\nhindered practical adoption: (i) unequalhyperparameter tuningand (ii) limited\nor misleadingevaluation setups. To address these two issues, we conduct a\nsystematic study of tendeep learning optimizersacross fourmodel scales(0.1B-1.2B parameters) anddata-to-model ratios(1-8x the Chinchilla optimum).\nWe find that fair and informative comparisons require rigorous hyperparameter\ntuning and evaluations across a range ofmodel scalesanddata-to-model ratios,\nperformed at the end of training. First, optimal hyperparameters for one\noptimizer may be suboptimal for another, making blind hyperparameter transfer\nunfair. Second, the actual speedup of many proposed optimizers over well-tuned\nbaselines is lower than claimed and decreases with model size to only 1.1x for\n1.2B parameter models. Thirdly, comparing intermediate checkpoints before\nreaching the target training budgets can be misleading, as rankings between two\noptimizers can flip during training due tolearning rate decay. Through our\nthorough investigation, we find that all the fastest optimizers such asMuonandSoap, use matrices aspreconditioners-- multiplying gradients with\nmatrices rather than entry-wise scalars. However",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2509,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2509.02046",
    "arxiv_url": "https://arxiv.org/abs/2509.02046",
    "num_models": 147,
    "models_list": "OptimizerStudy/adamw_1.2b_1, OptimizerStudy/adamw_1.2b_2, OptimizerStudy/adamw_1.2b_4, OptimizerStudy/adamw_1.2b_8, OptimizerStudy/adamw_130m_1, OptimizerStudy/adamw_130m_16, OptimizerStudy/adamw_130m_2, OptimizerStudy/adamw_130m_4, OptimizerStudy/adamw_130m_8, OptimizerStudy/adamw_300m_1, OptimizerStudy/adamw_300m_16, OptimizerStudy/adamw_300m_2, OptimizerStudy/adamw_300m_4, OptimizerStudy/adamw_300m_8, OptimizerStudy/adamw_520m_1, OptimizerStudy/adamw_520m_2, OptimizerStudy/adamw_520m_4, OptimizerStudy/adamw_520m_8, OptimizerStudy/cautious_130m_1, OptimizerStudy/cautious_130m_2, OptimizerStudy/cautious_130m_4, OptimizerStudy/cautious_130m_8, OptimizerStudy/cautious_300m_1, OptimizerStudy/cautious_300m_2, OptimizerStudy/cautious_300m_4, OptimizerStudy/cautious_300m_8, OptimizerStudy/cautious_520m_1, OptimizerStudy/cautious_520m_2, OptimizerStudy/cautious_520m_4, OptimizerStudy/cautious_520m_8, OptimizerStudy/kron_130m_1, OptimizerStudy/kron_130m_2, OptimizerStudy/kron_130m_4, OptimizerStudy/kron_130m_8, OptimizerStudy/kron_300m_1, OptimizerStudy/kron_300m_2, OptimizerStudy/kron_300m_4, OptimizerStudy/kron_300m_8, OptimizerStudy/kron_520m_1, OptimizerStudy/kron_520m_2, OptimizerStudy/kron_520m_4, OptimizerStudy/kron_520m_8, OptimizerStudy/lion_130m_1, OptimizerStudy/lion_130m_2, OptimizerStudy/lion_130m_4, OptimizerStudy/lion_130m_8, OptimizerStudy/lion_300m_1, OptimizerStudy/lion_300m_2, OptimizerStudy/lion_300m_4, OptimizerStudy/lion_300m_8, OptimizerStudy/lion_520m_1, OptimizerStudy/lion_520m_2, OptimizerStudy/lion_520m_4, OptimizerStudy/lion_520m_8, OptimizerStudy/mars_130m_1, OptimizerStudy/mars_130m_2, OptimizerStudy/mars_130m_4, OptimizerStudy/mars_130m_8, OptimizerStudy/mars_300m_1, OptimizerStudy/mars_300m_2, OptimizerStudy/mars_300m_4, OptimizerStudy/mars_300m_8, OptimizerStudy/mars_520m_1, OptimizerStudy/mars_520m_2, OptimizerStudy/mars_520m_4, OptimizerStudy/mars_520m_8, OptimizerStudy/mini_130m_1, OptimizerStudy/mini_130m_2, OptimizerStudy/mini_130m_4, OptimizerStudy/mini_130m_8, OptimizerStudy/mini_300m_1, OptimizerStudy/mini_300m_2, OptimizerStudy/mini_300m_4, OptimizerStudy/mini_300m_8, OptimizerStudy/mini_520m_1, OptimizerStudy/mini_520m_2, OptimizerStudy/mini_520m_4, OptimizerStudy/mini_520m_8, OptimizerStudy/muon_1.2b_1, OptimizerStudy/muon_1.2b_2, OptimizerStudy/muon_1.2b_4, OptimizerStudy/muon_1.2b_8, OptimizerStudy/muon_130m_1, OptimizerStudy/muon_130m_16, OptimizerStudy/muon_130m_2, OptimizerStudy/muon_130m_4, OptimizerStudy/muon_130m_8, OptimizerStudy/muon_300m_1, OptimizerStudy/muon_300m_2, OptimizerStudy/muon_300m_4, OptimizerStudy/muon_300m_8, OptimizerStudy/muon_520m_1, OptimizerStudy/muon_520m_2, OptimizerStudy/muon_520m_4, OptimizerStudy/muon_520m_8, OptimizerStudy/nadamw_1.2b_1, OptimizerStudy/nadamw_1.2b_2, OptimizerStudy/nadamw_1.2b_4, OptimizerStudy/nadamw_1.2b_8, OptimizerStudy/nadamw_130m_1, OptimizerStudy/nadamw_130m_16, OptimizerStudy/nadamw_130m_2, OptimizerStudy/nadamw_130m_4, OptimizerStudy/nadamw_130m_8, OptimizerStudy/nadamw_300m_1, OptimizerStudy/nadamw_300m_16, OptimizerStudy/nadamw_300m_2, OptimizerStudy/nadamw_300m_4, OptimizerStudy/nadamw_300m_8, OptimizerStudy/nadamw_520m_1, OptimizerStudy/nadamw_520m_2, OptimizerStudy/nadamw_520m_4, OptimizerStudy/nadamw_520m_8, OptimizerStudy/scion_130m_1, OptimizerStudy/scion_130m_2, OptimizerStudy/scion_130m_4, OptimizerStudy/scion_130m_8, OptimizerStudy/scion_300m_1, OptimizerStudy/scion_300m_2, OptimizerStudy/scion_300m_4, OptimizerStudy/scion_300m_8, OptimizerStudy/scion_520m_1, OptimizerStudy/scion_520m_2, OptimizerStudy/scion_520m_4, OptimizerStudy/scion_520m_8, OptimizerStudy/soape_1.2b_1, OptimizerStudy/soape_1.2b_2, OptimizerStudy/soape_1.2b_4, OptimizerStudy/soape_1.2b_8, OptimizerStudy/soape_130m_1, OptimizerStudy/soape_130m_16, OptimizerStudy/soape_130m_8, OptimizerStudy/soape_300m_1, OptimizerStudy/soape_300m_16, OptimizerStudy/soape_300m_2, OptimizerStudy/soape_300m_4, OptimizerStudy/soape_300m_8, OptimizerStudy/soape_520m_1, OptimizerStudy/soape_520m_2, OptimizerStudy/soape_520m_4, OptimizerStudy/soape_520m_8, OptimizerStudy/sophia_130m_1, OptimizerStudy/sophia_130m_2, OptimizerStudy/sophia_130m_4, OptimizerStudy/sophia_130m_8, OptimizerStudy/sophia_300m_1, OptimizerStudy/sophia_520m_1",
    "models_links": "https://huggingface.co/OptimizerStudy/adamw_1.2b_1, https://huggingface.co/OptimizerStudy/adamw_1.2b_2, https://huggingface.co/OptimizerStudy/adamw_1.2b_4, https://huggingface.co/OptimizerStudy/adamw_1.2b_8, https://huggingface.co/OptimizerStudy/adamw_130m_1, https://huggingface.co/OptimizerStudy/adamw_130m_16, https://huggingface.co/OptimizerStudy/adamw_130m_2, https://huggingface.co/OptimizerStudy/adamw_130m_4, https://huggingface.co/OptimizerStudy/adamw_130m_8, https://huggingface.co/OptimizerStudy/adamw_300m_1, https://huggingface.co/OptimizerStudy/adamw_300m_16, https://huggingface.co/OptimizerStudy/adamw_300m_2, https://huggingface.co/OptimizerStudy/adamw_300m_4, https://huggingface.co/OptimizerStudy/adamw_300m_8, https://huggingface.co/OptimizerStudy/adamw_520m_1, https://huggingface.co/OptimizerStudy/adamw_520m_2, https://huggingface.co/OptimizerStudy/adamw_520m_4, https://huggingface.co/OptimizerStudy/adamw_520m_8, https://huggingface.co/OptimizerStudy/cautious_130m_1, https://huggingface.co/OptimizerStudy/cautious_130m_2, https://huggingface.co/OptimizerStudy/cautious_130m_4, https://huggingface.co/OptimizerStudy/cautious_130m_8, https://huggingface.co/OptimizerStudy/cautious_300m_1, https://huggingface.co/OptimizerStudy/cautious_300m_2, https://huggingface.co/OptimizerStudy/cautious_300m_4, https://huggingface.co/OptimizerStudy/cautious_300m_8, https://huggingface.co/OptimizerStudy/cautious_520m_1, https://huggingface.co/OptimizerStudy/cautious_520m_2, https://huggingface.co/OptimizerStudy/cautious_520m_4, https://huggingface.co/OptimizerStudy/cautious_520m_8, https://huggingface.co/OptimizerStudy/kron_130m_1, https://huggingface.co/OptimizerStudy/kron_130m_2, https://huggingface.co/OptimizerStudy/kron_130m_4, https://huggingface.co/OptimizerStudy/kron_130m_8, https://huggingface.co/OptimizerStudy/kron_300m_1, https://huggingface.co/OptimizerStudy/kron_300m_2, https://huggingface.co/OptimizerStudy/kron_300m_4, https://huggingface.co/OptimizerStudy/kron_300m_8, https://huggingface.co/OptimizerStudy/kron_520m_1, https://huggingface.co/OptimizerStudy/kron_520m_2, https://huggingface.co/OptimizerStudy/kron_520m_4, https://huggingface.co/OptimizerStudy/kron_520m_8, https://huggingface.co/OptimizerStudy/lion_130m_1, https://huggingface.co/OptimizerStudy/lion_130m_2, https://huggingface.co/OptimizerStudy/lion_130m_4, https://huggingface.co/OptimizerStudy/lion_130m_8, https://huggingface.co/OptimizerStudy/lion_300m_1, https://huggingface.co/OptimizerStudy/lion_300m_2, https://huggingface.co/OptimizerStudy/lion_300m_4, https://huggingface.co/OptimizerStudy/lion_300m_8, https://huggingface.co/OptimizerStudy/lion_520m_1, https://huggingface.co/OptimizerStudy/lion_520m_2, https://huggingface.co/OptimizerStudy/lion_520m_4, https://huggingface.co/OptimizerStudy/lion_520m_8, https://huggingface.co/OptimizerStudy/mars_130m_1, https://huggingface.co/OptimizerStudy/mars_130m_2, https://huggingface.co/OptimizerStudy/mars_130m_4, https://huggingface.co/OptimizerStudy/mars_130m_8, https://huggingface.co/OptimizerStudy/mars_300m_1, https://huggingface.co/OptimizerStudy/mars_300m_2, https://huggingface.co/OptimizerStudy/mars_300m_4, https://huggingface.co/OptimizerStudy/mars_300m_8, https://huggingface.co/OptimizerStudy/mars_520m_1, https://huggingface.co/OptimizerStudy/mars_520m_2, https://huggingface.co/OptimizerStudy/mars_520m_4, https://huggingface.co/OptimizerStudy/mars_520m_8, https://huggingface.co/OptimizerStudy/mini_130m_1, https://huggingface.co/OptimizerStudy/mini_130m_2, https://huggingface.co/OptimizerStudy/mini_130m_4, https://huggingface.co/OptimizerStudy/mini_130m_8, https://huggingface.co/OptimizerStudy/mini_300m_1, https://huggingface.co/OptimizerStudy/mini_300m_2, https://huggingface.co/OptimizerStudy/mini_300m_4, https://huggingface.co/OptimizerStudy/mini_300m_8, https://huggingface.co/OptimizerStudy/mini_520m_1, https://huggingface.co/OptimizerStudy/mini_520m_2, https://huggingface.co/OptimizerStudy/mini_520m_4, https://huggingface.co/OptimizerStudy/mini_520m_8, https://huggingface.co/OptimizerStudy/muon_1.2b_1, https://huggingface.co/OptimizerStudy/muon_1.2b_2, https://huggingface.co/OptimizerStudy/muon_1.2b_4, https://huggingface.co/OptimizerStudy/muon_1.2b_8, https://huggingface.co/OptimizerStudy/muon_130m_1, https://huggingface.co/OptimizerStudy/muon_130m_16, https://huggingface.co/OptimizerStudy/muon_130m_2, https://huggingface.co/OptimizerStudy/muon_130m_4, https://huggingface.co/OptimizerStudy/muon_130m_8, https://huggingface.co/OptimizerStudy/muon_300m_1, https://huggingface.co/OptimizerStudy/muon_300m_2, https://huggingface.co/OptimizerStudy/muon_300m_4, https://huggingface.co/OptimizerStudy/muon_300m_8, https://huggingface.co/OptimizerStudy/muon_520m_1, https://huggingface.co/OptimizerStudy/muon_520m_2, https://huggingface.co/OptimizerStudy/muon_520m_4, https://huggingface.co/OptimizerStudy/muon_520m_8, https://huggingface.co/OptimizerStudy/nadamw_1.2b_1, https://huggingface.co/OptimizerStudy/nadamw_1.2b_2, https://huggingface.co/OptimizerStudy/nadamw_1.2b_4, https://huggingface.co/OptimizerStudy/nadamw_1.2b_8, https://huggingface.co/OptimizerStudy/nadamw_130m_1, https://huggingface.co/OptimizerStudy/nadamw_130m_16, https://huggingface.co/OptimizerStudy/nadamw_130m_2, https://huggingface.co/OptimizerStudy/nadamw_130m_4, https://huggingface.co/OptimizerStudy/nadamw_130m_8, https://huggingface.co/OptimizerStudy/nadamw_300m_1, https://huggingface.co/OptimizerStudy/nadamw_300m_16, https://huggingface.co/OptimizerStudy/nadamw_300m_2, https://huggingface.co/OptimizerStudy/nadamw_300m_4, https://huggingface.co/OptimizerStudy/nadamw_300m_8, https://huggingface.co/OptimizerStudy/nadamw_520m_1, https://huggingface.co/OptimizerStudy/nadamw_520m_2, https://huggingface.co/OptimizerStudy/nadamw_520m_4, https://huggingface.co/OptimizerStudy/nadamw_520m_8, https://huggingface.co/OptimizerStudy/scion_130m_1, https://huggingface.co/OptimizerStudy/scion_130m_2, https://huggingface.co/OptimizerStudy/scion_130m_4, https://huggingface.co/OptimizerStudy/scion_130m_8, https://huggingface.co/OptimizerStudy/scion_300m_1, https://huggingface.co/OptimizerStudy/scion_300m_2, https://huggingface.co/OptimizerStudy/scion_300m_4, https://huggingface.co/OptimizerStudy/scion_300m_8, https://huggingface.co/OptimizerStudy/scion_520m_1, https://huggingface.co/OptimizerStudy/scion_520m_2, https://huggingface.co/OptimizerStudy/scion_520m_4, https://huggingface.co/OptimizerStudy/scion_520m_8, https://huggingface.co/OptimizerStudy/soape_1.2b_1, https://huggingface.co/OptimizerStudy/soape_1.2b_2, https://huggingface.co/OptimizerStudy/soape_1.2b_4, https://huggingface.co/OptimizerStudy/soape_1.2b_8, https://huggingface.co/OptimizerStudy/soape_130m_1, https://huggingface.co/OptimizerStudy/soape_130m_16, https://huggingface.co/OptimizerStudy/soape_130m_8, https://huggingface.co/OptimizerStudy/soape_300m_1, https://huggingface.co/OptimizerStudy/soape_300m_16, https://huggingface.co/OptimizerStudy/soape_300m_2, https://huggingface.co/OptimizerStudy/soape_300m_4, https://huggingface.co/OptimizerStudy/soape_300m_8, https://huggingface.co/OptimizerStudy/soape_520m_1, https://huggingface.co/OptimizerStudy/soape_520m_2, https://huggingface.co/OptimizerStudy/soape_520m_4, https://huggingface.co/OptimizerStudy/soape_520m_8, https://huggingface.co/OptimizerStudy/sophia_130m_1, https://huggingface.co/OptimizerStudy/sophia_130m_2, https://huggingface.co/OptimizerStudy/sophia_130m_4, https://huggingface.co/OptimizerStudy/sophia_130m_8, https://huggingface.co/OptimizerStudy/sophia_300m_1, https://huggingface.co/OptimizerStudy/sophia_520m_1",
    "models_detailed": "[{\"name\": \"OptimizerStudy/adamw_1.2b_1\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_1.2b_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_1.2b_2\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_1.2b_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_1.2b_4\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_1.2b_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_1.2b_8\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_1.2b_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_130m_16\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_130m_16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_300m_16\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_300m_16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/adamw_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/adamw_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/cautious_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/cautious_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/kron_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/kron_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/lion_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/lion_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mars_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/mars_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/mini_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/mini_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_1.2b_1\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_1.2b_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_1.2b_2\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_1.2b_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_1.2b_4\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_1.2b_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_1.2b_8\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_1.2b_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_130m_16\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_130m_16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"OptimizerStudy/muon_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/muon_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/muon_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_1.2b_1\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_1.2b_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_1.2b_2\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_1.2b_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_1.2b_4\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_1.2b_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_1.2b_8\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_1.2b_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_130m_16\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_130m_16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_300m_16\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_300m_16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/nadamw_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/nadamw_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/scion_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/scion_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_1.2b_1\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_1.2b_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_1.2b_2\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_1.2b_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_1.2b_4\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_1.2b_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_1.2b_8\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_1.2b_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_130m_16\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_130m_16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_300m_16\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_300m_16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_300m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_300m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_300m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_300m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_300m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_300m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_520m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_520m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_520m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_520m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/soape_520m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/soape_520m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/sophia_130m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/sophia_130m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/sophia_130m_2\", \"link\": \"https://huggingface.co/OptimizerStudy/sophia_130m_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/sophia_130m_4\", \"link\": \"https://huggingface.co/OptimizerStudy/sophia_130m_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/sophia_130m_8\", \"link\": \"https://huggingface.co/OptimizerStudy/sophia_130m_8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/sophia_300m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/sophia_300m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"OptimizerStudy/sophia_520m_1\", \"link\": \"https://huggingface.co/OptimizerStudy/sophia_520m_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.20751",
    "first_seen_date": "2025-08-29",
    "title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.20751Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement LearningPublished on Aug 28\u00b7Submitted bySII-Yibin Wangon Aug 29Upvote89+81Authors:Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi WangAbstractPref-GRPO, a pairwise preference reward-based GRPO method, enhances text-to-image generation by mitigating reward hacking and improving stability, while UniGenBench provides a comprehensive benchmark for evaluating T2I models.AI-generated summaryRecent advancements highlight the importance ofGRPO-based reinforcement\nlearning methods and benchmarking in enhancingtext-to-image(T2I) generation.\nHowever, current methods usingpointwise reward models(RM) for scoring\ngenerated images are susceptible toreward hacking. We reveal that this happens\nwhen minimal score differences between images are amplified after\nnormalization, creating illusory advantages that drive the model to\nover-optimize for trivial gains, ultimately destabilizing the image generation\nprocess. To address this, we propose Pref-GRPO, apairwise preferencereward-basedGRPOmethod that shifts the optimization objective from score\nmaximization topreference fitting, ensuring more stable training. In\nPref-GRPO, images are pairwise compared within each group using preference RM,\nand thewin rateis used as the reward signal. Extensive experiments\ndemonstrate that PREF-GRPOdifferentiates subtle image quality differences,\nproviding more stable advantages and mitigatingreward hacking. Additionally,\nexisting T2I benchmarks are limited by coarse evaluation criteria, hindering\ncomprehensive model assessment. To solve this, we introduceUniGenBench, a\nunified T2I benchmark comprising 600 prompts across 5 main themes and 20\nsubthemes. It evaluatessemantic consistencythrough 10 primary and 27\nsub-criteria, leveragingMLLMfor benchmark construction and evaluation",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/CodeGoat24/Pref-GRPO",
    "hf_paper_url": "https://huggingface.co/papers/2508.20751",
    "arxiv_url": "https://arxiv.org/abs/2508.20751",
    "num_models": 3,
    "models_list": "CodeGoat24/FLUX.1-dev-PrefGRPO, CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1, CodeGoat24/UniGenBench-EvalModel-qwen3vl-32b-v1",
    "models_links": "https://huggingface.co/CodeGoat24/FLUX.1-dev-PrefGRPO, https://huggingface.co/CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1, https://huggingface.co/CodeGoat24/UniGenBench-EvalModel-qwen3vl-32b-v1",
    "models_detailed": "[{\"name\": \"CodeGoat24/FLUX.1-dev-PrefGRPO\", \"link\": \"https://huggingface.co/CodeGoat24/FLUX.1-dev-PrefGRPO\", \"task\": \"\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1\", \"link\": \"https://huggingface.co/CodeGoat24/UniGenBench-EvalModel-qwen-72b-v1\", \"task\": \"\", \"likes\": \"142\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"CodeGoat24/UniGenBench-EvalModel-qwen3vl-32b-v1\", \"link\": \"https://huggingface.co/CodeGoat24/UniGenBench-EvalModel-qwen3vl-32b-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}]",
    "num_datasets": 2,
    "datasets_list": "CodeGoat24/UniGenBench-Eval-Images, CodeGoat24/UniGenBench",
    "datasets_links": "https://huggingface.co/datasets/CodeGoat24/UniGenBench-Eval-Images, https://huggingface.co/datasets/CodeGoat24/UniGenBench",
    "datasets_detailed": "[{\"name\": \"CodeGoat24/UniGenBench-Eval-Images\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/UniGenBench-Eval-Images\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\", \"size\": \"\"}, {\"name\": \"CodeGoat24/UniGenBench\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/UniGenBench\", \"task\": \"\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"Oct 25\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.17445",
    "first_seen_date": "2025-08-27",
    "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.17445TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based ModelingPublished on Aug 24\u00b7Submitted byGe Zhangon Aug 27#2 Paper of the day\u00b7ByteDance SeedUpvote80+72Authors:Yizhi Li,Qingshui Gu,Zhoufutu Wen,Ziniu Li,Tianshun Xing,Shuyue Guo,Tianyu Zheng,Xin Zhou,Xingwei Qu,Wangchunshu Zhou,Zheng Zhang,Wei Shen,Qian Liu,Chenghua Lin,Jian Yang,Ge Zhang,Wenhao HuangAbstractTreePO, a self-guided rollout algorithm for sequence generation, reduces computational cost and enhances exploration diversity in reinforcement learning for large language models.AI-generated summaryRecent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that viewssequence generationas atree-structured searchingprocess. Composed ofdynamic tree sampling policyandfixed-length segment decoding, TreePO leverageslocal uncertaintyto warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) asegment-wise samplingalgorithm that alleviates\ntheKV cache burdenthrough contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and localproximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/multimodal-art-projection/TreePO",
    "hf_paper_url": "https://huggingface.co/papers/2508.17445",
    "arxiv_url": "https://arxiv.org/abs/2508.17445",
    "num_models": 2,
    "models_list": "m-a-p/TreePO-Qwen2.5-7B, m-a-p/TreePO-Qwen2.5-7B_fixed-div",
    "models_links": "https://huggingface.co/m-a-p/TreePO-Qwen2.5-7B, https://huggingface.co/m-a-p/TreePO-Qwen2.5-7B_fixed-div",
    "models_detailed": "[{\"name\": \"m-a-p/TreePO-Qwen2.5-7B\", \"link\": \"https://huggingface.co/m-a-p/TreePO-Qwen2.5-7B\", \"task\": \"Text Generation\", \"likes\": \"61\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"m-a-p/TreePO-Qwen2.5-7B_fixed-div\", \"link\": \"https://huggingface.co/m-a-p/TreePO-Qwen2.5-7B_fixed-div\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 31\"}]",
    "num_datasets": 1,
    "datasets_list": "m-a-p/TreePO_data",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/TreePO_data",
    "datasets_detailed": "[{\"name\": \"m-a-p/TreePO_data\", \"link\": \"https://huggingface.co/datasets/m-a-p/TreePO_data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.18621",
    "first_seen_date": "2025-08-27",
    "title": "Wan-S2V: Audio-Driven Cinematic Video Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.18621Wan-S2V: Audio-Driven Cinematic Video GenerationPublished on Aug 26\u00b7Submitted bytaesirion Aug 27Upvote20+12Authors:Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou+1 authorsAbstractWan-S2V, an audio-driven model built on Wan, enhances expressiveness and fidelity in cinematic character animation compared to existing methods.AI-generated summaryCurrent state-of-the-art (SOTA) methods for audio-driven character animation\ndemonstrate promising performance for scenarios primarily involving speech and\nsinging. However, they often fall short in more complex film and television\nproductions, which demand sophisticated elements such as nuanced character\ninteractions, realistic body movements, and dynamic camera work. To address\nthis long-standing challenge of achieving film-level character animation, we\npropose anaudio-driven model, which we refere to asWan-S2V, built uponWan.\nOur model achieves significantly enhanced expressiveness and fidelity in\ncinematic contexts compared to existing approaches. We conducted extensive\nexperiments, benchmarking our method against cutting-edge models such asHunyuan-AvatarandOmnihuman. The experimental results consistently demonstrate\nthat our approach significantly outperforms these existing solutions.\nAdditionally, we explore the versatility of our method through its applications\ninlong-form video generationand precisevideo lip-sync editing.View arXiv pageView PDFAdd to collectionCommunitytaesiriPaper submitterAug 27Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, w",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2508.18621",
    "arxiv_url": "https://arxiv.org/abs/2508.18621",
    "num_models": 1,
    "models_list": "Wan-AI/Wan2.2-S2V-14B",
    "models_links": "https://huggingface.co/Wan-AI/Wan2.2-S2V-14B",
    "models_detailed": "[{\"name\": \"Wan-AI/Wan2.2-S2V-14B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-S2V-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.18672",
    "first_seen_date": "2025-08-27",
    "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.18672Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  TasksPublished on Aug 26\u00b7Submitted byTaishion Aug 27Upvote10+2Authors:Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio YokotaAbstractMoE models introduce sparsity that affects memorization and reasoning capabilities differently in large language models, with reasoning performance potentially regressing despite increased parameters.AI-generated summaryEmpirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes.Mixture-of-Experts(MoE) models, now standard in\nstate-of-the-art systems, introduce a newsparsitydimension that current\ndense-model frontiers overlook. We investigate howMoEsparsityinfluences two\ndistinct capability regimes:memorizationandreasoning. We train families ofMoETransformersthat systematically vary total parameters,active parameters,\nand top-k routing while holding the compute budget fixed. For every model we\nrecordpre-training loss,downstream task loss, and task accuracy, allowing us\nto separate the train-testgeneralization gapfrom theloss-accuracy gap.Memorizationbenchmarks improve monotonically with total parameters, mirroring\ntraining loss. By contrast,reasoningperformance saturates and can even\nregress despite continued gains in both total parameters and training loss.\nAltering top-k alone has little effect whenactive parametersare constant,\nand classic hyperparameters such aslearning rateandinitializationmodulate\nthegeneralization gapin the same direction assparsity. Neither post-training\nreinforcement learning (GRPO) nor extra test-time compute rescues thereasoningdeficit of overly sparse models. Our model checkpoints, code and logs are\nopen-source at https://github.com/rioyokotalab/optimal-sparsity.View arXiv pageView PDFGitHub5a",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/rioyokotalab/optimal-sparsity",
    "hf_paper_url": "https://huggingface.co/papers/2508.18672",
    "arxiv_url": "https://arxiv.org/abs/2508.18672",
    "num_models": 130,
    "models_list": "llm-jp/optimal-sparsity-math-d512-E8-k2-320M-A170M, llm-jp/optimal-sparsity-math-d512-E16-k2-520M-A170M, llm-jp/optimal-sparsity-math-d2048-E8-k2-3.9B-A1.5B, llm-jp/optimal-sparsity-math-d2048-E16-k2-7.1B-A1.5B, llm-jp/optimal-sparsity-math-d2048-E32-k2-13.6B-A1.5B, llm-jp/optimal-sparsity-math-d2048-E16-k16-7.1B-A7.1B, llm-jp/optimal-sparsity-math-d2048-E8-k8-3.9B-A3.9B, llm-jp/optimal-sparsity-math-d2048-E64-k2-26.4B-A1.5B, llm-jp/optimal-sparsity-math-d2048-E32-k16-13.6B-A7.1B, llm-jp/optimal-sparsity-math-d2048-E16-k8-7.1B-A3.9B, llm-jp/optimal-sparsity-math-d1024-E16-k16-1.9B-A1.9B, llm-jp/optimal-sparsity-math-d2048-E32-k8-13.6B-A3.9B, llm-jp/optimal-sparsity-math-d2048-E64-k16-26.4B-A7.1B, llm-jp/optimal-sparsity-math-d1024-E32-k16-3.5B-A1.9B, llm-jp/optimal-sparsity-math-d2048-E128-k2-52.2B-A1.5B, llm-jp/optimal-sparsity-math-d1024-E64-k16-6.7B-A1.9B, llm-jp/optimal-sparsity-math-d2048-E64-k8-26.4B-A3.9B, llm-jp/optimal-sparsity-math-d1024-E128-k16-13.2B-A1.9B, llm-jp/optimal-sparsity-math-d512-E32-k2-920M-A170M, llm-jp/optimal-sparsity-math-d1024-E8-k2-1.1B-A470M, llm-jp/optimal-sparsity-math-d512-E64-k2-1.7B-A170M, llm-jp/optimal-sparsity-math-d2048-E128-k16-52.2B-A7.1B, llm-jp/optimal-sparsity-math-d512-E8-k4-320M-A220M, llm-jp/optimal-sparsity-math-d1024-E16-k2-1.9B-A470M, llm-jp/optimal-sparsity-math-d512-E16-k4-520M-A220M, llm-jp/optimal-sparsity-math-d512-E8-k8-320M-A320M, llm-jp/optimal-sparsity-math-d512-E16-k8-520M-A320M, llm-jp/optimal-sparsity-math-d512-E16-k16-520M-A520M, llm-jp/optimal-sparsity-math-d512-E32-k4-920M-A220M, llm-jp/optimal-sparsity-math-d512-E128-k2-3.3B-A170M, llm-jp/optimal-sparsity-math-d512-E32-k8-920M-A320M, llm-jp/optimal-sparsity-math-d512-E32-k16-920M-A520M, llm-jp/optimal-sparsity-math-d1024-E256-k16-26.0B-A1.9B, llm-jp/optimal-sparsity-math-d512-E64-k4-1.7B-A220M, llm-jp/optimal-sparsity-math-d1024-E32-k2-3.5B-A470M, llm-jp/optimal-sparsity-math-d512-E64-k8-1.7B-A320M, llm-jp/optimal-sparsity-math-d512-E64-k16-1.7B-A520M, llm-jp/optimal-sparsity-math-d2048-E128-k8-52.2B-A3.9B, llm-jp/optimal-sparsity-math-d512-E128-k4-3.3B-A220M, llm-jp/optimal-sparsity-math-d512-E256-k2-6.6B-A170M, llm-jp/optimal-sparsity-math-d512-E128-k8-3.3B-A320M, llm-jp/optimal-sparsity-math-d512-E128-k16-3.3B-A520M, llm-jp/optimal-sparsity-math-d1024-E64-k2-6.7B-A470M, llm-jp/optimal-sparsity-math-d512-E256-k4-6.6B-A220M, llm-jp/optimal-sparsity-math-d512-E256-k8-6.6B-A320M, llm-jp/optimal-sparsity-math-d512-E256-k16-6.6B-A520M, llm-jp/optimal-sparsity-math-d1024-E128-k2-13.2B-A470M, llm-jp/optimal-sparsity-math-d1024-E256-k2-26.0B-A470M, llm-jp/optimal-sparsity-math-d1024-E8-k4-1.1B-A670M, llm-jp/optimal-sparsity-math-d1024-E128-k4-13.2B-A670M, llm-jp/optimal-sparsity-math-d1024-E16-k4-1.9B-A670M, llm-jp/optimal-sparsity-math-d2048-E8-k4-3.9B-A2.3B, llm-jp/optimal-sparsity-math-d1024-E32-k4-3.5B-A670M, llm-jp/optimal-sparsity-math-d1024-E8-k8-1.1B-A1.1B, llm-jp/optimal-sparsity-math-d1024-E256-k4-26.0B-A670M, llm-jp/optimal-sparsity-math-d1024-E16-k8-1.9B-A1.1B, llm-jp/optimal-sparsity-math-d2048-E16-k4-7.1B-A2.3B, llm-jp/optimal-sparsity-math-d1024-E64-k4-6.7B-A670M, llm-jp/optimal-sparsity-math-d1024-E32-k8-3.5B-A1.1B, llm-jp/optimal-sparsity-math-d2048-E32-k4-13.6B-A2.3B, llm-jp/optimal-sparsity-math-d1024-E64-k8-6.7B-A1.1B, llm-jp/optimal-sparsity-math-d2048-E64-k4-26.4B-A2.3B, llm-jp/optimal-sparsity-math-d2048-E128-k4-52.2B-A2.3B, llm-jp/optimal-sparsity-math-d1024-E128-k8-13.2B-A1.1B, llm-jp/optimal-sparsity-math-d1024-E256-k8-26.0B-A1.1B, llm-jp/optimal-sparsity-code-d512-E8-k2-320M-A170M, llm-jp/optimal-sparsity-code-d512-E16-k2-520M-A170M, llm-jp/optimal-sparsity-code-d512-E32-k2-920M-A170M, llm-jp/optimal-sparsity-code-d512-E64-k2-1.7B-A170M, llm-jp/optimal-sparsity-code-d512-E128-k2-3.3B-A170M, llm-jp/optimal-sparsity-code-d512-E256-k2-6.6B-A170M, llm-jp/optimal-sparsity-code-d512-E8-k4-320M-A220M, llm-jp/optimal-sparsity-code-d512-E16-k4-520M-A220M, llm-jp/optimal-sparsity-code-d512-E32-k4-920M-A220M, llm-jp/optimal-sparsity-code-d512-E64-k4-1.7B-A220M, llm-jp/optimal-sparsity-code-d512-E128-k4-3.3B-A220M, llm-jp/optimal-sparsity-code-d512-E256-k4-6.6B-A220M, llm-jp/optimal-sparsity-code-d512-E8-k8-320M-A320M, llm-jp/optimal-sparsity-code-d512-E16-k8-520M-A320M, llm-jp/optimal-sparsity-code-d512-E32-k8-920M-A320M, llm-jp/optimal-sparsity-code-d512-E64-k8-1.7B-A320M, llm-jp/optimal-sparsity-code-d512-E128-k8-3.3B-A320M, llm-jp/optimal-sparsity-code-d512-E256-k8-6.6B-A320M, llm-jp/optimal-sparsity-code-d512-E16-k16-520M-A520M, llm-jp/optimal-sparsity-code-d512-E32-k16-920M-A520M, llm-jp/optimal-sparsity-code-d512-E64-k16-1.7B-A520M, llm-jp/optimal-sparsity-code-d512-E128-k16-3.3B-A520M, llm-jp/optimal-sparsity-code-d512-E256-k16-6.6B-A520M, llm-jp/optimal-sparsity-code-d1024-E8-k2-1.1B-A470M, llm-jp/optimal-sparsity-code-d1024-E16-k2-1.9B-A470M, llm-jp/optimal-sparsity-code-d1024-E32-k2-3.5B-A470M, llm-jp/optimal-sparsity-code-d1024-E64-k2-6.7B-A470M, llm-jp/optimal-sparsity-code-d1024-E128-k2-13.2B-A470M, llm-jp/optimal-sparsity-code-d1024-E256-k2-26.0B-A470M, llm-jp/optimal-sparsity-code-d1024-E8-k4-1.1B-A670M, llm-jp/optimal-sparsity-code-d1024-E16-k4-1.9B-A670M, llm-jp/optimal-sparsity-code-d1024-E32-k4-3.5B-A670M, llm-jp/optimal-sparsity-code-d1024-E64-k4-6.7B-A670M, llm-jp/optimal-sparsity-code-d1024-E128-k4-13.2B-A670M, llm-jp/optimal-sparsity-code-d1024-E256-k4-26.0B-A670M, llm-jp/optimal-sparsity-code-d1024-E8-k8-1.1B-A1.1B, llm-jp/optimal-sparsity-code-d1024-E16-k8-1.9B-A1.1B, llm-jp/optimal-sparsity-code-d1024-E32-k8-3.5B-A1.1B, llm-jp/optimal-sparsity-code-d1024-E64-k8-6.7B-A1.1B, llm-jp/optimal-sparsity-code-d1024-E128-k8-13.2B-A1.1B, llm-jp/optimal-sparsity-code-d1024-E256-k8-26.0B-A1.1B, llm-jp/optimal-sparsity-code-d1024-E16-k16-1.9B-A1.9B, llm-jp/optimal-sparsity-code-d1024-E32-k16-3.5B-A1.9B, llm-jp/optimal-sparsity-code-d1024-E64-k16-6.7B-A1.9B, llm-jp/optimal-sparsity-code-d1024-E128-k16-13.2B-A1.9B, llm-jp/optimal-sparsity-code-d1024-E256-k16-26.0B-A1.9B, llm-jp/optimal-sparsity-code-d2048-E8-k2-3.9B-A1.5B, llm-jp/optimal-sparsity-code-d2048-E16-k2-7.1B-A1.5B, llm-jp/optimal-sparsity-code-d2048-E32-k2-13.6B-A1.5B, llm-jp/optimal-sparsity-code-d2048-E64-k2-26.4B-A1.5B, llm-jp/optimal-sparsity-code-d2048-E128-k2-52.2B-A1.5B, llm-jp/optimal-sparsity-code-d2048-E8-k4-3.9B-A2.3B, llm-jp/optimal-sparsity-code-d2048-E16-k4-7.1B-A2.3B, llm-jp/optimal-sparsity-code-d2048-E32-k4-13.6B-A2.3B, llm-jp/optimal-sparsity-code-d2048-E64-k4-26.4B-A2.3B, llm-jp/optimal-sparsity-code-d2048-E128-k4-52.2B-A2.3B, llm-jp/optimal-sparsity-code-d2048-E8-k8-3.9B-A3.9B, llm-jp/optimal-sparsity-code-d2048-E16-k8-7.1B-A3.9B, llm-jp/optimal-sparsity-code-d2048-E32-k8-13.6B-A3.9B, llm-jp/optimal-sparsity-code-d2048-E64-k8-26.4B-A3.9B, llm-jp/optimal-sparsity-code-d2048-E128-k8-52.2B-A3.9B, llm-jp/optimal-sparsity-code-d2048-E16-k16-7.1B-A7.1B, llm-jp/optimal-sparsity-code-d2048-E32-k16-13.6B-A7.1B, llm-jp/optimal-sparsity-code-d2048-E64-k16-26.4B-A7.1B, llm-jp/optimal-sparsity-code-d2048-E128-k16-52.2B-A7.1B",
    "models_links": "https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E8-k2-320M-A170M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E16-k2-520M-A170M, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E8-k2-3.9B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E16-k2-7.1B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E32-k2-13.6B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E16-k16-7.1B-A7.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E8-k8-3.9B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E64-k2-26.4B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E32-k16-13.6B-A7.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E16-k8-7.1B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E16-k16-1.9B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E32-k8-13.6B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E64-k16-26.4B-A7.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E32-k16-3.5B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E128-k2-52.2B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E64-k16-6.7B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E64-k8-26.4B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E128-k16-13.2B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E32-k2-920M-A170M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E8-k2-1.1B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E64-k2-1.7B-A170M, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E128-k16-52.2B-A7.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E8-k4-320M-A220M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E16-k2-1.9B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E16-k4-520M-A220M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E8-k8-320M-A320M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E16-k8-520M-A320M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E16-k16-520M-A520M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E32-k4-920M-A220M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E128-k2-3.3B-A170M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E32-k8-920M-A320M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E32-k16-920M-A520M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E256-k16-26.0B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E64-k4-1.7B-A220M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E32-k2-3.5B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E64-k8-1.7B-A320M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E64-k16-1.7B-A520M, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E128-k8-52.2B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E128-k4-3.3B-A220M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E256-k2-6.6B-A170M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E128-k8-3.3B-A320M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E128-k16-3.3B-A520M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E64-k2-6.7B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E256-k4-6.6B-A220M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E256-k8-6.6B-A320M, https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E256-k16-6.6B-A520M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E128-k2-13.2B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E256-k2-26.0B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E8-k4-1.1B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E128-k4-13.2B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E16-k4-1.9B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E8-k4-3.9B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E32-k4-3.5B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E8-k8-1.1B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E256-k4-26.0B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E16-k8-1.9B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E16-k4-7.1B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E64-k4-6.7B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E32-k8-3.5B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E32-k4-13.6B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E64-k8-6.7B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E64-k4-26.4B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E128-k4-52.2B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E128-k8-13.2B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E256-k8-26.0B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E8-k2-320M-A170M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E16-k2-520M-A170M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E32-k2-920M-A170M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E64-k2-1.7B-A170M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E128-k2-3.3B-A170M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E256-k2-6.6B-A170M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E8-k4-320M-A220M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E16-k4-520M-A220M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E32-k4-920M-A220M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E64-k4-1.7B-A220M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E128-k4-3.3B-A220M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E256-k4-6.6B-A220M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E8-k8-320M-A320M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E16-k8-520M-A320M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E32-k8-920M-A320M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E64-k8-1.7B-A320M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E128-k8-3.3B-A320M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E256-k8-6.6B-A320M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E16-k16-520M-A520M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E32-k16-920M-A520M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E64-k16-1.7B-A520M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E128-k16-3.3B-A520M, https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E256-k16-6.6B-A520M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E8-k2-1.1B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E16-k2-1.9B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E32-k2-3.5B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E64-k2-6.7B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E128-k2-13.2B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E256-k2-26.0B-A470M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E8-k4-1.1B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E16-k4-1.9B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E32-k4-3.5B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E64-k4-6.7B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E128-k4-13.2B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E256-k4-26.0B-A670M, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E8-k8-1.1B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E16-k8-1.9B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E32-k8-3.5B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E64-k8-6.7B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E128-k8-13.2B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E256-k8-26.0B-A1.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E16-k16-1.9B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E32-k16-3.5B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E64-k16-6.7B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E128-k16-13.2B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E256-k16-26.0B-A1.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E8-k2-3.9B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E16-k2-7.1B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E32-k2-13.6B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E64-k2-26.4B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E128-k2-52.2B-A1.5B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E8-k4-3.9B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E16-k4-7.1B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E32-k4-13.6B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E64-k4-26.4B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E128-k4-52.2B-A2.3B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E8-k8-3.9B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E16-k8-7.1B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E32-k8-13.6B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E64-k8-26.4B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E128-k8-52.2B-A3.9B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E16-k16-7.1B-A7.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E32-k16-13.6B-A7.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E64-k16-26.4B-A7.1B, https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E128-k16-52.2B-A7.1B",
    "models_detailed": "[{\"name\": \"llm-jp/optimal-sparsity-math-d512-E8-k2-320M-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E8-k2-320M-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E16-k2-520M-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E16-k2-520M-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E8-k2-3.9B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E8-k2-3.9B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E16-k2-7.1B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E16-k2-7.1B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E32-k2-13.6B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E32-k2-13.6B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E16-k16-7.1B-A7.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E16-k16-7.1B-A7.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E8-k8-3.9B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E8-k8-3.9B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E64-k2-26.4B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E64-k2-26.4B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E32-k16-13.6B-A7.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E32-k16-13.6B-A7.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E16-k8-7.1B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E16-k8-7.1B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E16-k16-1.9B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E16-k16-1.9B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E32-k8-13.6B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E32-k8-13.6B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E64-k16-26.4B-A7.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E64-k16-26.4B-A7.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E32-k16-3.5B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E32-k16-3.5B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E128-k2-52.2B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E128-k2-52.2B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E64-k16-6.7B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E64-k16-6.7B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E64-k8-26.4B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E64-k8-26.4B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E128-k16-13.2B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E128-k16-13.2B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E32-k2-920M-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E32-k2-920M-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E8-k2-1.1B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E8-k2-1.1B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E64-k2-1.7B-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E64-k2-1.7B-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E128-k16-52.2B-A7.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E128-k16-52.2B-A7.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E8-k4-320M-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E8-k4-320M-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E16-k2-1.9B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E16-k2-1.9B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E16-k4-520M-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E16-k4-520M-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E8-k8-320M-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E8-k8-320M-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E16-k8-520M-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E16-k8-520M-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E16-k16-520M-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E16-k16-520M-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E32-k4-920M-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E32-k4-920M-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E128-k2-3.3B-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E128-k2-3.3B-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E32-k8-920M-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E32-k8-920M-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E32-k16-920M-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E32-k16-920M-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E256-k16-26.0B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E256-k16-26.0B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E64-k4-1.7B-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E64-k4-1.7B-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E32-k2-3.5B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E32-k2-3.5B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E64-k8-1.7B-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E64-k8-1.7B-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E64-k16-1.7B-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E64-k16-1.7B-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E128-k8-52.2B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E128-k8-52.2B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E128-k4-3.3B-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E128-k4-3.3B-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E256-k2-6.6B-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E256-k2-6.6B-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E128-k8-3.3B-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E128-k8-3.3B-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E128-k16-3.3B-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E128-k16-3.3B-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E64-k2-6.7B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E64-k2-6.7B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E256-k4-6.6B-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E256-k4-6.6B-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E256-k8-6.6B-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E256-k8-6.6B-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d512-E256-k16-6.6B-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d512-E256-k16-6.6B-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E128-k2-13.2B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E128-k2-13.2B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E256-k2-26.0B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E256-k2-26.0B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E8-k4-1.1B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E8-k4-1.1B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E128-k4-13.2B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E128-k4-13.2B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E16-k4-1.9B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E16-k4-1.9B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E8-k4-3.9B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E8-k4-3.9B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E32-k4-3.5B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E32-k4-3.5B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E8-k8-1.1B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E8-k8-1.1B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E256-k4-26.0B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E256-k4-26.0B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E16-k8-1.9B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E16-k8-1.9B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E16-k4-7.1B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E16-k4-7.1B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E64-k4-6.7B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E64-k4-6.7B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E32-k8-3.5B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E32-k8-3.5B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E32-k4-13.6B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E32-k4-13.6B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E64-k8-6.7B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E64-k8-6.7B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E64-k4-26.4B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E64-k4-26.4B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d2048-E128-k4-52.2B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d2048-E128-k4-52.2B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E128-k8-13.2B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E128-k8-13.2B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-math-d1024-E256-k8-26.0B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-math-d1024-E256-k8-26.0B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E8-k2-320M-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E8-k2-320M-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E16-k2-520M-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E16-k2-520M-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E32-k2-920M-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E32-k2-920M-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E64-k2-1.7B-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E64-k2-1.7B-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E128-k2-3.3B-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E128-k2-3.3B-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E256-k2-6.6B-A170M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E256-k2-6.6B-A170M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E8-k4-320M-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E8-k4-320M-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E16-k4-520M-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E16-k4-520M-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E32-k4-920M-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E32-k4-920M-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E64-k4-1.7B-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E64-k4-1.7B-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E128-k4-3.3B-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E128-k4-3.3B-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E256-k4-6.6B-A220M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E256-k4-6.6B-A220M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E8-k8-320M-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E8-k8-320M-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E16-k8-520M-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E16-k8-520M-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E32-k8-920M-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E32-k8-920M-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E64-k8-1.7B-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E64-k8-1.7B-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E128-k8-3.3B-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E128-k8-3.3B-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E256-k8-6.6B-A320M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E256-k8-6.6B-A320M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E16-k16-520M-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E16-k16-520M-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E32-k16-920M-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E32-k16-920M-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E64-k16-1.7B-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E64-k16-1.7B-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E128-k16-3.3B-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E128-k16-3.3B-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d512-E256-k16-6.6B-A520M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d512-E256-k16-6.6B-A520M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E8-k2-1.1B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E8-k2-1.1B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E16-k2-1.9B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E16-k2-1.9B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E32-k2-3.5B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E32-k2-3.5B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E64-k2-6.7B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E64-k2-6.7B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E128-k2-13.2B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E128-k2-13.2B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E256-k2-26.0B-A470M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E256-k2-26.0B-A470M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E8-k4-1.1B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E8-k4-1.1B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E16-k4-1.9B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E16-k4-1.9B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E32-k4-3.5B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E32-k4-3.5B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E64-k4-6.7B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E64-k4-6.7B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E128-k4-13.2B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E128-k4-13.2B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E256-k4-26.0B-A670M\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E256-k4-26.0B-A670M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E8-k8-1.1B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E8-k8-1.1B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E16-k8-1.9B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E16-k8-1.9B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E32-k8-3.5B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E32-k8-3.5B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E64-k8-6.7B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E64-k8-6.7B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E128-k8-13.2B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E128-k8-13.2B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E256-k8-26.0B-A1.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E256-k8-26.0B-A1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E16-k16-1.9B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E16-k16-1.9B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E32-k16-3.5B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E32-k16-3.5B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E64-k16-6.7B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E64-k16-6.7B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E128-k16-13.2B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E128-k16-13.2B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d1024-E256-k16-26.0B-A1.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d1024-E256-k16-26.0B-A1.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E8-k2-3.9B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E8-k2-3.9B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E16-k2-7.1B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E16-k2-7.1B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E32-k2-13.6B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E32-k2-13.6B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E64-k2-26.4B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E64-k2-26.4B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E128-k2-52.2B-A1.5B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E128-k2-52.2B-A1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E8-k4-3.9B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E8-k4-3.9B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E16-k4-7.1B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E16-k4-7.1B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E32-k4-13.6B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E32-k4-13.6B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E64-k4-26.4B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E64-k4-26.4B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E128-k4-52.2B-A2.3B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E128-k4-52.2B-A2.3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E8-k8-3.9B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E8-k8-3.9B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E16-k8-7.1B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E16-k8-7.1B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E32-k8-13.6B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E32-k8-13.6B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E64-k8-26.4B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E64-k8-26.4B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E128-k8-52.2B-A3.9B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E128-k8-52.2B-A3.9B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E16-k16-7.1B-A7.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E16-k16-7.1B-A7.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E32-k16-13.6B-A7.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E32-k16-13.6B-A7.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E64-k16-26.4B-A7.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E64-k16-26.4B-A7.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"llm-jp/optimal-sparsity-code-d2048-E128-k16-52.2B-A7.1B\", \"link\": \"https://huggingface.co/llm-jp/optimal-sparsity-code-d2048-E128-k16-52.2B-A7.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.18255",
    "first_seen_date": "2025-08-26",
    "title": "Hermes 4 Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.18255Hermes 4 Technical ReportPublished on Aug 25\u00b7Submitted bySumuk Shashidharon Aug 26#3 Paper of the day\u00b7NousResearchUpvote43+35Authors:Ryan Teknium,Roger Jin,Jai Suphavadeeprasit,Dakota Mahan,Jeffrey Quesnelle,Joe Li,Chen Guang,Shannon Sands,Karan MalhotraAbstractHermes 4, a hybrid reasoning model, integrates structured multi-turn reasoning with broad instruction-following, evaluated across various benchmarks including math, coding, knowledge, comprehension, and alignment.AI-generated summaryWe present Hermes 4, a family ofhybrid reasoning modelsthat combine\nstructured,multi-turn reasoningwith broadinstruction-followingability. We\ndescribe the challenges encountered duringdata curation, synthesis, training,\nand evaluation, and outline the solutions employed to address these challenges\nat scale. We comprehensively evaluate acrossmathematical reasoning,coding,\nknowledge, comprehension, andalignment benchmarks, and we report both\nquantitative performance and qualitative behavioral analysis. To support open\nresearch, all model weights are published publicly at\nhttps://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728View arXiv pageView PDFProject pageAdd to collectionCommunitysumuksPaper submitterAug 26Go NOUS!\ud83d\ude8011+Replylibrarian-botAug 27This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIGLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models(2025)InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling(2025)JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models(2025)Generalizing Verifiable Instruction Following(2025)Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following(2025)EXAONE 4.0: Unifi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2508.18255",
    "arxiv_url": "https://arxiv.org/abs/2508.18255",
    "num_models": 17,
    "models_list": "unsloth/Hermes-4-70B-GGUF, unsloth/Hermes-4-70B, unsloth/Hermes-4-405B, unsloth/Hermes-4-405B-GGUF, cyankiwi/Hermes-4-70B-AWQ-4bit, cyankiwi/Hermes-4-70B-AWQ-8bit, gabriellarson/Hermes-4-14B-GGUF, warshanks/Hermes-4-14B-AWQ, cyankiwi/Hermes-4-14B-AWQ-4bit, cyankiwi/Hermes-4-14B-AWQ-8bit, Mungert/Hermes-4-14B-GGUF, NobodyExistsOnTheInternet/hermes-4-405b-e2.5, cyankiwi/Hermes-4.3-36B-AWQ-8bit, cyankiwi/Hermes-4.3-36B-AWQ-4bit, Doradus-AI/Hermes-4.3-36B-FP8, arnomatic/Hermes-4.3-36B-heretic, n0kovo/Hermes-4.3-36B-heretic",
    "models_links": "https://huggingface.co/unsloth/Hermes-4-70B-GGUF, https://huggingface.co/unsloth/Hermes-4-70B, https://huggingface.co/unsloth/Hermes-4-405B, https://huggingface.co/unsloth/Hermes-4-405B-GGUF, https://huggingface.co/cyankiwi/Hermes-4-70B-AWQ-4bit, https://huggingface.co/cyankiwi/Hermes-4-70B-AWQ-8bit, https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF, https://huggingface.co/warshanks/Hermes-4-14B-AWQ, https://huggingface.co/cyankiwi/Hermes-4-14B-AWQ-4bit, https://huggingface.co/cyankiwi/Hermes-4-14B-AWQ-8bit, https://huggingface.co/Mungert/Hermes-4-14B-GGUF, https://huggingface.co/NobodyExistsOnTheInternet/hermes-4-405b-e2.5, https://huggingface.co/cyankiwi/Hermes-4.3-36B-AWQ-8bit, https://huggingface.co/cyankiwi/Hermes-4.3-36B-AWQ-4bit, https://huggingface.co/Doradus-AI/Hermes-4.3-36B-FP8, https://huggingface.co/arnomatic/Hermes-4.3-36B-heretic, https://huggingface.co/n0kovo/Hermes-4.3-36B-heretic",
    "models_detailed": "[{\"name\": \"unsloth/Hermes-4-70B-GGUF\", \"link\": \"https://huggingface.co/unsloth/Hermes-4-70B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"unsloth/Hermes-4-70B\", \"link\": \"https://huggingface.co/unsloth/Hermes-4-70B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"unsloth/Hermes-4-405B\", \"link\": \"https://huggingface.co/unsloth/Hermes-4-405B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"unsloth/Hermes-4-405B-GGUF\", \"link\": \"https://huggingface.co/unsloth/Hermes-4-405B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"741\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"cyankiwi/Hermes-4-70B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Hermes-4-70B-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"162\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"cyankiwi/Hermes-4-70B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Hermes-4-70B-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"88\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"gabriellarson/Hermes-4-14B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/Hermes-4-14B-GGUF\", \"task\": \"\", \"likes\": \"886\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"warshanks/Hermes-4-14B-AWQ\", \"link\": \"https://huggingface.co/warshanks/Hermes-4-14B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"51\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"cyankiwi/Hermes-4-14B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Hermes-4-14B-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"70\", \"downloads\": \"\", \"updated\": \"Sep 4\"}, {\"name\": \"cyankiwi/Hermes-4-14B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Hermes-4-14B-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 4\"}, {\"name\": \"Mungert/Hermes-4-14B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Hermes-4-14B-GGUF\", \"task\": \"\", \"likes\": \"230\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"NobodyExistsOnTheInternet/hermes-4-405b-e2.5\", \"link\": \"https://huggingface.co/NobodyExistsOnTheInternet/hermes-4-405b-e2.5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"cyankiwi/Hermes-4.3-36B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Hermes-4.3-36B-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"cyankiwi/Hermes-4.3-36B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Hermes-4.3-36B-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"Doradus-AI/Hermes-4.3-36B-FP8\", \"link\": \"https://huggingface.co/Doradus-AI/Hermes-4.3-36B-FP8\", \"task\": \"Text Generation\", \"likes\": \"79\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"arnomatic/Hermes-4.3-36B-heretic\", \"link\": \"https://huggingface.co/arnomatic/Hermes-4.3-36B-heretic\", \"task\": \"Text Generation\", \"likes\": \"38\", \"downloads\": \"\", \"updated\": \"8 days ago\"}, {\"name\": \"n0kovo/Hermes-4.3-36B-heretic\", \"link\": \"https://huggingface.co/n0kovo/Hermes-4.3-36B-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.18265",
    "first_seen_date": "2025-08-26",
    "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.18265InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and EfficiencyPublished on Aug 25\u00b7Submitted bytaesirion Aug 26#1 Paper of the dayUpvote210+202Authors:Weiyun Wang,Zhangwei Gao,Lixin Gu,Hengjun Pu,Long Cui,Xingguang Wei,Zhaoyang Liu,Linglin Jing,Shenglong Ye,Jie Shao,Zhaokai Wang,Zhe Chen,Hongjie Zhang,Ganlin Yang,Haomin Wang,Qi Wei,Jinhui Yin,Wenhao Li,Erfei Cui,Guanzhou Chen,Zichen Ding,Changyao Tian+39 authorsAbstractInternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.AI-generated summaryWe introduce InternVL 3.5, a new family of open-sourcemultimodal modelsthat\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process:offline RLfor stable convergence andonline RLfor\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose aVisual Resolution Router(ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled withViR, ourDecoupled Vision-Language Deployment(DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05timesinference speedupcompared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such asGUI interactionandembodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/OpenGVLab/InternVL",
    "hf_paper_url": "https://huggingface.co/papers/2508.18265",
    "arxiv_url": "https://arxiv.org/abs/2508.18265",
    "num_models": 124,
    "models_list": "OpenGVLab/InternVL3_5-241B-A28B, OpenGVLab/InternVL3_5-8B, OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview, OpenGVLab/InternVL3_5-38B, OpenGVLab/InternVL3_5-241B-A28B-MPO, OpenGVLab/InternVL3_5-241B-A28B-Pretrained, OpenGVLab/InternVL3_5-241B-A28B-Instruct, OpenGVLab/InternVL3_5-38B-MPO, OpenGVLab/InternVL3_5-38B-Pretrained, OpenGVLab/InternVL3_5-30B-A3B-MPO, OpenGVLab/InternVL3_5-30B-A3B, OpenGVLab/InternVL3_5-30B-A3B-Pretrained, OpenGVLab/InternVL3_5-38B-Instruct, OpenGVLab/InternVL3_5-30B-A3B-Instruct, OpenGVLab/InternVL3_5-4B, OpenGVLab/InternVL3_5-8B-Pretrained, OpenGVLab/InternVL3_5-8B-MPO, OpenGVLab/InternVL3_5-8B-Instruct, OpenGVLab/InternVL3_5-2B-Pretrained, OpenGVLab/InternVL3_5-2B-Instruct, OpenGVLab/InternVL3_5-4B-Instruct, OpenGVLab/InternVL3_5-1B, OpenGVLab/InternVL3_5-2B, OpenGVLab/InternVL3_5-2B-MPO, OpenGVLab/InternVL3_5-4B-MPO, OpenGVLab/InternVL3_5-1B-MPO, OpenGVLab/InternVL3_5-4B-Pretrained, OpenGVLab/InternVL3_5-14B-Pretrained, OpenGVLab/InternVL3_5-14B-MPO, OpenGVLab/InternVL3_5-14B-Instruct, OpenGVLab/InternVL3_5-14B, OpenGVLab/InternVL3_5-1B-Instruct, OpenGVLab/InternVL3_5-1B-Pretrained, OpenGVLab/InternVL3_5-38B-HF, OpenGVLab/InternVL3_5-30B-A3B-HF, OpenGVLab/InternVL3_5-8B-HF, OpenGVLab/InternVL3_5-1B-HF, OpenGVLab/InternVL3_5-4B-HF, OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF, OpenGVLab/InternVL3_5-2B-HF, OpenGVLab/InternVL3_5-241B-A28B-HF, OpenGVLab/InternVL3_5-14B-HF, cyankiwi/InternVL3_5-38B-AWQ-4bit, cyankiwi/InternVL3_5-8B-AWQ-4bit, cyankiwi/InternVL3_5-14B-AWQ-4bit, cyankiwi/InternVL3_5-38B-AWQ-8bit, cyankiwi/InternVL3_5-14B-AWQ-8bit, cyankiwi/InternVL3_5-8B-AWQ-8bit, vlnkane/InternVL3_5-GPT-OSS-20B-A4B-Preview, mertunsal/InternVL3_5-30B-A3B, mertunsal/InternVL3_5-4B, mertunsal/InternVL3_5-2B, OpenGVLab/InternVL3_5-2B-Flash, OpenGVLab/InternVL3_5-4B-Flash, OpenGVLab/InternVL3_5-1B-Flash, OpenGVLab/InternVL3_5-8B-Flash, OpenGVLab/InternVL3_5-14B-Flash, OpenGVLab/InternVL3_5-38B-Flash, OpenGVLab/InternVL3_5-30B-A3B-Flash, OpenGVLab/InternVL3_5-241B-A28B-Flash, iarchuk/InternVL, vocaela/Vocaela-500M, IVC-liuyuan/Intern_Vision_language, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s16000, lyx98/InternVL3_5_Flash-1B-HF, lyx98/InternVL3_5_Flash-2B-HF, lyx98/InternVL3_5_Flash-4B-HF, yiyangd/InternVL3_5-1B-HF-mix_base_oxe_1214_proximity_50pc_libero_text_5ac-s5000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_oxe_1214_proximity_50pc_libero_text_5ac-s5000, yiyangd/InternVL3_5-1B-HF-skip_libero_10_s16000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_10_s32000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_10_s48000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_10_s64000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s4000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s8000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s12000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s16000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_object_s4000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_object_s8000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_object_s12000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_object_s16000_new, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s8000",
    "models_links": "https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B, https://huggingface.co/OpenGVLab/InternVL3_5-8B, https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview, https://huggingface.co/OpenGVLab/InternVL3_5-38B, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-38B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-38B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-38B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-4B, https://huggingface.co/OpenGVLab/InternVL3_5-8B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-8B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-8B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-2B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-2B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-4B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-1B, https://huggingface.co/OpenGVLab/InternVL3_5-2B, https://huggingface.co/OpenGVLab/InternVL3_5-2B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-4B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-1B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-4B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-14B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-14B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-14B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-14B, https://huggingface.co/OpenGVLab/InternVL3_5-1B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-1B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-38B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-8B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-1B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-4B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF, https://huggingface.co/OpenGVLab/InternVL3_5-2B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-14B-HF, https://huggingface.co/cyankiwi/InternVL3_5-38B-AWQ-4bit, https://huggingface.co/cyankiwi/InternVL3_5-8B-AWQ-4bit, https://huggingface.co/cyankiwi/InternVL3_5-14B-AWQ-4bit, https://huggingface.co/cyankiwi/InternVL3_5-38B-AWQ-8bit, https://huggingface.co/cyankiwi/InternVL3_5-14B-AWQ-8bit, https://huggingface.co/cyankiwi/InternVL3_5-8B-AWQ-8bit, https://huggingface.co/vlnkane/InternVL3_5-GPT-OSS-20B-A4B-Preview, https://huggingface.co/mertunsal/InternVL3_5-30B-A3B, https://huggingface.co/mertunsal/InternVL3_5-4B, https://huggingface.co/mertunsal/InternVL3_5-2B, https://huggingface.co/OpenGVLab/InternVL3_5-2B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-4B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-14B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-38B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Flash, https://huggingface.co/iarchuk/InternVL, https://huggingface.co/vocaela/Vocaela-500M, https://huggingface.co/IVC-liuyuan/Intern_Vision_language, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s16000, https://huggingface.co/lyx98/InternVL3_5_Flash-1B-HF, https://huggingface.co/lyx98/InternVL3_5_Flash-2B-HF, https://huggingface.co/lyx98/InternVL3_5_Flash-4B-HF, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_oxe_1214_proximity_50pc_libero_text_5ac-s5000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_oxe_1214_proximity_50pc_libero_text_5ac-s5000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s16000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s32000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s48000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s64000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s4000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s8000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s12000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s16000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s4000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s8000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s12000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s16000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s8000",
    "models_detailed": "[{\"name\": \"OpenGVLab/InternVL3_5-241B-A28B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-MPO\", \"task\": \"\", \"likes\": \"67\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Pretrained\", \"task\": \"\", \"likes\": \"61\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Instruct\", \"task\": \"\", \"likes\": \"990\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-MPO\", \"task\": \"\", \"likes\": \"175\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-Pretrained\", \"task\": \"\", \"likes\": \"86\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-MPO\", \"task\": \"\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Pretrained\", \"task\": \"\", \"likes\": \"58\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-Instruct\", \"task\": \"\", \"likes\": \"676\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Instruct\", \"task\": \"\", \"likes\": \"144\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-Pretrained\", \"task\": \"\", \"likes\": \"416\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-MPO\", \"task\": \"\", \"likes\": \"535\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-Pretrained\", \"task\": \"\", \"likes\": \"88\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-MPO\", \"task\": \"\", \"likes\": \"168\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-MPO\", \"task\": \"\", \"likes\": \"683\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-MPO\", \"task\": \"\", \"likes\": \"169\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-Pretrained\", \"task\": \"\", \"likes\": \"114\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-Pretrained\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-MPO\", \"task\": \"\", \"likes\": \"159\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-Instruct\", \"task\": \"\", \"likes\": \"716\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-Instruct\", \"task\": \"\", \"likes\": \"780\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-Pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-HF\", \"task\": \"\", \"likes\": \"583\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-HF\", \"task\": \"\", \"likes\": \"147\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-HF\", \"task\": \"\", \"likes\": \"82\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"cyankiwi/InternVL3_5-38B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/InternVL3_5-38B-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"cyankiwi/InternVL3_5-8B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/InternVL3_5-8B-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"cyankiwi/InternVL3_5-14B-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/InternVL3_5-14B-AWQ-4bit\", \"task\": \"\", \"likes\": \"111\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"cyankiwi/InternVL3_5-38B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/InternVL3_5-38B-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"cyankiwi/InternVL3_5-14B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/InternVL3_5-14B-AWQ-8bit\", \"task\": \"\", \"likes\": \"118\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"cyankiwi/InternVL3_5-8B-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/InternVL3_5-8B-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"vlnkane/InternVL3_5-GPT-OSS-20B-A4B-Preview\", \"link\": \"https://huggingface.co/vlnkane/InternVL3_5-GPT-OSS-20B-A4B-Preview\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"mertunsal/InternVL3_5-30B-A3B\", \"link\": \"https://huggingface.co/mertunsal/InternVL3_5-30B-A3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"mertunsal/InternVL3_5-4B\", \"link\": \"https://huggingface.co/mertunsal/InternVL3_5-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"mertunsal/InternVL3_5-2B\", \"link\": \"https://huggingface.co/mertunsal/InternVL3_5-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 20\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-Flash\", \"task\": \"\", \"likes\": \"404\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-Flash\", \"task\": \"\", \"likes\": \"639\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash\", \"task\": \"\", \"likes\": \"810\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-Flash\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-Flash\", \"task\": \"\", \"likes\": \"158\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Flash\", \"task\": \"\", \"likes\": \"246\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Flash\", \"task\": \"\", \"likes\": \"134\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"iarchuk/InternVL\", \"link\": \"https://huggingface.co/iarchuk/InternVL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 13\"}, {\"name\": \"vocaela/Vocaela-500M\", \"link\": \"https://huggingface.co/vocaela/Vocaela-500M\", \"task\": \"\", \"likes\": \"57\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"IVC-liuyuan/Intern_Vision_language\", \"link\": \"https://huggingface.co/IVC-liuyuan/Intern_Vision_language\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"22 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"lyx98/InternVL3_5_Flash-1B-HF\", \"link\": \"https://huggingface.co/lyx98/InternVL3_5_Flash-1B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"8 days ago\"}, {\"name\": \"lyx98/InternVL3_5_Flash-2B-HF\", \"link\": \"https://huggingface.co/lyx98/InternVL3_5_Flash-2B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"8 days ago\"}, {\"name\": \"lyx98/InternVL3_5_Flash-4B-HF\", \"link\": \"https://huggingface.co/lyx98/InternVL3_5_Flash-4B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"8 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_oxe_1214_proximity_50pc_libero_text_5ac-s5000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_oxe_1214_proximity_50pc_libero_text_5ac-s5000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_oxe_1214_proximity_50pc_libero_text_5ac-s5000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_oxe_1214_proximity_50pc_libero_text_5ac-s5000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_10_s16000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s16000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_10_s32000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s32000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_10_s48000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s48000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_10_s64000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s64000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s4000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s4000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s8000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s8000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s12000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s12000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s16000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s16000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_object_s4000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s4000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_object_s8000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s8000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_object_s12000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s12000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_object_s16000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s16000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}]",
    "num_datasets": 2,
    "datasets_list": "OpenGVLab/MMPR-v1.2, OpenGVLab/MMPR-Tiny",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2, https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/MMPR-v1.2\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 31\", \"size\": \"\"}, {\"name\": \"OpenGVLab/MMPR-Tiny\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny\", \"task\": \"\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"Aug 31\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.16402",
    "first_seen_date": "2025-08-25",
    "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.16402AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  CompetitionsPublished on Aug 22\u00b7Submitted byZihan Wangon Aug 25Upvote14+6Authors:Zihan Wang,Jiaze Chen,Zhicheng Liu,Markus Mak,Yidi Du,Geonsik Moon,Luoqi Xu,Aaron Tua,Kunshuo Peng,Jiayi Lu,Mingfei Xia,Boqian Zou,Chenyang Ran,Guang Tian,Shoutai Zhu,Yeheng Duan,Zhenghui Kang,Zhenxing Lin,Shangshu Li,Qiang Luo,Qingshen Long,Zhiyong Chen+6 authorsAbstractAetherCode is a new benchmark for evaluating Large Language Models in competitive programming, offering more challenging and expert-validated test cases than existing benchmarks.AI-generated summaryCompetitive programminghas emerged as a criticalbenchmarkfor evaluating\nthereasoningandcoding capabilitiesofLarge Language Models(LLMs). Despite\nimpressive progress on existingbenchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope ofbenchmarkproblems, andevaluation biasfrom\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnewbenchmarkthat draws problems from premier programming competitions such asIOIandICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validatedtest suitesbuilt through\na hybrid ofautomated generationandhuman curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research incode reasoning.View arXiv pageView PDFProject pageAdd to collectionCommunityzhwang01Paper authorPaper submitterAug 25Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2508.16402",
    "arxiv_url": "https://arxiv.org/abs/2508.16402",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "m-a-p/AetherCode",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/AetherCode",
    "datasets_detailed": "[{\"name\": \"m-a-p/AetherCode\", \"link\": \"https://huggingface.co/datasets/m-a-p/AetherCode\", \"task\": \"\", \"likes\": \"456\", \"downloads\": \"\", \"updated\": \"Aug 26\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.15763",
    "first_seen_date": "2025-08-22",
    "title": "Intern-S1: A Scientific Multimodal Foundation Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.15763Intern-S1: A Scientific Multimodal Foundation ModelPublished on Aug 21\u00b7Submitted byWenwei Zhangon Aug 22Upvote257+249Authors:Lei Bai,Zhongrui Cai,Maosong Cao,Weihan Cao,Chiyu Chen,Haojiong Chen,Kai Chen,Pengcheng Chen,Ying Chen,Yongkang Chen,Yu Cheng,Yu Cheng,Pei Chu,Tao Chu,Erfei Cui,Ganqu Cui,Long Cui,Ziyun Cui,Nianchen Deng,Ning Ding,Nanqin Dong,Peijie Dong+153 authorsAbstractIntern-S1, a multimodal Mixture-of-Experts model with extensive pre-training and reinforcement learning, achieves top-tier performance in general reasoning and outperforms closed-source models in scientific tasks.AI-generated summaryIn recent years, a plethora of open-source foundation models have emerged,\nachieving remarkable progress in some widely attended fields, with performance\nbeing quite close to that of closed-source models. However, in high-value but\nmore challenging scientific professional fields, either the fields still rely\non expert models, or the progress of general foundation models lags\nsignificantly compared to those in popular areas, far from sufficient for\ntransforming scientific research and leaving substantial gap between\nopen-source models and closed-source models in these scientific domains. To\nmitigate this gap and explore a step further toward Artificial General\nIntelligence (AGI), we introduce Intern-S1, a specialized generalist equipped\nwith general understanding and reasoning capabilities with expertise to analyze\nmultiple science modal data. Intern-S1 is a multimodalMixture-of-Experts (MoE)model with 28 billion activated parameters and 241 billion total parameters,\ncontinually pre-trained on 5T tokens, including over 2.5T tokens from\nscientific domains. In the post-training stage, Intern-S1 undergoes offline and\nthen onlinereinforcement learning (RL)in InternBootCamp, where we proposeMixture-of-Rewards (MoR)to synergize the RL training on more than 1000 tasks\nsimulta",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/InternLM/Intern-S1",
    "hf_paper_url": "https://huggingface.co/papers/2508.15763",
    "arxiv_url": "https://arxiv.org/abs/2508.15763",
    "num_models": 4,
    "models_list": "internlm/Intern-S1, internlm/Intern-S1-mini, internlm/Intern-S1-FP8, internlm/Intern-S1-mini-FP8",
    "models_links": "https://huggingface.co/internlm/Intern-S1, https://huggingface.co/internlm/Intern-S1-mini, https://huggingface.co/internlm/Intern-S1-FP8, https://huggingface.co/internlm/Intern-S1-mini-FP8",
    "models_detailed": "[{\"name\": \"internlm/Intern-S1\", \"link\": \"https://huggingface.co/internlm/Intern-S1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"internlm/Intern-S1-mini\", \"link\": \"https://huggingface.co/internlm/Intern-S1-mini\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"internlm/Intern-S1-FP8\", \"link\": \"https://huggingface.co/internlm/Intern-S1-FP8\", \"task\": \"\", \"likes\": \"159\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"internlm/Intern-S1-mini-FP8\", \"link\": \"https://huggingface.co/internlm/Intern-S1-mini-FP8\", \"task\": \"\", \"likes\": \"784\", \"downloads\": \"\", \"updated\": \"Nov 4\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.14160",
    "first_seen_date": "2025-08-21",
    "title": "RynnEC: Bringing MLLMs into Embodied World",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.14160RynnEC: Bringing MLLMs into Embodied WorldPublished on Aug 19\u00b7Submitted byYuqianYuanon Aug 21Upvote19+11Authors:Ronghao Dang,Yuqian Yuan,Yunxuan Mao,Kehan Li,Jiangpin Liu,Zhikai Wang,Xin Li,Fan Wang,Deli ZhaoAbstractRynnEC, a video multimodal large language model with a region-centric approach, achieves state-of-the-art performance in object property understanding, segmentation, and spatial reasoning, using an egocentric video pipeline and a region-centered benchmark.AI-generated summaryWe introduce RynnEC, avideo multimodal large language modeldesigned forembodied cognition. Built upon a general-purpose vision-language foundation\nmodel, RynnEC incorporates aregion encoderand amask decoder, enabling\nflexibleregion-level video interaction. Despite its compact architecture,\nRynnEC achieves state-of-the-art performance inobject property understanding,object segmentation, andspatial reasoning. Conceptually, it offers a\nregion-centric video paradigm for the brain of embodied agents, providing\nfine-grained perception of the physical world and enabling more precise\ninteractions. To mitigate the scarcity of annotated 3D datasets, we propose anegocentric videobased pipeline for generatingembodied cognitiondata.\nFurthermore, we introduceRynnEC-Bench, a region-centered benchmark for\nevaluatingembodied cognitive capabilities. We anticipate that RynnEC will\nadvance the development of general-purpose cognitive cores for embodied agents\nand facilitate generalization across diverse embodied tasks. The code, model\ncheckpoints, and benchmark are available at:\nhttps://github.com/alibaba-damo-academy/RynnECView arXiv pageView PDFProject pageGitHub382Add to collectionCommunityCircleRadonPaper authorPaper submitterAug 21We introduceRynnEC, our first multi-modal large language model (MLLM) specially designed forembodied perception and understanding.1.RynnEC is\"object-centric\", supporting objec",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/alibaba-damo-academy/RynnEC",
    "hf_paper_url": "https://huggingface.co/papers/2508.14160",
    "arxiv_url": "https://arxiv.org/abs/2508.14160",
    "num_models": 3,
    "models_list": "Alibaba-DAMO-Academy/RynnEC-7B, Alibaba-DAMO-Academy/RynnEC-2B, Alibaba-DAMO-Academy/RynnVLA-002",
    "models_links": "https://huggingface.co/Alibaba-DAMO-Academy/RynnEC-7B, https://huggingface.co/Alibaba-DAMO-Academy/RynnEC-2B, https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-002",
    "models_detailed": "[{\"name\": \"Alibaba-DAMO-Academy/RynnEC-7B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/RynnEC-7B\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Alibaba-DAMO-Academy/RynnEC-2B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/RynnEC-2B\", \"task\": \"\", \"likes\": \"60\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Alibaba-DAMO-Academy/RynnVLA-002\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-002\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 1\"}]",
    "num_datasets": 1,
    "datasets_list": "Alibaba-DAMO-Academy/RynnEC-Bench",
    "datasets_links": "https://huggingface.co/datasets/Alibaba-DAMO-Academy/RynnEC-Bench",
    "datasets_detailed": "[{\"name\": \"Alibaba-DAMO-Academy/RynnEC-Bench\", \"link\": \"https://huggingface.co/datasets/Alibaba-DAMO-Academy/RynnEC-Bench\", \"task\": \"\", \"likes\": \"104\", \"downloads\": \"\", \"updated\": \"Oct 15\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.14444",
    "first_seen_date": "2025-08-21",
    "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid\n  Mamba-Transformer Reasoning Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.14444NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid\n  Mamba-Transformer Reasoning ModelPublished on Aug 20\u00b7Submitted bytaesirion Aug 21Upvote38+30Authors:NVIDIA,Aarti Basant,Abhijit Khairnar,Abhijit Paithankar,Abhinav Khattar,Adi Renduchintala,Adithya Renduchintala,Aditya Malte,Akhiad Bercovich,Akshay Hazare,Alejandra Rico,Aleksander Ficek,Alex Kondratenko,Alex Shaposhnikov,Ali Taghibakhshi,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amy Shen,Andrew Tao,Ann Guan,Anna Shors,Anubhav Mandarwal+189 authorsAbstractNemotron-Nano-9B-v2, a hybrid Mamba-Transformer model, enhances reasoning workload throughput and accuracy by replacing self-attention layers with Mamba-2 layers and using the Minitron strategy for compression.AI-generated summaryWe introduce Nemotron-Nano-9B-v2, a hybridMamba-Transformerlanguage model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of theself-attention layersin the common Transformer architecture\nare replaced withMamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ theMinitron strategyto\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy onreasoning benchmarkswhile achieving up to 6x higherinference throughputin reasoning settings like\n8k input and 16k output tokens. We are releasi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2508.14444",
    "arxiv_url": "https://arxiv.org/abs/2508.14444",
    "num_models": 19,
    "models_list": "nvidia/NVIDIA-Nemotron-Nano-9B-v2, nvidia/NVIDIA-Nemotron-Nano-12B-v2, nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base, nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base, dominguesm/NVIDIA-Nemotron-Nano-9B-v2-GGUF, gabriellarson/NVIDIA-Nemotron-Nano-12B-v2-GGUF, QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF, QuantFactory/NVIDIA-Nemotron-Nano-12B-v2-GGUF, GGUF-A-Lot/NVIDIA-Nemotron-Nano-9B-v2-GGUF, GGUF-A-Lot/NVIDIA-Nemotron-Nano-12B-v2-GGUF, cyankiwi/NVIDIA-Nemotron-Nano-9B-v2-AWQ-4bit, cyankiwi/NVIDIA-Nemotron-Nano-12B-v2-AWQ-4bit, cyankiwi/NVIDIA-Nemotron-Nano-12B-v2-AWQ-8bit, cyankiwi/NVIDIA-Nemotron-Nano-9B-v2-AWQ-8bit, Mungert/NVIDIA-Nemotron-Nano-12B-v2-GGUF, unsloth/NVIDIA-Nemotron-Nano-9B-v2, nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8, nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4, yueqis/NVIDIA-Nemotron-Nano-9B-v2",
    "models_links": "https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2, https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2, https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base, https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base, https://huggingface.co/dominguesm/NVIDIA-Nemotron-Nano-9B-v2-GGUF, https://huggingface.co/gabriellarson/NVIDIA-Nemotron-Nano-12B-v2-GGUF, https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF, https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-12B-v2-GGUF, https://huggingface.co/GGUF-A-Lot/NVIDIA-Nemotron-Nano-9B-v2-GGUF, https://huggingface.co/GGUF-A-Lot/NVIDIA-Nemotron-Nano-12B-v2-GGUF, https://huggingface.co/cyankiwi/NVIDIA-Nemotron-Nano-9B-v2-AWQ-4bit, https://huggingface.co/cyankiwi/NVIDIA-Nemotron-Nano-12B-v2-AWQ-4bit, https://huggingface.co/cyankiwi/NVIDIA-Nemotron-Nano-12B-v2-AWQ-8bit, https://huggingface.co/cyankiwi/NVIDIA-Nemotron-Nano-9B-v2-AWQ-8bit, https://huggingface.co/Mungert/NVIDIA-Nemotron-Nano-12B-v2-GGUF, https://huggingface.co/unsloth/NVIDIA-Nemotron-Nano-9B-v2, https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8, https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4, https://huggingface.co/yueqis/NVIDIA-Nemotron-Nano-9B-v2",
    "models_detailed": "[{\"name\": \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\"}, {\"name\": \"nvidia/NVIDIA-Nemotron-Nano-12B-v2\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"dominguesm/NVIDIA-Nemotron-Nano-9B-v2-GGUF\", \"link\": \"https://huggingface.co/dominguesm/NVIDIA-Nemotron-Nano-9B-v2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"379\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"gabriellarson/NVIDIA-Nemotron-Nano-12B-v2-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/NVIDIA-Nemotron-Nano-12B-v2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-9B-v2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"QuantFactory/NVIDIA-Nemotron-Nano-12B-v2-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/NVIDIA-Nemotron-Nano-12B-v2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"308\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"GGUF-A-Lot/NVIDIA-Nemotron-Nano-9B-v2-GGUF\", \"link\": \"https://huggingface.co/GGUF-A-Lot/NVIDIA-Nemotron-Nano-9B-v2-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 1\"}, {\"name\": \"GGUF-A-Lot/NVIDIA-Nemotron-Nano-12B-v2-GGUF\", \"link\": \"https://huggingface.co/GGUF-A-Lot/NVIDIA-Nemotron-Nano-12B-v2-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 1\"}, {\"name\": \"cyankiwi/NVIDIA-Nemotron-Nano-9B-v2-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/NVIDIA-Nemotron-Nano-9B-v2-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"268\", \"downloads\": \"\", \"updated\": \"Aug 31\"}, {\"name\": \"cyankiwi/NVIDIA-Nemotron-Nano-12B-v2-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/NVIDIA-Nemotron-Nano-12B-v2-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"260\", \"downloads\": \"\", \"updated\": \"Sep 13\"}, {\"name\": \"cyankiwi/NVIDIA-Nemotron-Nano-12B-v2-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/NVIDIA-Nemotron-Nano-12B-v2-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"122\", \"downloads\": \"\", \"updated\": \"Sep 13\"}, {\"name\": \"cyankiwi/NVIDIA-Nemotron-Nano-9B-v2-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/NVIDIA-Nemotron-Nano-9B-v2-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 31\"}, {\"name\": \"Mungert/NVIDIA-Nemotron-Nano-12B-v2-GGUF\", \"link\": \"https://huggingface.co/Mungert/NVIDIA-Nemotron-Nano-12B-v2-GGUF\", \"task\": \"Text Generation\", \"likes\": \"399\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"unsloth/NVIDIA-Nemotron-Nano-9B-v2\", \"link\": \"https://huggingface.co/unsloth/NVIDIA-Nemotron-Nano-9B-v2\", \"task\": \"Text Generation\", \"likes\": \"191\", \"downloads\": \"\", \"updated\": \"Sep 10\"}, {\"name\": \"nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4\", \"link\": \"https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"yueqis/NVIDIA-Nemotron-Nano-9B-v2\", \"link\": \"https://huggingface.co/yueqis/NVIDIA-Nemotron-Nano-9B-v2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}]",
    "num_datasets": 12,
    "datasets_list": "nvidia/Nemotron-CC-v2, nvidia/Nemotron-CC-v2.1, nvidia/Nemotron-Post-Training-Dataset-v2, nvidia/Nemotron-Pretraining-SFT-v1, nvidia/Nemotron-Pretraining-Code-v2, nvidia/Nemotron-Pretraining-Specialized-v1, nvidia/Nemotron-CC-Code-v1, nvidia/Nemotron-Pretraining-Dataset-sample, nvidia/Nemotron-Pretraining-Code-v1, nvidia/Nemotron-CC-Math-v1, Ericwang/nemotron-nano2-safety-distill-gptoss, semran1/Nemotron-Pretraining-Specialized-v1",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Nemotron-CC-v2, https://huggingface.co/datasets/nvidia/Nemotron-CC-v2.1, https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v2, https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-SFT-v1, https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Code-v2, https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Specialized-v1, https://huggingface.co/datasets/nvidia/Nemotron-CC-Code-v1, https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Dataset-sample, https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Code-v1, https://huggingface.co/datasets/nvidia/Nemotron-CC-Math-v1, https://huggingface.co/datasets/Ericwang/nemotron-nano2-safety-distill-gptoss, https://huggingface.co/datasets/semran1/Nemotron-Pretraining-Specialized-v1",
    "datasets_detailed": "[{\"name\": \"nvidia/Nemotron-CC-v2\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-CC-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-CC-v2.1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-CC-v2.1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Post-Training-Dataset-v2\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 21\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Pretraining-SFT-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-SFT-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Pretraining-Code-v2\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Code-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Pretraining-Specialized-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Specialized-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-CC-Code-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-CC-Code-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Pretraining-Dataset-sample\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Dataset-sample\", \"task\": \"\", \"likes\": \"935\", \"downloads\": \"\", \"updated\": \"Aug 26\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Pretraining-Code-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Code-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-CC-Math-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-CC-Math-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 27\", \"size\": \"\"}, {\"name\": \"Ericwang/nemotron-nano2-safety-distill-gptoss\", \"link\": \"https://huggingface.co/datasets/Ericwang/nemotron-nano2-safety-distill-gptoss\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 22\", \"size\": \"\"}, {\"name\": \"semran1/Nemotron-Pretraining-Specialized-v1\", \"link\": \"https://huggingface.co/datasets/semran1/Nemotron-Pretraining-Specialized-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.11737",
    "first_seen_date": "2025-08-19",
    "title": "Ovis2.5 Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.11737Ovis2.5 Technical ReportPublished on Aug 15\u00b7Submitted byShiyin Luon Aug 19#1 Paper of the dayUpvote111+103Authors:Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu+20 authorsAbstractOvis2.5, a native-resolution vision transformer with multimodal reasoning, achieves state-of-the-art performance on various benchmarks through advanced training techniques and efficient scaling methods.AI-generated summaryWe present Ovis2.5, a successor to Ovis2 designed fornative-resolutionvisual perception and strongmultimodal reasoning. Ovis2.5 integrates anative-resolutionvision transformerthat processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyondlinear chain-of-thoughtand performreflection-- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensivefive-phase curriculumthat progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement usingDPOandGRPO. To scale these upgrades efficiently, we employmultimodal data packingandhybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/AIDC-AI/Ovis",
    "hf_paper_url": "https://huggingface.co/papers/2508.11737",
    "arxiv_url": "https://arxiv.org/abs/2508.11737",
    "num_models": 5,
    "models_list": "AIDC-AI/Ovis2.5-9B, AIDC-AI/Ovis2.5-2B, ViFortune-AI/VOVis2.5-2B-pt, wsbagnsv1/Ovis2.5-9B-sinq-4bit-experimental, wsbagnsv1/Ovis2.5-2B-sinq-4bit-experimental",
    "models_links": "https://huggingface.co/AIDC-AI/Ovis2.5-9B, https://huggingface.co/AIDC-AI/Ovis2.5-2B, https://huggingface.co/ViFortune-AI/VOVis2.5-2B-pt, https://huggingface.co/wsbagnsv1/Ovis2.5-9B-sinq-4bit-experimental, https://huggingface.co/wsbagnsv1/Ovis2.5-2B-sinq-4bit-experimental",
    "models_detailed": "[{\"name\": \"AIDC-AI/Ovis2.5-9B\", \"link\": \"https://huggingface.co/AIDC-AI/Ovis2.5-9B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"AIDC-AI/Ovis2.5-2B\", \"link\": \"https://huggingface.co/AIDC-AI/Ovis2.5-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"ViFortune-AI/VOVis2.5-2B-pt\", \"link\": \"https://huggingface.co/ViFortune-AI/VOVis2.5-2B-pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 25\"}, {\"name\": \"wsbagnsv1/Ovis2.5-9B-sinq-4bit-experimental\", \"link\": \"https://huggingface.co/wsbagnsv1/Ovis2.5-9B-sinq-4bit-experimental\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"wsbagnsv1/Ovis2.5-2B-sinq-4bit-experimental\", \"link\": \"https://huggingface.co/wsbagnsv1/Ovis2.5-2B-sinq-4bit-experimental\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 6\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.10104",
    "first_seen_date": "2025-08-18",
    "title": "DINOv3",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.10104DINOv3Published on Aug 13\u00b7Submitted byNiels Roggeon Aug 18#1 Paper of the day\u00b7AI at MetaUpvote288+280Authors:Oriane Sim\u00e9oni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Micha\u00ebl Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timoth\u00e9e Darcet,Th\u00e9o Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie+4 authorsAbstractDINOv3, a self-supervised learning model, achieves superior performance across various vision tasks by scaling datasets and models, addressing dense feature degradation, and enhancing flexibility with post-hoc strategies.AI-generated summarySelf-supervised learningholds the promise of eliminating the need for manual\ndata annotation, enabling models to scale effortlessly to massive datasets and\nlarger architectures. By not being tailored to specific tasks or domains, this\ntraining paradigm has the potential to learn visual representations from\ndiverse sources, ranging from natural to aerial images -- using a single\nalgorithm. This technical report introducesDINOv3, a major milestone toward\nrealizing this vision by leveraging simple yet effective strategies. First, we\nleverage the benefit of scaling both dataset and model size by careful data\npreparation,design, andoptimization. Second, we introduce a new method calledGram anchoring, which effectively addresses the known yet unsolved issue ofdense feature mapsdegrading during long training schedules. Finally, we applypost-hoc strategiesthat further enhance our models' flexibility with respect\nto resolution, model size, and alignment with text. As a result, we present a\nversatilevision foundation modelthat outperforms the specialized state of the\nart across a broad range of settings, without fine-tuning.DINOv3produceshigh-quality dense featuresthat achieve outstanding performanc",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/facebookresearch/dinov3",
    "hf_paper_url": "https://huggingface.co/papers/2508.10104",
    "arxiv_url": "https://arxiv.org/abs/2508.10104",
    "num_models": 65,
    "models_list": "facebook/dinov3-vit7b16-pretrain-lvd1689m, facebook/dinov3-vitb16-pretrain-lvd1689m, facebook/dinov3-vitl16-pretrain-lvd1689m, facebook/dinov3-vits16-pretrain-lvd1689m, facebook/dinov3-convnext-tiny-pretrain-lvd1689m, facebook/dinov3-vith16plus-pretrain-lvd1689m, facebook/dinov3-convnext-large-pretrain-lvd1689m, facebook/dinov3-vitl16-pretrain-sat493m, facebook/dinov3-vit7b16-pretrain-sat493m, facebook/dinov3-convnext-small-pretrain-lvd1689m, facebook/dinov3-convnext-base-pretrain-lvd1689m, facebook/dinov3-vits16plus-pretrain-lvd1689m, IBBI-bio/dinov3-vitl16-pretrain-lvd1689m, PIA-SPACE-LAB/dinov3-vitl-pretrain-lvd1689m, timm/convnext_base.dinov3_lvd1689m, timm/convnext_large.dinov3_lvd1689m, timm/convnext_small.dinov3_lvd1689m, timm/convnext_tiny.dinov3_lvd1689m, PIA-SPACE-LAB/dinov3-vit7b16-pretrain-lvd1689m, timm/vit_base_patch16_dinov3.lvd1689m, timm/vit_base_patch16_dinov3_qkvb.lvd1689m, timm/vit_huge_plus_patch16_dinov3.lvd1689m, timm/vit_huge_plus_patch16_dinov3_qkvb.lvd1689m, timm/vit_large_patch16_dinov3.lvd1689m, timm/vit_large_patch16_dinov3.sat493m, timm/vit_large_patch16_dinov3_qkvb.lvd1689m, timm/vit_large_patch16_dinov3_qkvb.sat493m, timm/vit_small_patch16_dinov3.lvd1689m, timm/vit_small_patch16_dinov3_qkvb.lvd1689m, timm/vit_small_plus_patch16_dinov3.lvd1689m, timm/vit_small_plus_patch16_dinov3_qkvb.lvd1689m, timm/vit_7b_patch16_dinov3.lvd1689m, timm/vit_7b_patch16_dinov3.sat493m, dflorea/dinov3-convnext-tiny-pretrain-lvd1689m-andina, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-448x448, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-448x448, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-448x448, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-560x560, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-560x560, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-560x560, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-336x336, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-336x336, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-336x336, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-224x224, SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-224x224, SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP32-560x560, SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP16-560x560, SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-INT8-560x560, SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP32-448x448, SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP16-448x448, SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-INT8-448x448, dimidagd/dinov3-vit7b16-pretrain-lvd1689m-imagenet1k-lc, yberreby/dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe, yberreby/dinov3-vits16-lvd1689m-in1k-512x512-linear-clf-probe, yberreby/dinov3-vitl16-lvd1689m-in1k-512x512-linear-clf-probe, yberreby/dinov3-vits16plus-lvd1689m-in1k-512x512-linear-clf-probe, yberreby/dinov3-vith16plus-lvd1689m-in1k-512x512-linear-clf-probe, mirekphd/dinov3-vit7b16-pretrain-lvd1689m-fp16, mirekphd/dinov3-vit7b16-pretrain-sat493m-fp16, Simon-Kotchou/dinov3-convnext-small-geoguessr-25k-384, xycheni/facebook-dinov3-vitl16-pretrain-lvd1689m, yangpanqi/YUAN_dino, PIA-SPACE-LAB/dinov3-convnext-small-pretrain-lvd1689m, camenduru/dinov3-vitl16-pretrain-lvd1689m, tao-hunter/dinov3-vitl16-pretrain-lvd1689m",
    "models_links": "https://huggingface.co/facebook/dinov3-vit7b16-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-vitl16-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-vits16-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-convnext-tiny-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-vith16plus-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-convnext-large-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-vitl16-pretrain-sat493m, https://huggingface.co/facebook/dinov3-vit7b16-pretrain-sat493m, https://huggingface.co/facebook/dinov3-convnext-small-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-convnext-base-pretrain-lvd1689m, https://huggingface.co/facebook/dinov3-vits16plus-pretrain-lvd1689m, https://huggingface.co/IBBI-bio/dinov3-vitl16-pretrain-lvd1689m, https://huggingface.co/PIA-SPACE-LAB/dinov3-vitl-pretrain-lvd1689m, https://huggingface.co/timm/convnext_base.dinov3_lvd1689m, https://huggingface.co/timm/convnext_large.dinov3_lvd1689m, https://huggingface.co/timm/convnext_small.dinov3_lvd1689m, https://huggingface.co/timm/convnext_tiny.dinov3_lvd1689m, https://huggingface.co/PIA-SPACE-LAB/dinov3-vit7b16-pretrain-lvd1689m, https://huggingface.co/timm/vit_base_patch16_dinov3.lvd1689m, https://huggingface.co/timm/vit_base_patch16_dinov3_qkvb.lvd1689m, https://huggingface.co/timm/vit_huge_plus_patch16_dinov3.lvd1689m, https://huggingface.co/timm/vit_huge_plus_patch16_dinov3_qkvb.lvd1689m, https://huggingface.co/timm/vit_large_patch16_dinov3.lvd1689m, https://huggingface.co/timm/vit_large_patch16_dinov3.sat493m, https://huggingface.co/timm/vit_large_patch16_dinov3_qkvb.lvd1689m, https://huggingface.co/timm/vit_large_patch16_dinov3_qkvb.sat493m, https://huggingface.co/timm/vit_small_patch16_dinov3.lvd1689m, https://huggingface.co/timm/vit_small_patch16_dinov3_qkvb.lvd1689m, https://huggingface.co/timm/vit_small_plus_patch16_dinov3.lvd1689m, https://huggingface.co/timm/vit_small_plus_patch16_dinov3_qkvb.lvd1689m, https://huggingface.co/timm/vit_7b_patch16_dinov3.lvd1689m, https://huggingface.co/timm/vit_7b_patch16_dinov3.sat493m, https://huggingface.co/dflorea/dinov3-convnext-tiny-pretrain-lvd1689m-andina, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-448x448, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-448x448, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-448x448, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-560x560, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-560x560, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-560x560, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-336x336, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-336x336, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-336x336, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-224x224, https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-224x224, https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP32-560x560, https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP16-560x560, https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-INT8-560x560, https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP32-448x448, https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP16-448x448, https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-INT8-448x448, https://huggingface.co/dimidagd/dinov3-vit7b16-pretrain-lvd1689m-imagenet1k-lc, https://huggingface.co/yberreby/dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe, https://huggingface.co/yberreby/dinov3-vits16-lvd1689m-in1k-512x512-linear-clf-probe, https://huggingface.co/yberreby/dinov3-vitl16-lvd1689m-in1k-512x512-linear-clf-probe, https://huggingface.co/yberreby/dinov3-vits16plus-lvd1689m-in1k-512x512-linear-clf-probe, https://huggingface.co/yberreby/dinov3-vith16plus-lvd1689m-in1k-512x512-linear-clf-probe, https://huggingface.co/mirekphd/dinov3-vit7b16-pretrain-lvd1689m-fp16, https://huggingface.co/mirekphd/dinov3-vit7b16-pretrain-sat493m-fp16, https://huggingface.co/Simon-Kotchou/dinov3-convnext-small-geoguessr-25k-384, https://huggingface.co/xycheni/facebook-dinov3-vitl16-pretrain-lvd1689m, https://huggingface.co/yangpanqi/YUAN_dino, https://huggingface.co/PIA-SPACE-LAB/dinov3-convnext-small-pretrain-lvd1689m, https://huggingface.co/camenduru/dinov3-vitl16-pretrain-lvd1689m, https://huggingface.co/tao-hunter/dinov3-vitl16-pretrain-lvd1689m",
    "models_detailed": "[{\"name\": \"facebook/dinov3-vit7b16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-vit7b16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-vitb16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-vitl16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-vitl16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-vits16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-vits16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-convnext-tiny-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-vith16plus-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-vith16plus-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-convnext-large-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-convnext-large-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-vitl16-pretrain-sat493m\", \"link\": \"https://huggingface.co/facebook/dinov3-vitl16-pretrain-sat493m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-vit7b16-pretrain-sat493m\", \"link\": \"https://huggingface.co/facebook/dinov3-vit7b16-pretrain-sat493m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-convnext-small-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-convnext-small-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-convnext-base-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-convnext-base-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"facebook/dinov3-vits16plus-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/facebook/dinov3-vits16plus-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"IBBI-bio/dinov3-vitl16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/IBBI-bio/dinov3-vitl16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"PIA-SPACE-LAB/dinov3-vitl-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/PIA-SPACE-LAB/dinov3-vitl-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 10\"}, {\"name\": \"timm/convnext_base.dinov3_lvd1689m\", \"link\": \"https://huggingface.co/timm/convnext_base.dinov3_lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"timm/convnext_large.dinov3_lvd1689m\", \"link\": \"https://huggingface.co/timm/convnext_large.dinov3_lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"timm/convnext_small.dinov3_lvd1689m\", \"link\": \"https://huggingface.co/timm/convnext_small.dinov3_lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"timm/convnext_tiny.dinov3_lvd1689m\", \"link\": \"https://huggingface.co/timm/convnext_tiny.dinov3_lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"PIA-SPACE-LAB/dinov3-vit7b16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/PIA-SPACE-LAB/dinov3-vit7b16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"timm/vit_base_patch16_dinov3.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_dinov3.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_base_patch16_dinov3_qkvb.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_dinov3_qkvb.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_huge_plus_patch16_dinov3.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_huge_plus_patch16_dinov3.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_huge_plus_patch16_dinov3_qkvb.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_huge_plus_patch16_dinov3_qkvb.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_large_patch16_dinov3.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_dinov3.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_large_patch16_dinov3.sat493m\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_dinov3.sat493m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_large_patch16_dinov3_qkvb.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_dinov3_qkvb.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_large_patch16_dinov3_qkvb.sat493m\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_dinov3_qkvb.sat493m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_small_patch16_dinov3.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_small_patch16_dinov3.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_small_patch16_dinov3_qkvb.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_small_patch16_dinov3_qkvb.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_small_plus_patch16_dinov3.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_small_plus_patch16_dinov3.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_small_plus_patch16_dinov3_qkvb.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_small_plus_patch16_dinov3_qkvb.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_7b_patch16_dinov3.lvd1689m\", \"link\": \"https://huggingface.co/timm/vit_7b_patch16_dinov3.lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"timm/vit_7b_patch16_dinov3.sat493m\", \"link\": \"https://huggingface.co/timm/vit_7b_patch16_dinov3.sat493m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"dflorea/dinov3-convnext-tiny-pretrain-lvd1689m-andina\", \"link\": \"https://huggingface.co/dflorea/dinov3-convnext-tiny-pretrain-lvd1689m-andina\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-448x448\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-448x448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-448x448\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-448x448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-448x448\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-448x448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-560x560\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-560x560\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-560x560\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-560x560\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-560x560\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-560x560\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-336x336\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-336x336\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-336x336\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP16-336x336\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-336x336\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-336x336\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-224x224\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-INT8-224x224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-224x224\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vits16-pretrain-lvd1689m-coreml-FP32-224x224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP32-560x560\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP32-560x560\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP16-560x560\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP16-560x560\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-INT8-560x560\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-INT8-560x560\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP32-448x448\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP32-448x448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP16-448x448\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-FP16-448x448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-INT8-448x448\", \"link\": \"https://huggingface.co/SharpAI/dinov3-vitb16-pretrain-lvd1689m-coreml-INT8-448x448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"dimidagd/dinov3-vit7b16-pretrain-lvd1689m-imagenet1k-lc\", \"link\": \"https://huggingface.co/dimidagd/dinov3-vit7b16-pretrain-lvd1689m-imagenet1k-lc\", \"task\": \"Image Classification\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"yberreby/dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe\", \"link\": \"https://huggingface.co/yberreby/dinov3-vitb16-lvd1689m-in1k-512x512-linear-clf-probe\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"yberreby/dinov3-vits16-lvd1689m-in1k-512x512-linear-clf-probe\", \"link\": \"https://huggingface.co/yberreby/dinov3-vits16-lvd1689m-in1k-512x512-linear-clf-probe\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"yberreby/dinov3-vitl16-lvd1689m-in1k-512x512-linear-clf-probe\", \"link\": \"https://huggingface.co/yberreby/dinov3-vitl16-lvd1689m-in1k-512x512-linear-clf-probe\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"yberreby/dinov3-vits16plus-lvd1689m-in1k-512x512-linear-clf-probe\", \"link\": \"https://huggingface.co/yberreby/dinov3-vits16plus-lvd1689m-in1k-512x512-linear-clf-probe\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"yberreby/dinov3-vith16plus-lvd1689m-in1k-512x512-linear-clf-probe\", \"link\": \"https://huggingface.co/yberreby/dinov3-vith16plus-lvd1689m-in1k-512x512-linear-clf-probe\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"mirekphd/dinov3-vit7b16-pretrain-lvd1689m-fp16\", \"link\": \"https://huggingface.co/mirekphd/dinov3-vit7b16-pretrain-lvd1689m-fp16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"29 days ago\"}, {\"name\": \"mirekphd/dinov3-vit7b16-pretrain-sat493m-fp16\", \"link\": \"https://huggingface.co/mirekphd/dinov3-vit7b16-pretrain-sat493m-fp16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"29 days ago\"}, {\"name\": \"Simon-Kotchou/dinov3-convnext-small-geoguessr-25k-384\", \"link\": \"https://huggingface.co/Simon-Kotchou/dinov3-convnext-small-geoguessr-25k-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"xycheni/facebook-dinov3-vitl16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/xycheni/facebook-dinov3-vitl16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"yangpanqi/YUAN_dino\", \"link\": \"https://huggingface.co/yangpanqi/YUAN_dino\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"PIA-SPACE-LAB/dinov3-convnext-small-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/PIA-SPACE-LAB/dinov3-convnext-small-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"camenduru/dinov3-vitl16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/camenduru/dinov3-vitl16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"tao-hunter/dinov3-vitl16-pretrain-lvd1689m\", \"link\": \"https://huggingface.co/tao-hunter/dinov3-vitl16-pretrain-lvd1689m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 2,
    "datasets_list": "zhuangzhe1229/test_dataset, simon123905/vitl",
    "datasets_links": "https://huggingface.co/datasets/zhuangzhe1229/test_dataset, https://huggingface.co/datasets/simon123905/vitl",
    "datasets_detailed": "[{\"name\": \"zhuangzhe1229/test_dataset\", \"link\": \"https://huggingface.co/datasets/zhuangzhe1229/test_dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\", \"size\": \"\"}, {\"name\": \"simon123905/vitl\", \"link\": \"https://huggingface.co/datasets/simon123905/vitl\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"Aug 24\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.09736",
    "first_seen_date": "2025-08-14",
    "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.09736Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term MemoryPublished on Aug 13\u00b7Submitted byyichen heon Aug 14#2 Paper of the day\u00b7ByteDance SeedUpvote57+49Authors:Lin Long,Yichen He,Wentao Ye,Yiyuan Pan,Yuan Lin,Hang Li,Junbo Zhao,Wei LiAbstractM3-Agent, a multimodal agent with long-term memory, performs multi-turn reasoning and outperforms baselines on a new long-video question answering benchmark.AI-generated summaryWe introduce M3-Agent, a novelmultimodal agentframework equipped withlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update itslong-term memory. Beyond episodic\nmemory, it also developssemantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we developM3-Bench, a newlong-video question answering benchmark.M3-Benchcomprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such ashuman understanding,general knowledge extraction, andcross-modal reasoning. Experimental results\nshow that M3-Agent, trained viareinforcement learning, outperforms the\nstrongest baseline, a prompting agent usingGemini-1.5-proandGPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy onM3-Bench-robot,M3-Bench-web\nand VideoMME-long, respectively. Our work advances themultimodal agents toward\nmore human-likelong-term mem",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/ByteDance-Seed/m3-agent",
    "hf_paper_url": "https://huggingface.co/papers/2508.09736",
    "arxiv_url": "https://arxiv.org/abs/2508.09736",
    "num_models": 2,
    "models_list": "ByteDance-Seed/M3-Agent-Control, ByteDance-Seed/M3-Agent-Memorization",
    "models_links": "https://huggingface.co/ByteDance-Seed/M3-Agent-Control, https://huggingface.co/ByteDance-Seed/M3-Agent-Memorization",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/M3-Agent-Control\", \"link\": \"https://huggingface.co/ByteDance-Seed/M3-Agent-Control\", \"task\": \"\", \"likes\": \"151\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"ByteDance-Seed/M3-Agent-Memorization\", \"link\": \"https://huggingface.co/ByteDance-Seed/M3-Agent-Memorization\", \"task\": \"\", \"likes\": \"131\", \"downloads\": \"\", \"updated\": \"Aug 14\"}]",
    "num_datasets": 2,
    "datasets_list": "ByteDance-Seed/M3-Bench, David0219/TeleEgo",
    "datasets_links": "https://huggingface.co/datasets/ByteDance-Seed/M3-Bench, https://huggingface.co/datasets/David0219/TeleEgo",
    "datasets_detailed": "[{\"name\": \"ByteDance-Seed/M3-Bench\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/M3-Bench\", \"task\": \"\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"Aug 14\", \"size\": \"\"}, {\"name\": \"David0219/TeleEgo\", \"link\": \"https://huggingface.co/datasets/David0219/TeleEgo\", \"task\": \"\", \"likes\": \"563\", \"downloads\": \"\", \"updated\": \"Nov 11\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.05748",
    "first_seen_date": "2025-08-13",
    "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.05748WebWatcher: Breaking New Frontier of Vision-Language Deep Research AgentPublished on Aug 7\u00b7Submitted byPeng Xiaon Aug 13#1 Paper of the day\u00b7Alibaba-NLPUpvote141+133Authors:Xinyu Geng,Peng Xia,Zhen Zhang,Xinyu Wang,Qiuchen Wang,Ruixue Ding,Chenxi Wang,Jialong Wu,Yida Zhao,Kuan Li,Yong Jiang,Pengjun Xie,Fei Huang,Jingren ZhouAbstractWebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.AI-generated summaryWeb agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makesmultimodalDeep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality syntheticmultimodaltrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization throughreinforcement learning.\nTo better evaluate the capabilities ofmultimodalagents, we proposeBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complexmultimodalinformation-seeking tasks.View arXiv pageView PDFProject pageGitHub17.7kAdd to collectionCo",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/Alibaba-NLP/WebAgent",
    "hf_paper_url": "https://huggingface.co/papers/2508.05748",
    "arxiv_url": "https://arxiv.org/abs/2508.05748",
    "num_models": 2,
    "models_list": "Alibaba-NLP/WebWatcher-32B, Alibaba-NLP/WebWatcher-7B",
    "models_links": "https://huggingface.co/Alibaba-NLP/WebWatcher-32B, https://huggingface.co/Alibaba-NLP/WebWatcher-7B",
    "models_detailed": "[{\"name\": \"Alibaba-NLP/WebWatcher-32B\", \"link\": \"https://huggingface.co/Alibaba-NLP/WebWatcher-32B\", \"task\": \"\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"Alibaba-NLP/WebWatcher-7B\", \"link\": \"https://huggingface.co/Alibaba-NLP/WebWatcher-7B\", \"task\": \"\", \"likes\": \"119\", \"downloads\": \"\", \"updated\": \"Sep 1\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.07629",
    "first_seen_date": "2025-08-12",
    "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving\n  Clipping Policy Optimization",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.07629Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving\n  Clipping Policy OptimizationPublished on Aug 11\u00b7Submitted bysuuon Aug 12Upvote43+35Authors:Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui ZhouAbstractKlear-Reasoner, a model with long reasoning capabilities, achieves high performance across benchmarks through detailed post-training workflows, including long Chain-of-Thought supervised fine-tuning and reinforcement learning with Gradient-Preserving clipping Policy Optimization.AI-generated summaryWe present Klear-Reasoner, a model withlong reasoningcapabilities that\ndemonstrates careful deliberation during problem solving, achieving outstanding\nperformance across multiple benchmarks. Although there are already many\nexcellent works related to inference models in the current community, there are\nstill many problems with reproducing high-performance inference models due to\nincomplete disclosure of training details. This report provides an in-depth\nanalysis of the reasoning model, covering the entire post-training workflow\nfrom data preparation and longChain-of-Thought supervised fine-tuning(long\nCoT SFT) toreinforcement learning(RL), along with detailed ablation studies\nfor each experimental component. For SFT data, our experiments show that a\nsmall number of high-quality data sources are more effective than a large\nnumber of diverse data sources, and that difficult samples can achieve better\nresults without accuracy filtering. In addition, we investigate two key issues\nwith current clipping mechanisms inRL: Clipping suppresses criticalexploration signalsand ignoressuboptimal trajectories. To address these\nchallenges, we proposeGradient-Preserving clipping Policy Optimization(GPPO)\nthat gently backpropagatesgradientsfromclipped tokens.GPPOnot only\nenhances the model's exploration capacity but also improves its eff",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/suu990901/KlearReasoner",
    "hf_paper_url": "https://huggingface.co/papers/2508.07629",
    "arxiv_url": "https://arxiv.org/abs/2508.07629",
    "num_models": 2,
    "models_list": "Kwai-Klear/Klear-Reasoner-8B, Kwai-Klear/Klear-Reasoner-8B-SFT",
    "models_links": "https://huggingface.co/Kwai-Klear/Klear-Reasoner-8B, https://huggingface.co/Kwai-Klear/Klear-Reasoner-8B-SFT",
    "models_detailed": "[{\"name\": \"Kwai-Klear/Klear-Reasoner-8B\", \"link\": \"https://huggingface.co/Kwai-Klear/Klear-Reasoner-8B\", \"task\": \"\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Sep 27\"}, {\"name\": \"Kwai-Klear/Klear-Reasoner-8B-SFT\", \"link\": \"https://huggingface.co/Kwai-Klear/Klear-Reasoner-8B-SFT\", \"task\": \"\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Sep 27\"}]",
    "num_datasets": 7,
    "datasets_list": "allenai/Dolci-Think-RL-7B, allenai/Dolci-Think-RL-32B, allenai/Dolci-Instruct-RL, allenai/Dolci-Think-RL-7B-Completions-SFT, Kwai-Klear/KlearReasoner-MathSub-30K, Kwai-Klear/KlearReasoner-CodeSub-15K, allenai/Dolci-Think-RL-7B-Completions-DPO",
    "datasets_links": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B, https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B, https://huggingface.co/datasets/allenai/Dolci-Instruct-RL, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT, https://huggingface.co/datasets/Kwai-Klear/KlearReasoner-MathSub-30K, https://huggingface.co/datasets/Kwai-Klear/KlearReasoner-CodeSub-15K, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO",
    "datasets_detailed": "[{\"name\": \"allenai/Dolci-Think-RL-7B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-32B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Instruct-RL\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Instruct-RL\", \"task\": \"\", \"likes\": \"712\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-SFT\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT\", \"task\": \"\", \"likes\": \"414\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"Kwai-Klear/KlearReasoner-MathSub-30K\", \"link\": \"https://huggingface.co/datasets/Kwai-Klear/KlearReasoner-MathSub-30K\", \"task\": \"\", \"likes\": \"96\", \"downloads\": \"\", \"updated\": \"Sep 27\", \"size\": \"\"}, {\"name\": \"Kwai-Klear/KlearReasoner-CodeSub-15K\", \"link\": \"https://huggingface.co/datasets/Kwai-Klear/KlearReasoner-CodeSub-15K\", \"task\": \"\", \"likes\": \"167\", \"downloads\": \"\", \"updated\": \"Sep 27\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-DPO\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO\", \"task\": \"\", \"likes\": \"335\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.07917",
    "first_seen_date": "2025-08-12",
    "title": "MolmoAct: Action Reasoning Models that can Reason in Space",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.07917MolmoAct: Action Reasoning Models that can Reason in SpacePublished on Aug 11\u00b7Submitted byDuanon Aug 12\u00b7Ai2Upvote44+36Authors:Jason Lee,Jiafei Duan,Haoquan Fang,Yuquan Deng,Shuo Liu,Boyang Li,Bohan Fang,Jieyu Zhang,Yi Ru Wang,Sangho Lee,Winson Han,Wilbert Pumacay,Angelica Wu,Rose Hendrix,Karen Farley,Eli VanderBilt,Ali Farhadi,Dieter Fox,Ranjay KrishnaAbstractAction Reasoning Models (ARMs) integrate perception, planning, and control to enable adaptable and explainable robotic behavior, achieving superior performance across various tasks and settings.AI-generated summaryReasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduceAction Reasoning Models(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,MolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generatesmid-level spatial plansas editabletrajectory traces, and\npredicts preciselow-level actions, enabling explainable and steerable\nbehavior.MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy onSimplerEnv Visual Matchingtasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success onLIBERO, including an additional 6.3% gain overThinkActon long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores foropen-ended instruction followingand trajectory\nsteering. Furthermore, we release, for the first time, theMolmoAct Dataset--\na mid-training robot dataset compri",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/allenai/molmoact",
    "hf_paper_url": "https://huggingface.co/papers/2508.07917",
    "arxiv_url": "https://arxiv.org/abs/2508.07917",
    "num_models": 9,
    "models_list": "allenai/MolmoAct-7B-D-0812, allenai/MolmoAct-7B-D-Pretrain-0812, allenai/MolmoAct-7B-D-Pretrain-RT-1-0812, allenai/MolmoAct-7B-O-0812, allenai/MolmoAct-7B-D-LIBERO-Long-0812, allenai/MolmoAct-7B-D-LIBERO-Goal-0812, allenai/MolmoAct-7B-D-LIBERO-Object-0812, allenai/MolmoAct-7B-D-LIBERO-Spatial-0812, allenai/MolmoAct-7B-D-Captioner-0812",
    "models_links": "https://huggingface.co/allenai/MolmoAct-7B-D-0812, https://huggingface.co/allenai/MolmoAct-7B-D-Pretrain-0812, https://huggingface.co/allenai/MolmoAct-7B-D-Pretrain-RT-1-0812, https://huggingface.co/allenai/MolmoAct-7B-O-0812, https://huggingface.co/allenai/MolmoAct-7B-D-LIBERO-Long-0812, https://huggingface.co/allenai/MolmoAct-7B-D-LIBERO-Goal-0812, https://huggingface.co/allenai/MolmoAct-7B-D-LIBERO-Object-0812, https://huggingface.co/allenai/MolmoAct-7B-D-LIBERO-Spatial-0812, https://huggingface.co/allenai/MolmoAct-7B-D-Captioner-0812",
    "models_detailed": "[{\"name\": \"allenai/MolmoAct-7B-D-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-D-0812\", \"task\": \"\", \"likes\": \"608\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"allenai/MolmoAct-7B-D-Pretrain-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-D-Pretrain-0812\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"allenai/MolmoAct-7B-D-Pretrain-RT-1-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-D-Pretrain-RT-1-0812\", \"task\": \"\", \"likes\": \"131\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"allenai/MolmoAct-7B-O-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-O-0812\", \"task\": \"\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"allenai/MolmoAct-7B-D-LIBERO-Long-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-D-LIBERO-Long-0812\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"allenai/MolmoAct-7B-D-LIBERO-Goal-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-D-LIBERO-Goal-0812\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"allenai/MolmoAct-7B-D-LIBERO-Object-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-D-LIBERO-Object-0812\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"allenai/MolmoAct-7B-D-LIBERO-Spatial-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-D-LIBERO-Spatial-0812\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"allenai/MolmoAct-7B-D-Captioner-0812\", \"link\": \"https://huggingface.co/allenai/MolmoAct-7B-D-Captioner-0812\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}]",
    "num_datasets": 3,
    "datasets_list": "allenai/MolmoAct-Dataset, allenai/MolmoAct-Pretraining-Mixture, allenai/MolmoAct-Midtraining-Mixture",
    "datasets_links": "https://huggingface.co/datasets/allenai/MolmoAct-Dataset, https://huggingface.co/datasets/allenai/MolmoAct-Pretraining-Mixture, https://huggingface.co/datasets/allenai/MolmoAct-Midtraining-Mixture",
    "datasets_detailed": "[{\"name\": \"allenai/MolmoAct-Dataset\", \"link\": \"https://huggingface.co/datasets/allenai/MolmoAct-Dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"allenai/MolmoAct-Pretraining-Mixture\", \"link\": \"https://huggingface.co/datasets/allenai/MolmoAct-Pretraining-Mixture\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 10\", \"size\": \"\"}, {\"name\": \"allenai/MolmoAct-Midtraining-Mixture\", \"link\": \"https://huggingface.co/datasets/allenai/MolmoAct-Midtraining-Mixture\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.06471",
    "first_seen_date": "2025-08-11",
    "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.06471GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation ModelsPublished on Aug 8\u00b7Submitted byTiezhen WANGon Aug 11#1 Paper of the dayUpvote195+187Authors:GLM-4. 5 Team,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu+149 authorsAbstractGLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.AI-generated summaryWe present GLM-4.5, an open-sourceMixture-of-Experts(MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring ahybrid reasoning methodthat supports both thinking and direct response modes.\nThroughmulti-stage trainingon 23T tokens and comprehensive post-training withexpert model iterationandreinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% onTAU-Bench, 91.0% onAIME 24, and 64.2% onSWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd onagentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.View arXiv pageView PDFGitHub3.32kAdd to collectionCommunityxianbaoPaper submitterAug 11GLM 4.5 technical paperSee translation1 reply\u00b7MagistrTheOneOct 6We already did it on Moe's platform before your presentation. Thanks, though.See translationgrantsingAug 12arXiv explained breakdown of this paper \ud83d\udc49https://arxivexplained.com/papers/glm-45-agentic-reasoning-and-coding-arc-fo",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/zai-org/GLM-4.5",
    "hf_paper_url": "https://huggingface.co/papers/2508.06471",
    "arxiv_url": "https://arxiv.org/abs/2508.06471",
    "num_models": 49,
    "models_list": "zai-org/GLM-4.5, zai-org/GLM-4.6, zai-org/GLM-4.5-Air, unsloth/GLM-4.6-GGUF, ArliAI/GLM-4.6-Derestricted-v3, ArliAI/GLM-4.5-Air-Derestricted, zai-org/GLM-4.5-Air-FP8, zai-org/GLM-4.6-FP8, zai-org/GLM-4.5-Air-Base, zai-org/GLM-4.5-Base, QuantTrio/GLM-4.6-AWQ, cyankiwi/GLM-4.6-AWQ-4bit, zai-org/GLM-4.5-FP8, jobs-git/GLM-4.5, dasLOL/Affine-NEW2, jonlizardo/affine-glm-4.5-grpo-02, Jackmin108/glm-0.5B-old, Jackmin108/GLM-4.5-Air-Fast, mikasenghaas/GLM-4.5-Air-Qwen-Chat-Template, danikhan632/glm-4.5-1b, PrimeIntellect/GLM-0.5B, IntervitensInc/GLM-4.6-Channel-int8, unsloth/GLM-4.6, alpindale/GLM-4.6-INT8, xaura/affine-gki, alpindale/GLM-4.6-FP4, QuantTrio/GLM-4.6-GPTQ-Int4-Int8Mix, Pavvav/Affine-Model-G, maywell/GLM-4.5-Air-GLM-4.6-Distill, Chhfjfs/Porsche, keatone/air-base-lacuna, ginipick/GLM-4.6, aiqtech/GLM-4.6, seawolf2357/GLM-4.6, fantaxy/GLM-4.6, fantos/GLM-4.6, ArliAI/GLM-4.5-Air-Derestricted-FP8, ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8, noctrex/GLM-4.5-Air-Derestricted-MXFP4_MOE-GGUF, Alrott/GLM-4.5-Air-RS-256k, cyankiwi/GLM-4.5-Air-Derestricted-AWQ-4bit, cyankiwi/GLM-4.5-Air-Derestricted-AWQ-8bit, avtc/GLM-4.5-Air-GPTQMODEL-W8A16, ArliAI/GLM-4.5-Air-Derestricted-GPTQ-W4A16, ArliAI/GLM-4.6-Derestricted-GPTQ-W4A16, ArliAI/GLM-4.5-Air-Derestricted-GPTQ-Int4-Int8-Mixed, roleplaiapp/glm-air, avtc/GLM-4.5-Air-GPTQMODEL-W4A16, gecfdo/GLM-4.5-Air_EXL3_6.38bpw_H16",
    "models_links": "https://huggingface.co/zai-org/GLM-4.5, https://huggingface.co/zai-org/GLM-4.6, https://huggingface.co/zai-org/GLM-4.5-Air, https://huggingface.co/unsloth/GLM-4.6-GGUF, https://huggingface.co/ArliAI/GLM-4.6-Derestricted-v3, https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted, https://huggingface.co/zai-org/GLM-4.5-Air-FP8, https://huggingface.co/zai-org/GLM-4.6-FP8, https://huggingface.co/zai-org/GLM-4.5-Air-Base, https://huggingface.co/zai-org/GLM-4.5-Base, https://huggingface.co/QuantTrio/GLM-4.6-AWQ, https://huggingface.co/cyankiwi/GLM-4.6-AWQ-4bit, https://huggingface.co/zai-org/GLM-4.5-FP8, https://huggingface.co/jobs-git/GLM-4.5, https://huggingface.co/dasLOL/Affine-NEW2, https://huggingface.co/jonlizardo/affine-glm-4.5-grpo-02, https://huggingface.co/Jackmin108/glm-0.5B-old, https://huggingface.co/Jackmin108/GLM-4.5-Air-Fast, https://huggingface.co/mikasenghaas/GLM-4.5-Air-Qwen-Chat-Template, https://huggingface.co/danikhan632/glm-4.5-1b, https://huggingface.co/PrimeIntellect/GLM-0.5B, https://huggingface.co/IntervitensInc/GLM-4.6-Channel-int8, https://huggingface.co/unsloth/GLM-4.6, https://huggingface.co/alpindale/GLM-4.6-INT8, https://huggingface.co/xaura/affine-gki, https://huggingface.co/alpindale/GLM-4.6-FP4, https://huggingface.co/QuantTrio/GLM-4.6-GPTQ-Int4-Int8Mix, https://huggingface.co/Pavvav/Affine-Model-G, https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill, https://huggingface.co/Chhfjfs/Porsche, https://huggingface.co/keatone/air-base-lacuna, https://huggingface.co/ginipick/GLM-4.6, https://huggingface.co/aiqtech/GLM-4.6, https://huggingface.co/seawolf2357/GLM-4.6, https://huggingface.co/fantaxy/GLM-4.6, https://huggingface.co/fantos/GLM-4.6, https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-FP8, https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8, https://huggingface.co/noctrex/GLM-4.5-Air-Derestricted-MXFP4_MOE-GGUF, https://huggingface.co/Alrott/GLM-4.5-Air-RS-256k, https://huggingface.co/cyankiwi/GLM-4.5-Air-Derestricted-AWQ-4bit, https://huggingface.co/cyankiwi/GLM-4.5-Air-Derestricted-AWQ-8bit, https://huggingface.co/avtc/GLM-4.5-Air-GPTQMODEL-W8A16, https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-GPTQ-W4A16, https://huggingface.co/ArliAI/GLM-4.6-Derestricted-GPTQ-W4A16, https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-GPTQ-Int4-Int8-Mixed, https://huggingface.co/roleplaiapp/glm-air, https://huggingface.co/avtc/GLM-4.5-Air-GPTQMODEL-W4A16, https://huggingface.co/gecfdo/GLM-4.5-Air_EXL3_6.38bpw_H16",
    "models_detailed": "[{\"name\": \"zai-org/GLM-4.5\", \"link\": \"https://huggingface.co/zai-org/GLM-4.5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"zai-org/GLM-4.6\", \"link\": \"https://huggingface.co/zai-org/GLM-4.6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 30\"}, {\"name\": \"zai-org/GLM-4.5-Air\", \"link\": \"https://huggingface.co/zai-org/GLM-4.5-Air\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"unsloth/GLM-4.6-GGUF\", \"link\": \"https://huggingface.co/unsloth/GLM-4.6-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 9\"}, {\"name\": \"ArliAI/GLM-4.6-Derestricted-v3\", \"link\": \"https://huggingface.co/ArliAI/GLM-4.6-Derestricted-v3\", \"task\": \"Text Generation\", \"likes\": \"159\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"ArliAI/GLM-4.5-Air-Derestricted\", \"link\": \"https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"zai-org/GLM-4.5-Air-FP8\", \"link\": \"https://huggingface.co/zai-org/GLM-4.5-Air-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 12\"}, {\"name\": \"zai-org/GLM-4.6-FP8\", \"link\": \"https://huggingface.co/zai-org/GLM-4.6-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"zai-org/GLM-4.5-Air-Base\", \"link\": \"https://huggingface.co/zai-org/GLM-4.5-Air-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"zai-org/GLM-4.5-Base\", \"link\": \"https://huggingface.co/zai-org/GLM-4.5-Base\", \"task\": \"Text Generation\", \"likes\": \"596\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"QuantTrio/GLM-4.6-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/GLM-4.6-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"cyankiwi/GLM-4.6-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/GLM-4.6-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"686\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"zai-org/GLM-4.5-FP8\", \"link\": \"https://huggingface.co/zai-org/GLM-4.5-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 12\"}, {\"name\": \"jobs-git/GLM-4.5\", \"link\": \"https://huggingface.co/jobs-git/GLM-4.5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"dasLOL/Affine-NEW2\", \"link\": \"https://huggingface.co/dasLOL/Affine-NEW2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 16\"}, {\"name\": \"jonlizardo/affine-glm-4.5-grpo-02\", \"link\": \"https://huggingface.co/jonlizardo/affine-glm-4.5-grpo-02\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 22\"}, {\"name\": \"Jackmin108/glm-0.5B-old\", \"link\": \"https://huggingface.co/Jackmin108/glm-0.5B-old\", \"task\": \"Text Generation\", \"likes\": \"4\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"Jackmin108/GLM-4.5-Air-Fast\", \"link\": \"https://huggingface.co/Jackmin108/GLM-4.5-Air-Fast\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"mikasenghaas/GLM-4.5-Air-Qwen-Chat-Template\", \"link\": \"https://huggingface.co/mikasenghaas/GLM-4.5-Air-Qwen-Chat-Template\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 9\"}, {\"name\": \"danikhan632/glm-4.5-1b\", \"link\": \"https://huggingface.co/danikhan632/glm-4.5-1b\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"PrimeIntellect/GLM-0.5B\", \"link\": \"https://huggingface.co/PrimeIntellect/GLM-0.5B\", \"task\": \"Text Generation\", \"likes\": \"489\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"IntervitensInc/GLM-4.6-Channel-int8\", \"link\": \"https://huggingface.co/IntervitensInc/GLM-4.6-Channel-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 30\"}, {\"name\": \"unsloth/GLM-4.6\", \"link\": \"https://huggingface.co/unsloth/GLM-4.6\", \"task\": \"Text Generation\", \"likes\": \"55\", \"downloads\": \"\", \"updated\": \"Oct 1\"}, {\"name\": \"alpindale/GLM-4.6-INT8\", \"link\": \"https://huggingface.co/alpindale/GLM-4.6-INT8\", \"task\": \"Text Generation\", \"likes\": \"51\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"xaura/affine-gki\", \"link\": \"https://huggingface.co/xaura/affine-gki\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"alpindale/GLM-4.6-FP4\", \"link\": \"https://huggingface.co/alpindale/GLM-4.6-FP4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"QuantTrio/GLM-4.6-GPTQ-Int4-Int8Mix\", \"link\": \"https://huggingface.co/QuantTrio/GLM-4.6-GPTQ-Int4-Int8Mix\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"Pavvav/Affine-Model-G\", \"link\": \"https://huggingface.co/Pavvav/Affine-Model-G\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"maywell/GLM-4.5-Air-GLM-4.6-Distill\", \"link\": \"https://huggingface.co/maywell/GLM-4.5-Air-GLM-4.6-Distill\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Oct 11\"}, {\"name\": \"Chhfjfs/Porsche\", \"link\": \"https://huggingface.co/Chhfjfs/Porsche\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"keatone/air-base-lacuna\", \"link\": \"https://huggingface.co/keatone/air-base-lacuna\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"ginipick/GLM-4.6\", \"link\": \"https://huggingface.co/ginipick/GLM-4.6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"aiqtech/GLM-4.6\", \"link\": \"https://huggingface.co/aiqtech/GLM-4.6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"seawolf2357/GLM-4.6\", \"link\": \"https://huggingface.co/seawolf2357/GLM-4.6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"fantaxy/GLM-4.6\", \"link\": \"https://huggingface.co/fantaxy/GLM-4.6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"fantos/GLM-4.6\", \"link\": \"https://huggingface.co/fantos/GLM-4.6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"ArliAI/GLM-4.5-Air-Derestricted-FP8\", \"link\": \"https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8\", \"link\": \"https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-W8A8-INT8\", \"task\": \"Text Generation\", \"likes\": \"120\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"noctrex/GLM-4.5-Air-Derestricted-MXFP4_MOE-GGUF\", \"link\": \"https://huggingface.co/noctrex/GLM-4.5-Air-Derestricted-MXFP4_MOE-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"Alrott/GLM-4.5-Air-RS-256k\", \"link\": \"https://huggingface.co/Alrott/GLM-4.5-Air-RS-256k\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"cyankiwi/GLM-4.5-Air-Derestricted-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/GLM-4.5-Air-Derestricted-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"cyankiwi/GLM-4.5-Air-Derestricted-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/GLM-4.5-Air-Derestricted-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"avtc/GLM-4.5-Air-GPTQMODEL-W8A16\", \"link\": \"https://huggingface.co/avtc/GLM-4.5-Air-GPTQMODEL-W8A16\", \"task\": \"Text Generation\", \"likes\": \"32\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"ArliAI/GLM-4.5-Air-Derestricted-GPTQ-W4A16\", \"link\": \"https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-GPTQ-W4A16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"21 days ago\"}, {\"name\": \"ArliAI/GLM-4.6-Derestricted-GPTQ-W4A16\", \"link\": \"https://huggingface.co/ArliAI/GLM-4.6-Derestricted-GPTQ-W4A16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"ArliAI/GLM-4.5-Air-Derestricted-GPTQ-Int4-Int8-Mixed\", \"link\": \"https://huggingface.co/ArliAI/GLM-4.5-Air-Derestricted-GPTQ-Int4-Int8-Mixed\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\"}, {\"name\": \"roleplaiapp/glm-air\", \"link\": \"https://huggingface.co/roleplaiapp/glm-air\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"avtc/GLM-4.5-Air-GPTQMODEL-W4A16\", \"link\": \"https://huggingface.co/avtc/GLM-4.5-Air-GPTQMODEL-W4A16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"gecfdo/GLM-4.5-Air_EXL3_6.38bpw_H16\", \"link\": \"https://huggingface.co/gecfdo/GLM-4.5-Air_EXL3_6.38bpw_H16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.02038",
    "first_seen_date": "2025-08-08",
    "title": "Marco-Voice Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.02038Marco-Voice Technical ReportPublished on Aug 4\u00b7Submitted byChenyang Lyuon Aug 8Upvote16+8Authors:Fengping Tian,Chenyang Lyu,Xuanfan Ni,Haoqin Sun,Qingjuan Li,Zhiqiang Qian,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu ZhangAbstractA multifunctional speech synthesis system integrates voice cloning and emotion control using speaker-emotion disentanglement and rotational emotional embeddings, achieving high expressive and natural speech.AI-generated summaryThis paper presents a multifunctional speech synthesis system that integratesvoice cloningandemotion controlspeech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effectivespeaker-emotion disentanglementmechanism\nwithin-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smoothemotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-qualityemotional speech datasetcontaining 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system,Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field ofexpressive neural speech synthesis.View arXiv pageView PDFProject pageGitHub398Add to collectionCommunityChenyangLyuPaper submitterAug 8See translationReplylibrarian-botAug 9This is an automated message from th",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/AIDC-AI/Marco-Voice",
    "hf_paper_url": "https://huggingface.co/papers/2508.02038",
    "arxiv_url": "https://arxiv.org/abs/2508.02038",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "AIDC-AI/CSEMOTIONS, sdcsdccdsd/CSEMOTIONS",
    "datasets_links": "https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS, https://huggingface.co/datasets/sdcsdccdsd/CSEMOTIONS",
    "datasets_detailed": "[{\"name\": \"AIDC-AI/CSEMOTIONS\", \"link\": \"https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS\", \"task\": \"\", \"likes\": \"676\", \"downloads\": \"\", \"updated\": \"Aug 12\", \"size\": \"\"}, {\"name\": \"sdcsdccdsd/CSEMOTIONS\", \"link\": \"https://huggingface.co/datasets/sdcsdccdsd/CSEMOTIONS\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.05618",
    "first_seen_date": "2025-08-08",
    "title": "Learning to Reason for Factuality",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.05618Learning to Reason for FactualityPublished on Aug 7\u00b7Submitted byXilun Chenon Aug 8Upvote6Authors:Xilun Chen,Ilia Kulikov,Vincent-Pierre Berges,Barlas O\u011fuz,Rulin Shao,Gargi Ghosh,Jason Weston,Wen-tau YihAbstractA novel reward function for online reinforcement learning improves factuality and detail in reasoning large language models without reducing helpfulness.AI-generated summaryReasoning Large Language Models(R-LLMs) have significantly advanced complex\nreasoning tasks but often struggle with factuality, generating substantially\nmore hallucinations than their non-reasoning counterparts on long-form\nfactuality benchmarks. However, extending onlineReinforcement Learning(RL), a\nkey component in recent R-LLM advancements, to the long-form factuality setting\nposes several unique challenges due to the lack of reliable verification\nmethods. Previous work has utilized automatic factuality evaluation frameworks\nsuch asFActScoreto curate preference data in theoffline RLsetting, yet we\nfind that directly leveraging such methods as the reward inonline RLleads toreward hackingin multiple ways, such as producing less detailed or relevant\nresponses. We propose a novel reward function that simultaneously considers thefactual precision, response detail level, andanswer relevance, and appliesonline RLto learn high quality factual reasoning. Evaluated on six long-form\nfactuality benchmarks, our factual reasoning model achieves an average\nreduction of 23.1 percentage points in hallucination rate, a 23% increase in\nanswer detail level, and no degradation in the overall response helpfulness.View arXiv pageView PDFAdd to collectionCommunityccsasukePaper submitterAug 8Online RL to learn reasoning strategies for long-form factualitySee translationReplylibrarian-botAug 9This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers wer",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2508.05618",
    "arxiv_url": "https://arxiv.org/abs/2508.05618",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "facebook/factual_reasoning",
    "datasets_links": "https://huggingface.co/datasets/facebook/factual_reasoning",
    "datasets_detailed": "[{\"name\": \"facebook/factual_reasoning\", \"link\": \"https://huggingface.co/datasets/facebook/factual_reasoning\", \"task\": \"\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Sep 25\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.00109",
    "first_seen_date": "2025-08-07",
    "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form\n  Factuality",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.00109FACTORY: A Challenging Human-Verified Prompt Set for Long-Form\n  FactualityPublished on Jul 31\u00b7Submitted byMingda Chenon Aug 7Upvote4Authors:Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott YihAbstractFACTORY, a human-verified prompt set, evaluates the factuality of long-form responses from language models, revealing higher factual accuracy compared to existing datasets.AI-generated summaryLong-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using amodel-in-the-loopapproach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conducthuman evaluationson 6state-of-the-art language modelsusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.View arXiv pageView PDFProject pageAdd to collectionCommunitymingdachenmetaPaper submitterAug 7FACTORY is a large-scale, human-verified, and challenging prompt set for long-form factuality.See translationReplylibrarian-botAug 8This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIVerifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers(2025)RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking(2025)CoT-Self-I",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2508.00109",
    "arxiv_url": "https://arxiv.org/abs/2508.00109",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "facebook/FACTORY",
    "datasets_links": "https://huggingface.co/datasets/facebook/FACTORY",
    "datasets_detailed": "[{\"name\": \"facebook/FACTORY\", \"link\": \"https://huggingface.co/datasets/facebook/FACTORY\", \"task\": \"\", \"likes\": \"193\", \"downloads\": \"\", \"updated\": \"Aug 7\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.01630",
    "first_seen_date": "2025-08-07",
    "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.01630OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public DatasetsPublished on Aug 3\u00b7Submitted byMaziyar Panahion Aug 7Upvote12+4Authors:Maziyar PanahiAbstractOpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.AI-generated summaryNamed-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adaptedtransformer modelsthat combinelightweight domain-adaptive pre-training (DAPT)withparameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) usingDeBERTa-v3,PubMedBERT, andBioELECTRAbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases,genes, and\nspecies. OpenMed NER achieves new state-of-the-artmicro-F1 scoreson 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g.,BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specializedgeneandclinical cell line corpora. This work demonstrates that ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2508.01630",
    "arxiv_url": "https://arxiv.org/abs/2508.01630",
    "num_models": 301,
    "models_list": "OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M, OpenMed/OpenMed-NER-PathologyDetect-BigMed-560M, OpenMed/OpenMed-NER-OncologyDetect-SuperClinical-434M, OpenMed/OpenMed-NER-DiseaseDetect-SuperClinical-434M, OpenMed/OpenMed-ZeroShot-NER-Pharma-Medium-209M, OpenMed/OpenMed-ZeroShot-NER-Chemical-Medium-209M, OpenMed/OpenMed-ZeroShot-NER-Pharma-Large-459M, OpenMed/OpenMed-ZeroShot-NER-Oncology-Base-220M, OpenMed/OpenMed-NER-ChemicalDetect-PubMed-109M, OpenMed/OpenMed-NER-PathologyDetect-PubMed-109M, OpenMed/OpenMed-NER-DNADetect-BioMed-109M, OpenMed/OpenMed-NER-DiseaseDetect-SnowMed-568M, OpenMed/OpenMed-NER-DiseaseDetect-BioMed-335M, OpenMed/OpenMed-NER-OrganismDetect-BioMed-109M, OpenMed/OpenMed-ZeroShot-NER-Pharma-XLarge-770M, OpenMed/OpenMed-ZeroShot-NER-Chemical-Multi-209M, OpenMed/OpenMed-ZeroShot-NER-Anatomy-Small-166M, OpenMed/OpenMed-ZeroShot-NER-Disease-Medium-209M, OpenMed/OpenMed-ZeroShot-NER-DNA-Large-459M, OpenMed/OpenMed-NER-PharmaDetect-SuperMedical-125M, OpenMed/OpenMed-NER-OncologyDetect-TinyMed-65M, OpenMed/OpenMed-NER-ChemicalDetect-PubMed-335M, OpenMed/OpenMed-NER-SpeciesDetect-BioClinical-108M, OpenMed/OpenMed-NER-ChemicalDetect-BigMed-278M, OpenMed/OpenMed-NER-ChemicalDetect-EuroMed-212M, OpenMed/OpenMed-NER-DNADetect-BioClinical-108M, OpenMed/OpenMed-NER-OncologyDetect-SuperClinical-141M, OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-135M, OpenMed/OpenMed-NER-ProteinDetect-PubMed-109M, OpenMed/OpenMed-NER-PharmaDetect-SuperMedical-355M, OpenMed/OpenMed-NER-PharmaDetect-BioMed-335M, OpenMed/OpenMed-NER-ProteinDetect-MultiMed-335M, OpenMed/OpenMed-NER-SpeciesDetect-ModernMed-395M, OpenMed/OpenMed-NER-PathologyDetect-ModernClinical-395M, OpenMed/OpenMed-NER-GenomeDetect-BioClinical-108M, OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-135M, OpenMed/OpenMed-NER-OncologyDetect-SuperMedical-125M, OpenMed/OpenMed-NER-BloodCancerDetect-SuperClinical-434M, OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-560M, OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-109M, OpenMed/OpenMed-NER-PharmaDetect-MultiMed-568M, OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-335M, OpenMed/OpenMed-NER-PharmaDetect-BigMed-560M, OpenMed/OpenMed-NER-ProteinDetect-PubMed-v2-109M, OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-184M, OpenMed/OpenMed-NER-OncologyDetect-BioPatient-108M, OpenMed/OpenMed-NER-AnatomyDetect-BioMed-109M, OpenMed/OpenMed-NER-ChemicalDetect-SuperMedical-355M, OpenMed/OpenMed-NER-DiseaseDetect-PubMed-335M, OpenMed/OpenMed-NER-AnatomyDetect-BigMed-560M, OpenMed/OpenMed-NER-OncologyDetect-BioClinical-108M, OpenMed/OpenMed-NER-DNADetect-SuperClinical-434M, OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-135M, OpenMed/OpenMed-NER-ProteinDetect-MultiMed-568M, OpenMed/OpenMed-NER-AnatomyDetect-ElectraMed-560M, OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-335M, OpenMed/OpenMed-NER-ProteinDetect-SuperMedical-355M, OpenMed/OpenMed-NER-DNADetect-PubMed-v2-109M, OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-33M, OpenMed/OpenMed-NER-PharmaDetect-MultiMed-335M, OpenMed/OpenMed-NER-PathologyDetect-SuperClinical-141M, OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-135M, OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-33M, OpenMed/OpenMed-NER-OrganismDetect-TinyMed-66M, OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-33M, OpenMed/OpenMed-NER-GenomicDetect-BioPatient-108M, OpenMed/OpenMed-NER-AnatomyDetect-PubMed-109M, OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-82M, OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-184M, OpenMed/OpenMed-NER-GenomeDetect-EuroMed-212M, OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-v2-109M, OpenMed/OpenMed-NER-OncologyDetect-MultiMed-568M, OpenMed/OpenMed-NER-DNADetect-ElectraMed-109M, OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-109M, OpenMed/OpenMed-NER-PharmaDetect-ModernClinical-395M, OpenMed/OpenMed-NER-AnatomyDetect-SuperMedical-355M, OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-560M, OpenMed/OpenMed-NER-PathologyDetect-TinyMed-66M, OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-65M, OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-335M, OpenMed/OpenMed-NER-DNADetect-ElectraMed-560M, OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-109M, OpenMed/OpenMed-NER-DNADetect-TinyMed-82M, OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-65M, OpenMed/OpenMed-NER-OrganismDetect-BigMed-560M, OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-33M, OpenMed/OpenMed-NER-ProteinDetect-BioMed-109M, OpenMed/OpenMed-NER-DiseaseDetect-ModernClinical-149M, OpenMed/OpenMed-NER-GenomicDetect-PubMed-v2-109M, OpenMed/OpenMed-NER-OncologyDetect-ModernClinical-395M, OpenMed/OpenMed-NER-DNADetect-BioPatient-108M, OpenMed/OpenMed-NER-PathologyDetect-BioMed-335M, OpenMed/OpenMed-NER-PathologyDetect-TinyMed-82M, OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-335M, OpenMed/OpenMed-NER-GenomicDetect-ModernClinical-395M, OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-33M, OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-82M, OpenMed/OpenMed-NER-AnatomyDetect-BioClinical-108M, OpenMed/OpenMed-NER-PharmaDetect-PubMed-109M, OpenMed/OpenMed-NER-AnatomyDetect-ModernMed-395M, OpenMed/OpenMed-NER-OrganismDetect-TinyMed-82M, OpenMed/OpenMed-NER-PathologyDetect-ModernMed-149M, OpenMed/OpenMed-NER-SpeciesDetect-BioMed-335M, OpenMed/OpenMed-NER-PharmaDetect-ModernMed-395M, OpenMed/OpenMed-NER-PharmaDetect-ModernMed-149M, OpenMed/OpenMed-NER-DNADetect-SnowMed-568M, OpenMed/OpenMed-NER-OncologyDetect-SuperMedical-355M, OpenMed/OpenMed-NER-SpeciesDetect-BioMed-109M, OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-560M, OpenMed/OpenMed-NER-AnatomyDetect-ModernClinical-149M, OpenMed/OpenMed-NER-PathologyDetect-SnowMed-568M, OpenMed/OpenMed-NER-OrganismDetect-TinyMed-135M, OpenMed/OpenMed-NER-PathologyDetect-SuperClinical-434M, OpenMed/OpenMed-NER-ProteinDetect-TinyMed-82M, OpenMed/OpenMed-NER-AnatomyDetect-ModernClinical-395M, OpenMed/OpenMed-NER-AnatomyDetect-SuperMedical-125M, OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-109M, OpenMed/OpenMed-NER-BloodCancerDetect-ModernMed-395M, OpenMed/OpenMed-NER-AnatomyDetect-BioMed-335M, OpenMed/OpenMed-NER-DNADetect-BioMed-335M, OpenMed/OpenMed-NER-OrganismDetect-ModernClinical-395M, OpenMed/OpenMed-NER-AnatomyDetect-MultiMed-568M, OpenMed/OpenMed-NER-SpeciesDetect-PubMed-335M, OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-66M, OpenMed/OpenMed-NER-ChemicalDetect-BigMed-560M, OpenMed/OpenMed-NER-OrganismDetect-PubMed-335M, OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-560M, OpenMed/OpenMed-NER-GenomeDetect-BioPatient-108M, OpenMed/OpenMed-NER-ProteinDetect-TinyMed-135M, OpenMed/OpenMed-NER-OrganismDetect-ModernClinical-149M, OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-434M, OpenMed/OpenMed-NER-DNADetect-SuperMedical-355M, OpenMed/OpenMed-NER-ProteinDetect-TinyMed-65M, OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-33M, OpenMed/OpenMed-NER-GenomeDetect-PubMed-109M, OpenMed/OpenMed-NER-GenomicDetect-BioMed-109M, OpenMed/OpenMed-NER-PharmaDetect-TinyMed-66M, OpenMed/OpenMed-NER-DNADetect-MultiMed-568M, OpenMed/OpenMed-NER-OrganismDetect-SuperMedical-355M, OpenMed/OpenMed-NER-ProteinDetect-ModernClinical-395M, OpenMed/OpenMed-NER-ProteinDetect-PubMed-335M, OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-66M, OpenMed/OpenMed-NER-DNADetect-SuperClinical-141M, OpenMed/OpenMed-NER-OrganismDetect-BioPatient-108M, OpenMed/OpenMed-NER-DNADetect-ElectraMed-33M, OpenMed/OpenMed-NER-PharmaDetect-BioMed-109M, OpenMed/OpenMed-NER-ChemicalDetect-BioMed-335M, OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-184M, OpenMed/OpenMed-NER-OrganismDetect-SnowMed-568M, OpenMed/OpenMed-NER-DiseaseDetect-MultiMed-335M, OpenMed/OpenMed-NER-GenomeDetect-TinyMed-82M, OpenMed/OpenMed-NER-ProteinDetect-SnowMed-568M, OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-335M, OpenMed/OpenMed-NER-AnatomyDetect-PubMed-335M, OpenMed/OpenMed-NER-GenomeDetect-SuperMedical-125M, OpenMed/OpenMed-NER-GenomicDetect-ModernMed-395M, OpenMed/OpenMed-NER-OncologyDetect-TinyMed-66M, OpenMed/OpenMed-NER-SpeciesDetect-PubMed-109M, OpenMed/OpenMed-NER-BloodCancerDetect-MultiMed-568M, OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-560M, OpenMed/OpenMed-NER-GenomeDetect-ModernMed-149M, OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-109M, OpenMed/OpenMed-NER-PathologyDetect-BioMed-109M, OpenMed/OpenMed-NER-SpeciesDetect-SnowMed-568M, OpenMed/OpenMed-NER-GenomicDetect-EuroMed-212M, OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-184M, OpenMed/OpenMed-NER-PharmaDetect-PubMed-v2-109M, OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-434M, OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-184M, OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-33M, OpenMed/OpenMed-NER-GenomicDetect-TinyMed-135M, OpenMed/OpenMed-NER-PathologyDetect-BioPatient-108M, OpenMed/OpenMed-NER-PathologyDetect-ModernMed-395M, OpenMed/OpenMed-NER-BloodCancerDetect-BioClinical-108M, OpenMed/OpenMed-NER-SpeciesDetect-BigMed-278M, OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-33M, OpenMed/OpenMed-NER-OncologyDetect-BigMed-278M, OpenMed/OpenMed-NER-OncologyDetect-SnowMed-568M, OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-141M, OpenMed/OpenMed-NER-OrganismDetect-BigMed-278M, OpenMed/OpenMed-NER-PathologyDetect-SuperMedical-125M, OpenMed/OpenMed-NER-GenomeDetect-BigMed-560M, OpenMed/OpenMed-NER-GenomicDetect-ModernClinical-149M, OpenMed/OpenMed-NER-OncologyDetect-PubMed-109M, OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-141M, OpenMed/OpenMed-NER-OrganismDetect-PubMed-v2-109M, OpenMed/OpenMed-NER-BloodCancerDetect-BioMed-335M, OpenMed/OpenMed-NER-PathologyDetect-ModernClinical-149M, OpenMed/OpenMed-NER-GenomeDetect-SuperMedical-355M, OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-335M, OpenMed/OpenMed-NER-GenomeDetect-ModernMed-395M, OpenMed/OpenMed-NER-PathologyDetect-PubMed-v2-109M, OpenMed/OpenMed-NER-PathologyDetect-MultiMed-568M, OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-66M, OpenMed/OpenMed-NER-OrganismDetect-ModernMed-395M, OpenMed/OpenMed-NER-GenomicDetect-ModernMed-149M, OpenMed/OpenMed-NER-ProteinDetect-BigMed-278M, OpenMed/OpenMed-NER-GenomeDetect-MultiMed-568M, OpenMed/OpenMed-NER-PharmaDetect-PubMed-335M, OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-184M, OpenMed/OpenMed-NER-OncologyDetect-BioMed-335M, OpenMed/OpenMed-NER-OncologyDetect-PubMed-v2-109M, OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-82M, OpenMed/OpenMed-NER-ProteinDetect-SuperClinical-184M, OpenMed/OpenMed-NER-GenomicDetect-PubMed-335M, OpenMed/OpenMed-NER-BloodCancerDetect-SnowMed-568M, OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-560M, OpenMed/OpenMed-NER-BloodCancerDetect-SuperClinical-141M, OpenMed/OpenMed-NER-ChemicalDetect-ElectraMed-560M, OpenMed/OpenMed-NER-DiseaseDetect-BigMed-560M, OpenMed/OpenMed-NER-PharmaDetect-SnowMed-568M, OpenMed/OpenMed-NER-PharmaDetect-TinyMed-82M, OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-141M, OpenMed/OpenMed-NER-ProteinDetect-EuroMed-212M, OpenMed/OpenMed-NER-DNADetect-ModernClinical-149M, OpenMed/OpenMed-NER-GenomicDetect-BigMed-560M, OpenMed/OpenMed-NER-BloodCancerDetect-SuperMedical-355M, OpenMed/OpenMed-NER-DNADetect-MultiMed-335M, OpenMed/OpenMed-NER-GenomeDetect-SnowMed-568M, OpenMed/OpenMed-NER-PharmaDetect-TinyMed-65M, OpenMed/OpenMed-NER-OncologyDetect-PubMed-335M, OpenMed/OpenMed-NER-ProteinDetect-BioClinical-108M, OpenMed/OpenMed-NER-GenomeDetect-BioMed-335M, OpenMed/OpenMed-NER-DiseaseDetect-BioClinical-108M, OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-335M, OpenMed/OpenMed-NER-ProteinDetect-BioPatient-108M, OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-109M, OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-109M, OpenMed/OpenMed-NER-BloodCancerDetect-ModernMed-149M, OpenMed/OpenMed-NER-ChemicalDetect-BioClinical-108M, OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-335M, OpenMed/OpenMed-NER-BloodCancerDetect-BigMed-560M, OpenMed/OpenMed-NER-GenomeDetect-TinyMed-66M, OpenMed/OpenMed-NER-OncologyDetect-TinyMed-82M, OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-109M, OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-560M, OpenMed/OpenMed-NER-GenomeDetect-ModernClinical-149M, OpenMed/OpenMed-NER-GenomeDetect-TinyMed-65M, OpenMed/OpenMed-NER-GenomicDetect-SnowMed-568M, OpenMed/OpenMed-NER-GenomicDetect-BigMed-278M, OpenMed/OpenMed-NER-AnatomyDetect-MultiMed-335M, OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-335M, OpenMed/OpenMed-NER-DNADetect-BigMed-278M, OpenMed/OpenMed-NER-GenomicDetect-TinyMed-82M, OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-141M, OpenMed/OpenMed-NER-DiseaseDetect-BigMed-278M, OpenMed/OpenMed-NER-ProteinDetect-ModernMed-395M, OpenMed/OpenMed-NER-PharmaDetect-BigMed-278M, OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-434M, OpenMed/OpenMed-NER-ProteinDetect-BigMed-560M, OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-33M, OpenMed/OpenMed-NER-ChemicalDetect-PubMed-v2-109M, OpenMed/OpenMed-NER-PharmaDetect-TinyMed-135M, OpenMed/OpenMed-NER-DNADetect-PubMed-335M, OpenMed/OpenMed-NER-DNADetect-ElectraMed-335M, OpenMed/OpenMed-NER-BloodCancerDetect-BioPatient-108M, OpenMed/OpenMed-NER-OncologyDetect-ModernClinical-149M, OpenMed/OpenMed-NER-SpeciesDetect-ModernMed-149M, OpenMed/OpenMed-NER-GenomicDetect-MultiMed-335M, OpenMed/OpenMed-NER-ChemicalDetect-ModernClinical-395M, OpenMed/OpenMed-NER-BloodCancerDetect-SuperMedical-125M, OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-560M, OpenMed/OpenMed-NER-PharmaDetect-BioPatient-108M, OpenMed/OpenMed-NER-GenomicDetect-SuperMedical-125M, OpenMed/OpenMed-NER-SpeciesDetect-MultiMed-335M, OpenMed/OpenMed-NER-OncologyDetect-TinyMed-135M, OpenMed/OpenMed-NER-ChemicalDetect-ModernMed-395M, OpenMed/OpenMed-NER-OrganismDetect-TinyMed-65M, OpenMed/OpenMed-NER-ChemicalDetect-BioPatient-108M, OpenMed/OpenMed-NER-DNADetect-ModernMed-395M, OpenMed/OpenMed-NER-DNADetect-PubMed-109M, OpenMed/OpenMed-NER-DiseaseDetect-ModernClinical-395M, OpenMed/OpenMed-NER-AnatomyDetect-SnowMed-568M, OpenMed/OpenMed-NER-ProteinDetect-ModernMed-149M, OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-434M, OpenMed/OpenMed-NER-BloodCancerDetect-MultiMed-335M, OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-109M, OpenMed/OpenMed-NER-OncologyDetect-ModernMed-395M, OpenMed/OpenMed-NER-DiseaseDetect-BioPatient-108M, OpenMed/OpenMed-NER-GenomicDetect-BioClinical-108M, OpenMed/OpenMed-NER-ChemicalDetect-ModernClinical-149M, OpenMed/OpenMed-NER-DNADetect-BigMed-560M, OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-141M, OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-434M, OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-560M, OpenMed/OpenMed-NER-ChemicalDetect-MultiMed-335M, OpenMed/OpenMed-NER-OncologyDetect-MultiMed-335M, OpenMed/OpenMed-NER-AnatomyDetect-ElectraMed-335M, OpenMed/OpenMed-NER-BloodCancerDetect-ModernClinical-149M, OpenMed/OpenMed-NER-DiseaseDetect-PubMed-109M, OpenMed/OpenMed-NER-SpeciesDetect-SuperMedical-125M, OpenMed/OpenMed-NER-PharmaDetect-ModernClinical-149M, OpenMed/OpenMed-NER-GenomeDetect-BioMed-109M, OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-65M, OpenMed/OpenMed-NER-DNADetect-ModernClinical-395M, OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-65M, OpenMed/OpenMed-NER-BloodCancerDetect-ModernClinical-395M, OpenMed/OpenMed-NER-SpeciesDetect-MultiMed-568M, OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-82M, OpenMed/OpenMed-NER-DiseaseDetect-ModernMed-395M, OpenMed/OpenMed-NER-AnatomyDetect-BioPatient-108M",
    "models_links": "https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Pharma-Medium-209M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Chemical-Medium-209M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Pharma-Large-459M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Oncology-Base-220M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Pharma-XLarge-770M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Chemical-Multi-209M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Anatomy-Small-166M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Disease-Medium-209M, https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-DNA-Large-459M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SuperMedical-125M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-EuroMed-212M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SuperMedical-125M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-184M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-TinyMed-66M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-184M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-EuroMed-212M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-TinyMed-66M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ModernMed-149M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ModernMed-149M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperMedical-125M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-66M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-TinyMed-66M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-66M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-184M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperMedical-125M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-TinyMed-66M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ModernMed-149M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-EuroMed-212M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-184M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-184M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-SuperMedical-125M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-66M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ModernMed-149M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-184M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-SuperClinical-184M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-EuroMed-212M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SuperMedical-355M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BioMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ModernMed-149M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-TinyMed-66M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BigMed-278M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-33M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-PubMed-v2-109M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-PubMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ModernMed-149M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SuperMedical-125M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SuperMedical-125M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-TinyMed-135M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SnowMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ModernMed-149M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BioPatient-108M, https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BioClinical-108M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BigMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-141M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-434M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-560M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-MultiMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ElectraMed-335M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-PubMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-SuperMedical-125M, https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ModernClinical-149M, https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BioMed-109M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-65M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ModernClinical-395M, https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-MultiMed-568M, https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-82M, https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ModernMed-395M, https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BioPatient-108M",
    "models_detailed": "[{\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-Pharma-Medium-209M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Pharma-Medium-209M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-Chemical-Medium-209M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Chemical-Medium-209M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-Pharma-Large-459M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Pharma-Large-459M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-Oncology-Base-220M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Oncology-Base-220M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-Pharma-XLarge-770M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Pharma-XLarge-770M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 18\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-Chemical-Multi-209M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Chemical-Multi-209M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-Anatomy-Small-166M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Anatomy-Small-166M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-Disease-Medium-209M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-Disease-Medium-209M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"OpenMed/OpenMed-ZeroShot-NER-DNA-Large-459M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-ZeroShot-NER-DNA-Large-459M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-SuperMedical-125M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SuperMedical-125M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-EuroMed-212M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-EuroMed-212M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-SuperMedical-125M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SuperMedical-125M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-184M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-184M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-TinyMed-66M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-TinyMed-66M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-184M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-184M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-EuroMed-212M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-EuroMed-212M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-TinyMed-66M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-TinyMed-66M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-ModernMed-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ModernMed-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-ModernMed-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ModernMed-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-SuperMedical-125M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperMedical-125M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-66M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-66M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-TinyMed-66M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-TinyMed-66M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-66M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-TinyMed-66M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-184M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-184M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-SuperMedical-125M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperMedical-125M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-TinyMed-66M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-TinyMed-66M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-ModernMed-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ModernMed-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-EuroMed-212M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-EuroMed-212M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-184M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-184M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-184M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-184M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-SuperMedical-125M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-SuperMedical-125M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-66M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-TinyMed-66M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-ModernMed-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-ModernMed-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-184M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-184M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-SuperClinical-184M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-SuperClinical-184M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-EuroMed-212M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-EuroMed-212M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-SuperMedical-355M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SuperMedical-355M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-BioMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BioMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-ModernMed-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ModernMed-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-TinyMed-66M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-TinyMed-66M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PathologyDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-BigMed-278M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BigMed-278M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-33M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ElectraMed-33M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-PubMed-v2-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-PubMed-v2-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-PubMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-PubMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-ModernMed-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ModernMed-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-SuperMedical-125M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-SuperMedical-125M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-SuperMedical-125M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-SuperMedical-125M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-TinyMed-135M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-TinyMed-135M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-SnowMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SnowMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ProteinDetect-ModernMed-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ProteinDetect-ModernMed-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-ElectraMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomicDetect-BioClinical-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomicDetect-BioClinical-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-BigMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-BigMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-141M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OrganismDetect-SuperClinical-141M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-434M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-SuperClinical-434M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-560M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ElectraMed-560M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-OncologyDetect-MultiMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-OncologyDetect-MultiMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-ElectraMed-335M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-ElectraMed-335M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-PubMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-PubMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-SuperMedical-125M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-SuperMedical-125M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-PharmaDetect-ModernClinical-149M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-PharmaDetect-ModernClinical-149M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-GenomeDetect-BioMed-109M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-GenomeDetect-BioMed-109M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DNADetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DNADetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-65M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-ChemicalDetect-TinyMed-65M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-ModernClinical-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-ModernClinical-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-SpeciesDetect-MultiMed-568M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-SpeciesDetect-MultiMed-568M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-82M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-82M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-DiseaseDetect-ModernMed-395M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-DiseaseDetect-ModernMed-395M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"OpenMed/OpenMed-NER-AnatomyDetect-BioPatient-108M\", \"link\": \"https://huggingface.co/OpenMed/OpenMed-NER-AnatomyDetect-BioPatient-108M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2508.03178",
    "first_seen_date": "2025-08-07",
    "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and\n  Self-Checking for Complex Instruction Following",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.03178Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and\n  Self-Checking for Complex Instruction FollowingPublished on Aug 5\u00b7Submitted bywenliangon Aug 7Upvote5Authors:Chenyang Wang,Liang Wen,Shousheng Jia,Xiangzheng Zhang,Liang XuAbstractA framework using entropy-preserving supervised fine-tuning and token-wise entropy-adaptive reinforcement learning improves instruction adherence in LLMs by fostering rigorous reasoning processes.AI-generated summaryWhile advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifieslazy reasoningduring the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involvingpreviewandself-checking, essential for\nsatisfying strictinstruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply afiltering processto obtain\nvalid prompts, resulting in three distinctprompt datasetscategorized as hard,\neasy, and pass. Then, we employrejection samplingon the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ anentropy-preserving supervised fine-tuning(Entropy-SFT) strategy coupled withtoken-wise entropy-adaptive(TEA-RL)reinforcement learningguided byrule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompasspreviewandself-checking.\nExtensive experiments conducted oni",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/Qihoo360/Light-IF",
    "hf_paper_url": "https://huggingface.co/papers/2508.03178",
    "arxiv_url": "https://arxiv.org/abs/2508.03178",
    "num_models": 5,
    "models_list": "qihoo360/Light-IF-32B, qihoo360/Light-IF-4B, qihoo360/Light-IF-8B, qihoo360/Light-IF-14B, jpbwin/Light-IF-32B_exl3_4.5bpw_calcols-4096",
    "models_links": "https://huggingface.co/qihoo360/Light-IF-32B, https://huggingface.co/qihoo360/Light-IF-4B, https://huggingface.co/qihoo360/Light-IF-8B, https://huggingface.co/qihoo360/Light-IF-14B, https://huggingface.co/jpbwin/Light-IF-32B_exl3_4.5bpw_calcols-4096",
    "models_detailed": "[{\"name\": \"qihoo360/Light-IF-32B\", \"link\": \"https://huggingface.co/qihoo360/Light-IF-32B\", \"task\": \"Text Generation\", \"likes\": \"105\", \"downloads\": \"\", \"updated\": \"Aug 10\"}, {\"name\": \"qihoo360/Light-IF-4B\", \"link\": \"https://huggingface.co/qihoo360/Light-IF-4B\", \"task\": \"Text Generation\", \"likes\": \"29\", \"downloads\": \"\", \"updated\": \"Aug 10\"}, {\"name\": \"qihoo360/Light-IF-8B\", \"link\": \"https://huggingface.co/qihoo360/Light-IF-8B\", \"task\": \"Text Generation\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Aug 10\"}, {\"name\": \"qihoo360/Light-IF-14B\", \"link\": \"https://huggingface.co/qihoo360/Light-IF-14B\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Nov 20\"}, {\"name\": \"jpbwin/Light-IF-32B_exl3_4.5bpw_calcols-4096\", \"link\": \"https://huggingface.co/jpbwin/Light-IF-32B_exl3_4.5bpw_calcols-4096\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 12\"}]",
    "num_datasets": 1,
    "datasets_list": "qihoo360/Light-IF-SFTData",
    "datasets_links": "https://huggingface.co/datasets/qihoo360/Light-IF-SFTData",
    "datasets_detailed": "[{\"name\": \"qihoo360/Light-IF-SFTData\", \"link\": \"https://huggingface.co/datasets/qihoo360/Light-IF-SFTData\", \"task\": \"\", \"likes\": \"75\", \"downloads\": \"\", \"updated\": \"Aug 10\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.03613",
    "first_seen_date": "2025-08-06",
    "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.03613Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-CorrectionPublished on Aug 5\u00b7Submitted byBohan22on Aug 6Upvote11+3Authors:Yong Lin,Shange Tang,Bohan Lyu,Ziran Yang,Jui-Hui Chung,Haoyu Zhao,Lai Jiang,Yihan Geng,Jiawei Ge,Jingruo Sun,Jiayun Wu,Jiri Gesi,Ximing Lu,David Acuna,Kaiyu Yang,Hongzhou Lin,Yejin Choi,Danqi Chen,Sanjeev Arora,Chi JinAbstractGoedel-Prover-V2, a series of open-source language models, achieves state-of-the-art performance in automated theorem proving through scaffolded data synthesis, verifier-guided self-correction, and model averaging.AI-generated summaryWe introduce Goedel-Prover-V2, a series of open-source language models that\nset a new state-of-the-art inautomated theorem proving. Built on the standard\nexpert iteration and reinforcement learning pipeline, our approach incorporates\nthree key innovations: (1)Scaffolded data synthesis: We generate synthetic\ntasks of increasing difficulty to train the model to master increasingly\ncomplex theorems; (2)Verifier-guided self-correction: We enable the model to\niteratively revise its proofs by leveraging feedback from the Lean compiler;\n(3)Model averaging: We merge model checkpoints to mitigate the decrease in\nmodel output diversity in later stages of training. Our small model,\nGoedel-Prover-V2-8B, reaches 84.6%pass@32onMiniF2Fand outperforms\nDeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our\nflagship model, Goedel-Prover-V2-32B, achieves 88.1% onMiniF2Fatpass@32in\nstandard mode and 90.4% in self-correction mode, outperforming prior SOTA by a\nlarge margin. Additionally, our flagship model solves 86 problems onPutnamBenchatpass@184, securing the first place among open-source models on\nthe leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47\nproblems bypass@1024with a significantly smaller model size and compute\nbudget. A",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/Goedel-LM/Goedel-Prover-V2",
    "hf_paper_url": "https://huggingface.co/papers/2508.03613",
    "arxiv_url": "https://arxiv.org/abs/2508.03613",
    "num_models": 2,
    "models_list": "Goedel-LM/Goedel-Prover-V2-32B, Goedel-LM/Goedel-Prover-V2-8B",
    "models_links": "https://huggingface.co/Goedel-LM/Goedel-Prover-V2-32B, https://huggingface.co/Goedel-LM/Goedel-Prover-V2-8B",
    "models_detailed": "[{\"name\": \"Goedel-LM/Goedel-Prover-V2-32B\", \"link\": \"https://huggingface.co/Goedel-LM/Goedel-Prover-V2-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"Goedel-LM/Goedel-Prover-V2-8B\", \"link\": \"https://huggingface.co/Goedel-LM/Goedel-Prover-V2-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 9\"}]",
    "num_datasets": 3,
    "datasets_list": "nvidia/Nemotron-Math-Proofs-v1, Goedel-LM/MathOlympiadBench, edgerunner-ai/nvidia__Nemotron-Math-Proofs-v1",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Nemotron-Math-Proofs-v1, https://huggingface.co/datasets/Goedel-LM/MathOlympiadBench, https://huggingface.co/datasets/edgerunner-ai/nvidia__Nemotron-Math-Proofs-v1",
    "datasets_detailed": "[{\"name\": \"nvidia/Nemotron-Math-Proofs-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Math-Proofs-v1\", \"task\": \"\", \"likes\": \"711\", \"downloads\": \"\", \"updated\": \"4 days ago\", \"size\": \"\"}, {\"name\": \"Goedel-LM/MathOlympiadBench\", \"link\": \"https://huggingface.co/datasets/Goedel-LM/MathOlympiadBench\", \"task\": \"\", \"likes\": \"360\", \"downloads\": \"\", \"updated\": \"Aug 6\", \"size\": \"\"}, {\"name\": \"edgerunner-ai/nvidia__Nemotron-Math-Proofs-v1\", \"link\": \"https://huggingface.co/datasets/edgerunner-ai/nvidia__Nemotron-Math-Proofs-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2508.02324",
    "first_seen_date": "2025-08-05",
    "title": "Qwen-Image Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2508.02324Qwen-Image Technical ReportPublished on Aug 4\u00b7Submitted byXiao Xuon Aug 5#1 Paper of the dayUpvote266+258Authors:Chenfei Wu,Jiahao Li,Jingren Zhou,Junyang Lin,Kaiyuan Gao,Kun Yan,Sheng-ming Yin,Shuai Bai,Xiao Xu,Yilei Chen,Yuxiang Chen,Zecheng Tang,Zekai Zhang,Zhengyi Wang,An Yang,Bowen Yu,Chen Cheng,Dayiheng Liu,Deqing Li,Hang Zhang,Hao Meng,Hu Wei+17 authorsAbstractQwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.AI-generated summaryWe present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensivedata pipelinethat includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt aprogressive trainingstrategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. Thiscurriculum learningapproach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditionaltext-to-image(T2I) andtext-image-to-image(TI2I) tasks but alsoimage-to-image(I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. Thisdual-encoding mechanismenables",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2508,
    "github_repo": "https://github.com/QwenLM/Qwen-Image",
    "hf_paper_url": "https://huggingface.co/papers/2508.02324",
    "arxiv_url": "https://arxiv.org/abs/2508.02324",
    "num_models": 24,
    "models_list": "Qwen/Qwen-Image, Qwen/Qwen-Image-Edit, Qwen/Qwen-Image-Edit-2509, ovedrive/qwen-image-edit-4bit, unsloth/Qwen-Image-GGUF, unsloth/Qwen-Image-Edit-GGUF, unsloth/Qwen-Image-Edit-2509-GGUF, Runware/Qwen-Image, ovedrive/qwen-image-4bit, Runware/Qwen-Image-Edit, offfragnor123/neeratoneera, frankjoshua/Qwen-Image, chaitnya26/Qwen-Image-fork, chaitnya26/Qwen-Image-Edit-fork, wajiha02/image-edit, kp-forks/Qwen-Image-Edit, oguzm/qwen-image-backup, lrzjason/qwen_image_edit_plus_nf4, Runware/Qwen-Image-Edit-2509, ovedrive/Qwen-Image-Edit-2509-4bit, Zillis/Qwen-Image, ovedrive/Qwen-Image-testing, smthem/Qwen-Image-GGUF, gradients-io-tournaments/Qwen-Image",
    "models_links": "https://huggingface.co/Qwen/Qwen-Image, https://huggingface.co/Qwen/Qwen-Image-Edit, https://huggingface.co/Qwen/Qwen-Image-Edit-2509, https://huggingface.co/ovedrive/qwen-image-edit-4bit, https://huggingface.co/unsloth/Qwen-Image-GGUF, https://huggingface.co/unsloth/Qwen-Image-Edit-GGUF, https://huggingface.co/unsloth/Qwen-Image-Edit-2509-GGUF, https://huggingface.co/Runware/Qwen-Image, https://huggingface.co/ovedrive/qwen-image-4bit, https://huggingface.co/Runware/Qwen-Image-Edit, https://huggingface.co/offfragnor123/neeratoneera, https://huggingface.co/frankjoshua/Qwen-Image, https://huggingface.co/chaitnya26/Qwen-Image-fork, https://huggingface.co/chaitnya26/Qwen-Image-Edit-fork, https://huggingface.co/wajiha02/image-edit, https://huggingface.co/kp-forks/Qwen-Image-Edit, https://huggingface.co/oguzm/qwen-image-backup, https://huggingface.co/lrzjason/qwen_image_edit_plus_nf4, https://huggingface.co/Runware/Qwen-Image-Edit-2509, https://huggingface.co/ovedrive/Qwen-Image-Edit-2509-4bit, https://huggingface.co/Zillis/Qwen-Image, https://huggingface.co/ovedrive/Qwen-Image-testing, https://huggingface.co/smthem/Qwen-Image-GGUF, https://huggingface.co/gradients-io-tournaments/Qwen-Image",
    "models_detailed": "[{\"name\": \"Qwen/Qwen-Image\", \"link\": \"https://huggingface.co/Qwen/Qwen-Image\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"Qwen/Qwen-Image-Edit\", \"link\": \"https://huggingface.co/Qwen/Qwen-Image-Edit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 25\"}, {\"name\": \"Qwen/Qwen-Image-Edit-2509\", \"link\": \"https://huggingface.co/Qwen/Qwen-Image-Edit-2509\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\"}, {\"name\": \"ovedrive/qwen-image-edit-4bit\", \"link\": \"https://huggingface.co/ovedrive/qwen-image-edit-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 25\"}, {\"name\": \"unsloth/Qwen-Image-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen-Image-GGUF\", \"task\": \"\", \"likes\": \"494\", \"downloads\": \"\", \"updated\": \"about 1\"}, {\"name\": \"unsloth/Qwen-Image-Edit-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen-Image-Edit-GGUF\", \"task\": \"\", \"likes\": \"662\", \"downloads\": \"\", \"updated\": \"about 1\"}, {\"name\": \"unsloth/Qwen-Image-Edit-2509-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen-Image-Edit-2509-GGUF\", \"task\": \"\", \"likes\": \"838\", \"downloads\": \"\", \"updated\": \"about 1\"}, {\"name\": \"Runware/Qwen-Image\", \"link\": \"https://huggingface.co/Runware/Qwen-Image\", \"task\": \"\", \"likes\": \"191\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"ovedrive/qwen-image-4bit\", \"link\": \"https://huggingface.co/ovedrive/qwen-image-4bit\", \"task\": \"\", \"likes\": \"660\", \"downloads\": \"\", \"updated\": \"Sep 1\"}, {\"name\": \"Runware/Qwen-Image-Edit\", \"link\": \"https://huggingface.co/Runware/Qwen-Image-Edit\", \"task\": \"\", \"likes\": \"196\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"offfragnor123/neeratoneera\", \"link\": \"https://huggingface.co/offfragnor123/neeratoneera\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 23\"}, {\"name\": \"frankjoshua/Qwen-Image\", \"link\": \"https://huggingface.co/frankjoshua/Qwen-Image\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"chaitnya26/Qwen-Image-fork\", \"link\": \"https://huggingface.co/chaitnya26/Qwen-Image-fork\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"chaitnya26/Qwen-Image-Edit-fork\", \"link\": \"https://huggingface.co/chaitnya26/Qwen-Image-Edit-fork\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"wajiha02/image-edit\", \"link\": \"https://huggingface.co/wajiha02/image-edit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"kp-forks/Qwen-Image-Edit\", \"link\": \"https://huggingface.co/kp-forks/Qwen-Image-Edit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 25\"}, {\"name\": \"oguzm/qwen-image-backup\", \"link\": \"https://huggingface.co/oguzm/qwen-image-backup\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"lrzjason/qwen_image_edit_plus_nf4\", \"link\": \"https://huggingface.co/lrzjason/qwen_image_edit_plus_nf4\", \"task\": \"\", \"likes\": \"70\", \"downloads\": \"\", \"updated\": \"Sep 23\"}, {\"name\": \"Runware/Qwen-Image-Edit-2509\", \"link\": \"https://huggingface.co/Runware/Qwen-Image-Edit-2509\", \"task\": \"\", \"likes\": \"189\", \"downloads\": \"\", \"updated\": \"Sep 23\"}, {\"name\": \"ovedrive/Qwen-Image-Edit-2509-4bit\", \"link\": \"https://huggingface.co/ovedrive/Qwen-Image-Edit-2509-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 23\"}, {\"name\": \"Zillis/Qwen-Image\", \"link\": \"https://huggingface.co/Zillis/Qwen-Image\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 12\"}, {\"name\": \"ovedrive/Qwen-Image-testing\", \"link\": \"https://huggingface.co/ovedrive/Qwen-Image-testing\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"smthem/Qwen-Image-GGUF\", \"link\": \"https://huggingface.co/smthem/Qwen-Image-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"22 days ago\"}, {\"name\": \"gradients-io-tournaments/Qwen-Image\", \"link\": \"https://huggingface.co/gradients-io-tournaments/Qwen-Image\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.22062",
    "first_seen_date": "2025-07-31",
    "title": "MetaCLIP 2: A Worldwide Scaling Recipe",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.22062MetaCLIP 2: A Worldwide Scaling RecipePublished on Jul 29\u00b7Submitted byNiels Roggeon Jul 31Upvote36+28Authors:Yung-Sung Chuang,Yang Li,Dong Wang,Ching-Feng Yeh,Kehan Lyu,Ramya Raghavendra,James Glass,Lifei Huang,Jason Weston,Luke Zettlemoyer,Xinlei Chen,Zhuang Liu,Saining Xie,Wen-tau Yih,Shang-Wen Li,Hu XuAbstractMetaCLIP 2, trained on worldwide web-scale image-text pairs, improves zero-shot classification and multilingual benchmarks without system-level confounding factors.AI-generated summaryContrastive Language-Image Pretraining(CLIP) is a popular foundation model,\nsupporting fromzero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). AlthoughCLIPis successfully trained on\nbillion-scaleimage-text pairsfrom the English world, scalingCLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existingmultilingual CLIPis worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we presentMetaCLIP 2, the first recipe trainingCLIPfrom scratch\non worldwide web-scaleimage-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification,MetaCLIP 2ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such asCVQAwith 57.4%,Babel-ImageNetwith 50.2% andXM3600with\n64.3% onimage-to-text retrieval.View arXiv pageView PDFGitHub1.78kAdd to collectionCommunitynielsrPaper submitterJul 31C",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/facebookresearch/MetaCLIP",
    "hf_paper_url": "https://huggingface.co/papers/2507.22062",
    "arxiv_url": "https://arxiv.org/abs/2507.22062",
    "num_models": 25,
    "models_list": "facebook/metaclip-2-worldwide-huge-quickgelu, facebook/metaclip-2-worldwide-l14, facebook/metaclip-2-worldwide-giant-378, facebook/metaclip-2-worldwide-s16, facebook/metaclip-2-worldwide-m16-384, facebook/metaclip-2-worldwide-huge-378, timm/vit_gigantic_patch14_clip_378.metaclip2_worldwide, timm/vit_huge_patch14_clip_378.metaclip2_worldwide, timm/vit_huge_patch14_clip_224.metaclip2_worldwide, timm/vit_gigantic_patch14_clip_224.metaclip2_worldwide, facebook/metaclip-2-worldwide-giant, onnx-community/metaclip-2-worldwide-huge-378-ONNX, facebook/metaclip-2-mt5-worldwide-b32, facebook/metaclip-2-mt5-worldwide-m16, facebook/metaclip-2-mt5-worldwide-s16, facebook/metaclip-2-worldwide-b16, facebook/metaclip-2-worldwide-b32, facebook/metaclip-2-worldwide-m16, prithivMLmods/MetaCLIP-2-Age-Range-Estimator, prithivMLmods/MetaCLIP-2-Gender-Identifier, prithivMLmods/MetaCLIP-2-Open-Scene, prithivMLmods/MetaCLIP-2-Cifar10, facebook/metaclip-2-worldwide-b16-384, facebook/metaclip-2-worldwide-b32-384, facebook/metaclip-2-worldwide-s16-384",
    "models_links": "https://huggingface.co/facebook/metaclip-2-worldwide-huge-quickgelu, https://huggingface.co/facebook/metaclip-2-worldwide-l14, https://huggingface.co/facebook/metaclip-2-worldwide-giant-378, https://huggingface.co/facebook/metaclip-2-worldwide-s16, https://huggingface.co/facebook/metaclip-2-worldwide-m16-384, https://huggingface.co/facebook/metaclip-2-worldwide-huge-378, https://huggingface.co/timm/vit_gigantic_patch14_clip_378.metaclip2_worldwide, https://huggingface.co/timm/vit_huge_patch14_clip_378.metaclip2_worldwide, https://huggingface.co/timm/vit_huge_patch14_clip_224.metaclip2_worldwide, https://huggingface.co/timm/vit_gigantic_patch14_clip_224.metaclip2_worldwide, https://huggingface.co/facebook/metaclip-2-worldwide-giant, https://huggingface.co/onnx-community/metaclip-2-worldwide-huge-378-ONNX, https://huggingface.co/facebook/metaclip-2-mt5-worldwide-b32, https://huggingface.co/facebook/metaclip-2-mt5-worldwide-m16, https://huggingface.co/facebook/metaclip-2-mt5-worldwide-s16, https://huggingface.co/facebook/metaclip-2-worldwide-b16, https://huggingface.co/facebook/metaclip-2-worldwide-b32, https://huggingface.co/facebook/metaclip-2-worldwide-m16, https://huggingface.co/prithivMLmods/MetaCLIP-2-Age-Range-Estimator, https://huggingface.co/prithivMLmods/MetaCLIP-2-Gender-Identifier, https://huggingface.co/prithivMLmods/MetaCLIP-2-Open-Scene, https://huggingface.co/prithivMLmods/MetaCLIP-2-Cifar10, https://huggingface.co/facebook/metaclip-2-worldwide-b16-384, https://huggingface.co/facebook/metaclip-2-worldwide-b32-384, https://huggingface.co/facebook/metaclip-2-worldwide-s16-384",
    "models_detailed": "[{\"name\": \"facebook/metaclip-2-worldwide-huge-quickgelu\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-huge-quickgelu\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"facebook/metaclip-2-worldwide-l14\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-l14\", \"task\": \"Image Classification\", \"likes\": \"265\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"facebook/metaclip-2-worldwide-giant-378\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-giant-378\", \"task\": \"Image Classification\", \"likes\": \"698\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"facebook/metaclip-2-worldwide-s16\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-s16\", \"task\": \"Image Classification\", \"likes\": \"594\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"facebook/metaclip-2-worldwide-m16-384\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-m16-384\", \"task\": \"Image Classification\", \"likes\": \"315\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"facebook/metaclip-2-worldwide-huge-378\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-huge-378\", \"task\": \"Image Classification\", \"likes\": \"604\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"timm/vit_gigantic_patch14_clip_378.metaclip2_worldwide\", \"link\": \"https://huggingface.co/timm/vit_gigantic_patch14_clip_378.metaclip2_worldwide\", \"task\": \"Image Classification\", \"likes\": \"139\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"timm/vit_huge_patch14_clip_378.metaclip2_worldwide\", \"link\": \"https://huggingface.co/timm/vit_huge_patch14_clip_378.metaclip2_worldwide\", \"task\": \"Image Classification\", \"likes\": \"112\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"timm/vit_huge_patch14_clip_224.metaclip2_worldwide\", \"link\": \"https://huggingface.co/timm/vit_huge_patch14_clip_224.metaclip2_worldwide\", \"task\": \"Image Classification\", \"likes\": \"417\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"timm/vit_gigantic_patch14_clip_224.metaclip2_worldwide\", \"link\": \"https://huggingface.co/timm/vit_gigantic_patch14_clip_224.metaclip2_worldwide\", \"task\": \"Image Classification\", \"likes\": \"124\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"facebook/metaclip-2-worldwide-giant\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-giant\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"onnx-community/metaclip-2-worldwide-huge-378-ONNX\", \"link\": \"https://huggingface.co/onnx-community/metaclip-2-worldwide-huge-378-ONNX\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"facebook/metaclip-2-mt5-worldwide-b32\", \"link\": \"https://huggingface.co/facebook/metaclip-2-mt5-worldwide-b32\", \"task\": \"Image Classification\", \"likes\": \"166\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"facebook/metaclip-2-mt5-worldwide-m16\", \"link\": \"https://huggingface.co/facebook/metaclip-2-mt5-worldwide-m16\", \"task\": \"Image Classification\", \"likes\": \"51\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"facebook/metaclip-2-mt5-worldwide-s16\", \"link\": \"https://huggingface.co/facebook/metaclip-2-mt5-worldwide-s16\", \"task\": \"Image Classification\", \"likes\": \"160\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"facebook/metaclip-2-worldwide-b16\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-b16\", \"task\": \"Image Classification\", \"likes\": \"82\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"facebook/metaclip-2-worldwide-b32\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-b32\", \"task\": \"Image Classification\", \"likes\": \"68\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"facebook/metaclip-2-worldwide-m16\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-m16\", \"task\": \"Image Classification\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"prithivMLmods/MetaCLIP-2-Age-Range-Estimator\", \"link\": \"https://huggingface.co/prithivMLmods/MetaCLIP-2-Age-Range-Estimator\", \"task\": \"Image Classification\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"prithivMLmods/MetaCLIP-2-Gender-Identifier\", \"link\": \"https://huggingface.co/prithivMLmods/MetaCLIP-2-Gender-Identifier\", \"task\": \"Image Classification\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Nov 13\"}, {\"name\": \"prithivMLmods/MetaCLIP-2-Open-Scene\", \"link\": \"https://huggingface.co/prithivMLmods/MetaCLIP-2-Open-Scene\", \"task\": \"Image Classification\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Nov 13\"}, {\"name\": \"prithivMLmods/MetaCLIP-2-Cifar10\", \"link\": \"https://huggingface.co/prithivMLmods/MetaCLIP-2-Cifar10\", \"task\": \"Image Classification\", \"likes\": \"35\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"facebook/metaclip-2-worldwide-b16-384\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-b16-384\", \"task\": \"Image Classification\", \"likes\": \"96\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"facebook/metaclip-2-worldwide-b32-384\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-b32-384\", \"task\": \"Image Classification\", \"likes\": \"127\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"facebook/metaclip-2-worldwide-s16-384\", \"link\": \"https://huggingface.co/facebook/metaclip-2-worldwide-s16-384\", \"task\": \"Image Classification\", \"likes\": \"163\", \"downloads\": \"\", \"updated\": \"28 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.20984",
    "first_seen_date": "2025-07-29",
    "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.20984SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local DeploymentPublished on Jul 28\u00b7Submitted byJeremy Songon Jul 29#3 Paper of the dayUpvote57+49Authors:Yixin Song,Zhenliang Xue,Dongliang Wei,Feiyang Chen,Jianxiang Gao,Junchen Liu,Hangyu Liang,Guangshuo Qin,Chengrong Tian,Bo Wen,Longyu Zhao,Xinrui Zheng,Zeyu Mi,Haibo ChenAbstractSmallThinker, a family of LLMs designed for local devices, uses a deployment-aware architecture with sparse structures, pre-attention routing, and hybrid sparse attention to achieve high performance on consumer hardware.AI-generated summaryWhile frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grainedMixture-of-Experts (MoE)with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design apre-attention routerthat enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilizeNoPE-RoPE hybrid sparse attention mechanismto\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2507.20984",
    "arxiv_url": "https://arxiv.org/abs/2507.20984",
    "num_models": 3,
    "models_list": "Tiiny/SmallThinker-21BA3B-Instruct, Tiiny/SmallThinker-4BA0.6B-Instruct, Mungert/SmallThinker-21BA3B-Instruct-GGUF",
    "models_links": "https://huggingface.co/Tiiny/SmallThinker-21BA3B-Instruct, https://huggingface.co/Tiiny/SmallThinker-4BA0.6B-Instruct, https://huggingface.co/Mungert/SmallThinker-21BA3B-Instruct-GGUF",
    "models_detailed": "[{\"name\": \"Tiiny/SmallThinker-21BA3B-Instruct\", \"link\": \"https://huggingface.co/Tiiny/SmallThinker-21BA3B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"108\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"Tiiny/SmallThinker-4BA0.6B-Instruct\", \"link\": \"https://huggingface.co/Tiiny/SmallThinker-4BA0.6B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"105\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"Mungert/SmallThinker-21BA3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/SmallThinker-21BA3B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"316\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.19478",
    "first_seen_date": "2025-07-28",
    "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI\n  Agents",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.19478MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI\n  AgentsPublished on Jul 25\u00b7Submitted byXuehui Wangon Jul 28Upvote31+23Authors:Xuehui Wang,Zhenyu Wu,JingJing Xie,Zichen Ding,Bowen Yang,Zehao Li,Zhaoyang Liu,Qingyun Li,Xuan Dong,Zhe Chen,Weiyun Wang,Xiangyu Zhao,Jixuan Chen,Haodong Duan,Tianbao Xie,Chenyu Yang,Shiqian Su,Yue Yu,Yuan Huang,Yiqian Liu,Xiao Zhang,Yanting Zhang+6 authorsAbstractMMBench-GUI evaluates GUI automation agents across multiple platforms using a hierarchical benchmark and Efficiency-Quality Area metric, highlighting the importance of visual grounding, task planning, and efficiency.AI-generated summaryWe introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels:GUI Content Understanding, Element\nGrounding,Task Automation, andTask Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novelEfficiency-Quality Area (EQA)metric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accuratevisual groundingas a critical\ndeterminant of overall task success, emphasizing the substantial benefits ofmodular frameworksthat integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strongtask planningandcross-platform generalizationabilities, withlong-context memory, a broad\naction space, andlong-term reasoningplaying a critical role. More important,task efficiencyremains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration ofprecise localization,effective planning, andearly stopping strategiesis indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmar",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/open-compass/MMBench-GUI",
    "hf_paper_url": "https://huggingface.co/papers/2507.19478",
    "arxiv_url": "https://arxiv.org/abs/2507.19478",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "OpenGVLab/MMBench-GUI",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/MMBench-GUI",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/MMBench-GUI\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/MMBench-GUI\", \"task\": \"\", \"likes\": \"75\", \"downloads\": \"\", \"updated\": \"Aug 15\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.18071",
    "first_seen_date": "2025-07-25",
    "title": "Group Sequence Policy Optimization",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.18071Group Sequence Policy OptimizationPublished on Jul 24\u00b7Submitted byChujie Zhengon Jul 25#1 Paper of the day\u00b7QwenUpvote316+308Authors:Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang LinAbstractGroup Sequence Policy Optimization (GSPO) is a reinforcement learning algorithm that improves training efficiency and performance of large language models by using sequence-level importance ratios and operations.AI-generated summaryThis paper introducesGroup Sequence Policy Optimization(GSPO), our stable,\nefficient, and performantreinforcement learningalgorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios,GSPOdefines the importance ratio based onsequence likelihoodand\nperformssequence-level clipping,rewarding, andoptimization. We demonstrate\nthatGSPOachieves superior training efficiency and performance compared to theGRPOalgorithm, notably stabilizesMixture-of-Experts(MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nofGSPOhave contributed to the remarkable improvements in the latestQwen3models.View arXiv pageView PDFAdd to collectionCommunitychujiezhengPaper authorPaper submitterJul 25This paper introducesGroup Sequence Policy Optimization (GSPO), a stable, efficient,and performant RL algorithm for training the latest Qwen3 models(Instruct, Coder, and Thinking)See translation1 reply\u00b7\u2764\ufe0f1919+apachavesJul 26Awesome!eliebakJul 25beautiful \ud83d\udd25Replylibrarian-botJul 26This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIStaQ it! Growing neural networks for Policy Mirror Descent(2025)Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning(2025)On-P",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2507.18071",
    "arxiv_url": "https://arxiv.org/abs/2507.18071",
    "num_models": 12,
    "models_list": "ServiceNow-AI/Apriel-1.6-15b-Thinker, driaforall/mem-agent, driaforall/mem-agent-mlx-4bit, vivekvar/GSPO-DeepSeek-R1-Distill-Qwen-1.5B, cyankiwi/Apriel-1.6-15b-Thinker-AWQ-4bit, hanchaow/QTuneVL1_5-3B, gabriellarson/Qwen2.5-GSPO-experiment, driaforall/mem-agent-mlx-bf16, driaforall/mem-agent-mlx-8bit, QuantFactory/mem-agent-GGUF, Mungert/mem-agent-GGUF, cyankiwi/Apriel-1.6-15b-Thinker-AWQ-8bit",
    "models_links": "https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker, https://huggingface.co/driaforall/mem-agent, https://huggingface.co/driaforall/mem-agent-mlx-4bit, https://huggingface.co/vivekvar/GSPO-DeepSeek-R1-Distill-Qwen-1.5B, https://huggingface.co/cyankiwi/Apriel-1.6-15b-Thinker-AWQ-4bit, https://huggingface.co/hanchaow/QTuneVL1_5-3B, https://huggingface.co/gabriellarson/Qwen2.5-GSPO-experiment, https://huggingface.co/driaforall/mem-agent-mlx-bf16, https://huggingface.co/driaforall/mem-agent-mlx-8bit, https://huggingface.co/QuantFactory/mem-agent-GGUF, https://huggingface.co/Mungert/mem-agent-GGUF, https://huggingface.co/cyankiwi/Apriel-1.6-15b-Thinker-AWQ-8bit",
    "models_detailed": "[{\"name\": \"ServiceNow-AI/Apriel-1.6-15b-Thinker\", \"link\": \"https://huggingface.co/ServiceNow-AI/Apriel-1.6-15b-Thinker\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"driaforall/mem-agent\", \"link\": \"https://huggingface.co/driaforall/mem-agent\", \"task\": \"Text Generation\", \"likes\": \"115\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"driaforall/mem-agent-mlx-4bit\", \"link\": \"https://huggingface.co/driaforall/mem-agent-mlx-4bit\", \"task\": \"Text Generation\", \"likes\": \"57\", \"downloads\": \"\", \"updated\": \"Sep 9\"}, {\"name\": \"vivekvar/GSPO-DeepSeek-R1-Distill-Qwen-1.5B\", \"link\": \"https://huggingface.co/vivekvar/GSPO-DeepSeek-R1-Distill-Qwen-1.5B\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"cyankiwi/Apriel-1.6-15b-Thinker-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Apriel-1.6-15b-Thinker-AWQ-4bit\", \"task\": \"\", \"likes\": \"408\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"hanchaow/QTuneVL1_5-3B\", \"link\": \"https://huggingface.co/hanchaow/QTuneVL1_5-3B\", \"task\": \"\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Sep 1\"}, {\"name\": \"gabriellarson/Qwen2.5-GSPO-experiment\", \"link\": \"https://huggingface.co/gabriellarson/Qwen2.5-GSPO-experiment\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"driaforall/mem-agent-mlx-bf16\", \"link\": \"https://huggingface.co/driaforall/mem-agent-mlx-bf16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 9\"}, {\"name\": \"driaforall/mem-agent-mlx-8bit\", \"link\": \"https://huggingface.co/driaforall/mem-agent-mlx-8bit\", \"task\": \"Text Generation\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Sep 9\"}, {\"name\": \"QuantFactory/mem-agent-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/mem-agent-GGUF\", \"task\": \"Text Generation\", \"likes\": \"180\", \"downloads\": \"\", \"updated\": \"Sep 14\"}, {\"name\": \"Mungert/mem-agent-GGUF\", \"link\": \"https://huggingface.co/Mungert/mem-agent-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"cyankiwi/Apriel-1.6-15b-Thinker-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Apriel-1.6-15b-Thinker-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "OpenGVLab/MMPR-Tiny",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/MMPR-Tiny\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/MMPR-Tiny\", \"task\": \"\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"Aug 31\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.18192",
    "first_seen_date": "2025-07-25",
    "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.18192TeEFusion: Blending Text Embeddings to Distill Classifier-Free GuidancePublished on Jul 24\u00b7Submitted byGuo-Hua Wangon Jul 25Upvote7Authors:Minghao Fu,Guo-Hua Wang,Xiaohao Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu ZhangAbstractTeEFusion improves text-to-image synthesis by efficiently distilling classifier-free guidance into text embeddings, enabling faster inference without sacrificing image quality.AI-generated summaryRecent advances in text-to-image synthesis largely benefit from sophisticatedsampling strategiesandclassifier-free guidance(CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (Text\nEmbeddings Fusion), a novel and efficientdistillation methodthat directly incorporates the guidance magnitude into thetext embeddingsand\ndistills theteacher model's complex sampling strategy. By simply fusing\nconditional andunconditional text embeddingsusinglinear operations,\nTeEFusion reconstructs the desired guidance without adding extra parameters,\nsimultaneously enabling thestudent modelto learn from the teacher's output\nproduced via itssophisticated sampling approach. Extensive experiments on\nstate-of-the-art models such as SD3 demonstrate that our method allows the\nstudent to closely mimic the teacher's performance with a far simpler and more\nefficient sampling strategy. Consequently, thestudent modelachieves inference\nspeeds up to 6times faster than theteacher model, while maintaining image\nquality at levels comparable to those obtained through the teacher's complex\nsampling approach. The code is publicly available at\nhttps://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.View arXiv pageView PDFGitHub8Add to collectionCommunityFlourishPaper authorPaper submitterJul 25GitHub:http",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/AIDC-AI/TeEFusion",
    "hf_paper_url": "https://huggingface.co/papers/2507.18192",
    "arxiv_url": "https://arxiv.org/abs/2507.18192",
    "num_models": 1,
    "models_list": "AIDC-AI/TeEFusion",
    "models_links": "https://huggingface.co/AIDC-AI/TeEFusion",
    "models_detailed": "[{\"name\": \"AIDC-AI/TeEFusion\", \"link\": \"https://huggingface.co/AIDC-AI/TeEFusion\", \"task\": \"\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Jul 30\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.15061",
    "first_seen_date": "2025-07-22",
    "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.15061WebShaper: Agentically Data Synthesizing via Information-Seeking\n  FormalizationPublished on Jul 20\u00b7Submitted byJialong Wuon Jul 22\u00b7Alibaba-NLPUpvote60+52Authors:Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren ZhouAbstractWebShaper, a formalization-driven framework, synthesizes information-seeking datasets using set theory and Knowledge Projections to enhance reasoning structure and achieve top performance in open-sourced benchmarks.AI-generated summaryThe advent ofLarge Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks throughset theory. Central to the formalization is the\nconcept ofKnowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, anagentic Expanderexpands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents onGAIAandWebWalkerQA benchmarks.View arXiv pageView PDFGitHub17.",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/Alibaba-NLP/WebWalker",
    "hf_paper_url": "https://huggingface.co/papers/2507.15061",
    "arxiv_url": "https://arxiv.org/abs/2507.15061",
    "num_models": 1,
    "models_list": "Alibaba-NLP/WebShaper-32B",
    "models_links": "https://huggingface.co/Alibaba-NLP/WebShaper-32B",
    "models_detailed": "[{\"name\": \"Alibaba-NLP/WebShaper-32B\", \"link\": \"https://huggingface.co/Alibaba-NLP/WebShaper-32B\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Aug 28\"}]",
    "num_datasets": 1,
    "datasets_list": "Alibaba-NLP/WebShaper",
    "datasets_links": "https://huggingface.co/datasets/Alibaba-NLP/WebShaper",
    "datasets_detailed": "[{\"name\": \"Alibaba-NLP/WebShaper\", \"link\": \"https://huggingface.co/datasets/Alibaba-NLP/WebShaper\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Jul 22\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.12566",
    "first_seen_date": "2025-07-21",
    "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal\n  Large Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.12566Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal\n  Large Language ModelsPublished on Jul 16\u00b7Submitted byZhaokai Wangon Jul 21Upvote14+6Authors:Gen Luo,Wenhan Dou,Wenhao Li,Zhaokai Wang,Xue Yang,Changyao Tian,Hao Li,Weiyun Wang,Wenhai Wang,Xizhou Zhu,Yu Qiao,Jifeng DaiAbstractMono-InternVL, an advanced monolithic Multimodal Large Language Model, integrates visual experts and employs Endogenous Visual Pre-training to enhance visual learning and reduce computational costs.AI-generated summaryThis paper focuses on monolithicMultimodal Large Language Models(MLLMs),\nwhich integrate visual encoding and language decoding into a single model.\nExisting structures and pre-training strategies formonolithic MLLMsoften\nsuffer from unstable optimization and catastrophic forgetting. To address these\nchallenges, our key idea is to embed a newvisual parameter spaceinto a\npre-trained LLM, enabling stable learning of visual knowledge from noisy data\nviadelta tuning. Based on this principle, we first introduce Mono-InternVL, an\nadvanced monolithic MLLM that incorporates a set of visual experts through amultimodal mixture-of-experts architecture. In addition, we design an\ninnovativeEndogenous Visual Pre-training(EViP) for Mono-InternVL to maximize\nits visual capabilities via progressive learning. Mono-InternVL achieves\ncompetitive performance against existing MLLMs but also leads to relatively\nexpensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper\nand stronger monolithic MLLM equipped with an improvedEViP(EViP++).EViP++introduces additionalvisual attention expertsto Mono-InternVL-1.5 and\nre-organizes the pre-training process in an efficient manner. During inference,\nit includes a fusedCUDA kernelto speed up itsMoE operations. With these\ndesigns, Mono-InternVL-1.5 significantly reduces training and inference costs,\nwhile still maintaining competitive",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/OpenGVLab/Mono-InternVL",
    "hf_paper_url": "https://huggingface.co/papers/2507.12566",
    "arxiv_url": "https://arxiv.org/abs/2507.12566",
    "num_models": 4,
    "models_list": "OpenGVLab/Mono-InternVL-2B, OpenGVLab/Mono-InternVL-2B-S1-2, OpenGVLab/Mono-InternVL-2B-S1-3, OpenGVLab/Mono-InternVL-2B-S1-1",
    "models_links": "https://huggingface.co/OpenGVLab/Mono-InternVL-2B, https://huggingface.co/OpenGVLab/Mono-InternVL-2B-S1-2, https://huggingface.co/OpenGVLab/Mono-InternVL-2B-S1-3, https://huggingface.co/OpenGVLab/Mono-InternVL-2B-S1-1",
    "models_detailed": "[{\"name\": \"OpenGVLab/Mono-InternVL-2B\", \"link\": \"https://huggingface.co/OpenGVLab/Mono-InternVL-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"OpenGVLab/Mono-InternVL-2B-S1-2\", \"link\": \"https://huggingface.co/OpenGVLab/Mono-InternVL-2B-S1-2\", \"task\": \"\", \"likes\": \"69\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"OpenGVLab/Mono-InternVL-2B-S1-3\", \"link\": \"https://huggingface.co/OpenGVLab/Mono-InternVL-2B-S1-3\", \"task\": \"\", \"likes\": \"70\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"OpenGVLab/Mono-InternVL-2B-S1-1\", \"link\": \"https://huggingface.co/OpenGVLab/Mono-InternVL-2B-S1-1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\"}]",
    "num_datasets": 1,
    "datasets_list": "OpenGVLab/Mono-InternVL-2B-Synthetic-Data",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/Mono-InternVL-2B-Synthetic-Data",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/Mono-InternVL-2B-Synthetic-Data\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/Mono-InternVL-2B-Synthetic-Data\", \"task\": \"\", \"likes\": \"135\", \"downloads\": \"\", \"updated\": \"Jul 22\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.14129",
    "first_seen_date": "2025-07-21",
    "title": "OpenBEATs: A Fully Open-Source General-Purpose Audio Encoder",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.14129OpenBEATs: A Fully Open-Source General-Purpose Audio EncoderPublished on Jul 18\u00b7Submitted byShikhar Bharadwajon Jul 21Upvote9+1Authors:Shikhar Bharadwaj,Samuele Cornell,Kwanghee Choi,Satoru Fukayama,Hye-jin Shim,Soham Deshmukh,Shinji WatanabeAbstractOpenBEATs, an open-source framework extending BEATs with multi-domain audio pre-training, achieves state-of-the-art performance on various audio tasks with a smaller parameter size compared to larger models.AI-generated summaryMasked token predictionhas emerged as a powerfulpre-trainingobjective\nacross language, vision, and speech, offering the potential to unify these\ndiverse modalities through a singlepre-trainingtask. However, its application\nfor general audio understanding remains underexplored, with BEATs being the\nonly notable example. BEATs has seen limited modifications due to the absence\nof open-sourcepre-trainingcode. Furthermore, BEATs was trained only onAudioSet, restricting its broader downstream applicability. To address these\ngaps, we presentOpenBEATs, an open-source framework that extends BEATs viamulti-domain audio pre-training. We conduct comprehensive evaluations across\nsix types of tasks, twenty five datasets, and three audio domains, includingaudio reasoningtasks such asaudio question answering,entailment, andcaptioning.OpenBEATsachieves state-of-the-art performance on six bioacoustics\ndatasets, twoenvironmental sound datasetsand fivereasoning datasets,\nperforming better than models exceeding a billion parameters at one-fourth\ntheir parameter size. These results demonstrate the effectiveness of\nmulti-domain datasets andmasked token predictiontask to learn general-purpose\naudio representations. To promote further research and reproducibility, we\nrelease allpre-trainingand evaluation code, pretrained and fine-tuned\ncheckpoints, and training logs at https://shikhar-s.github.io/OpenBEATsView arXiv pageView PD",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2507.14129",
    "arxiv_url": "https://arxiv.org/abs/2507.14129",
    "num_models": 75,
    "models_list": "espnet/OpenBEATS-Large-AQAyn-BERT, shikhar7ssu/OpenBEATS-Large-i1-esc50f1, shikhar7ssu/OpenBEATS-Large-i1-esc50f2, shikhar7ssu/OpenBEATS-Large-i1-esc50f3, shikhar7ssu/OpenBEATS-Large-i1-esc50f4, shikhar7ssu/OpenBEATS-Large-i1-esc50f5, shikhar7ssu/OpenBEATS-Large-i2-esc50f1, shikhar7ssu/OpenBEATS-Large-i2-esc50f2, shikhar7ssu/OpenBEATS-Large-i2-esc50f3, shikhar7ssu/OpenBEATS-Large-i2-esc50f4, shikhar7ssu/OpenBEATS-Large-i2-esc50f5, shikhar7ssu/OpenBEATS-Large-i3-esc50f1, shikhar7ssu/OpenBEATS-Large-i3-esc50f2, shikhar7ssu/OpenBEATS-Large-i3-esc50f3, shikhar7ssu/OpenBEATS-Large-i3-esc50f4, shikhar7ssu/OpenBEATS-Large-i3-esc50f5, shikhar7ssu/OpenBEATS-Large-i1-as20k, shikhar7ssu/OpenBEATS-Large-i2-as20k, shikhar7ssu/OpenBEATS-Large-i3-as20k, espnet/OpenBEATS-Large-i3-as2m, espnet/OpenBEATS-Large-i2-as2m, espnet/OpenBEATS-Large-i1-as2m, espnet/OpenBEATS-Large-i3-as20k, espnet/OpenBEATS-Large-i2-as20k, espnet/OpenBEATS-Large-i1-as20k, espnet/OpenBEATS-Large-i3-fsd50k, espnet/OpenBEATS-Large-i2-fsd50k, espnet/OpenBEATS-Large-i1-fsd50k, espnet/OpenBEATS-Base-i3-fsd50k, espnet/OpenBEATS-Base-i2-fsd50k, espnet/OpenBEATS-Base-i1-fsd50k, espnet/OpenBEATS-Base-i3-as20k, espnet/OpenBEATS-Base-i2-as20k, espnet/OpenBEATS-Base-i1-as20k, espnet/OpenBEATS-Base-i3-as2m, espnet/OpenBEATS-Base-i2-as2m, espnet/OpenBEATS-Base-i1-as2m, espnet/OpenBEATS-Large-i3-watkins, espnet/OpenBEATS-Large-i2-watkins, espnet/OpenBEATS-Large-i1-watkins, espnet/OpenBEATS-Large-i3-cbi, espnet/OpenBEATS-Large-i2-cbi, espnet/OpenBEATS-Large-i1-cbi, espnet/OpenBEATS-Large-i3-humbugdb, espnet/OpenBEATS-Large-i2-humbugdb, espnet/OpenBEATS-Large-i3-dogs, espnet/OpenBEATS-Large-i1-humbugdb, espnet/OpenBEATS-Large-i2-dogs, espnet/OpenBEATS-Large-i3-bats, espnet/OpenBEATS-Large-i1-dogs, espnet/OpenBEATS-Large-i2-bats, espnet/OpenBEATS-Large-i1-bats, espnet/OpenBEATS-Large-i3-dcase21_task5, espnet/OpenBEATS-Large-i2-dcase21_task5, espnet/OpenBEATS-Large-i3-rfcx, espnet/OpenBEATS-Large-i1-dcase21_task5, espnet/OpenBEATS-Large-i2-rfcx, espnet/OpenBEATS-Large-i3-gibbons, espnet/OpenBEATS-Large-i1-rfcx, espnet/OpenBEATS-Large-i3-hiceas, espnet/OpenBEATS-Large-i2-gibbons, espnet/OpenBEATS-Large-i2-hiceas, espnet/OpenBEATS-Large-i1-gibbons, espnet/OpenBEATS-Large-i3-enabirds, espnet/OpenBEATS-Large-i1-hiceas, espnet/OpenBEATS-Large-i2-enabirds, espnet/OpenBEATS-Large-i1-enabirds, espnet/OpenBEATS-Large-AQAOpen-BERT, espnet/OpenBEATS-Large-AQAOpen-CLAP, espnet/OpenBEATS-Large-AQAyn-CLAP, espnet/OpenBEATS-Large-Entailment-CLAP, espnet/OpenBEATS-Large-Entailment-BERT, espnet/OpenBEATS-Large-GTZAN, espnet/OpenBEATS-Large-NsynthInstrument, espnet/OpenBEATS-Large-NsynthPitch",
    "models_links": "https://huggingface.co/espnet/OpenBEATS-Large-AQAyn-BERT, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f1, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f2, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f3, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f4, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f5, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f1, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f2, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f3, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f4, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f5, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f1, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f2, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f3, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f4, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f5, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-as20k, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-as20k, https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-as20k, https://huggingface.co/espnet/OpenBEATS-Large-i3-as2m, https://huggingface.co/espnet/OpenBEATS-Large-i2-as2m, https://huggingface.co/espnet/OpenBEATS-Large-i1-as2m, https://huggingface.co/espnet/OpenBEATS-Large-i3-as20k, https://huggingface.co/espnet/OpenBEATS-Large-i2-as20k, https://huggingface.co/espnet/OpenBEATS-Large-i1-as20k, https://huggingface.co/espnet/OpenBEATS-Large-i3-fsd50k, https://huggingface.co/espnet/OpenBEATS-Large-i2-fsd50k, https://huggingface.co/espnet/OpenBEATS-Large-i1-fsd50k, https://huggingface.co/espnet/OpenBEATS-Base-i3-fsd50k, https://huggingface.co/espnet/OpenBEATS-Base-i2-fsd50k, https://huggingface.co/espnet/OpenBEATS-Base-i1-fsd50k, https://huggingface.co/espnet/OpenBEATS-Base-i3-as20k, https://huggingface.co/espnet/OpenBEATS-Base-i2-as20k, https://huggingface.co/espnet/OpenBEATS-Base-i1-as20k, https://huggingface.co/espnet/OpenBEATS-Base-i3-as2m, https://huggingface.co/espnet/OpenBEATS-Base-i2-as2m, https://huggingface.co/espnet/OpenBEATS-Base-i1-as2m, https://huggingface.co/espnet/OpenBEATS-Large-i3-watkins, https://huggingface.co/espnet/OpenBEATS-Large-i2-watkins, https://huggingface.co/espnet/OpenBEATS-Large-i1-watkins, https://huggingface.co/espnet/OpenBEATS-Large-i3-cbi, https://huggingface.co/espnet/OpenBEATS-Large-i2-cbi, https://huggingface.co/espnet/OpenBEATS-Large-i1-cbi, https://huggingface.co/espnet/OpenBEATS-Large-i3-humbugdb, https://huggingface.co/espnet/OpenBEATS-Large-i2-humbugdb, https://huggingface.co/espnet/OpenBEATS-Large-i3-dogs, https://huggingface.co/espnet/OpenBEATS-Large-i1-humbugdb, https://huggingface.co/espnet/OpenBEATS-Large-i2-dogs, https://huggingface.co/espnet/OpenBEATS-Large-i3-bats, https://huggingface.co/espnet/OpenBEATS-Large-i1-dogs, https://huggingface.co/espnet/OpenBEATS-Large-i2-bats, https://huggingface.co/espnet/OpenBEATS-Large-i1-bats, https://huggingface.co/espnet/OpenBEATS-Large-i3-dcase21_task5, https://huggingface.co/espnet/OpenBEATS-Large-i2-dcase21_task5, https://huggingface.co/espnet/OpenBEATS-Large-i3-rfcx, https://huggingface.co/espnet/OpenBEATS-Large-i1-dcase21_task5, https://huggingface.co/espnet/OpenBEATS-Large-i2-rfcx, https://huggingface.co/espnet/OpenBEATS-Large-i3-gibbons, https://huggingface.co/espnet/OpenBEATS-Large-i1-rfcx, https://huggingface.co/espnet/OpenBEATS-Large-i3-hiceas, https://huggingface.co/espnet/OpenBEATS-Large-i2-gibbons, https://huggingface.co/espnet/OpenBEATS-Large-i2-hiceas, https://huggingface.co/espnet/OpenBEATS-Large-i1-gibbons, https://huggingface.co/espnet/OpenBEATS-Large-i3-enabirds, https://huggingface.co/espnet/OpenBEATS-Large-i1-hiceas, https://huggingface.co/espnet/OpenBEATS-Large-i2-enabirds, https://huggingface.co/espnet/OpenBEATS-Large-i1-enabirds, https://huggingface.co/espnet/OpenBEATS-Large-AQAOpen-BERT, https://huggingface.co/espnet/OpenBEATS-Large-AQAOpen-CLAP, https://huggingface.co/espnet/OpenBEATS-Large-AQAyn-CLAP, https://huggingface.co/espnet/OpenBEATS-Large-Entailment-CLAP, https://huggingface.co/espnet/OpenBEATS-Large-Entailment-BERT, https://huggingface.co/espnet/OpenBEATS-Large-GTZAN, https://huggingface.co/espnet/OpenBEATS-Large-NsynthInstrument, https://huggingface.co/espnet/OpenBEATS-Large-NsynthPitch",
    "models_detailed": "[{\"name\": \"espnet/OpenBEATS-Large-AQAyn-BERT\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-AQAyn-BERT\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i1-esc50f1\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i1-esc50f2\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i1-esc50f3\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i1-esc50f4\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i1-esc50f5\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-esc50f5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i2-esc50f1\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i2-esc50f2\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i2-esc50f3\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i2-esc50f4\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i2-esc50f5\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-esc50f5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i3-esc50f1\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i3-esc50f2\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i3-esc50f3\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i3-esc50f4\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i3-esc50f5\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-esc50f5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i1-as20k\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i1-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i2-as20k\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i2-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"shikhar7ssu/OpenBEATS-Large-i3-as20k\", \"link\": \"https://huggingface.co/shikhar7ssu/OpenBEATS-Large-i3-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-as2m\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-as2m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-as2m\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-as2m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-as2m\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-as2m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-as20k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-as20k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-as20k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-fsd50k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-fsd50k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-fsd50k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-fsd50k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-fsd50k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-fsd50k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i3-fsd50k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i3-fsd50k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i2-fsd50k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i2-fsd50k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i1-fsd50k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i1-fsd50k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i3-as20k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i3-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i2-as20k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i2-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i1-as20k\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i1-as20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i3-as2m\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i3-as2m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i2-as2m\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i2-as2m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Base-i1-as2m\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Base-i1-as2m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-watkins\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-watkins\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-watkins\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-watkins\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-watkins\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-watkins\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-cbi\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-cbi\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-cbi\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-cbi\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-cbi\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-cbi\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-humbugdb\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-humbugdb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-humbugdb\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-humbugdb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-dogs\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-dogs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-humbugdb\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-humbugdb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-dogs\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-dogs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-bats\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-bats\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-dogs\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-dogs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-bats\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-bats\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-bats\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-bats\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-dcase21_task5\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-dcase21_task5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-dcase21_task5\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-dcase21_task5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-rfcx\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-rfcx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-dcase21_task5\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-dcase21_task5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-rfcx\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-rfcx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-gibbons\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-gibbons\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-rfcx\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-rfcx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-hiceas\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-hiceas\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-gibbons\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-gibbons\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-hiceas\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-hiceas\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-gibbons\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-gibbons\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i3-enabirds\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i3-enabirds\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-hiceas\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-hiceas\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i2-enabirds\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i2-enabirds\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-i1-enabirds\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-i1-enabirds\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-AQAOpen-BERT\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-AQAOpen-BERT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-AQAOpen-CLAP\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-AQAOpen-CLAP\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-AQAyn-CLAP\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-AQAyn-CLAP\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-Entailment-CLAP\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-Entailment-CLAP\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-Entailment-BERT\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-Entailment-BERT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-GTZAN\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-GTZAN\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-NsynthInstrument\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-NsynthInstrument\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"espnet/OpenBEATS-Large-NsynthPitch\", \"link\": \"https://huggingface.co/espnet/OpenBEATS-Large-NsynthPitch\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}]",
    "num_datasets": 1,
    "datasets_list": "Bencr/beats-checkpoints",
    "datasets_links": "https://huggingface.co/datasets/Bencr/beats-checkpoints",
    "datasets_detailed": "[{\"name\": \"Bencr/beats-checkpoints\", \"link\": \"https://huggingface.co/datasets/Bencr/beats-checkpoints\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 15\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.12956",
    "first_seen_date": "2025-07-18",
    "title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.12956FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion TransformersPublished on Jul 17\u00b7Submitted bywangqiangon Jul 18Upvote24+16Authors:Qiang Wang,Mengchao Wang,Fan Jiang,Yaqi Fan,Yonggang Qi,Mu XuAbstractFantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.AI-generated summaryProducing expressive facial animations from static images is a challenging\ntask. Prior methods relying on explicit geometric priors (e.g., facial\nlandmarks or 3DMM) often suffer from artifacts in cross reenactment and\nstruggle to capture subtle emotions. Furthermore, existing approaches lack\nsupport for multi-character animation, as driving features from different\nindividuals frequently interfere with one another, complicating the task. To\naddress these challenges, we propose FantasyPortrait, adiffusion transformerbased framework capable of generating high-fidelity and emotion-rich animations\nfor both single- and multi-character scenarios. Our method introduces anexpression-augmented learningstrategy that utilizesimplicit representationsto capture identity-agnostic facial dynamics, enhancing the model's ability to\nrender fine-grained emotions. For multi-character control, we design a masked\ncross-attention mechanism that ensures independent yet coordinated expression\ngeneration, effectively preventing feature interference. To advance research in\nthis area, we propose theMulti-Expr datasetandExprBench, which are\nspecifically designed datasets and benchmarks for training and evaluating\nmulti-character portrait animations. Extensive experiments demonstrate that\nFantasyPortrait significantly outperforms state-of-the-art methods in both\nquantitative metrics and qualitative evaluations, excelli",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/Fantasy-AMAP/fantasy-portrait",
    "hf_paper_url": "https://huggingface.co/papers/2507.12956",
    "arxiv_url": "https://arxiv.org/abs/2507.12956",
    "num_models": 1,
    "models_list": "acvlab/FantasyPortrait",
    "models_links": "https://huggingface.co/acvlab/FantasyPortrait",
    "models_detailed": "[{\"name\": \"acvlab/FantasyPortrait\", \"link\": \"https://huggingface.co/acvlab/FantasyPortrait\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 12\"}]",
    "num_datasets": 1,
    "datasets_list": "acvlab/FantasyPortrait-Multi-Expr",
    "datasets_links": "https://huggingface.co/datasets/acvlab/FantasyPortrait-Multi-Expr",
    "datasets_detailed": "[{\"name\": \"acvlab/FantasyPortrait-Multi-Expr\", \"link\": \"https://huggingface.co/datasets/acvlab/FantasyPortrait-Multi-Expr\", \"task\": \"\", \"likes\": \"110\", \"downloads\": \"\", \"updated\": \"Aug 20\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.09075",
    "first_seen_date": "2025-07-16",
    "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via\n  Self-Critique",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.09075OpenCodeReasoning-II: A Simple Test Time Scaling Approach via\n  Self-CritiquePublished on Jul 11\u00b7Submitted bySomshubra Majumdaron Jul 16Upvote15+7Authors:Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris GinsburgAbstractOpenCodeReasoning-II, a large dataset for code reasoning, enhances code generation and critique through a two-stage fine-tuning strategy, improving competitive coding performance and extending LiveCodeBench for C++.AI-generated summaryRecent advancements in reasoning-basedLarge Language Models(LLMs),\nparticularly their potential throughtest-time scaling, have created\nsignificant opportunities fordistillationincode generationandcritique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduceOpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critiquetriples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stagesupervised fine-tuningstrategy. The first stage focuses on fine-tuning forcode generation, while the second stage involves the joint training of models\nfor bothcode generationandcritique. Our resulting finetunedQwen2.5-Instructmodels achieve performance incode generationthat either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration andcritiquemodels leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of theLiveCodeBenchbenchmark to specifically support theC++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.View arXiv pageView PDFAdd to collectionCommunitysmajumdar94Paper authorPaper submitterJul 16Recent a",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2507.09075",
    "arxiv_url": "https://arxiv.org/abs/2507.09075",
    "num_models": 23,
    "models_list": "nvidia/OpenReasoning-Nemotron-32B, nvidia/OpenReasoning-Nemotron-1.5B, nvidia/OpenReasoning-Nemotron-7B, nvidia/OpenReasoning-Nemotron-14B, gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF, gabriellarson/OpenReasoning-Nemotron-7B-GGUF, gabriellarson/OpenReasoning-Nemotron-14B-GGUF, gabriellarson/OpenReasoning-Nemotron-32B-GGUF, unsloth/OpenReasoning-Nemotron-32B, unsloth/OpenReasoning-Nemotron-32B-GGUF, Mungert/OpenReasoning-Nemotron-32B-GGUF, codys12/OpenReasoning-Nemotron-32B, Mungert/OpenReasoning-Nemotron-7B-GGUF, Mungert/OpenReasoning-Nemotron-1.5B-GGUF, jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8, Mungert/OpenReasoning-Nemotron-14B-GGUF, Prince-1/OpenReasoning-Nemotron-7B, onnx-community/OpenReasoning-Nemotron-7B, onnx-community/OpenReasoning-Nemotron-1.5B, Prince-1/OpenReasoning-Nemotron-1.5B, pamanseau/OpenReasoning-Nemotron-32B, Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit, QuantFactory/OpenReasoning-Nemotron-7B-GGUF",
    "models_links": "https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B, https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B, https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B, https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-7B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-14B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B, https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/Mungert/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/codys12/OpenReasoning-Nemotron-32B, https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF, https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF, https://huggingface.co/jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8, https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF, https://huggingface.co/Prince-1/OpenReasoning-Nemotron-7B, https://huggingface.co/onnx-community/OpenReasoning-Nemotron-7B, https://huggingface.co/onnx-community/OpenReasoning-Nemotron-1.5B, https://huggingface.co/Prince-1/OpenReasoning-Nemotron-1.5B, https://huggingface.co/pamanseau/OpenReasoning-Nemotron-32B, https://huggingface.co/Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit, https://huggingface.co/QuantFactory/OpenReasoning-Nemotron-7B-GGUF",
    "models_detailed": "[{\"name\": \"nvidia/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"452\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"408\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-14B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"303\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"182\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"148\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"unsloth/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"30\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"unsloth/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"610\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"codys12/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/codys12/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"273\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"188\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8\", \"link\": \"https://huggingface.co/jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Prince-1/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/Prince-1/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"onnx-community/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/onnx-community/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"onnx-community/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/onnx-community/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"Prince-1/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/Prince-1/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"pamanseau/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/pamanseau/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit\", \"link\": \"https://huggingface.co/Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"QuantFactory/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"131\", \"downloads\": \"\", \"updated\": \"Sep 1\"}]",
    "num_datasets": 2,
    "datasets_list": "nvidia/Nemotron-Competitive-Programming-v1, edgerunner-ai/nvidia__Nemotron-Competitive-Programming-v1",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Nemotron-Competitive-Programming-v1, https://huggingface.co/datasets/edgerunner-ai/nvidia__Nemotron-Competitive-Programming-v1",
    "datasets_detailed": "[{\"name\": \"nvidia/Nemotron-Competitive-Programming-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Competitive-Programming-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"edgerunner-ai/nvidia__Nemotron-Competitive-Programming-v1\", \"link\": \"https://huggingface.co/datasets/edgerunner-ai/nvidia__Nemotron-Competitive-Programming-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.11407",
    "first_seen_date": "2025-07-16",
    "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and\n  Reasoning Modes",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.11407EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and\n  Reasoning ModesPublished on Jul 15\u00b7Submitted byYireun Kimon Jul 16#1 Paper of the dayUpvote58+50Authors:LG AI Research,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim+19 authorsAbstractEXAONE 4.0 integrates non-reasoning and reasoning modes, supports agentic tool use, and offers multilingual capabilities, demonstrating superior performance in its class.AI-generated summaryThis technical report introducesEXAONE 4.0, which integrates a Non-reasoning\nmode and aReasoning modeto achieve both the excellent usability ofEXAONE 3.5and the advanced reasoning abilities ofEXAONE Deep. To pave the way for the\nagentic AI era,EXAONE 4.0incorporates essential features such as agentic tool\nuse, and itsmultilingual capabilitiesare extended to support Spanish in\naddition to English and Korean. TheEXAONE 4.0model series consists of two\nsizes: amid-size 32B modeloptimized for high performance, and a small-size\n1.2B model designed for on-device applications. TheEXAONE 4.0demonstrates\nsuperior performance compared toopen-weight modelsin its class and remains\ncompetitive even againstfrontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE.View arXiv pageView PDFAdd to collectionCommunityyireunPaper authorPaper submitterJul 16This comment has been hiddenyireunPaper authorPaper submitterJul 16EXAONE 4.0: Unified Model Integrating Non-reasoning and Reasoning Modeshttps://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32Bhttps://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2BSee translation\ud83d\udc4d66+ReplyBuiDoanJul 19How many NVIDIA H10",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2507.11407",
    "arxiv_url": "https://arxiv.org/abs/2507.11407",
    "num_models": 17,
    "models_list": "LGAI-EXAONE/EXAONE-4.0-32B, LGAI-EXAONE/EXAONE-4.0-1.2B, LGAI-EXAONE/EXAONE-4.0-32B-GGUF, LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF, LGAI-EXAONE/EXAONE-4.0-32B-FP8, LGAI-EXAONE/EXAONE-4.0.1-32B, LGAI-EXAONE/EXAONE-4.0-32B-GPTQ, LGAI-EXAONE/EXAONE-4.0-32B-AWQ, LGAI-EXAONE/EXAONE-4.0-1.2B-FP8, LGAI-EXAONE/EXAONE-4.0-1.2B-GPTQ-Int8, LGAI-EXAONE/EXAONE-4.0-1.2B-AWQ, Mungert/EXAONE-4.0-32B-GGUF, Mungert/EXAONE-4.0-1.2B-GGUF, unsloth/EXAONE-4.0-32B, OpenLLM-Korea/EXAONE-4.0-32B, OpenLLM-Korea/EXAONE-4.0-1.2B, OpenLLM-Korea/EXAONE-4.0.1-32B",
    "models_links": "https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GGUF, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-FP8, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GPTQ, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-AWQ, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-FP8, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GPTQ-Int8, https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-AWQ, https://huggingface.co/Mungert/EXAONE-4.0-32B-GGUF, https://huggingface.co/Mungert/EXAONE-4.0-1.2B-GGUF, https://huggingface.co/unsloth/EXAONE-4.0-32B, https://huggingface.co/OpenLLM-Korea/EXAONE-4.0-32B, https://huggingface.co/OpenLLM-Korea/EXAONE-4.0-1.2B, https://huggingface.co/OpenLLM-Korea/EXAONE-4.0.1-32B",
    "models_detailed": "[{\"name\": \"LGAI-EXAONE/EXAONE-4.0-32B\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-1.2B\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-32B-GGUF\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"681\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-32B-FP8\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0.1-32B\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0.1-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-32B-GPTQ\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GPTQ\", \"task\": \"Text Generation\", \"likes\": \"348\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-32B-AWQ\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"867\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-1.2B-FP8\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-FP8\", \"task\": \"Text Generation\", \"likes\": \"241\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-1.2B-GPTQ-Int8\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"127\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"LGAI-EXAONE/EXAONE-4.0-1.2B-AWQ\", \"link\": \"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"117\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"Mungert/EXAONE-4.0-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/EXAONE-4.0-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"276\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/EXAONE-4.0-1.2B-GGUF\", \"link\": \"https://huggingface.co/Mungert/EXAONE-4.0-1.2B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"257\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"unsloth/EXAONE-4.0-32B\", \"link\": \"https://huggingface.co/unsloth/EXAONE-4.0-32B\", \"task\": \"Text Generation\", \"likes\": \"29\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"OpenLLM-Korea/EXAONE-4.0-32B\", \"link\": \"https://huggingface.co/OpenLLM-Korea/EXAONE-4.0-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/EXAONE-4.0-1.2B\", \"link\": \"https://huggingface.co/OpenLLM-Korea/EXAONE-4.0-1.2B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/EXAONE-4.0.1-32B\", \"link\": \"https://huggingface.co/OpenLLM-Korea/EXAONE-4.0.1-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.08128",
    "first_seen_date": "2025-07-14",
    "title": "Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large\n  Audio Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.08128Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large\n  Audio Language ModelsPublished on Jul 10\u00b7Submitted byGhoshon Jul 14Upvote10+2Authors:Arushi Goel,Sreyan Ghosh,Jaehyeon Kim,Sonal Kumar,Zhifeng Kong,Sang-gil Lee,Chao-Han Huck Yang,Ramani Duraiswami,Dinesh Manocha,Rafael Valle,Bryan CatanzaroAbstractAudio Flamingo 3, a state-of-the-art audio-language model, advances reasoning and understanding across speech, sound, and music through a unified audio encoder and novel training strategies.AI-generated summaryWe present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large\naudio-language model that advances reasoning and understanding across speech,\nsound, and music. AF3 introduces: (i) AF-Whisper, a unifiedaudio encodertrained using a novel strategy forjoint representation learningacross all 3\nmodalities of speech, sound, and music; (ii) flexible, on-demand thinking,\nallowing the model to dochain-of-thought-type reasoningbefore answering;\n(iii) multi-turn, multi-audio chat; (iv)long audio understandingand reasoning\n(including speech) up to 10 minutes; and (v)voice-to-voice interaction. To\nenable these capabilities, we propose several large-scale training datasets\ncurated using novel strategies, includingAudioSkills-XL,LongAudio-XL,AF-Think, andAF-Chat, and train AF3 with a novel five-stage curriculum-based\ntraining strategy. Trained on only open-source audio data, AF3 achieves new\nSOTA results on over 20+ (long) audio understanding and reasoning benchmarks,\nsurpassing both open-weight and closed-source models trained on much larger\ndatasets.View arXiv pageView PDFAdd to collectionCommunitySreyan88Paper authorPaper submitterJul 15Model, weights, and code:https://research.nvidia.com/labs/adlr/AF3/See translationReplytdingman-scaleJul 18\u2022edited Jul 18I listed to a couple responses in the voice-to-voice section that were pretty inaccurate. The ki",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2507.08128",
    "arxiv_url": "https://arxiv.org/abs/2507.08128",
    "num_models": 4,
    "models_list": "nvidia/audio-flamingo-3-hf, nvidia/audio-flamingo-3, nvidia/audio-flamingo-3-chat, nvidia/audio-flamingo-2-SoundCoT",
    "models_links": "https://huggingface.co/nvidia/audio-flamingo-3-hf, https://huggingface.co/nvidia/audio-flamingo-3, https://huggingface.co/nvidia/audio-flamingo-3-chat, https://huggingface.co/nvidia/audio-flamingo-2-SoundCoT",
    "models_detailed": "[{\"name\": \"nvidia/audio-flamingo-3-hf\", \"link\": \"https://huggingface.co/nvidia/audio-flamingo-3-hf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"nvidia/audio-flamingo-3\", \"link\": \"https://huggingface.co/nvidia/audio-flamingo-3\", \"task\": \"\", \"likes\": \"751\", \"downloads\": \"\", \"updated\": \"23 days ago\"}, {\"name\": \"nvidia/audio-flamingo-3-chat\", \"link\": \"https://huggingface.co/nvidia/audio-flamingo-3-chat\", \"task\": \"\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"nvidia/audio-flamingo-2-SoundCoT\", \"link\": \"https://huggingface.co/nvidia/audio-flamingo-2-SoundCoT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}]",
    "num_datasets": 5,
    "datasets_list": "nvidia/AudioSkills, nvidia/AF-Think, nvidia/LongAudio, nvidia/AF-Chat, sonalkum/AudioSkills-Llama3",
    "datasets_links": "https://huggingface.co/datasets/nvidia/AudioSkills, https://huggingface.co/datasets/nvidia/AF-Think, https://huggingface.co/datasets/nvidia/LongAudio, https://huggingface.co/datasets/nvidia/AF-Chat, https://huggingface.co/datasets/sonalkum/AudioSkills-Llama3",
    "datasets_detailed": "[{\"name\": \"nvidia/AudioSkills\", \"link\": \"https://huggingface.co/datasets/nvidia/AudioSkills\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 12\", \"size\": \"\"}, {\"name\": \"nvidia/AF-Think\", \"link\": \"https://huggingface.co/datasets/nvidia/AF-Think\", \"task\": \"\", \"likes\": \"519\", \"downloads\": \"\", \"updated\": \"Aug 7\", \"size\": \"\"}, {\"name\": \"nvidia/LongAudio\", \"link\": \"https://huggingface.co/datasets/nvidia/LongAudio\", \"task\": \"\", \"likes\": \"377\", \"downloads\": \"\", \"updated\": \"Aug 8\", \"size\": \"\"}, {\"name\": \"nvidia/AF-Chat\", \"link\": \"https://huggingface.co/datasets/nvidia/AF-Chat\", \"task\": \"\", \"likes\": \"165\", \"downloads\": \"\", \"updated\": \"Jul 21\", \"size\": \"\"}, {\"name\": \"sonalkum/AudioSkills-Llama3\", \"link\": \"https://huggingface.co/datasets/sonalkum/AudioSkills-Llama3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.08801",
    "first_seen_date": "2025-07-14",
    "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.08801Lumos-1: On Autoregressive Video Generation from a Unified Model\n  PerspectivePublished on Jul 11\u00b7Submitted byHangjie Yuanon Jul 14Upvote30+22Authors:Hangjie Yuan,Weihua Chen,Jun Cen,Hu Yu,Jingyun Liang,Shuning Chang,Zhihui Lin,Tao Feng,Pengwei Liu,Jiazheng Xing,Hao Luo,Jiasheng Tang,Fan Wang,Yi YangAbstractLumos-1 is an autoregressive video generator that uses modified LLM architecture with MM-RoPE and AR-DF to handle spatiotemporal data efficiently, achieving competitive performance with reduced computational resources.AI-generated summaryAutoregressive large language models(LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\ninLLMs, we identify the efficacy of incorporating3D RoPEand diagnose its\nimbalanced frequency spectrum ranges. Therefore, we proposeMM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to atoken dependency strategythat obeysintra-frame bidirectionalityandinter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposingAutoregressive Discrete Diffusion Forcing(AR-DF).AR-DFintroduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By usingmemory-efficient trainingtechniques, we\npre-train",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/alibaba-damo-academy/Lumos",
    "hf_paper_url": "https://huggingface.co/papers/2507.08801",
    "arxiv_url": "https://arxiv.org/abs/2507.08801",
    "num_models": 1,
    "models_list": "Alibaba-DAMO-Academy/Lumos-1",
    "models_links": "https://huggingface.co/Alibaba-DAMO-Academy/Lumos-1",
    "models_detailed": "[{\"name\": \"Alibaba-DAMO-Academy/Lumos-1\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/Lumos-1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.07024",
    "first_seen_date": "2025-07-10",
    "title": "FlexOlmo: Open Language Models for Flexible Data Use",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.07024FlexOlmo: Open Language Models for Flexible Data UsePublished on Jul 9\u00b7Submitted byNiklas Muennighoffon Jul 10Upvote7Authors:Weijia Shi,Akshita Bhagia,Kevin Farhat,Niklas Muennighoff,Pete Walsh,Jacob Morrison,Dustin Schwenk,Shayne Longpre,Jake Poznanski,Allyson Ettinger,Daogao Liu,Margaret Li,Dirk Groeneveld,Mike Lewis,Wen-tau Yih,Luca Soldaini,Kyle Lo,Noah A. Smith,Luke Zettlemoyer,Pang Wei Koh,Hannaneh Hajishirzi,Ali Farhadi+1 authorsAbstractFlexOlmo, a distributed language model using a mixture-of-experts architecture, allows independent training on closed datasets and flexible inference without further training, improving performance while respecting data privacy.AI-generated summaryWe introduce FlexOlmo, a new class of language models (LMs) that supports (1)distributed trainingwithout data sharing, where different model parameters are\nindependently trained onclosed datasets, and (2)data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys amixture-of-experts(MoE) architecture where each expert is trained\nindependently onclosed datasetsand later integrated through a newdomain-informed routingwithout any joint training. FlexOlmo is trained onFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based ondata licensingor permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/allenai/FlexOlmo",
    "hf_paper_url": "https://huggingface.co/papers/2507.07024",
    "arxiv_url": "https://arxiv.org/abs/2507.07024",
    "num_models": 1,
    "models_list": "allenai/FlexOlmo-7x7B-1T",
    "models_links": "https://huggingface.co/allenai/FlexOlmo-7x7B-1T",
    "models_detailed": "[{\"name\": \"allenai/FlexOlmo-7x7B-1T\", \"link\": \"https://huggingface.co/allenai/FlexOlmo-7x7B-1T\", \"task\": \"Text Generation\", \"likes\": \"282\", \"downloads\": \"\", \"updated\": \"Oct 9\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.05201",
    "first_seen_date": "2025-07-09",
    "title": "MedGemma Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.05201MedGemma Technical ReportPublished on Jul 7\u00b7Submitted byNiels Roggeon Jul 9Upvote14+6Authors:Andrew Sellergren,Sahar Kazemzadeh,Tiam Jaroensri,Atilla Kiraly,Madeleine Traverse,Timo Kohlberger,Shawn Xu,Fayaz Jamil,C\u00edan Hughes,Charles Lau,Justin Chen,Fereshteh Mahvar,Liron Yatziv,Tiffany Chen,Bram Sterling,Stefanie Anna Baby,Susanna Maria Baby,Jeremy Lai,Samuel Schmidgall,Lu Yang,Kejia Chen,Per Bjornsson+58 authorsAbstractMedGemma, a collection of medical vision-language foundation models, demonstrates advanced medical understanding and reasoning, outperforming similar-sized generative models and approaching task-specific models' performance.AI-generated summaryArtificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.Foundation modelsthat perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-languagefoundation modelsbased onGemma 34B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of theGemma 3base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement onmedical multimodal question answering,\n15.5-18.1% improvement onchest X-ray finding classification, and 10.8%\nimprovement onagentic evaluationscompared to the base models.Fine-tuningMedGemma further improves performance in subdomains, reducing errors inelectronic health record information retrievalby 50% and reaching comparable\nperformance to existing specialized state-of-the-art meth",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/google-gemini/gemma-cookbook",
    "hf_paper_url": "https://huggingface.co/papers/2507.05201",
    "arxiv_url": "https://arxiv.org/abs/2507.05201",
    "num_models": 27,
    "models_list": "google/medgemma-4b-it, google/medgemma-27b-text-it, google/medgemma-27b-it, google/medgemma-4b-pt, google/medsiglip-448, unsloth/medgemma-4b-it-GGUF, unsloth/medgemma-4b-it, Mungert/medgemma-4b-it-GGUF, gabriellarson/medgemma-27b-it-GGUF, unsloth/medgemma-27b-it, unsloth/medgemma-27b-it-GGUF, Prince-1/medgemma-27b-it, onnx-community/MedGemma-27B-IT, ZhangQiao123/medgemma-27b-it-chinese-medical-qa-lora, FastFlowLM/medgemma-4b-it-NPU2, Muhammadidrees/my-medgamma, Muhammadidrees/Medgamma27B, erjui/medgemma-4b-csrrg-findings, erjui/medgemma-4b-srrg-findings, erjui/medgemma-4b-srrg-impression, erjui/medgemma-4b-csrrg-impression, fokan/MedSigLIP, OpenModels4all/mgemma, zarnow/medgemma-4b-it-bnb-4bit, pszemraj/medgemma-4b-it-heretic, pszemraj/medgemma-27b-text-heretic_med, tanujd/mg4b",
    "models_links": "https://huggingface.co/google/medgemma-4b-it, https://huggingface.co/google/medgemma-27b-text-it, https://huggingface.co/google/medgemma-27b-it, https://huggingface.co/google/medgemma-4b-pt, https://huggingface.co/google/medsiglip-448, https://huggingface.co/unsloth/medgemma-4b-it-GGUF, https://huggingface.co/unsloth/medgemma-4b-it, https://huggingface.co/Mungert/medgemma-4b-it-GGUF, https://huggingface.co/gabriellarson/medgemma-27b-it-GGUF, https://huggingface.co/unsloth/medgemma-27b-it, https://huggingface.co/unsloth/medgemma-27b-it-GGUF, https://huggingface.co/Prince-1/medgemma-27b-it, https://huggingface.co/onnx-community/MedGemma-27B-IT, https://huggingface.co/ZhangQiao123/medgemma-27b-it-chinese-medical-qa-lora, https://huggingface.co/FastFlowLM/medgemma-4b-it-NPU2, https://huggingface.co/Muhammadidrees/my-medgamma, https://huggingface.co/Muhammadidrees/Medgamma27B, https://huggingface.co/erjui/medgemma-4b-csrrg-findings, https://huggingface.co/erjui/medgemma-4b-srrg-findings, https://huggingface.co/erjui/medgemma-4b-srrg-impression, https://huggingface.co/erjui/medgemma-4b-csrrg-impression, https://huggingface.co/fokan/MedSigLIP, https://huggingface.co/OpenModels4all/mgemma, https://huggingface.co/zarnow/medgemma-4b-it-bnb-4bit, https://huggingface.co/pszemraj/medgemma-4b-it-heretic, https://huggingface.co/pszemraj/medgemma-27b-text-heretic_med, https://huggingface.co/tanujd/mg4b",
    "models_detailed": "[{\"name\": \"google/medgemma-4b-it\", \"link\": \"https://huggingface.co/google/medgemma-4b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"google/medgemma-27b-text-it\", \"link\": \"https://huggingface.co/google/medgemma-27b-text-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"google/medgemma-27b-it\", \"link\": \"https://huggingface.co/google/medgemma-27b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"google/medgemma-4b-pt\", \"link\": \"https://huggingface.co/google/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"google/medsiglip-448\", \"link\": \"https://huggingface.co/google/medsiglip-448\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"unsloth/medgemma-4b-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"unsloth/medgemma-4b-it\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"Mungert/medgemma-4b-it-GGUF\", \"link\": \"https://huggingface.co/Mungert/medgemma-4b-it-GGUF\", \"task\": \"\", \"likes\": \"413\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"gabriellarson/medgemma-27b-it-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/medgemma-27b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"unsloth/medgemma-27b-it\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-it\", \"task\": \"\", \"likes\": \"183\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"unsloth/medgemma-27b-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"Prince-1/medgemma-27b-it\", \"link\": \"https://huggingface.co/Prince-1/medgemma-27b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"onnx-community/MedGemma-27B-IT\", \"link\": \"https://huggingface.co/onnx-community/MedGemma-27B-IT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"ZhangQiao123/medgemma-27b-it-chinese-medical-qa-lora\", \"link\": \"https://huggingface.co/ZhangQiao123/medgemma-27b-it-chinese-medical-qa-lora\", \"task\": \"Text Generation\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"FastFlowLM/medgemma-4b-it-NPU2\", \"link\": \"https://huggingface.co/FastFlowLM/medgemma-4b-it-NPU2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 26\"}, {\"name\": \"Muhammadidrees/my-medgamma\", \"link\": \"https://huggingface.co/Muhammadidrees/my-medgamma\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\"}, {\"name\": \"Muhammadidrees/Medgamma27B\", \"link\": \"https://huggingface.co/Muhammadidrees/Medgamma27B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"erjui/medgemma-4b-csrrg-findings\", \"link\": \"https://huggingface.co/erjui/medgemma-4b-csrrg-findings\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"erjui/medgemma-4b-srrg-findings\", \"link\": \"https://huggingface.co/erjui/medgemma-4b-srrg-findings\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"erjui/medgemma-4b-srrg-impression\", \"link\": \"https://huggingface.co/erjui/medgemma-4b-srrg-impression\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"erjui/medgemma-4b-csrrg-impression\", \"link\": \"https://huggingface.co/erjui/medgemma-4b-csrrg-impression\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"fokan/MedSigLIP\", \"link\": \"https://huggingface.co/fokan/MedSigLIP\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"OpenModels4all/mgemma\", \"link\": \"https://huggingface.co/OpenModels4all/mgemma\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"zarnow/medgemma-4b-it-bnb-4bit\", \"link\": \"https://huggingface.co/zarnow/medgemma-4b-it-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"pszemraj/medgemma-4b-it-heretic\", \"link\": \"https://huggingface.co/pszemraj/medgemma-4b-it-heretic\", \"task\": \"\", \"likes\": \"87\", \"downloads\": \"\", \"updated\": \"29 days ago\"}, {\"name\": \"pszemraj/medgemma-27b-text-heretic_med\", \"link\": \"https://huggingface.co/pszemraj/medgemma-27b-text-heretic_med\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"29 days ago\"}, {\"name\": \"tanujd/mg4b\", \"link\": \"https://huggingface.co/tanujd/mg4b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.06181",
    "first_seen_date": "2025-07-09",
    "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  Formalization",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.06181CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  FormalizationPublished on Jul 8\u00b7Submitted byzhongyuan pengon Jul 9Upvote44+36Authors:Zhongyuan Peng,Yifan Yao,Kaijing Ma,Shuyue Guo,Yizhe Li,Yichi Zhang,Chenchen Zhang,Yifan Zhang,Zhouliang Yu,Luming Li,Minghao Liu,Yihang Xia,Jiawei Shen,Yuchen Wu,Yixin Cao,Zhaoxiang Zhang,Wenhao Huang,Jiaheng Liu,Ge ZhangAbstractCriticLean, a critic-guided reinforcement learning framework, enhances the evaluation of formalizations in automated theorem proving by actively learning and distinguishing semantically correct formalizations.AI-generated summaryTranslating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to thecritic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduceCriticLean, a novel critic-guidedreinforcement learningframework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose theCriticLeanGPT, trained\nviasupervised fine-tuningandreinforcement learning, to rigorously assess thesemantic fidelityofLean 4 formalizations. Then, we introduceCriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trainedCriticLeanGPTmodels can significantly outperform strong open- and\nclosed-source baselines. Building on theCriticLeanframework, we constructFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is esse",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/multimodal-art-projection/CriticLean",
    "hf_paper_url": "https://huggingface.co/papers/2507.06181",
    "arxiv_url": "https://arxiv.org/abs/2507.06181",
    "num_models": 3,
    "models_list": "m-a-p/CriticLeanGPT-Qwen3-8B-RL, m-a-p/CriticLeanGPT-Qwen3-14B-RL, m-a-p/CriticLeanGPT-Qwen3-32B-RL",
    "models_links": "https://huggingface.co/m-a-p/CriticLeanGPT-Qwen3-8B-RL, https://huggingface.co/m-a-p/CriticLeanGPT-Qwen3-14B-RL, https://huggingface.co/m-a-p/CriticLeanGPT-Qwen3-32B-RL",
    "models_detailed": "[{\"name\": \"m-a-p/CriticLeanGPT-Qwen3-8B-RL\", \"link\": \"https://huggingface.co/m-a-p/CriticLeanGPT-Qwen3-8B-RL\", \"task\": \"\", \"likes\": \"64\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"m-a-p/CriticLeanGPT-Qwen3-14B-RL\", \"link\": \"https://huggingface.co/m-a-p/CriticLeanGPT-Qwen3-14B-RL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"m-a-p/CriticLeanGPT-Qwen3-32B-RL\", \"link\": \"https://huggingface.co/m-a-p/CriticLeanGPT-Qwen3-32B-RL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}]",
    "num_datasets": 3,
    "datasets_list": "m-a-p/FineLeanCorpus, m-a-p/CriticLeanBench, m-a-p/CriticLeanInstruct",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/FineLeanCorpus, https://huggingface.co/datasets/m-a-p/CriticLeanBench, https://huggingface.co/datasets/m-a-p/CriticLeanInstruct",
    "datasets_detailed": "[{\"name\": \"m-a-p/FineLeanCorpus\", \"link\": \"https://huggingface.co/datasets/m-a-p/FineLeanCorpus\", \"task\": \"\", \"likes\": \"481\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}, {\"name\": \"m-a-p/CriticLeanBench\", \"link\": \"https://huggingface.co/datasets/m-a-p/CriticLeanBench\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Jul 9\", \"size\": \"\"}, {\"name\": \"m-a-p/CriticLeanInstruct\", \"link\": \"https://huggingface.co/datasets/m-a-p/CriticLeanInstruct\", \"task\": \"\", \"likes\": \"131\", \"downloads\": \"\", \"updated\": \"Jul 10\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.04590",
    "first_seen_date": "2025-07-08",
    "title": "VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual Documents",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.04590VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and\n  Visual DocumentsPublished on Jul 7\u00b7Submitted byZiyan Jiangon Jul 8Upvote16+8Authors:Rui Meng,Ziyan Jiang,Ye Liu,Mingyi Su,Xinyi Yang,Yuepeng Fu,Can Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Yingbo Zhou,Wenhu Chen,Semih YavuzAbstractA unified framework VLM2Vec-V2 for multimodal embedding supports diverse visual forms, including videos and visual documents, improving performance across various tasks and benchmarks.AI-generated summaryMultimodal embedding modelshave been crucial in enabling various downstream\ntasks such as semantic similarity, information retrieval, and clustering over\ndifferent modalities. However, existing multimodal embeddings like VLM2Vec,\nE5-V, GME are predominantly focused on natural images, with limited support for\nother visual forms such as videos and visual documents. This restricts their\napplicability in real-world scenarios, including AI agents, multi-modal search\nand recommendation, and retrieval-augmented generation (RAG). To close this\ngap, we proposeVLM2Vec-V2, a unified framework for learning embeddings across\ndiverse visual forms. First, we introduceMMEB-V2, a comprehensive benchmark\nthat extends MMEB with five new task types:visual document retrieval, video\nretrieval,temporal grounding,video classificationand video question\nanswering - spanning text, image, video, and visual document inputs. Next, we\ntrainVLM2Vec-V2, a general-purpose embedding model that supports text, image,\nvideo, and visual document inputs. Extensive experiments show thatVLM2Vec-V2achieves strong performance not only on the newly introduced video and document\nretrieval tasks, but also improves over prior baselines on the original image\nbenchmarks. Through extensive evaluation, our study offers insights into the\ngeneralizability of variousmultimodal embedding modelsand highlights\neffective strategies foruni",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/TIGER-AI-Lab/VLM2Vec",
    "hf_paper_url": "https://huggingface.co/papers/2507.04590",
    "arxiv_url": "https://arxiv.org/abs/2507.04590",
    "num_models": 1,
    "models_list": "VLM2Vec/VLM2Vec-V2.0",
    "models_links": "https://huggingface.co/VLM2Vec/VLM2Vec-V2.0",
    "models_detailed": "[{\"name\": \"VLM2Vec/VLM2Vec-V2.0\", \"link\": \"https://huggingface.co/VLM2Vec/VLM2Vec-V2.0\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 13\"}]",
    "num_datasets": 2,
    "datasets_list": "TIGER-Lab/MMEB-V2, TIGER-Lab/MMEB_Raw_Video",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/MMEB-V2, https://huggingface.co/datasets/TIGER-Lab/MMEB_Raw_Video",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/MMEB-V2\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/MMEB-V2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 11\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/MMEB_Raw_Video\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/MMEB_Raw_Video\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 9\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2507.05197",
    "first_seen_date": "2025-07-08",
    "title": "Pre-Trained Policy Discriminators are General Reward Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.05197Pre-Trained Policy Discriminators are General Reward ModelsPublished on Jul 7\u00b7Submitted byYicheng Zouon Jul 8Upvote39+31Authors:Shihan Dou,Shichun Liu,Yuming Yang,Yicheng Zou,Yunhua Zhou,Shuhao Xing,Chenhao Huang,Qiming Ge,Demin Song,Haijun Lv,Songyang Gao,Chengqi Lv,Enyu Zhou,Honglin Guo,Zhiheng Xi,Wenwei Zhang,Qipeng Guo,Qi Zhang,Xipeng Qiu,Xuanjing Huang,Tao Gui,Kai ChenAbstractA novel reward modeling approach, Policy Discriminative Learning (POLAR), enhances reward model performance and generalization in reinforcement learning by focusing on relative policy differences.AI-generated summaryWe offer a novel perspective onreward modeling by formulating it as a policy\ndiscriminator, which quantifies the difference between two policies to generate\na reward signal, guiding the training policy towards a target policy with\ndesired behaviors. Based on this conceptual insight, we propose a scalable\npre-training method named Policy Discriminative Learning (POLAR), which trains\nareward model(RM) to discern identical policies and discriminate different\nones. Unlike traditionalreward modeling methods relying on absolute\npreferences,POLARcaptures the relative difference between one policy and an\narbitrary target policy, which is a scalable, high-level optimization objective\nsuitable for modeling generic ranking relationships. Leveraging thePOLARpre-training paradigm, we present a series of RMs withparameter scalesfrom\n1.8B to 7B. Empirical results show thatPOLARsubstantially outperforms\ntraditional non-pre-trained methods, significantly enhancing RM performance.\nFor instance,POLAR-7B could improvepreference accuracyfrom 54.8% to 81.0% on\nSTEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA\nbaselines.POLARalso shows robust generalization capabilities inRLHFusingReinforcement Fine-tuning(RFT), providing reliable reward signals and markedly\nenhancing policy pe",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/InternLM/POLAR",
    "hf_paper_url": "https://huggingface.co/papers/2507.05197",
    "arxiv_url": "https://arxiv.org/abs/2507.05197",
    "num_models": 4,
    "models_list": "internlm/POLAR-7B, internlm/POLAR-1_8B, internlm/POLAR-7B-Base, internlm/POLAR-1_8B-Base",
    "models_links": "https://huggingface.co/internlm/POLAR-7B, https://huggingface.co/internlm/POLAR-1_8B, https://huggingface.co/internlm/POLAR-7B-Base, https://huggingface.co/internlm/POLAR-1_8B-Base",
    "models_detailed": "[{\"name\": \"internlm/POLAR-7B\", \"link\": \"https://huggingface.co/internlm/POLAR-7B\", \"task\": \"\", \"likes\": \"104\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"internlm/POLAR-1_8B\", \"link\": \"https://huggingface.co/internlm/POLAR-1_8B\", \"task\": \"\", \"likes\": \"105\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"internlm/POLAR-7B-Base\", \"link\": \"https://huggingface.co/internlm/POLAR-7B-Base\", \"task\": \"\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"internlm/POLAR-1_8B-Base\", \"link\": \"https://huggingface.co/internlm/POLAR-1_8B-Base\", \"task\": \"\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"Jul 15\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2507.01006",
    "first_seen_date": "2025-07-02",
    "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2507.01006GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement LearningPublished on Jul 1\u00b7Submitted byWenyi Hongon Jul 2#1 Paper of the dayUpvote246+238Authors:Wenyi Hong,Wenmeng Yu,Xiaotao Gu,Guo Wang,Guobing Gan,Haomiao Tang,Jiale Cheng,Ji Qi,Junhui Ji,Lihang Pan,Shuaiqi Duan,Weihan Wang,Yan Wang,Yean Cheng,Zehai He,Zhe Su,Zhen Yang,Ziyang Pan,Aohan Zeng,Baoxu Wang,Boyan Shi,Changyu Pang+55 authorsAbstractA vision-language model (VLM) named GLM-4.1V-Thinking, developed with a reasoning-centric training framework, achieves state-of-the-art performance across various tasks, including STEM problem solving, video understanding, and long document understanding, outperforming larger models on many benchmarks.AI-generated summaryWe present GLM-4.1V-Thinking, avision-language model(VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of thereasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthroughlarge-scale pre-training, which arguably sets the upper bound for the\nfinal performance.Reinforcement Learning with Curriculum Sampling(RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, includingSTEM problem solving,video understanding,content recognition,coding,grounding,GUI-based agents,\nandlong document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28public benchmarks, our model outperformsQwen2.5-VL-7Bon nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly largerQwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking al",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2507,
    "github_repo": "https://github.com/THUDM/GLM-4.1V-Thinking",
    "hf_paper_url": "https://huggingface.co/papers/2507.01006",
    "arxiv_url": "https://arxiv.org/abs/2507.01006",
    "num_models": 33,
    "models_list": "zai-org/GLM-4.1V-9B-Thinking, zai-org/GLM-4.5V, zai-org/GLM-4.6V-Flash, zai-org/GLM-4.6V, unsloth/GLM-4.6V-Flash-GGUF, unsloth/GLM-4.6V-GGUF, unsloth/GLM-4.6V, unsloth/GLM-4.6V-Flash, zai-org/GLM-4.5V-FP8, cyankiwi/GLM-4.6V-AWQ-4bit, zai-org/GLM-4.1V-9B-Base, dengcao/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix, dengcao/GLM-4.1V-9B-Thinking-AWQ, FriendliAI/GLM-4.1V-9B-Thinking, QuantTrio/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix, QuantTrio/GLM-4.1V-9B-Thinking-AWQ, Mungert/GLM-4.1V-9B-Thinking-GGUF, unsloth/GLM-4.1V-9B-Thinking, unsloth/GLM-4.1V-9B-Thinking-GGUF, ronx-labs/affine-081115, QuantTrio/GLM-4.5V-AWQ, cpatonn/GLM-4.5V-AWQ-4bit, cpatonn/GLM-4.5V-AWQ-8bit, CPU-Hybrid-MoE/GLM-4.5V-GPU-weight, zai-org/GLM-4.6V-FP8, neody/glm-4.6v-flash-gguf-text-only, cyankiwi/GLM-4.6V-AWQ-8bit, cyankiwi/GLM-4.6V-Flash-AWQ-4bit, cyankiwi/GLM-4.6V-Flash-AWQ-8bit, Gapeleon/GLM-4.6V-Flash-exl3-6.0bpw, lucyknada/zai-org_GLM-4.6V-Flash-exl3, AiAsistent/GLM-4.6V-Flash-heretic, alecccdd/GLM-4.6V-Flash-W8A8-INT8",
    "models_links": "https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking, https://huggingface.co/zai-org/GLM-4.5V, https://huggingface.co/zai-org/GLM-4.6V-Flash, https://huggingface.co/zai-org/GLM-4.6V, https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF, https://huggingface.co/unsloth/GLM-4.6V-GGUF, https://huggingface.co/unsloth/GLM-4.6V, https://huggingface.co/unsloth/GLM-4.6V-Flash, https://huggingface.co/zai-org/GLM-4.5V-FP8, https://huggingface.co/cyankiwi/GLM-4.6V-AWQ-4bit, https://huggingface.co/zai-org/GLM-4.1V-9B-Base, https://huggingface.co/dengcao/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix, https://huggingface.co/dengcao/GLM-4.1V-9B-Thinking-AWQ, https://huggingface.co/FriendliAI/GLM-4.1V-9B-Thinking, https://huggingface.co/QuantTrio/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix, https://huggingface.co/QuantTrio/GLM-4.1V-9B-Thinking-AWQ, https://huggingface.co/Mungert/GLM-4.1V-9B-Thinking-GGUF, https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking, https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF, https://huggingface.co/ronx-labs/affine-081115, https://huggingface.co/QuantTrio/GLM-4.5V-AWQ, https://huggingface.co/cpatonn/GLM-4.5V-AWQ-4bit, https://huggingface.co/cpatonn/GLM-4.5V-AWQ-8bit, https://huggingface.co/CPU-Hybrid-MoE/GLM-4.5V-GPU-weight, https://huggingface.co/zai-org/GLM-4.6V-FP8, https://huggingface.co/neody/glm-4.6v-flash-gguf-text-only, https://huggingface.co/cyankiwi/GLM-4.6V-AWQ-8bit, https://huggingface.co/cyankiwi/GLM-4.6V-Flash-AWQ-4bit, https://huggingface.co/cyankiwi/GLM-4.6V-Flash-AWQ-8bit, https://huggingface.co/Gapeleon/GLM-4.6V-Flash-exl3-6.0bpw, https://huggingface.co/lucyknada/zai-org_GLM-4.6V-Flash-exl3, https://huggingface.co/AiAsistent/GLM-4.6V-Flash-heretic, https://huggingface.co/alecccdd/GLM-4.6V-Flash-W8A8-INT8",
    "models_detailed": "[{\"name\": \"zai-org/GLM-4.1V-9B-Thinking\", \"link\": \"https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"zai-org/GLM-4.5V\", \"link\": \"https://huggingface.co/zai-org/GLM-4.5V\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"zai-org/GLM-4.6V-Flash\", \"link\": \"https://huggingface.co/zai-org/GLM-4.6V-Flash\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"zai-org/GLM-4.6V\", \"link\": \"https://huggingface.co/zai-org/GLM-4.6V\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"unsloth/GLM-4.6V-Flash-GGUF\", \"link\": \"https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"unsloth/GLM-4.6V-GGUF\", \"link\": \"https://huggingface.co/unsloth/GLM-4.6V-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"unsloth/GLM-4.6V\", \"link\": \"https://huggingface.co/unsloth/GLM-4.6V\", \"task\": \"\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"unsloth/GLM-4.6V-Flash\", \"link\": \"https://huggingface.co/unsloth/GLM-4.6V-Flash\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"zai-org/GLM-4.5V-FP8\", \"link\": \"https://huggingface.co/zai-org/GLM-4.5V-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"cyankiwi/GLM-4.6V-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/GLM-4.6V-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"zai-org/GLM-4.1V-9B-Base\", \"link\": \"https://huggingface.co/zai-org/GLM-4.1V-9B-Base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"dengcao/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix\", \"link\": \"https://huggingface.co/dengcao/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix\", \"task\": \"\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"dengcao/GLM-4.1V-9B-Thinking-AWQ\", \"link\": \"https://huggingface.co/dengcao/GLM-4.1V-9B-Thinking-AWQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"FriendliAI/GLM-4.1V-9B-Thinking\", \"link\": \"https://huggingface.co/FriendliAI/GLM-4.1V-9B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"QuantTrio/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix\", \"link\": \"https://huggingface.co/QuantTrio/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix\", \"task\": \"Text Generation\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"QuantTrio/GLM-4.1V-9B-Thinking-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/GLM-4.1V-9B-Thinking-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"Mungert/GLM-4.1V-9B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Mungert/GLM-4.1V-9B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"932\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"unsloth/GLM-4.1V-9B-Thinking\", \"link\": \"https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"unsloth/GLM-4.1V-9B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/GLM-4.1V-9B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"ronx-labs/affine-081115\", \"link\": \"https://huggingface.co/ronx-labs/affine-081115\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"QuantTrio/GLM-4.5V-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/GLM-4.5V-AWQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 25\"}, {\"name\": \"cpatonn/GLM-4.5V-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/GLM-4.5V-AWQ-4bit\", \"task\": \"\", \"likes\": \"645\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"cpatonn/GLM-4.5V-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/GLM-4.5V-AWQ-8bit\", \"task\": \"\", \"likes\": \"292\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"CPU-Hybrid-MoE/GLM-4.5V-GPU-weight\", \"link\": \"https://huggingface.co/CPU-Hybrid-MoE/GLM-4.5V-GPU-weight\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"zai-org/GLM-4.6V-FP8\", \"link\": \"https://huggingface.co/zai-org/GLM-4.6V-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"neody/glm-4.6v-flash-gguf-text-only\", \"link\": \"https://huggingface.co/neody/glm-4.6v-flash-gguf-text-only\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"cyankiwi/GLM-4.6V-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/GLM-4.6V-AWQ-8bit\", \"task\": \"\", \"likes\": \"321\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"cyankiwi/GLM-4.6V-Flash-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/GLM-4.6V-Flash-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"cyankiwi/GLM-4.6V-Flash-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/GLM-4.6V-Flash-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"Gapeleon/GLM-4.6V-Flash-exl3-6.0bpw\", \"link\": \"https://huggingface.co/Gapeleon/GLM-4.6V-Flash-exl3-6.0bpw\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"lucyknada/zai-org_GLM-4.6V-Flash-exl3\", \"link\": \"https://huggingface.co/lucyknada/zai-org_GLM-4.6V-Flash-exl3\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"AiAsistent/GLM-4.6V-Flash-heretic\", \"link\": \"https://huggingface.co/AiAsistent/GLM-4.6V-Flash-heretic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"alecccdd/GLM-4.6V-Flash-W8A8-INT8\", \"link\": \"https://huggingface.co/alecccdd/GLM-4.6V-Flash-W8A8-INT8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.20639",
    "first_seen_date": "2025-07-02",
    "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.20639DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  GenerationPublished on Jun 25\u00b7Submitted byShansan Gongon Jul 2Upvote30+22Authors:Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe ZhangAbstractDiffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.AI-generated summaryDiffusion large language models(dLLMs) are compelling alternatives to\nautoregressive (AR) models because theirdenoising modelsoperate over the\nentire sequence. Theglobal planninganditerative refinementfeatures of dLLMs\nare particularly useful forcode generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthedecoding behaviorof dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code.\nUsing this model as a testbed, we analyze itsdecoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthesampling temperaturediversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space forRL rollouts.\nFor RL training, to reduce the variance oftoken log-likelihood estimatesand\nmaintain training efficiency, we proposecoupled-GRPO, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments,coupled-GRPOsignificantly improves\nDiffuCoder's performance oncode generationbenchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation a",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/apple/ml-diffucoder",
    "hf_paper_url": "https://huggingface.co/papers/2506.20639",
    "arxiv_url": "https://arxiv.org/abs/2506.20639",
    "num_models": 6,
    "models_list": "apple/DiffuCoder-7B-cpGRPO, apple/DiffuCoder-7B-Instruct, apple/DiffuCoder-7B-Base, Mungert/DiffuCoder-7B-cpGRPO-GGUF, bachngo/DiffuCoder-7B-Q4KM, Mungert/DiffuCoder-7B-Instruct-GGUF",
    "models_links": "https://huggingface.co/apple/DiffuCoder-7B-cpGRPO, https://huggingface.co/apple/DiffuCoder-7B-Instruct, https://huggingface.co/apple/DiffuCoder-7B-Base, https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF, https://huggingface.co/bachngo/DiffuCoder-7B-Q4KM, https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF",
    "models_detailed": "[{\"name\": \"apple/DiffuCoder-7B-cpGRPO\", \"link\": \"https://huggingface.co/apple/DiffuCoder-7B-cpGRPO\", \"task\": \"\", \"likes\": \"533\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"apple/DiffuCoder-7B-Instruct\", \"link\": \"https://huggingface.co/apple/DiffuCoder-7B-Instruct\", \"task\": \"\", \"likes\": \"650\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"apple/DiffuCoder-7B-Base\", \"link\": \"https://huggingface.co/apple/DiffuCoder-7B-Base\", \"task\": \"\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"Mungert/DiffuCoder-7B-cpGRPO-GGUF\", \"link\": \"https://huggingface.co/Mungert/DiffuCoder-7B-cpGRPO-GGUF\", \"task\": \"\", \"likes\": \"356\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"bachngo/DiffuCoder-7B-Q4KM\", \"link\": \"https://huggingface.co/bachngo/DiffuCoder-7B-Q4KM\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"Mungert/DiffuCoder-7B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/DiffuCoder-7B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.23044",
    "first_seen_date": "2025-07-01",
    "title": "Ovis-U1 Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.23044Ovis-U1 Technical ReportPublished on Jun 29\u00b7Submitted byGuo-Hua Wangon Jul 1#1 Paper of the dayUpvote61+53Authors:Guo-Hua Wang,Shanshan Zhao,Xinjie Zhang,Liangfu Cao,Pengxin Zhan,Lunhao Duan,Shiyin Lu,Minghao Fu,Xiaohao Chen,Jianshan Zhao,Yang Li,Qing-Guo ChenAbstractOvis-U1, a 3-billion-parameter unified model, integrates multimodal understanding, text-to-image generation, and image editing using a diffusion-based visual decoder and bidirectional token refiner, achieving state-of-the-art performance across various benchmarks.AI-generated summaryIn this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integratesmultimodal understanding,text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates adiffusion-based visual decoderpaired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a newunified trainingapproach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks,unified trainingyields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on theOpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. Intext-to-image generation, it excels with scores of 83.72 and 0.89 on theDPG-BenchandGenEvalbenchmarks, respectively. Forimage editing, it achieves\n4.00 and 6.42 on theImgEdit-BenchandGEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nofmultimodal understanding, generation, and editing.View arXiv pageView PDFGitHub445Add to collectionCommunityFlourishPaper authorPaper submitterJul 1The code and m",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/AIDC-AI/Ovis-U1",
    "hf_paper_url": "https://huggingface.co/papers/2506.23044",
    "arxiv_url": "https://arxiv.org/abs/2506.23044",
    "num_models": 2,
    "models_list": "AIDC-AI/Ovis-U1-3B, Ashenone3/Ovis-U1-03B-DiT-SANA_VAE-0901",
    "models_links": "https://huggingface.co/AIDC-AI/Ovis-U1-3B, https://huggingface.co/Ashenone3/Ovis-U1-03B-DiT-SANA_VAE-0901",
    "models_detailed": "[{\"name\": \"AIDC-AI/Ovis-U1-3B\", \"link\": \"https://huggingface.co/AIDC-AI/Ovis-U1-3B\", \"task\": \"\", \"likes\": \"494\", \"downloads\": \"\", \"updated\": \"Jul 3\"}, {\"name\": \"Ashenone3/Ovis-U1-03B-DiT-SANA_VAE-0901\", \"link\": \"https://huggingface.co/Ashenone3/Ovis-U1-03B-DiT-SANA_VAE-0901\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 12\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.24119",
    "first_seen_date": "2025-07-01",
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.24119SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement LearningPublished on Jun 30\u00b7Submitted byBo Liuon Jul 1#2 Paper of the dayUpvote50+42Authors:Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha JaquesAbstractSelf-play in zero-sum games using SPIRAL framework enhances reasoning capabilities in language models through continuous adaptation and transfer learning.AI-generated summaryRecent advances inreinforcement learninghave shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, aself-playframework\nwhere models learn by playingmulti-turn,zero-sum gamesagainst continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThroughself-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable thisself-playtraining at scale, We implement a fully online,multi-turn,multi-agent reinforcement learningsystem for LLMs and proposerole-conditioned advantage estimation(RAE) to stabilize multi-agent training.\nUsing SPIRAL,self-playonzero-sum gamesproduces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base onKuhn Pokeralone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe,Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applyi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/spiral-rl/spiral",
    "hf_paper_url": "https://huggingface.co/papers/2506.24119",
    "arxiv_url": "https://arxiv.org/abs/2506.24119",
    "num_models": 25,
    "models_list": "spiral-rl/Spiral-Qwen3-4B, spiral-rl/Spiral-DeepSeek-R1-Distill-Qwen-7B, caiyuchen/Spiral-step-0, caiyuchen/Spiral-step-1, caiyuchen/Spiral-step-2, caiyuchen/Spiral-step-4, caiyuchen/Spiral-step-3, caiyuchen/Spiral-step-6, caiyuchen/Spiral-step-5, caiyuchen/Spiral-step-8, caiyuchen/Spiral-step-7, caiyuchen/Spiral-step-9, caiyuchen/Spiral-step-10, caiyuchen/Spiral-step-11, caiyuchen/Spiral-step-12, caiyuchen/Spiral-step-14, caiyuchen/Spiral-step-13, caiyuchen/Spiral-step-15, caiyuchen/Spiral-step-16, caiyuchen/Spiral-step-18, caiyuchen/Spiral-step-17, caiyuchen/Spiral-step-20, caiyuchen/Spiral-step-19, caiyuchen/Spiral-step-22, caiyuchen/Spiral-step-21",
    "models_links": "https://huggingface.co/spiral-rl/Spiral-Qwen3-4B, https://huggingface.co/spiral-rl/Spiral-DeepSeek-R1-Distill-Qwen-7B, https://huggingface.co/caiyuchen/Spiral-step-0, https://huggingface.co/caiyuchen/Spiral-step-1, https://huggingface.co/caiyuchen/Spiral-step-2, https://huggingface.co/caiyuchen/Spiral-step-4, https://huggingface.co/caiyuchen/Spiral-step-3, https://huggingface.co/caiyuchen/Spiral-step-6, https://huggingface.co/caiyuchen/Spiral-step-5, https://huggingface.co/caiyuchen/Spiral-step-8, https://huggingface.co/caiyuchen/Spiral-step-7, https://huggingface.co/caiyuchen/Spiral-step-9, https://huggingface.co/caiyuchen/Spiral-step-10, https://huggingface.co/caiyuchen/Spiral-step-11, https://huggingface.co/caiyuchen/Spiral-step-12, https://huggingface.co/caiyuchen/Spiral-step-14, https://huggingface.co/caiyuchen/Spiral-step-13, https://huggingface.co/caiyuchen/Spiral-step-15, https://huggingface.co/caiyuchen/Spiral-step-16, https://huggingface.co/caiyuchen/Spiral-step-18, https://huggingface.co/caiyuchen/Spiral-step-17, https://huggingface.co/caiyuchen/Spiral-step-20, https://huggingface.co/caiyuchen/Spiral-step-19, https://huggingface.co/caiyuchen/Spiral-step-22, https://huggingface.co/caiyuchen/Spiral-step-21",
    "models_detailed": "[{\"name\": \"spiral-rl/Spiral-Qwen3-4B\", \"link\": \"https://huggingface.co/spiral-rl/Spiral-Qwen3-4B\", \"task\": \"Text Generation\", \"likes\": \"40\", \"downloads\": \"\", \"updated\": \"Jul 5\"}, {\"name\": \"spiral-rl/Spiral-DeepSeek-R1-Distill-Qwen-7B\", \"link\": \"https://huggingface.co/spiral-rl/Spiral-DeepSeek-R1-Distill-Qwen-7B\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Jul 5\"}, {\"name\": \"caiyuchen/Spiral-step-0\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-1\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-2\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-4\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-3\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-6\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-5\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-8\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-7\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-7\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-9\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-9\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-10\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-10\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-11\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-11\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-12\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-12\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-14\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-14\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-13\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-13\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-15\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-15\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-16\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-18\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-18\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-17\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-17\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-20\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-20\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-19\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-19\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-22\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-22\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"caiyuchen/Spiral-step-21\", \"link\": \"https://huggingface.co/caiyuchen/Spiral-step-21\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}]",
    "num_datasets": 1,
    "datasets_list": "spiral-rl/Spiral-Kuhn-Poker-Qwen3-32B-SFT",
    "datasets_links": "https://huggingface.co/datasets/spiral-rl/Spiral-Kuhn-Poker-Qwen3-32B-SFT",
    "datasets_detailed": "[{\"name\": \"spiral-rl/Spiral-Kuhn-Poker-Qwen3-32B-SFT\", \"link\": \"https://huggingface.co/datasets/spiral-rl/Spiral-Kuhn-Poker-Qwen3-32B-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 5\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.21416",
    "first_seen_date": "2025-06-30",
    "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.21416XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT ModulationPublished on Jun 26\u00b7Submitted byZhaoon Jun 30Upvote28+20Authors:Bowen Chen,Mengyi Zhao,Haomiao Sun,Li Chen,Xu Wang,Kang Du,Xinglong WuAbstractXVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.AI-generated summaryAchieving fine-grained control over subject identity andsemantic attributes(pose, style, lighting) intext-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novelmulti-subject controlled generationmodel XVerse. By transforming reference\nimages into offsets fortoken-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disruptingimage latentsor features. Consequently, XVerse offers high-fidelity, editablemulti-subject image synthesiswith robust control over individual subject\ncharacteristics andsemantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.View arXiv pageView PDFProject pageGitHub616Add to collectionCommunityMengyiPaper submitterJun 30XVerse introduces a novel approach to multi-subject image synthesis, offering precise and independent control over individual subjects without disrupting the overall image latents or features. We achieve this by transforming reference images into offsets for token-specific text-stream modulation.This innovation enables high-fidelity, editable image generation where you can robustly control both individual subject characteristics (identity) and their semantic attributes. XV",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/bytedance/XVerse",
    "hf_paper_url": "https://huggingface.co/papers/2506.21416",
    "arxiv_url": "https://arxiv.org/abs/2506.21416",
    "num_models": 1,
    "models_list": "ByteDance/XVerse",
    "models_links": "https://huggingface.co/ByteDance/XVerse",
    "models_detailed": "[{\"name\": \"ByteDance/XVerse\", \"link\": \"https://huggingface.co/ByteDance/XVerse\", \"task\": \"\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Jul 1\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.16655",
    "first_seen_date": "2025-06-27",
    "title": "Arch-Router: Aligning LLM Routing with Human Preferences",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.16655Arch-Router: Aligning LLM Routing with Human PreferencesPublished on Jun 19\u00b7Submitted bySalmanon Jun 27Upvote17+9Authors:Co Tran,Salman Paracha,Adil Hafeez,Shuguang ChenAbstractA preference-aligned routing framework using a compact 1.5B model effectively matches queries to user-defined domains and action types, outperforming proprietary models in subjective evaluation criteria.AI-generated summaryWith the rapid proliferation oflarge language models(LLMs) -- each\noptimized for different strengths, style, or latency/cost profile -- routing\nhas become an essential technique to operationalize the use of different\nmodels. However, existingLLM routingapproaches are limited in two key ways:\nthey evaluate performance using benchmarks that often fail to capture human\npreferences driven by subjective evaluation criteria, and they typically select\nfrom a limited pool of models. In this work, we propose a preference-aligned\nrouting framework that guides model selection by matching queries to\nuser-defined domains (e.g., travel) or action types (e.g., image editing) --\noffering a practical mechanism to encode preferences in routing decisions.\nSpecifically, we introduceArch-Router, a compact 1.5B model that\nlearns to map queries todomain-action preferencesfor model routing decisions.\nOur approach also supports seamlessly adding new models for routing without\nrequiring retraining or architectural modifications. Experiments on\nconversational datasets demonstrate that our approach achieves state-of-the-art\n(SOTA) results in matching queries with human preferences, outperforming top\nproprietary models. Our approach captures subjective evaluation criteria and\nmakes routing decisions more transparent and flexible. Our model is available\nat: https://huggingface.co/katanemo/Arch-Router-1.5B.View arXiv pageView PDFProject pageGitHub4.65kAdd to collectionCommunityparachasPaper authorPaper submit",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/katanemo/archgw",
    "hf_paper_url": "https://huggingface.co/papers/2506.16655",
    "arxiv_url": "https://arxiv.org/abs/2506.16655",
    "num_models": 12,
    "models_list": "katanemo/Arch-Router-1.5B, katanemo/Arch-Router-1.5B.gguf, Mungert/Arch-Router-1.5B-GGUF, ThomasTheMaker/Arch-Router-1.5B-rkllm, mradermacher/Arch-Router-1.5B-GGUF, jedisct1/Arch-Router-1.5B, huangang/Arch-Router-1.5B-mlx-4Bit, huangang/Arch-Router-1.5B-mlx-fp16, N19hty/Arch-Router-1.5B-IQ3_XXS-GGUF, tensorblock/katanemo_Arch-Router-1.5B-GGUF, mlx-community/Arch-Router-1.5B-mlx-8Bit, vidavox/SKK-Router-1.5B",
    "models_links": "https://huggingface.co/katanemo/Arch-Router-1.5B, https://huggingface.co/katanemo/Arch-Router-1.5B.gguf, https://huggingface.co/Mungert/Arch-Router-1.5B-GGUF, https://huggingface.co/ThomasTheMaker/Arch-Router-1.5B-rkllm, https://huggingface.co/mradermacher/Arch-Router-1.5B-GGUF, https://huggingface.co/jedisct1/Arch-Router-1.5B, https://huggingface.co/huangang/Arch-Router-1.5B-mlx-4Bit, https://huggingface.co/huangang/Arch-Router-1.5B-mlx-fp16, https://huggingface.co/N19hty/Arch-Router-1.5B-IQ3_XXS-GGUF, https://huggingface.co/tensorblock/katanemo_Arch-Router-1.5B-GGUF, https://huggingface.co/mlx-community/Arch-Router-1.5B-mlx-8Bit, https://huggingface.co/vidavox/SKK-Router-1.5B",
    "models_detailed": "[{\"name\": \"katanemo/Arch-Router-1.5B\", \"link\": \"https://huggingface.co/katanemo/Arch-Router-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"katanemo/Arch-Router-1.5B.gguf\", \"link\": \"https://huggingface.co/katanemo/Arch-Router-1.5B.gguf\", \"task\": \"Text Generation\", \"likes\": \"467\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"Mungert/Arch-Router-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Arch-Router-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"ThomasTheMaker/Arch-Router-1.5B-rkllm\", \"link\": \"https://huggingface.co/ThomasTheMaker/Arch-Router-1.5B-rkllm\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 29\"}, {\"name\": \"mradermacher/Arch-Router-1.5B-GGUF\", \"link\": \"https://huggingface.co/mradermacher/Arch-Router-1.5B-GGUF\", \"task\": \"\", \"likes\": \"263\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"jedisct1/Arch-Router-1.5B\", \"link\": \"https://huggingface.co/jedisct1/Arch-Router-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 29\"}, {\"name\": \"huangang/Arch-Router-1.5B-mlx-4Bit\", \"link\": \"https://huggingface.co/huangang/Arch-Router-1.5B-mlx-4Bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"huangang/Arch-Router-1.5B-mlx-fp16\", \"link\": \"https://huggingface.co/huangang/Arch-Router-1.5B-mlx-fp16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"N19hty/Arch-Router-1.5B-IQ3_XXS-GGUF\", \"link\": \"https://huggingface.co/N19hty/Arch-Router-1.5B-IQ3_XXS-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"tensorblock/katanemo_Arch-Router-1.5B-GGUF\", \"link\": \"https://huggingface.co/tensorblock/katanemo_Arch-Router-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"mlx-community/Arch-Router-1.5B-mlx-8Bit\", \"link\": \"https://huggingface.co/mlx-community/Arch-Router-1.5B-mlx-8Bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 17\"}, {\"name\": \"vidavox/SKK-Router-1.5B\", \"link\": \"https://huggingface.co/vidavox/SKK-Router-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.21539",
    "first_seen_date": "2025-06-27",
    "title": "WorldVLA: Towards Autoregressive Action World Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.21539WorldVLA: Towards Autoregressive Action World ModelPublished on Jun 26\u00b7Submitted byHangjie Yuanon Jun 27Upvote40+32Authors:Jun Cen,Chaohui Yu,Hangjie Yuan,Yuming Jiang,Siteng Huang,Jiayan Guo,Xin Li,Yibing Song,Hao Luo,Fan Wang,Deli Zhao,Hao ChenAbstractWorldVLA, an autoregressive action world model integrating vision-language-action (VLA) and world models, enhances performance through mutual understanding and generation, improving action prediction and sequence generation with an attention mask strategy.AI-generated summaryWe present WorldVLA, anautoregressive action world modelthat unifies action\nand image understanding and generation. Our WorldVLA intergratesVision-Language-Action (VLA) modelandworld modelin one single framework. Theworld modelpredicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improveaction generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of theworld model. We\ndemonstrate that WorldVLA outperforms standalone action andworld models,\nhighlighting the mutual enhancement between theworld modeland the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability foraction prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose anattention mask strategythat selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.View arXiv pageView PDFProject pageGitHub781Add to collectionCommunityJacobYuanPaper author",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/alibaba-damo-academy/WorldVLA",
    "hf_paper_url": "https://huggingface.co/papers/2506.21539",
    "arxiv_url": "https://arxiv.org/abs/2506.21539",
    "num_models": 9,
    "models_list": "Alibaba-DAMO-Academy/WorldVLA, jcenaa/WorldVLA-ActionModel-LIBERO-Goal-256, jcenaa/WorldVLA-ActionModel-LIBERO-10-256, jcenaa/WorldVLA-ActionModel-LIBERO-Object-256, jcenaa/WorldVLA-ActionModel-LIBERO-Spatial-256, jcenaa/WorldVLA-ActionModel-LIBERO-10-512, jcenaa/WorldVLA-ActionModel-LIBERO-Goal-512, jcenaa/WorldVLA-ActionModel-LIBERO-Object-512, jcenaa/WorldVLA-ActionModel-LIBERO-Spatial-512",
    "models_links": "https://huggingface.co/Alibaba-DAMO-Academy/WorldVLA, https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Goal-256, https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-10-256, https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Object-256, https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Spatial-256, https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-10-512, https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Goal-512, https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Object-512, https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Spatial-512",
    "models_detailed": "[{\"name\": \"Alibaba-DAMO-Academy/WorldVLA\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/WorldVLA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 10\"}, {\"name\": \"jcenaa/WorldVLA-ActionModel-LIBERO-Goal-256\", \"link\": \"https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Goal-256\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"jcenaa/WorldVLA-ActionModel-LIBERO-10-256\", \"link\": \"https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-10-256\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"jcenaa/WorldVLA-ActionModel-LIBERO-Object-256\", \"link\": \"https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Object-256\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"jcenaa/WorldVLA-ActionModel-LIBERO-Spatial-256\", \"link\": \"https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Spatial-256\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"jcenaa/WorldVLA-ActionModel-LIBERO-10-512\", \"link\": \"https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-10-512\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"jcenaa/WorldVLA-ActionModel-LIBERO-Goal-512\", \"link\": \"https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Goal-512\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"jcenaa/WorldVLA-ActionModel-LIBERO-Object-512\", \"link\": \"https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Object-512\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"jcenaa/WorldVLA-ActionModel-LIBERO-Spatial-512\", \"link\": \"https://huggingface.co/jcenaa/WorldVLA-ActionModel-LIBERO-Spatial-512\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.20920",
    "first_seen_date": "2025-06-26",
    "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data\n  Processing to Every Language",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.20920FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data\n  Processing to Every LanguagePublished on Jun 26\u00b7Submitted byGuilherme Penedoon Jun 26#1 Paper of the day\u00b7FineDataUpvote75+67Authors:Guilherme Penedo,Hynek Kydl\u00ed\u010dek,Vinko Sabol\u010dec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas WolfAbstractA new pre-training dataset curation pipeline based on FineWeb supports multilingual LLMs by automatically adapting to any language, improving model performance and balancing dataset quality.AI-generated summaryPre-training state-of-the-artlarge language models(LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality Englishpre-training datasetshas seen substantial recent\nprogress, training performantmultilingual LLMsremains a challenge, in large\npart due to the inherent difficulty of tailoringfilteringanddeduplicationpipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based onFineWebthat can be\nautomatically adapted to support any language. We extensively ablate ourpipeline designchoices on a set of nine diverse languages, guided by a set of\nmeaningful and informativeevaluation tasksthat were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produceFineWeb2, a new 20 terabyte (5 billion document)multilingual datasetwhich we release along with our pipel",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/huggingface/fineweb-2",
    "hf_paper_url": "https://huggingface.co/papers/2506.20920",
    "arxiv_url": "https://arxiv.org/abs/2506.20920",
    "num_models": 11,
    "models_list": "yanolja/YanoljaNEXT-Rosetta-27B-2511, yanolja/YanoljaNEXT-Rosetta-12B-2510, yanolja/YanoljaNEXT-Rosetta-4B-2510, yanolja/YanoljaNEXT-Rosetta-4B-2511, Bedovyy/YanoljaNEXT-Rosetta-12B-2510-FP8-Dynamic, yanolja/YanoljaNEXT-Rosetta-4B-2510-GGUF, yanolja/YanoljaNEXT-Rosetta-12B-2510-GGUF, yanolja/YanoljaNEXT-Rosetta-4B-2511-FP8, yanolja/YanoljaNEXT-Rosetta-4B-2511-GGUF, yanolja/YanoljaNEXT-Rosetta-27B-2511-FP8, yanolja/YanoljaNEXT-Rosetta-27B-2511-GGUF",
    "models_links": "https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-27B-2511, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2510, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2511, https://huggingface.co/Bedovyy/YanoljaNEXT-Rosetta-12B-2510-FP8-Dynamic, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2510-GGUF, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510-GGUF, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2511-FP8, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2511-GGUF, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-27B-2511-FP8, https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-27B-2511-GGUF",
    "models_detailed": "[{\"name\": \"yanolja/YanoljaNEXT-Rosetta-27B-2511\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-27B-2511\", \"task\": \"\", \"likes\": \"112\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-12B-2510\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-4B-2510\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2510\", \"task\": \"\", \"likes\": \"252\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-4B-2511\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2511\", \"task\": \"\", \"likes\": \"393\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"Bedovyy/YanoljaNEXT-Rosetta-12B-2510-FP8-Dynamic\", \"link\": \"https://huggingface.co/Bedovyy/YanoljaNEXT-Rosetta-12B-2510-FP8-Dynamic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 10\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-4B-2510-GGUF\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2510-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-12B-2510-GGUF\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-12B-2510-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-4B-2511-FP8\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2511-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-4B-2511-GGUF\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-4B-2511-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-27B-2511-FP8\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-27B-2511-FP8\", \"task\": \"\", \"likes\": \"282\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"yanolja/YanoljaNEXT-Rosetta-27B-2511-GGUF\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-Rosetta-27B-2511-GGUF\", \"task\": \"\", \"likes\": \"753\", \"downloads\": \"\", \"updated\": \"Nov 2\"}]",
    "num_datasets": 3,
    "datasets_list": "HuggingFaceFW/fineweb-2, caoyuji/fineweb-2, ReactiveAI/fineweb-2-pol-latest",
    "datasets_links": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-2, https://huggingface.co/datasets/caoyuji/fineweb-2, https://huggingface.co/datasets/ReactiveAI/fineweb-2-pol-latest",
    "datasets_detailed": "[{\"name\": \"HuggingFaceFW/fineweb-2\", \"link\": \"https://huggingface.co/datasets/HuggingFaceFW/fineweb-2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 27\", \"size\": \"\"}, {\"name\": \"caoyuji/fineweb-2\", \"link\": \"https://huggingface.co/datasets/caoyuji/fineweb-2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\", \"size\": \"\"}, {\"name\": \"ReactiveAI/fineweb-2-pol-latest\", \"link\": \"https://huggingface.co/datasets/ReactiveAI/fineweb-2-pol-latest\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.18841",
    "first_seen_date": "2025-06-24",
    "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.18841LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  LearningPublished on Jun 23\u00b7Submitted bywuyuhaoon Jun 24#3 Paper of the dayUpvote56+48Authors:Yuhao Wu,Yushi Bai,Zhiqiang Hu,Roy Ka-Wei Lee,Juanzi LiAbstractAn incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.AI-generated summaryUltra-long generationbylarge language models(LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leveragesreinforcement learning(RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specializedreward modelsthat steer the LLM towards improvedlength control,writing quality, andstructural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics onWritingBenchandArena-Write, an",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2506.18841",
    "arxiv_url": "https://arxiv.org/abs/2506.18841",
    "num_models": 2,
    "models_list": "THU-KEG/LongWriter-Zero-32B, Mungert/LongWriter-Zero-32B-GGUF",
    "models_links": "https://huggingface.co/THU-KEG/LongWriter-Zero-32B, https://huggingface.co/Mungert/LongWriter-Zero-32B-GGUF",
    "models_detailed": "[{\"name\": \"THU-KEG/LongWriter-Zero-32B\", \"link\": \"https://huggingface.co/THU-KEG/LongWriter-Zero-32B\", \"task\": \"Text Generation\", \"likes\": \"61\", \"downloads\": \"\", \"updated\": \"Jul 3\"}, {\"name\": \"Mungert/LongWriter-Zero-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/LongWriter-Zero-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"289\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 2,
    "datasets_list": "THU-KEG/Arena-Write, THU-KEG/LongWriter-Zero-RLData",
    "datasets_links": "https://huggingface.co/datasets/THU-KEG/Arena-Write, https://huggingface.co/datasets/THU-KEG/LongWriter-Zero-RLData",
    "datasets_detailed": "[{\"name\": \"THU-KEG/Arena-Write\", \"link\": \"https://huggingface.co/datasets/THU-KEG/Arena-Write\", \"task\": \"\", \"likes\": \"595\", \"downloads\": \"\", \"updated\": \"Jun 30\", \"size\": \"\"}, {\"name\": \"THU-KEG/LongWriter-Zero-RLData\", \"link\": \"https://huggingface.co/datasets/THU-KEG/LongWriter-Zero-RLData\", \"task\": \"\", \"likes\": \"79\", \"downloads\": \"\", \"updated\": \"Jul 10\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.18896",
    "first_seen_date": "2025-06-24",
    "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.18896ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMsPublished on Jun 23\u00b7Submitted byLing Yangon Jun 24Upvote29+21Authors:Jiaru Zou,Ling Yang,Jingwen Gu,Jiahao Qiu,Ke Shen,Jingrui He,Mengdi WangAbstractReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.AI-generated summaryProcess Reward Models(PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting oftrajectory-response outputsgenerated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnoveltrajectory-aware PRMexplicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level andtrajectory-level supervision, enabling fine-grained reward\nassignment aligned with structuredchain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-qualitymodel distillationdata for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards forpolicy optimizationduringreinforcement learning,\nand (iii) enabling reward-guidedBest-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such asAIME,MATH500, andGPQA-Diamonddemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\nav",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/Gen-Verse/ReasonFlux",
    "hf_paper_url": "https://huggingface.co/papers/2506.18896",
    "arxiv_url": "https://arxiv.org/abs/2506.18896",
    "num_models": 3,
    "models_list": "Gen-Verse/ReasonFlux-PRM-7B, Gen-Verse/ReasonFlux-PRM-1.5B, Gen-Verse/ReasonFlux-PRM-Qwen-2.5-7B",
    "models_links": "https://huggingface.co/Gen-Verse/ReasonFlux-PRM-7B, https://huggingface.co/Gen-Verse/ReasonFlux-PRM-1.5B, https://huggingface.co/Gen-Verse/ReasonFlux-PRM-Qwen-2.5-7B",
    "models_detailed": "[{\"name\": \"Gen-Verse/ReasonFlux-PRM-7B\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-PRM-7B\", \"task\": \"Text Generation\", \"likes\": \"353\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"Gen-Verse/ReasonFlux-PRM-1.5B\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-PRM-1.5B\", \"task\": \"Text Generation\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"Gen-Verse/ReasonFlux-PRM-Qwen-2.5-7B\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-PRM-Qwen-2.5-7B\", \"task\": \"Text Generation\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"Jun 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.18898",
    "first_seen_date": "2025-06-24",
    "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.18898Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned RepresentationsPublished on Jun 23\u00b7Submitted byJiaming Hanon Jun 24Upvote33+25Authors:Jiaming Han,Hao Chen,Yang Zhao,Hanyu Wang,Qi Zhao,Ziyan Yang,Hao He,Xiangyu Yue,Lu JiangAbstractA multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.AI-generated summaryThis paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is theText-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, ourmultimodal LLM,Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we proposescale-adaptive encodingand decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fastautoregressive modeland adiffusion-based model. To enhancemodality fusion, we investigate advancedpre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show thatTarmatches or surpasses\nexistingmultimodal LLMmethods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.comView arXiv pageView PDFProject pageGitHub192Add to collectionCommunitycsuhanPaper authorPaper submitterJun 24Project Page:https://tar.csuhan.comSee tr",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/csuhan/Tar",
    "hf_paper_url": "https://huggingface.co/papers/2506.18898",
    "arxiv_url": "https://arxiv.org/abs/2506.18898",
    "num_models": 6,
    "models_list": "ByteDance-Seed/Tar-7B, ByteDance-Seed/Tar-1.5B, ByteDance-Seed/Tar-TA-Tok, csuhan/TA-Tok, csuhan/Tar-1.5B, csuhan/Tar-7B-v0.1",
    "models_links": "https://huggingface.co/ByteDance-Seed/Tar-7B, https://huggingface.co/ByteDance-Seed/Tar-1.5B, https://huggingface.co/ByteDance-Seed/Tar-TA-Tok, https://huggingface.co/csuhan/TA-Tok, https://huggingface.co/csuhan/Tar-1.5B, https://huggingface.co/csuhan/Tar-7B-v0.1",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/Tar-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/Tar-7B\", \"task\": \"\", \"likes\": \"90\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"ByteDance-Seed/Tar-1.5B\", \"link\": \"https://huggingface.co/ByteDance-Seed/Tar-1.5B\", \"task\": \"\", \"likes\": \"88\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"ByteDance-Seed/Tar-TA-Tok\", \"link\": \"https://huggingface.co/ByteDance-Seed/Tar-TA-Tok\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"csuhan/TA-Tok\", \"link\": \"https://huggingface.co/csuhan/TA-Tok\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"csuhan/Tar-1.5B\", \"link\": \"https://huggingface.co/csuhan/Tar-1.5B\", \"task\": \"\", \"likes\": \"714\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"csuhan/Tar-7B-v0.1\", \"link\": \"https://huggingface.co/csuhan/Tar-7B-v0.1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.09827",
    "first_seen_date": "2025-06-20",
    "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.09827EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion DetectionPublished on Jun 11\u00b7Submitted byFelix Friedrichon Jun 20#3 Paper of the dayUpvote20+12Authors:Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,S\u00f6ren AuerAbstractEmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.AI-generated summaryThe advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Currentspeech emotion recognition(SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introducesEmoNet-Voice, a new resource for speech\nemotion detection, which includesEmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), andEmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations.EmoNet-Voiceis designed to evaluateSERmodels on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation bypsychology expertswho\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduceEmpathic Insight Voice modelsthat set\na new standard inspeech emotion recognitionwith high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuab",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2506.09827",
    "arxiv_url": "https://arxiv.org/abs/2506.09827",
    "num_models": 10,
    "models_list": "nineninesix/kani-tts-400m-en, nineninesix/kani-tts-400m-0.3-pt, nineninesix/kani-tts-400m-ko, nineninesix/kani-tts-400m-ar, nineninesix/kani-tts-400m-ky, nineninesix/kani-tts-400m-es, nineninesix/kani-tts-400m-de, nineninesix/kani-tts-400m-zh, nineninesix/kani-tts-400m-ky-kani, Mungert/kani-tts-400m-en-GGUF",
    "models_links": "https://huggingface.co/nineninesix/kani-tts-400m-en, https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt, https://huggingface.co/nineninesix/kani-tts-400m-ko, https://huggingface.co/nineninesix/kani-tts-400m-ar, https://huggingface.co/nineninesix/kani-tts-400m-ky, https://huggingface.co/nineninesix/kani-tts-400m-es, https://huggingface.co/nineninesix/kani-tts-400m-de, https://huggingface.co/nineninesix/kani-tts-400m-zh, https://huggingface.co/nineninesix/kani-tts-400m-ky-kani, https://huggingface.co/Mungert/kani-tts-400m-en-GGUF",
    "models_detailed": "[{\"name\": \"nineninesix/kani-tts-400m-en\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-en\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-0.3-pt\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-ko\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-ko\", \"task\": \"Text-to-Speech\", \"likes\": \"48\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-ar\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-ar\", \"task\": \"Text-to-Speech\", \"likes\": \"225\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-ky\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-ky\", \"task\": \"Text-to-Speech\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-es\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-es\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-de\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-de\", \"task\": \"Text-to-Speech\", \"likes\": \"322\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-zh\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-zh\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-ky-kani\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-ky-kani\", \"task\": \"Text-to-Speech\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"Mungert/kani-tts-400m-en-GGUF\", \"link\": \"https://huggingface.co/Mungert/kani-tts-400m-en-GGUF\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 3\"}]",
    "num_datasets": 2,
    "datasets_list": "laion/Emolia, laion/Emilia-with-Emotion-Annotations",
    "datasets_links": "https://huggingface.co/datasets/laion/Emolia, https://huggingface.co/datasets/laion/Emilia-with-Emotion-Annotations",
    "datasets_detailed": "[{\"name\": \"laion/Emolia\", \"link\": \"https://huggingface.co/datasets/laion/Emolia\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"laion/Emilia-with-Emotion-Annotations\", \"link\": \"https://huggingface.co/datasets/laion/Emilia-with-Emotion-Annotations\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 20\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.14028",
    "first_seen_date": "2025-06-18",
    "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.14028MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM EvaluationPublished on Jun 16\u00b7Submitted byXueqing Pengon Jun 18#1 Paper of the dayUpvote93+85Authors:Xueqing Peng,Lingfei Qian,Yan Wang,Ruoyu Xiang,Yueru He,Yang Ren,Mingyang Jiang,Jeff Zhao,Huan He,Yi Han,Yun Feng,Yuechen Jiang,Yupeng Cao,Haohang Li,Yangyang Yu,Xiaoyu Wang,Penglei Gao,Shengyuan Lin,Keyi Wang,Shanshan Yang,Yilun Zhao,Zhiwei Liu+22 authorsAbstractMultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.AI-generated summaryRecent advances in large language models (LLMs) have accelerated progress infinancial NLPand applications, yet existingbenchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the firstmultilingualandmultimodalbenchmarktailored\nto the global financial domain, evaluatingLLMsacross modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual,multilingual)\nondomain-specific tasks. We introduce two novel tasks, includingPolyFiQA-EasyandPolyFiQA-Expert, the firstmultilingualfinancialbenchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; andEnglishOCRandSpanishOCR, the firstOCR-embeddedfinancial QAtasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic,difficulty-awareselection mechanism and curate\na compact, balancedbenchmarkrather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their generalmultimodalandmultilingualcapabilities, ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/xueqingpeng/MultiFinBen",
    "hf_paper_url": "https://huggingface.co/papers/2506.14028",
    "arxiv_url": "https://arxiv.org/abs/2506.14028",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 5,
    "datasets_list": "TheFinAI/MultiFinBen-EnglishOCR, TheFinAI/MultiFinBen-SpanishOCR, BhaskarAmrit/MultiFinBen-SpanishOCR_ss, TheFinAI/PolyFiQA-Easy, TheFinAI/PolyFiQA-Expert",
    "datasets_links": "https://huggingface.co/datasets/TheFinAI/MultiFinBen-EnglishOCR, https://huggingface.co/datasets/TheFinAI/MultiFinBen-SpanishOCR, https://huggingface.co/datasets/BhaskarAmrit/MultiFinBen-SpanishOCR_ss, https://huggingface.co/datasets/TheFinAI/PolyFiQA-Easy, https://huggingface.co/datasets/TheFinAI/PolyFiQA-Expert",
    "datasets_detailed": "[{\"name\": \"TheFinAI/MultiFinBen-EnglishOCR\", \"link\": \"https://huggingface.co/datasets/TheFinAI/MultiFinBen-EnglishOCR\", \"task\": \"\", \"likes\": \"326\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"TheFinAI/MultiFinBen-SpanishOCR\", \"link\": \"https://huggingface.co/datasets/TheFinAI/MultiFinBen-SpanishOCR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"BhaskarAmrit/MultiFinBen-SpanishOCR_ss\", \"link\": \"https://huggingface.co/datasets/BhaskarAmrit/MultiFinBen-SpanishOCR_ss\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\", \"size\": \"\"}, {\"name\": \"TheFinAI/PolyFiQA-Easy\", \"link\": \"https://huggingface.co/datasets/TheFinAI/PolyFiQA-Easy\", \"task\": \"\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"TheFinAI/PolyFiQA-Expert\", \"link\": \"https://huggingface.co/datasets/TheFinAI/PolyFiQA-Expert\", \"task\": \"\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.14606",
    "first_seen_date": "2025-06-18",
    "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.14606Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing GuaranteesPublished on Jun 17\u00b7Submitted byAhmed Heaklon Jun 18Upvote11+3Authors:Ahmed Heakl,Sarim Hashmi,Chaimaa Abi,Celine Lee,Abdulrahman MahmoudAbstractA novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.AI-generated summaryThehardware ecosystemis rapidly evolving, with increasing interest in\ntranslatinglow-level programsacross different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability andlongevityof existing code. A particularly challenging class of thistranspilationproblem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity,memory models, andexecution paradigms. In this work, we introduce\nGG (Guaranteed Guess), anISA-centric transpilationpipeline that combines the\ntranslation power ofpre-trained large language models(LLMs) with the rigor of\nestablishedsoftware testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% onHumanEvalprograms and 49% onBringupBenchprograms,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We wil",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/ahmedheakl/Guaranteed-Guess",
    "hf_paper_url": "https://huggingface.co/papers/2506.14606",
    "arxiv_url": "https://arxiv.org/abs/2506.14606",
    "num_models": 6,
    "models_list": "ahmedheakl/gg-armv5-O0, ahmedheakl/ex13_qwen2.5_0.5b_1M_stack_armv5_O0, ahmedheakl/gg-armv8-O0, ahmedheakl/gg-armv5-O2, ahmedheakl/gg-armv8-O2, ahmedheakl/gg-risc-O0",
    "models_links": "https://huggingface.co/ahmedheakl/gg-armv5-O0, https://huggingface.co/ahmedheakl/ex13_qwen2.5_0.5b_1M_stack_armv5_O0, https://huggingface.co/ahmedheakl/gg-armv8-O0, https://huggingface.co/ahmedheakl/gg-armv5-O2, https://huggingface.co/ahmedheakl/gg-armv8-O2, https://huggingface.co/ahmedheakl/gg-risc-O0",
    "models_detailed": "[{\"name\": \"ahmedheakl/gg-armv5-O0\", \"link\": \"https://huggingface.co/ahmedheakl/gg-armv5-O0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"ahmedheakl/ex13_qwen2.5_0.5b_1M_stack_armv5_O0\", \"link\": \"https://huggingface.co/ahmedheakl/ex13_qwen2.5_0.5b_1M_stack_armv5_O0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"ahmedheakl/gg-armv8-O0\", \"link\": \"https://huggingface.co/ahmedheakl/gg-armv8-O0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"ahmedheakl/gg-armv5-O2\", \"link\": \"https://huggingface.co/ahmedheakl/gg-armv5-O2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"ahmedheakl/gg-armv8-O2\", \"link\": \"https://huggingface.co/ahmedheakl/gg-armv8-O2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"ahmedheakl/gg-risc-O0\", \"link\": \"https://huggingface.co/ahmedheakl/gg-risc-O0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}]",
    "num_datasets": 13,
    "datasets_list": "ahmedheakl/gg-armv8-O2-instruct, ahmedheakl/gg-armv8-O0-instruct, ahmedheakl/gg-armv5-O0-instruct, ahmedheakl/gg-armv5-O2-instruct, ahmedheakl/gg-bench-armv8-O2, ahmedheakl/gg-bench-armv8-O0, ahmedheakl/gg-bench-bringup-O0, ahmedheakl/gg-bench-bringup-O2, ahmedheakl/gg-bench-risc-O0, ahmedheakl/gg-bench-risc-O2, ahmedheakl/gg-bench-armv5-O0, ahmedheakl/gg-bench-armv5-O2, ahmedheakl/gg-risc-O0-instruct",
    "datasets_links": "https://huggingface.co/datasets/ahmedheakl/gg-armv8-O2-instruct, https://huggingface.co/datasets/ahmedheakl/gg-armv8-O0-instruct, https://huggingface.co/datasets/ahmedheakl/gg-armv5-O0-instruct, https://huggingface.co/datasets/ahmedheakl/gg-armv5-O2-instruct, https://huggingface.co/datasets/ahmedheakl/gg-bench-armv8-O2, https://huggingface.co/datasets/ahmedheakl/gg-bench-armv8-O0, https://huggingface.co/datasets/ahmedheakl/gg-bench-bringup-O0, https://huggingface.co/datasets/ahmedheakl/gg-bench-bringup-O2, https://huggingface.co/datasets/ahmedheakl/gg-bench-risc-O0, https://huggingface.co/datasets/ahmedheakl/gg-bench-risc-O2, https://huggingface.co/datasets/ahmedheakl/gg-bench-armv5-O0, https://huggingface.co/datasets/ahmedheakl/gg-bench-armv5-O2, https://huggingface.co/datasets/ahmedheakl/gg-risc-O0-instruct",
    "datasets_detailed": "[{\"name\": \"ahmedheakl/gg-armv8-O2-instruct\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-armv8-O2-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-armv8-O0-instruct\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-armv8-O0-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-armv5-O0-instruct\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-armv5-O0-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-armv5-O2-instruct\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-armv5-O2-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-bench-armv8-O2\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-bench-armv8-O2\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-bench-armv8-O0\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-bench-armv8-O0\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-bench-bringup-O0\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-bench-bringup-O0\", \"task\": \"\", \"likes\": \"66\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-bench-bringup-O2\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-bench-bringup-O2\", \"task\": \"\", \"likes\": \"66\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-bench-risc-O0\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-bench-risc-O0\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-bench-risc-O2\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-bench-risc-O2\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-bench-armv5-O0\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-bench-armv5-O0\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-bench-armv5-O2\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-bench-armv5-O2\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"ahmedheakl/gg-risc-O0-instruct\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/gg-risc-O0-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.13284",
    "first_seen_date": "2025-06-17",
    "title": "AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.13284AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL SynergyPublished on Jun 16\u00b7Submitted byZihan Liuon Jun 17Upvote26+18Authors:Zihan Liu,Zhuolin Yang,Yang Chen,Chankyu Lee,Mohammad Shoeybi,Bryan Catanzaro,Wei PingAbstractCombining supervised fine-tuning and reinforcement learning enhances reasoning models, especially when optimizing sampling temperature and leveraging strong initial fine-tuning, as demonstrated by the improved AceReason-Nemotron-1.1 model.AI-generated summaryIn this work, we investigate the synergy betweensupervised fine-tuning(SFT)\nandreinforcement learning(RL) in developing strongreasoning models. We begin\nby curating the SFT training data through two scaling strategies: increasing\nthe number of collected prompts and the number of generated responses per\nprompt. Both approaches yield notable improvements in reasoning performance,\nwith scaling the number of prompts resulting in more substantial gains. We then\nexplore the following questions regarding the synergy between SFT and RL: (i)\nDoes a stronger SFT model consistently lead to better final performance after\nlarge-scale RL training? (ii) How can we determine an appropriate sampling\ntemperature during RL training to effectively balance exploration and\nexploitation for a given SFT initialization? Our findings suggest that (i)\nholds true, provided effective RL training is conducted, particularly when thesampling temperatureis carefully chosen to maintain the temperature-adjustedentropyaround 0.3, a setting that strikes a good balance between exploration\nand exploitation. Notably, the performance gap between initial SFT models\nnarrows significantly throughout the RL process. Leveraging a strong SFT\nfoundation and insights into the synergistic interplay between SFT and RL, our\nAceReason-Nemotron-1.1 7B model significantly outperforms\nAceReason-Nemotron-1.0 and achieves new sta",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2506.13284",
    "arxiv_url": "https://arxiv.org/abs/2506.13284",
    "num_models": 16,
    "models_list": "nvidia/AceReason-Nemotron-14B, nvidia/AceReason-Nemotron-1.1-7B, nvidia/AceReason-Nemotron-7B, gabriellarson/AceReason-Nemotron-1.1-7B-GGUF, lmstudio-community/AceReason-Nemotron-1.1-7B-GGUF, QuantFactory/AceReason-Nemotron-1.1-7B-GGUF, Mungert/AceReason-Nemotron-1.1-7B-GGUF, Prince-1/AceReason-Nemotron-1.1-7B-Onnx, Prince-1/AceReason-Nemotron-14B-Onnx, RivianG/AceReason-Nemotron-1.1-7B_quant, RivianG/AceReason-Nemotron-1.1-7B-bnb-4bit, onnx-community/AceReason-Nemotron-1.1-7B-Onnx, onnx-community/AceReason-Nemotron-14B-Onnx, duyntnet/AceReason-Nemotron-1.1-7B-imatrix-GGUF, Mungert/AceReason-Nemotron-7B-GGUF, Kwai-Klear/Klear-Qwen3-Thinking-Preview",
    "models_links": "https://huggingface.co/nvidia/AceReason-Nemotron-14B, https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B, https://huggingface.co/nvidia/AceReason-Nemotron-7B, https://huggingface.co/gabriellarson/AceReason-Nemotron-1.1-7B-GGUF, https://huggingface.co/lmstudio-community/AceReason-Nemotron-1.1-7B-GGUF, https://huggingface.co/QuantFactory/AceReason-Nemotron-1.1-7B-GGUF, https://huggingface.co/Mungert/AceReason-Nemotron-1.1-7B-GGUF, https://huggingface.co/Prince-1/AceReason-Nemotron-1.1-7B-Onnx, https://huggingface.co/Prince-1/AceReason-Nemotron-14B-Onnx, https://huggingface.co/RivianG/AceReason-Nemotron-1.1-7B_quant, https://huggingface.co/RivianG/AceReason-Nemotron-1.1-7B-bnb-4bit, https://huggingface.co/onnx-community/AceReason-Nemotron-1.1-7B-Onnx, https://huggingface.co/onnx-community/AceReason-Nemotron-14B-Onnx, https://huggingface.co/duyntnet/AceReason-Nemotron-1.1-7B-imatrix-GGUF, https://huggingface.co/Mungert/AceReason-Nemotron-7B-GGUF, https://huggingface.co/Kwai-Klear/Klear-Qwen3-Thinking-Preview",
    "models_detailed": "[{\"name\": \"nvidia/AceReason-Nemotron-14B\", \"link\": \"https://huggingface.co/nvidia/AceReason-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"nvidia/AceReason-Nemotron-1.1-7B\", \"link\": \"https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"nvidia/AceReason-Nemotron-7B\", \"link\": \"https://huggingface.co/nvidia/AceReason-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"gabriellarson/AceReason-Nemotron-1.1-7B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/AceReason-Nemotron-1.1-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"191\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"lmstudio-community/AceReason-Nemotron-1.1-7B-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/AceReason-Nemotron-1.1-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"62\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"QuantFactory/AceReason-Nemotron-1.1-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/AceReason-Nemotron-1.1-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"90\", \"downloads\": \"\", \"updated\": \"Jun 21\"}, {\"name\": \"Mungert/AceReason-Nemotron-1.1-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/AceReason-Nemotron-1.1-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Prince-1/AceReason-Nemotron-1.1-7B-Onnx\", \"link\": \"https://huggingface.co/Prince-1/AceReason-Nemotron-1.1-7B-Onnx\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"Prince-1/AceReason-Nemotron-14B-Onnx\", \"link\": \"https://huggingface.co/Prince-1/AceReason-Nemotron-14B-Onnx\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 23\"}, {\"name\": \"RivianG/AceReason-Nemotron-1.1-7B_quant\", \"link\": \"https://huggingface.co/RivianG/AceReason-Nemotron-1.1-7B_quant\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"RivianG/AceReason-Nemotron-1.1-7B-bnb-4bit\", \"link\": \"https://huggingface.co/RivianG/AceReason-Nemotron-1.1-7B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"onnx-community/AceReason-Nemotron-1.1-7B-Onnx\", \"link\": \"https://huggingface.co/onnx-community/AceReason-Nemotron-1.1-7B-Onnx\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"onnx-community/AceReason-Nemotron-14B-Onnx\", \"link\": \"https://huggingface.co/onnx-community/AceReason-Nemotron-14B-Onnx\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"duyntnet/AceReason-Nemotron-1.1-7B-imatrix-GGUF\", \"link\": \"https://huggingface.co/duyntnet/AceReason-Nemotron-1.1-7B-imatrix-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"Mungert/AceReason-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/AceReason-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"216\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Kwai-Klear/Klear-Qwen3-Thinking-Preview\", \"link\": \"https://huggingface.co/Kwai-Klear/Klear-Qwen3-Thinking-Preview\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Jul 26\"}]",
    "num_datasets": 2,
    "datasets_list": "nvidia/AceReason-1.1-SFT, nvidia/AceReason-Math",
    "datasets_links": "https://huggingface.co/datasets/nvidia/AceReason-1.1-SFT, https://huggingface.co/datasets/nvidia/AceReason-Math",
    "datasets_detailed": "[{\"name\": \"nvidia/AceReason-1.1-SFT\", \"link\": \"https://huggingface.co/datasets/nvidia/AceReason-1.1-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"nvidia/AceReason-Math\", \"link\": \"https://huggingface.co/datasets/nvidia/AceReason-Math\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.09038",
    "first_seen_date": "2025-06-16",
    "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.09038AbstentionBench: Reasoning LLMs Fail on Unanswerable QuestionsPublished on Jun 10\u00b7Submitted byMark Ibrahimon Jun 16Upvote6Authors:Polina Kirichenko,Mark Ibrahim,Kamalika Chaudhuri,Samuel J. BellAbstractAbstentionBench evaluates the ability of LLMs to abstain from answering uncertain or unanswerable questions, revealing that reasoning fine-tuning often degrades this capability.AI-generated summaryForLarge Language Models(LLMs) to be reliably deployed in both everyday and\nhigh-stakes domains, knowing when not to answer is equally critical as\nanswering correctly. Real-world user queries, which can be underspecified,\nill-posed, or fundamentally unanswerable, requireLLMsto reason about\nuncertainty and selectively abstain -- i.e., refuse to answer definitively.\nHowever,abstentionremains understudied, without a systematic evaluation\nframework for modernLLMs. In this work, we introduceAbstentionBench, a\nlarge-scale benchmark for holistically evaluatingabstentionacross 20 diverse\ndatasets, including questions with unknown answers,underspecification, false\npremises,subjective interpretations, andoutdated information. Evaluating 20frontier LLMsrevealsabstentionis an unsolved problem, and one where scaling\nmodels is of little use. While recent reasoningLLMshave shown impressive\nresults in complex problem solving, surprisingly, we find that reasoning\nfine-tuning degradesabstention(by 24% on average), even for math and\nscience domains on which reasoning models are explicitly trained. We find that\nwhile a carefully crafted system prompt can boostabstentionin practice, it\ndoes not resolve models' fundamental inability to reason about uncertainty. We\nreleaseAbstentionBenchto foster research into advancing LLM reliability.View arXiv pageView PDFGitHub67autoAdd to collectionCommunitymarksibrahimPaper submitterJun 16A good language model should say \u201cI don\u2019t know\u201d by reasoning about the limi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/facebookresearch/abstentionbench",
    "hf_paper_url": "https://huggingface.co/papers/2506.09038",
    "arxiv_url": "https://arxiv.org/abs/2506.09038",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "facebook/AbstentionBench",
    "datasets_links": "https://huggingface.co/datasets/facebook/AbstentionBench",
    "datasets_detailed": "[{\"name\": \"facebook/AbstentionBench\", \"link\": \"https://huggingface.co/datasets/facebook/AbstentionBench\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.09513",
    "first_seen_date": "2025-06-13",
    "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.09513ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  ReasoningPublished on Jun 11\u00b7Submitted byYu Sunon Jun 13#1 Paper of the dayUpvote101+93Authors:Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang XuAbstractReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.AI-generated summaryThoughreasoning-based large language models(LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduceReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvariousLLMs.ReasonMedis constructed through a multi-agent\nverification and refinement process, where we design anError Refinerto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. LeveragingReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailedChain-of-Thought(CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we trainReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B onPubMedQAby 4.60\\%.View arXiv pageView PDFGitHub104Add to collectionCommunityYuSun-AIPaper authorPaper submitterJun 13Welcome to follow our latest achievement\u2014ReasonMed! We are dedicated to tackling knowledge-intensive reasoning challenges in the medical domain. To this end, we have built the largest open-source medical reasoning dataset in the industry and developed state-of-the-a",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/YuSun-Work/ReasonMed",
    "hf_paper_url": "https://huggingface.co/papers/2506.09513",
    "arxiv_url": "https://arxiv.org/abs/2506.09513",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "lingshu-medical-mllm/ReasonMed, EtoUbivaetMnya1997/ReasonMed",
    "datasets_links": "https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed, https://huggingface.co/datasets/EtoUbivaetMnya1997/ReasonMed",
    "datasets_detailed": "[{\"name\": \"lingshu-medical-mllm/ReasonMed\", \"link\": \"https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed\", \"task\": \"\", \"likes\": \"954\", \"downloads\": \"\", \"updated\": \"Jun 24\", \"size\": \"\"}, {\"name\": \"EtoUbivaetMnya1997/ReasonMed\", \"link\": \"https://huggingface.co/datasets/EtoUbivaetMnya1997/ReasonMed\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.10857",
    "first_seen_date": "2025-06-13",
    "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.10857VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative VideosPublished on Jun 12\u00b7Submitted byJiashuo Yuon Jun 13Upvote30+22Authors:Jiashuo Yu,Yue Wu,Meng Chu,Zhifei Ren,Zizheng Huang,Pei Chu,Ruijie Zhang,Yinan He,Qirui Li,Songze Li,Zhenxiang Li,Zhongying Tu,Conghui He,Yu Qiao,Yali Wang,Yi Wang,Limin WangAbstractVRBench is a long narrative video benchmark designed to evaluate models' multi-step reasoning and procedural validity through human-labeled question-answering pairs and a human-AI collaborative framework with a multi-phase evaluation pipeline.AI-generated summaryWe presentVRBench, the first long narrative video benchmark crafted for\nevaluating large models'multi-step reasoningcapabilities, addressing\nlimitations in existing evaluations that overlooktemporal reasoningandprocedural validity. It comprises 1,010long videos(with an average duration\nof 1.6 hours), along with 9,468human-labeledmulti-step question-answeringpairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process includingexpert inter-rater reviewingto\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngeneratescoherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g.,event attribution, implicit\ninference).VRBenchdesigns amulti-phase evaluationpipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose aprogress-level LLM-guided scoring metricto\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12LLMsand 16VLMsonVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field ofmulti-step reasoning.View arXiv pageView PDFProject pageGitHub23Add to collectionCommunityawojustinPaper authorPaper submitterJun 13A Be",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/OpenGVLab/VRBench",
    "hf_paper_url": "https://huggingface.co/papers/2506.10857",
    "arxiv_url": "https://arxiv.org/abs/2506.10857",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "OpenGVLab/VRBench",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/VRBench",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/VRBench\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/VRBench\", \"task\": \"\", \"likes\": \"720\", \"downloads\": \"\", \"updated\": \"Sep 8\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.10910",
    "first_seen_date": "2025-06-13",
    "title": "Magistral",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.10910MagistralPublished on Jun 12\u00b7Submitted byStefan Schweteron Jun 13#2 Paper of the dayUpvote66+58Authors:Mistral-AI,Abhinav Rastogi,Albert Q. Jiang,Andy Lo,Gabrielle Berrada,Guillaume Lample,Jason Rute,Joep Barmentlo,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,L\u00e9onard Blier,Lucile Saulnier,Matthieu Dinot,Maxime Darrin,Neha Gupta,Roman Soletskyi,Sagar Vaze,Teven Le Scao,Yihan Wang,Adam Yang,Alexander H. Liu+78 authorsAbstractMagistral, a scalable reinforcement learning pipeline, demonstrates that RL can enhance multimodal understanding and instruction following in large language models without requiring existing RL traces.AI-generated summaryWe introduce Magistral, Mistral's first reasoning model and our own scalablereinforcement learning(RL) pipeline. Instead of relying on existing\nimplementations andRLtraces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pureRLtraining\nofLLMs, present a simple method to force the reasoning language of the model,\nand show thatRLon text data alone maintains most of the initial checkpoint's\ncapabilities. We find thatRLon text maintains or improves multimodal\nunderstanding,instruction followingandfunction calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 withRLalone, and we\nopen-source Magistral Small (Apache 2.0) which further includescold-start datafrom Magistral Medium.View arXiv pageView PDFAdd to collectionCommunitystefan-itPaper submitterJun 13Mistral's first reasoning model \ud83e\udd73See translation\u2764\ufe0f1010+Replylibrarian-botJun 14This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIPhi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Langu",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2506.10910",
    "arxiv_url": "https://arxiv.org/abs/2506.10910",
    "num_models": 31,
    "models_list": "mistralai/Magistral-Small-2506, mistralai/Magistral-Small-2509, mistralai/Magistral-Small-2507, unsloth/Magistral-Small-2509-GGUF, csalab/Magistral-24B, gabriellarson/Magistral-Small-2507-GGUF, unsloth/Magistral-Small-2507-GGUF, unsloth/Magistral-Small-2507, cpatonn/Magistral-Small-2507-AWQ-4bit, m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA, unsloth/Magistral-Small-2507-unsloth-bnb-4bit, unsloth/Magistral-Small-2507-bnb-4bit, DavidAU/Mistral-Magistral-Devstral-Instruct-FUSED-CODER-Reasoning-36B, m-polignano/ANITA-NEXT-24B-Magistral-2506-ITA, m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA, m-polignano/ANITA-NEXT-24B-Magistral-2506-ITA-GGUF, m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA-GGUF, DavidAU/Mistral-2x24B-MOE-Magistral-2506-Devstral-2507-1.1-Coder-Reasoning-Ultimate-44B, unsloth/Magistral-Small-2509, unsloth/Magistral-Small-2509-bnb-4bit, unsloth/Magistral-Small-2509-unsloth-bnb-4bit, unsloth/Magistral-Small-2509-FP8-Dynamic, unsloth/Magistral-Small-2509-FP8-torchao, cpatonn/Magistral-Small-2509-AWQ-4bit, cpatonn/Magistral-Small-2509-AWQ-8bit, Mungert/Magistral-Small-2509-GGUF, GaleneAI/Magistral-Small-2509-FP8-Dynamic, m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA-GGUF, ExaltedSlayer/unsloth-Magistral-Small-2509-mlx-mxfp4, EnlistedGhost/Magistral-Small-2509-Vision, EnlistedGhost/Magistral-Small-2509-Vision-GGUF",
    "models_links": "https://huggingface.co/mistralai/Magistral-Small-2506, https://huggingface.co/mistralai/Magistral-Small-2509, https://huggingface.co/mistralai/Magistral-Small-2507, https://huggingface.co/unsloth/Magistral-Small-2509-GGUF, https://huggingface.co/csalab/Magistral-24B, https://huggingface.co/gabriellarson/Magistral-Small-2507-GGUF, https://huggingface.co/unsloth/Magistral-Small-2507-GGUF, https://huggingface.co/unsloth/Magistral-Small-2507, https://huggingface.co/cpatonn/Magistral-Small-2507-AWQ-4bit, https://huggingface.co/m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA, https://huggingface.co/unsloth/Magistral-Small-2507-unsloth-bnb-4bit, https://huggingface.co/unsloth/Magistral-Small-2507-bnb-4bit, https://huggingface.co/DavidAU/Mistral-Magistral-Devstral-Instruct-FUSED-CODER-Reasoning-36B, https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-ITA, https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA, https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-ITA-GGUF, https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA-GGUF, https://huggingface.co/DavidAU/Mistral-2x24B-MOE-Magistral-2506-Devstral-2507-1.1-Coder-Reasoning-Ultimate-44B, https://huggingface.co/unsloth/Magistral-Small-2509, https://huggingface.co/unsloth/Magistral-Small-2509-bnb-4bit, https://huggingface.co/unsloth/Magistral-Small-2509-unsloth-bnb-4bit, https://huggingface.co/unsloth/Magistral-Small-2509-FP8-Dynamic, https://huggingface.co/unsloth/Magistral-Small-2509-FP8-torchao, https://huggingface.co/cpatonn/Magistral-Small-2509-AWQ-4bit, https://huggingface.co/cpatonn/Magistral-Small-2509-AWQ-8bit, https://huggingface.co/Mungert/Magistral-Small-2509-GGUF, https://huggingface.co/GaleneAI/Magistral-Small-2509-FP8-Dynamic, https://huggingface.co/m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA-GGUF, https://huggingface.co/ExaltedSlayer/unsloth-Magistral-Small-2509-mlx-mxfp4, https://huggingface.co/EnlistedGhost/Magistral-Small-2509-Vision, https://huggingface.co/EnlistedGhost/Magistral-Small-2509-Vision-GGUF",
    "models_detailed": "[{\"name\": \"mistralai/Magistral-Small-2506\", \"link\": \"https://huggingface.co/mistralai/Magistral-Small-2506\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"mistralai/Magistral-Small-2509\", \"link\": \"https://huggingface.co/mistralai/Magistral-Small-2509\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"mistralai/Magistral-Small-2507\", \"link\": \"https://huggingface.co/mistralai/Magistral-Small-2507\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"unsloth/Magistral-Small-2509-GGUF\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2509-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"csalab/Magistral-24B\", \"link\": \"https://huggingface.co/csalab/Magistral-24B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 27\"}, {\"name\": \"gabriellarson/Magistral-Small-2507-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/Magistral-Small-2507-GGUF\", \"task\": \"Text Generation\", \"likes\": \"166\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"unsloth/Magistral-Small-2507-GGUF\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2507-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"unsloth/Magistral-Small-2507\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2507\", \"task\": \"\", \"likes\": \"62\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"cpatonn/Magistral-Small-2507-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Magistral-Small-2507-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 8\"}, {\"name\": \"m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA\", \"link\": \"https://huggingface.co/m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA\", \"task\": \"Text Generation\", \"likes\": \"172\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"unsloth/Magistral-Small-2507-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2507-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"954\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"unsloth/Magistral-Small-2507-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2507-bnb-4bit\", \"task\": \"\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"DavidAU/Mistral-Magistral-Devstral-Instruct-FUSED-CODER-Reasoning-36B\", \"link\": \"https://huggingface.co/DavidAU/Mistral-Magistral-Devstral-Instruct-FUSED-CODER-Reasoning-36B\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"m-polignano/ANITA-NEXT-24B-Magistral-2506-ITA\", \"link\": \"https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-ITA\", \"task\": \"Text Generation\", \"likes\": \"586\", \"downloads\": \"\", \"updated\": \"Aug 16\"}, {\"name\": \"m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA\", \"link\": \"https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA\", \"task\": \"Text Generation\", \"likes\": \"643\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"m-polignano/ANITA-NEXT-24B-Magistral-2506-ITA-GGUF\", \"link\": \"https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-ITA-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA-GGUF\", \"link\": \"https://huggingface.co/m-polignano/ANITA-NEXT-24B-Magistral-2506-VISION-ITA-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"DavidAU/Mistral-2x24B-MOE-Magistral-2506-Devstral-2507-1.1-Coder-Reasoning-Ultimate-44B\", \"link\": \"https://huggingface.co/DavidAU/Mistral-2x24B-MOE-Magistral-2506-Devstral-2507-1.1-Coder-Reasoning-Ultimate-44B\", \"task\": \"Text Generation\", \"likes\": \"30\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"unsloth/Magistral-Small-2509\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2509\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 29\"}, {\"name\": \"unsloth/Magistral-Small-2509-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2509-bnb-4bit\", \"task\": \"\", \"likes\": \"208\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"unsloth/Magistral-Small-2509-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2509-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"989\", \"downloads\": \"\", \"updated\": \"Sep 29\"}, {\"name\": \"unsloth/Magistral-Small-2509-FP8-Dynamic\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2509-FP8-Dynamic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 29\"}, {\"name\": \"unsloth/Magistral-Small-2509-FP8-torchao\", \"link\": \"https://huggingface.co/unsloth/Magistral-Small-2509-FP8-torchao\", \"task\": \"\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"cpatonn/Magistral-Small-2509-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Magistral-Small-2509-AWQ-4bit\", \"task\": \"\", \"likes\": \"627\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"cpatonn/Magistral-Small-2509-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Magistral-Small-2509-AWQ-8bit\", \"task\": \"\", \"likes\": \"497\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"Mungert/Magistral-Small-2509-GGUF\", \"link\": \"https://huggingface.co/Mungert/Magistral-Small-2509-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"GaleneAI/Magistral-Small-2509-FP8-Dynamic\", \"link\": \"https://huggingface.co/GaleneAI/Magistral-Small-2509-FP8-Dynamic\", \"task\": \"\", \"likes\": \"29\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA-GGUF\", \"link\": \"https://huggingface.co/m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA-GGUF\", \"task\": \"Text Generation\", \"likes\": \"316\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"ExaltedSlayer/unsloth-Magistral-Small-2509-mlx-mxfp4\", \"link\": \"https://huggingface.co/ExaltedSlayer/unsloth-Magistral-Small-2509-mlx-mxfp4\", \"task\": \"Text Generation\", \"likes\": \"80\", \"downloads\": \"\", \"updated\": \"Oct 26\"}, {\"name\": \"EnlistedGhost/Magistral-Small-2509-Vision\", \"link\": \"https://huggingface.co/EnlistedGhost/Magistral-Small-2509-Vision\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"EnlistedGhost/Magistral-Small-2509-Vision-GGUF\", \"link\": \"https://huggingface.co/EnlistedGhost/Magistral-Small-2509-Vision-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "ChuGyouk/DeepMath-Filtered-59.9K",
    "datasets_links": "https://huggingface.co/datasets/ChuGyouk/DeepMath-Filtered-59.9K",
    "datasets_detailed": "[{\"name\": \"ChuGyouk/DeepMath-Filtered-59.9K\", \"link\": \"https://huggingface.co/datasets/ChuGyouk/DeepMath-Filtered-59.9K\", \"task\": \"\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Sep 18\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.09980",
    "first_seen_date": "2025-06-12",
    "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.09980Efficient Part-level 3D Object Generation via Dual Volume PackingPublished on Jun 11\u00b7Submitted bykiuion Jun 12Upvote7Authors:Jiaxiang Tang,Ruijie Lu,Zhaoshuo Li,Zekun Hao,Xuan Li,Fangyin Wei,Shuran Song,Gang Zeng,Ming-Yu Liu,Tsung-Yi LinAbstractA new end-to-end framework generates high-quality 3D objects with part-level detail from a single image using a dual volume packing strategy.AI-generated summaryRecent progress in3D object generationhas greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce adual volume packing strategythat organizes\nall parts into two complementary volumes, allowing for the creation of complete\nandinterleaved partsthat assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-basedpart-level generationmethods.View arXiv pageView PDFProject pageGitHub788Add to collectionCommunityashawkeyPaper submitterJun 13Project Page:https://research.nvidia.com/labs/dir/partpacker/Demo:https://huggingface.co/spaces/nvidia/PartPackerCode:https://github.com/NVlabs/PartPackerSee translationReplylibrarian-botJun 13This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIPartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers(2025)Direct Numerical Layout Generation for 3D Indoor ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/NVlabs/PartPacker",
    "hf_paper_url": "https://huggingface.co/papers/2506.09980",
    "arxiv_url": "https://arxiv.org/abs/2506.09980",
    "num_models": 1,
    "models_list": "nvidia/PartPacker",
    "models_links": "https://huggingface.co/nvidia/PartPacker",
    "models_detailed": "[{\"name\": \"nvidia/PartPacker\", \"link\": \"https://huggingface.co/nvidia/PartPacker\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 16\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.07927",
    "first_seen_date": "2025-06-11",
    "title": "Solving Inequality Proofs with Large Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.07927Solving Inequality Proofs with Large Language ModelsPublished on Jun 9\u00b7Submitted byPan Luon Jun 11\u00b7Stanford AIUpvote20+12Authors:Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan LuAbstractThe investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.AI-generated summaryInequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks:bound estimationandrelation prediction.\nBuilding on this, we releaseIneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leadingLLMsonIneqMathreveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for currentLLMsbetween merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in ove",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/lupantech/ineqmath",
    "hf_paper_url": "https://huggingface.co/papers/2506.07927",
    "arxiv_url": "https://arxiv.org/abs/2506.07927",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "AI4Math/IneqMath, m-a-p/FineLeanCorpus",
    "datasets_links": "https://huggingface.co/datasets/AI4Math/IneqMath, https://huggingface.co/datasets/m-a-p/FineLeanCorpus",
    "datasets_detailed": "[{\"name\": \"AI4Math/IneqMath\", \"link\": \"https://huggingface.co/datasets/AI4Math/IneqMath\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"m-a-p/FineLeanCorpus\", \"link\": \"https://huggingface.co/datasets/m-a-p/FineLeanCorpus\", \"task\": \"\", \"likes\": \"481\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.07044",
    "first_seen_date": "2025-06-10",
    "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.07044Lingshu: A Generalist Foundation Model for Unified Multimodal Medical\n  Understanding and ReasoningPublished on Jun 8\u00b7Submitted byHou Pong (Ken) Chanon Jun 10Upvote114+106Authors:LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu RongAbstractA medical-specialized multimodal large language model, Lingshu, is introduced with enhanced data curation and reinforcement learning to address limitations in medical applications.AI-generated summaryMultimodal Large Language Models(MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medicalMLLMsface the following critical\nlimitations: (1) limited coverage ofmedical knowledgebeyond imaging, (2)\nheightened susceptibility tohallucinationsdue to suboptimaldata curationprocesses, (3) lack ofreasoning capabilitiestailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires richmedical knowledgedata\nnot only from medical imaging but also from extensivemedical textsandgeneral-domain data; and (2) synthesizesaccurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensivemedical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoesmulti-stage trainingto embedmedical expertiseand enhance its\ntask-solving capabilities progressively. Besides, we ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2506.07044",
    "arxiv_url": "https://arxiv.org/abs/2506.07044",
    "num_models": 7,
    "models_list": "lingshu-medical-mllm/Lingshu-32B, lingshu-medical-mllm/Lingshu-7B, Mungert/Lingshu-32B-GGUF, erjui/Lingshu-7b-csrrg-findings, erjui/Lingshu-7b-srrg-findings, erjui/Lingshu-7b-srrg-impression, erjui/Lingshu-7b-csrrg-impression",
    "models_links": "https://huggingface.co/lingshu-medical-mllm/Lingshu-32B, https://huggingface.co/lingshu-medical-mllm/Lingshu-7B, https://huggingface.co/Mungert/Lingshu-32B-GGUF, https://huggingface.co/erjui/Lingshu-7b-csrrg-findings, https://huggingface.co/erjui/Lingshu-7b-srrg-findings, https://huggingface.co/erjui/Lingshu-7b-srrg-impression, https://huggingface.co/erjui/Lingshu-7b-csrrg-impression",
    "models_detailed": "[{\"name\": \"lingshu-medical-mllm/Lingshu-32B\", \"link\": \"https://huggingface.co/lingshu-medical-mllm/Lingshu-32B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"lingshu-medical-mllm/Lingshu-7B\", \"link\": \"https://huggingface.co/lingshu-medical-mllm/Lingshu-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Mungert/Lingshu-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Lingshu-32B-GGUF\", \"task\": \"\", \"likes\": \"668\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"erjui/Lingshu-7b-csrrg-findings\", \"link\": \"https://huggingface.co/erjui/Lingshu-7b-csrrg-findings\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"erjui/Lingshu-7b-srrg-findings\", \"link\": \"https://huggingface.co/erjui/Lingshu-7b-srrg-findings\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"erjui/Lingshu-7b-srrg-impression\", \"link\": \"https://huggingface.co/erjui/Lingshu-7b-srrg-impression\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"erjui/Lingshu-7b-csrrg-impression\", \"link\": \"https://huggingface.co/erjui/Lingshu-7b-csrrg-impression\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}]",
    "num_datasets": 2,
    "datasets_list": "lingshu-medical-mllm/ReasonMed, EtoUbivaetMnya1997/ReasonMed",
    "datasets_links": "https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed, https://huggingface.co/datasets/EtoUbivaetMnya1997/ReasonMed",
    "datasets_detailed": "[{\"name\": \"lingshu-medical-mllm/ReasonMed\", \"link\": \"https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed\", \"task\": \"\", \"likes\": \"954\", \"downloads\": \"\", \"updated\": \"Jun 24\", \"size\": \"\"}, {\"name\": \"EtoUbivaetMnya1997/ReasonMed\", \"link\": \"https://huggingface.co/datasets/EtoUbivaetMnya1997/ReasonMed\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.07900",
    "first_seen_date": "2025-06-10",
    "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.07900MiniCPM4: Ultra-Efficient LLMs on End DevicesPublished on Jun 9\u00b7Submitted byChaojun XIAOon Jun 10\u00b7OpenBMBUpvote93+85Authors:MiniCPM Team,Chaojun Xiao,Yuxuan Li,Xu Han,Yuzhuo Bai,Jie Cai,Haotian Chen,Wentong Chen,Xin Cong,Ganqu Cui,Ning Ding,Shengdan Fan,Yewei Fang,Zixuan Fu,Wenyu Guan,Yitong Guan,Junshao Guo,Yufeng Han,Bingxiang He,Yuxiang Huang,Cunliang Kong,Qiuzuo Li+53 authorsAbstractMiniCPM4, a highly efficient large language model for end-side devices, achieves superior performance using innovations in sparse attention, pre-training datasets, training algorithms, and inference systems.AI-generated summaryThis paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we proposeInfLLM v2, a trainable sparse attention\nmechanism that accelerates bothprefillinganddecodingphases for long-context\nprocessing. Regarding training data, we proposeUltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, andUltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we proposeModelTunnel v2for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducingchunk-wise rolloutfor load-balanced reinforcement learning anddata-efficient tenary LLM,BitCPM. Regarding inference systems, we proposeCPM.cuthat integrates sparse attention,model quantization, and speculative\nsampling to achieve efficientprefillinganddecoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/openbmb/minicpm",
    "hf_paper_url": "https://huggingface.co/papers/2506.07900",
    "arxiv_url": "https://arxiv.org/abs/2506.07900",
    "num_models": 14,
    "models_list": "openbmb/MiniCPM4.1-8B, openbmb/MiniCPM4-8B, openbmb/MiniCPM4.1-8B-GGUF, openbmb/MiniCPM4-Survey, JunHowie/MiniCPM4-8B, QuantFactory/MiniCPM4-8B-GGUF, openbmb/MiniCPM4.1-8B-MLX, openbmb/MiniCPM4.1-8B-GPTQ, openbmb/MiniCPM4.1-8B-AutoAWQ, openbmb/MiniCPM4.1-8B-Eagle3, openbmb/MiniCPM4.1-8B-Marlin, Mungert/MiniCPM4.1-8B-GGUF, QuantFactory/MiniCPM4.1-8B-GGUF, eshiryae/MiniCPM4-8B",
    "models_links": "https://huggingface.co/openbmb/MiniCPM4.1-8B, https://huggingface.co/openbmb/MiniCPM4-8B, https://huggingface.co/openbmb/MiniCPM4.1-8B-GGUF, https://huggingface.co/openbmb/MiniCPM4-Survey, https://huggingface.co/JunHowie/MiniCPM4-8B, https://huggingface.co/QuantFactory/MiniCPM4-8B-GGUF, https://huggingface.co/openbmb/MiniCPM4.1-8B-MLX, https://huggingface.co/openbmb/MiniCPM4.1-8B-GPTQ, https://huggingface.co/openbmb/MiniCPM4.1-8B-AutoAWQ, https://huggingface.co/openbmb/MiniCPM4.1-8B-Eagle3, https://huggingface.co/openbmb/MiniCPM4.1-8B-Marlin, https://huggingface.co/Mungert/MiniCPM4.1-8B-GGUF, https://huggingface.co/QuantFactory/MiniCPM4.1-8B-GGUF, https://huggingface.co/eshiryae/MiniCPM4-8B",
    "models_detailed": "[{\"name\": \"openbmb/MiniCPM4.1-8B\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4.1-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"openbmb/MiniCPM4-8B\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"openbmb/MiniCPM4.1-8B-GGUF\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4.1-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"241\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"openbmb/MiniCPM4-Survey\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4-Survey\", \"task\": \"Text Generation\", \"likes\": \"78\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"JunHowie/MiniCPM4-8B\", \"link\": \"https://huggingface.co/JunHowie/MiniCPM4-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 13\"}, {\"name\": \"QuantFactory/MiniCPM4-8B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/MiniCPM4-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"openbmb/MiniCPM4.1-8B-MLX\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4.1-8B-MLX\", \"task\": \"Text Generation\", \"likes\": \"61\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"openbmb/MiniCPM4.1-8B-GPTQ\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4.1-8B-GPTQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"openbmb/MiniCPM4.1-8B-AutoAWQ\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4.1-8B-AutoAWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"openbmb/MiniCPM4.1-8B-Eagle3\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4.1-8B-Eagle3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"openbmb/MiniCPM4.1-8B-Marlin\", \"link\": \"https://huggingface.co/openbmb/MiniCPM4.1-8B-Marlin\", \"task\": \"Text Generation\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"Mungert/MiniCPM4.1-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/MiniCPM4.1-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"QuantFactory/MiniCPM4.1-8B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/MiniCPM4.1-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Sep 14\"}, {\"name\": \"eshiryae/MiniCPM4-8B\", \"link\": \"https://huggingface.co/eshiryae/MiniCPM4-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 17\"}]",
    "num_datasets": 1,
    "datasets_list": "openbmb/Ultra-FineWeb",
    "datasets_links": "https://huggingface.co/datasets/openbmb/Ultra-FineWeb",
    "datasets_detailed": "[{\"name\": \"openbmb/Ultra-FineWeb\", \"link\": \"https://huggingface.co/datasets/openbmb/Ultra-FineWeb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.05817",
    "first_seen_date": "2025-06-09",
    "title": "CodeContests+: High-Quality Test Case Generation for Competitive\n  Programming",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.05817CodeContests+: High-Quality Test Case Generation for Competitive\n  ProgrammingPublished on Jun 6\u00b7Submitted byZihan Wangon Jun 9Upvote9+1Authors:Zihan Wang,Siyao Liu,Yang Sun,Hongyan Li,Kai ShenAbstractAn LLM-based system generates high-quality test cases for competitive programming problems, enhancing the accuracy of model evaluation and RL performance.AI-generated summaryCompetitive programming, due to its high reasoning difficulty and precise\ncorrectness feedback, has become a key task for both training and evaluating\nthe reasoning capabilities of large language models (LLMs). However, while a\nlarge amount of public problem data, such as problem statements and solutions,\nis available, the test cases of these problems are often difficult to obtain.\nTherefore,test case generationis a necessary task for building large-scale\ndatasets, and the quality of the test cases directly determines the accuracy of\nthe evaluation. In this paper, we introduce anLLM-based agent systemthat\ncreates high-quality test cases for competitive programming problems. We apply\nthis system to theCodeContestsdataset and propose a new version with improved\ntest cases, namedCodeContests+. We evaluated the quality of test cases inCodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels\nto examine the accuracy of these test cases in evaluation. The results\nindicated thatCodeContests+achieves significantly higher accuracy thanCodeContests, particularly with a notably higherTrue Positive Rate (TPR).\nSubsequently, our experiments inLLM Reinforcement Learning (RL)further\nconfirmed that improvements in test case quality yield considerable advantages\nfor RL.View arXiv pageView PDFProject pageAdd to collectionCommunityzhwang01Paper authorPaper submitterJun 9IntroductionCodeContests+is a competitive programming problem dataset built uponCodeContests. It includes 11,690 competitive program",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2506.05817",
    "arxiv_url": "https://arxiv.org/abs/2506.05817",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "ByteDance-Seed/Code-Contests-Plus, HexQuant/Code-Contests-Plus",
    "datasets_links": "https://huggingface.co/datasets/ByteDance-Seed/Code-Contests-Plus, https://huggingface.co/datasets/HexQuant/Code-Contests-Plus",
    "datasets_detailed": "[{\"name\": \"ByteDance-Seed/Code-Contests-Plus\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/Code-Contests-Plus\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 6\", \"size\": \"\"}, {\"name\": \"HexQuant/Code-Contests-Plus\", \"link\": \"https://huggingface.co/datasets/HexQuant/Code-Contests-Plus\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 1\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.02865",
    "first_seen_date": "2025-06-06",
    "title": "Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.02865Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open WeightsPublished on Jun 3\u00b7Submitted byHamza Benchekrounon Jun 6Upvote33+25Authors:Mathieu Andreux,Breno Baldas Skuk,Hamza Benchekroun,Emilien Bir\u00e9,Antoine Bonnet,Riaz Bordie,Matthias Brunel,Pierre-Louis Cedoz,Antoine Chassang,Micka\u00ebl Chen,Alexandra D. Constantinou,Antoine d'Andign\u00e9,Hubert de La Jonqui\u00e8re,Aur\u00e9lien Delfosse,Ludovic Denoyer,Alexis Deprez,Augustin Derupti,Michael Eickenberg,Math\u00efs Federico,Charles Kantor,Xavier Koegler,Yann Labb\u00e9+21 authorsAbstractSurfer-H, paired with Holo1, an open-weight collection of Vision-Language Models, achieves top performance in web navigation tasks with high cost-efficiency.AI-generated summaryWe present Surfer-H, a cost-efficient web agent that integratesVision-Language Models(VLM) to perform user-defined tasks on the web. We pair\nit with Holo1, a new open-weight collection ofVLMs specialized in web\nnavigation andinformation extraction. Holo1 was trained on carefully curated\ndata sources, including open-access web content, synthetic examples, and\nself-produced agentic data. Holo1 topsgeneralist User Interface(UI)\nbenchmarks as well as our new webUIlocalization benchmark,WebClick. When\npowered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance onWebVoyager, striking a Pareto-optimal balance between accuracy and\ncost-efficiency. To accelerate research advancement in agentic systems, we areopen-sourcingboth ourWebClickevaluation dataset and the Holo1model weights.View arXiv pageView PDFProject pageAdd to collectionCommunityhamza-hcompanyPaper authorPaper submitterJun 6We present Surfer-H, a cost-efficient web agent that integrates Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair it with Holo1, a new open-weight collection of VLMs specialized in web navigation and information extraction. Holo1 was trained on carefully curated da",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2506.02865",
    "arxiv_url": "https://arxiv.org/abs/2506.02865",
    "num_models": 4,
    "models_list": "Hcompany/Holo1-7B, Hcompany/Holo1-3B, Mungert/Holo1-3B-GGUF, Mungert/Holo1-7B-GGUF",
    "models_links": "https://huggingface.co/Hcompany/Holo1-7B, https://huggingface.co/Hcompany/Holo1-3B, https://huggingface.co/Mungert/Holo1-3B-GGUF, https://huggingface.co/Mungert/Holo1-7B-GGUF",
    "models_detailed": "[{\"name\": \"Hcompany/Holo1-7B\", \"link\": \"https://huggingface.co/Hcompany/Holo1-7B\", \"task\": \"\", \"likes\": \"403\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"Hcompany/Holo1-3B\", \"link\": \"https://huggingface.co/Hcompany/Holo1-3B\", \"task\": \"\", \"likes\": \"416\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"Mungert/Holo1-3B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Holo1-3B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/Holo1-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Holo1-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 1,
    "datasets_list": "Hcompany/WebClick",
    "datasets_links": "https://huggingface.co/datasets/Hcompany/WebClick",
    "datasets_detailed": "[{\"name\": \"Hcompany/WebClick\", \"link\": \"https://huggingface.co/datasets/Hcompany/WebClick\", \"task\": \"\", \"likes\": \"969\", \"downloads\": \"\", \"updated\": \"Jun 9\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.04308",
    "first_seen_date": "2025-06-06",
    "title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.04308RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for RoboticsPublished on Jun 4\u00b7Submitted byZhoueson Jun 6\u00b7Beijing Academy of Artificial IntelligenceUpvote43+35Authors:Enshen Zhou,Jingkun An,Cheng Chi,Yi Han,Shanyu Rong,Chi Zhang,Pengwei Wang,Zhongyuan Wang,Tiejun Huang,Lu Sheng,Shanghang ZhangAbstractRoboRefer, a 3D-aware vision language model, enhances spatial understanding and multi-step reasoning in embodied robots through supervised and reinforcement fine-tuning, using the RefSpatial dataset and RefSpatial-Bench benchmark.AI-generated summarySpatial referring is a fundamental capability of embodied robots to interact\nwith the 3D physical world. However, even with the powerful pretrained vision\nlanguage models (VLMs), recent approaches are still not qualified to accurately\nunderstand the complex 3D scenes and dynamically reason about the\ninstruction-indicated locations for interaction. To this end, we propose\nRoboRefer, a3D-aware VLMthat can first achieve precise spatial understanding\nby integrating a disentangled but dedicated depth encoder via supervised\nfine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial\nreasoning viareinforcement fine-tuning (RFT), with metric-sensitive process\nreward functions tailored forspatial referring tasks. To support SFT and RFT\ntraining, we introduceRefSpatial, a large-scale dataset of 20M QA pairs (2x\nprior), covering 31 spatial relations (vs. 15 prior) and supporting complex\nreasoning processes (up to 5 steps). In addition, we introduceRefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial\nreferring withmulti-step reasoning. Experiments show that SFT-trained\nRoboRefer achievesstate-of-the-art spatial understanding, with an average\nsuccess rate of 89.6%. RFT-trained RoboRefer further outperforms all other\nbaselines by a large margin, even surpassing Gemin",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/Zhoues/RoboRefer",
    "hf_paper_url": "https://huggingface.co/papers/2506.04308",
    "arxiv_url": "https://arxiv.org/abs/2506.04308",
    "num_models": 12,
    "models_list": "BAAI/RoboBrain2.0-7B, BAAI/RoboBrain2.0-32B, BAAI/RoboBrain-X0-Preview, Zhoues/RoboRefer-2B-SFT, Mungert/RoboBrain2.0-7B-GGUF, Zhoues/RoboRefer-2B-Depth-Align, Zhoues/NVILA-2B-Depth, Zhoues/NVILA-8B-Depth, BAAI/RoboBrain2.0-7B-FP8, BAAI/RoboBrain2.0-7B-W8A16, BAAI/RoboBrain2.0-3B, Zhoues/RoboRefer-8B-SFT",
    "models_links": "https://huggingface.co/BAAI/RoboBrain2.0-7B, https://huggingface.co/BAAI/RoboBrain2.0-32B, https://huggingface.co/BAAI/RoboBrain-X0-Preview, https://huggingface.co/Zhoues/RoboRefer-2B-SFT, https://huggingface.co/Mungert/RoboBrain2.0-7B-GGUF, https://huggingface.co/Zhoues/RoboRefer-2B-Depth-Align, https://huggingface.co/Zhoues/NVILA-2B-Depth, https://huggingface.co/Zhoues/NVILA-8B-Depth, https://huggingface.co/BAAI/RoboBrain2.0-7B-FP8, https://huggingface.co/BAAI/RoboBrain2.0-7B-W8A16, https://huggingface.co/BAAI/RoboBrain2.0-3B, https://huggingface.co/Zhoues/RoboRefer-8B-SFT",
    "models_detailed": "[{\"name\": \"BAAI/RoboBrain2.0-7B\", \"link\": \"https://huggingface.co/BAAI/RoboBrain2.0-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"BAAI/RoboBrain2.0-32B\", \"link\": \"https://huggingface.co/BAAI/RoboBrain2.0-32B\", \"task\": \"\", \"likes\": \"53\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"BAAI/RoboBrain-X0-Preview\", \"link\": \"https://huggingface.co/BAAI/RoboBrain-X0-Preview\", \"task\": \"\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"Zhoues/RoboRefer-2B-SFT\", \"link\": \"https://huggingface.co/Zhoues/RoboRefer-2B-SFT\", \"task\": \"\", \"likes\": \"62\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"Mungert/RoboBrain2.0-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/RoboBrain2.0-7B-GGUF\", \"task\": \"\", \"likes\": \"234\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Zhoues/RoboRefer-2B-Depth-Align\", \"link\": \"https://huggingface.co/Zhoues/RoboRefer-2B-Depth-Align\", \"task\": \"\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"Zhoues/NVILA-2B-Depth\", \"link\": \"https://huggingface.co/Zhoues/NVILA-2B-Depth\", \"task\": \"\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"Zhoues/NVILA-8B-Depth\", \"link\": \"https://huggingface.co/Zhoues/NVILA-8B-Depth\", \"task\": \"\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"BAAI/RoboBrain2.0-7B-FP8\", \"link\": \"https://huggingface.co/BAAI/RoboBrain2.0-7B-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"BAAI/RoboBrain2.0-7B-W8A16\", \"link\": \"https://huggingface.co/BAAI/RoboBrain2.0-7B-W8A16\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"BAAI/RoboBrain2.0-3B\", \"link\": \"https://huggingface.co/BAAI/RoboBrain2.0-3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"Zhoues/RoboRefer-8B-SFT\", \"link\": \"https://huggingface.co/Zhoues/RoboRefer-8B-SFT\", \"task\": \"\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Jul 29\"}]",
    "num_datasets": 3,
    "datasets_list": "JingkunAn/RefSpatial, BAAI/RefSpatial-Bench, JingkunAn/RefSpatial-Expand-Bench",
    "datasets_links": "https://huggingface.co/datasets/JingkunAn/RefSpatial, https://huggingface.co/datasets/BAAI/RefSpatial-Bench, https://huggingface.co/datasets/JingkunAn/RefSpatial-Expand-Bench",
    "datasets_detailed": "[{\"name\": \"JingkunAn/RefSpatial\", \"link\": \"https://huggingface.co/datasets/JingkunAn/RefSpatial\", \"task\": \"\", \"likes\": \"800\", \"downloads\": \"\", \"updated\": \"Jul 20\", \"size\": \"\"}, {\"name\": \"BAAI/RefSpatial-Bench\", \"link\": \"https://huggingface.co/datasets/BAAI/RefSpatial-Bench\", \"task\": \"\", \"likes\": \"277\", \"downloads\": \"\", \"updated\": \"Oct 23\", \"size\": \"\"}, {\"name\": \"JingkunAn/RefSpatial-Expand-Bench\", \"link\": \"https://huggingface.co/datasets/JingkunAn/RefSpatial-Expand-Bench\", \"task\": \"\", \"likes\": \"441\", \"downloads\": \"\", \"updated\": \"Oct 28\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.05176",
    "first_seen_date": "2025-06-06",
    "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.05176Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation ModelsPublished on Jun 5\u00b7Submitted byDingkun Longon Jun 6#2 Paper of the dayUpvote76+68Authors:Yanzhao Zhang,Mingxin Li,Dingkun Long,Xin Zhang,Huan Lin,Baosong Yang,Pengjun Xie,An Yang,Dayiheng Liu,Junyang Lin,Fei Huang,Jingren ZhouAbstractThe Qwen3 Embedding series, built on Qwen3 foundation models, offers advanced text embedding and reranking capabilities through a multi-stage training pipeline, achieving state-of-the-art performance across multilingual and retrieval benchmarks.AI-generated summaryIn this work, we introduce theQwen3 Embedding series, a significant\nadvancement over its predecessor, theGTE-Qwen series, in textembeddingandrerankingcapabilities, built upon the Qwen3 foundation models. Leveraging theQwen3 LLMs' robust capabilities inmultilingual text understandingand\ngeneration, our innovative multi-stage training pipeline combines large-scaleunsupervised pre-trainingwithsupervised fine-tuningon high-quality datasets.\nEffectivemodel mergingstrategies further ensure the robustness and\nadaptability of theQwen3 Embedding series. During the training process, theQwen3 LLMsserve not only as backbone models but also play a crucial role in\nsynthesizing high-quality, rich, and diverse training data across multiple\ndomains and languages, thus enhancing the training pipeline. The Qwen3Embeddingseries offers a spectrum of model sizes (0.6B, 4B, 8B) for bothembeddingandrerankingtasks, addressing diverse deployment scenarios where\nusers can optimize for either efficiency or effectiveness. Empirical\nevaluations demonstrate that theQwen3 Embedding seriesachieves\nstate-of-the-art results across diverse benchmarks. Notably, it excels on the\nmultilingual evaluation benchmarkMTEBfor textembedding, as well as in\nvarious retrieval tasks, includingcode retrieval,cross-lingual retrievalandmultilingual ret",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/QwenLM/Qwen3-Embedding",
    "hf_paper_url": "https://huggingface.co/papers/2506.05176",
    "arxiv_url": "https://arxiv.org/abs/2506.05176",
    "num_models": 58,
    "models_list": "Qwen/Qwen3-Embedding-0.6B, Qwen/Qwen3-Embedding-8B, Qwen/Qwen3-Embedding-0.6B-GGUF, Qwen/Qwen3-Reranker-0.6B, Qwen/Qwen3-Embedding-4B, Qwen/Qwen3-Reranker-8B, Qwen/Qwen3-Embedding-4B-GGUF, Qwen/Qwen3-Reranker-4B, Qwen/Qwen3-Embedding-8B-GGUF, woodx/Qwen3-Embedding-0.6B-SGLang, zhlo/Qwen3-Embedding-0.6B-deploy, dulimov/Qwen3-Reranker-0.6B-rk3588-1.2.1, dulimov/Qwen3-Embedding-0.6B-rk3588-1.2.1, Mungert/Qwen3-Embedding-0.6B-GGUF, Mungert/Qwen3-Embedding-8B-GGUF, ushakov15/MNLP_M3_document_encoder, QuantFactory/Qwen3-Reranker-0.6B-GGUF, QuantFactory/Qwen3-Reranker-8B-GGUF, QuantFactory/Qwen3-Reranker-4B-GGUF, tomaarsen/Qwen3-Reranker-4B-seq-cls, tomaarsen/Qwen3-Reranker-8B-seq-cls, dengcao/Qwen3-Reranker-0.6B, dengcao/Qwen3-Reranker-4B, dengcao/Qwen3-Reranker-8B, dengcao/Qwen3-Embedding-0.6B-GGUF, yourleige/test_model_upload, DefaultDF/Qwen3-Embedding-0.6B-GGUF, MrDragonFox/Qwen3, dengcao/Qwen3-Reranker-4B-seq-cls, dengcao/Qwen3-Embedding-8B, dengcao/Qwen3-Embedding-4B, dengcao/Qwen3-Embedding-0.6B, Muhammed164/embedding, Tnt3o5/Qwen3-Embedding-0.6B-base, LeoMandtler/Qwen3-Embedding-8B-bnb-4bit, Tnt3o5/Qwen3-Reranker-0.6B-legal, heyanzhuo/Qwen3-Embedding-0.6B-Base-Mod, FukayaTakashi/Qwen3-Reranker-8B, alexisriot/insight-qwen3-reranker, alexisriot/qwen3-06b, talentguide/qwen06-base, anyidea/Qwen3-Embedding-8B, AXERA-TECH/Qwen3-Embedding-0.6B-GPTQ-Int8, socrate-tech/Qwen3-Embedding-0.6B-GGUF, zumainternal/Qwen3-Reranker-0.6B, leoikaichen/MyOwn_Qwen3-Embedding-0.8B, tonywng/Qwen3-Embedding-0.6B, Echo9Zulu/Qwen3-Embedding-0.6B-int8_asym-ov, Casual-Autopsy/Qwen3-Embedding-4B-GGUFs, ManiKumarAdapala/Qwen3-Reranker-0.6B-Q8_0-Safetensors, techAInewb/Qwen3-Embedding-0.6B-INT8, ZanderM98/Qwen3-Embedding-8B-bnb-4bit, akahana/qwen3-4b-text-embedding-4bit, homesome/qwen3-reranker-4b-homesome, Casual-Autopsy/Qwen3-Embedding-0.6B-GGUFs, DocDBrown/Qwen3-rerank-8B-onnx, ManiKumarAdapala/Qwen3-Embedding-0.6B-Q8_0-Safetensors, bflhc/MoD-Embedding",
    "models_links": "https://huggingface.co/Qwen/Qwen3-Embedding-0.6B, https://huggingface.co/Qwen/Qwen3-Embedding-8B, https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF, https://huggingface.co/Qwen/Qwen3-Reranker-0.6B, https://huggingface.co/Qwen/Qwen3-Embedding-4B, https://huggingface.co/Qwen/Qwen3-Reranker-8B, https://huggingface.co/Qwen/Qwen3-Embedding-4B-GGUF, https://huggingface.co/Qwen/Qwen3-Reranker-4B, https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF, https://huggingface.co/woodx/Qwen3-Embedding-0.6B-SGLang, https://huggingface.co/zhlo/Qwen3-Embedding-0.6B-deploy, https://huggingface.co/dulimov/Qwen3-Reranker-0.6B-rk3588-1.2.1, https://huggingface.co/dulimov/Qwen3-Embedding-0.6B-rk3588-1.2.1, https://huggingface.co/Mungert/Qwen3-Embedding-0.6B-GGUF, https://huggingface.co/Mungert/Qwen3-Embedding-8B-GGUF, https://huggingface.co/ushakov15/MNLP_M3_document_encoder, https://huggingface.co/QuantFactory/Qwen3-Reranker-0.6B-GGUF, https://huggingface.co/QuantFactory/Qwen3-Reranker-8B-GGUF, https://huggingface.co/QuantFactory/Qwen3-Reranker-4B-GGUF, https://huggingface.co/tomaarsen/Qwen3-Reranker-4B-seq-cls, https://huggingface.co/tomaarsen/Qwen3-Reranker-8B-seq-cls, https://huggingface.co/dengcao/Qwen3-Reranker-0.6B, https://huggingface.co/dengcao/Qwen3-Reranker-4B, https://huggingface.co/dengcao/Qwen3-Reranker-8B, https://huggingface.co/dengcao/Qwen3-Embedding-0.6B-GGUF, https://huggingface.co/yourleige/test_model_upload, https://huggingface.co/DefaultDF/Qwen3-Embedding-0.6B-GGUF, https://huggingface.co/MrDragonFox/Qwen3, https://huggingface.co/dengcao/Qwen3-Reranker-4B-seq-cls, https://huggingface.co/dengcao/Qwen3-Embedding-8B, https://huggingface.co/dengcao/Qwen3-Embedding-4B, https://huggingface.co/dengcao/Qwen3-Embedding-0.6B, https://huggingface.co/Muhammed164/embedding, https://huggingface.co/Tnt3o5/Qwen3-Embedding-0.6B-base, https://huggingface.co/LeoMandtler/Qwen3-Embedding-8B-bnb-4bit, https://huggingface.co/Tnt3o5/Qwen3-Reranker-0.6B-legal, https://huggingface.co/heyanzhuo/Qwen3-Embedding-0.6B-Base-Mod, https://huggingface.co/FukayaTakashi/Qwen3-Reranker-8B, https://huggingface.co/alexisriot/insight-qwen3-reranker, https://huggingface.co/alexisriot/qwen3-06b, https://huggingface.co/talentguide/qwen06-base, https://huggingface.co/anyidea/Qwen3-Embedding-8B, https://huggingface.co/AXERA-TECH/Qwen3-Embedding-0.6B-GPTQ-Int8, https://huggingface.co/socrate-tech/Qwen3-Embedding-0.6B-GGUF, https://huggingface.co/zumainternal/Qwen3-Reranker-0.6B, https://huggingface.co/leoikaichen/MyOwn_Qwen3-Embedding-0.8B, https://huggingface.co/tonywng/Qwen3-Embedding-0.6B, https://huggingface.co/Echo9Zulu/Qwen3-Embedding-0.6B-int8_asym-ov, https://huggingface.co/Casual-Autopsy/Qwen3-Embedding-4B-GGUFs, https://huggingface.co/ManiKumarAdapala/Qwen3-Reranker-0.6B-Q8_0-Safetensors, https://huggingface.co/techAInewb/Qwen3-Embedding-0.6B-INT8, https://huggingface.co/ZanderM98/Qwen3-Embedding-8B-bnb-4bit, https://huggingface.co/akahana/qwen3-4b-text-embedding-4bit, https://huggingface.co/homesome/qwen3-reranker-4b-homesome, https://huggingface.co/Casual-Autopsy/Qwen3-Embedding-0.6B-GGUFs, https://huggingface.co/DocDBrown/Qwen3-rerank-8B-onnx, https://huggingface.co/ManiKumarAdapala/Qwen3-Embedding-0.6B-Q8_0-Safetensors, https://huggingface.co/bflhc/MoD-Embedding",
    "models_detailed": "[{\"name\": \"Qwen/Qwen3-Embedding-0.6B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Qwen/Qwen3-Embedding-8B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Embedding-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-Embedding-0.6B-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"Qwen/Qwen3-Reranker-0.6B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Reranker-0.6B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"Qwen/Qwen3-Embedding-4B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Embedding-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Qwen/Qwen3-Reranker-8B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Reranker-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"Qwen/Qwen3-Embedding-4B-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Embedding-4B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"Qwen/Qwen3-Reranker-4B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Reranker-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"Qwen/Qwen3-Embedding-8B-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"woodx/Qwen3-Embedding-0.6B-SGLang\", \"link\": \"https://huggingface.co/woodx/Qwen3-Embedding-0.6B-SGLang\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 7\"}, {\"name\": \"zhlo/Qwen3-Embedding-0.6B-deploy\", \"link\": \"https://huggingface.co/zhlo/Qwen3-Embedding-0.6B-deploy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 8\"}, {\"name\": \"dulimov/Qwen3-Reranker-0.6B-rk3588-1.2.1\", \"link\": \"https://huggingface.co/dulimov/Qwen3-Reranker-0.6B-rk3588-1.2.1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"dulimov/Qwen3-Embedding-0.6B-rk3588-1.2.1\", \"link\": \"https://huggingface.co/dulimov/Qwen3-Embedding-0.6B-rk3588-1.2.1\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"Mungert/Qwen3-Embedding-0.6B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-Embedding-0.6B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/Qwen3-Embedding-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-Embedding-8B-GGUF\", \"task\": \"\", \"likes\": \"477\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"ushakov15/MNLP_M3_document_encoder\", \"link\": \"https://huggingface.co/ushakov15/MNLP_M3_document_encoder\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"QuantFactory/Qwen3-Reranker-0.6B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/Qwen3-Reranker-0.6B-GGUF\", \"task\": \"\", \"likes\": \"249\", \"downloads\": \"\", \"updated\": \"Jun 11\"}, {\"name\": \"QuantFactory/Qwen3-Reranker-8B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/Qwen3-Reranker-8B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 11\"}, {\"name\": \"QuantFactory/Qwen3-Reranker-4B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/Qwen3-Reranker-4B-GGUF\", \"task\": \"\", \"likes\": \"270\", \"downloads\": \"\", \"updated\": \"Jun 11\"}, {\"name\": \"tomaarsen/Qwen3-Reranker-4B-seq-cls\", \"link\": \"https://huggingface.co/tomaarsen/Qwen3-Reranker-4B-seq-cls\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 16\"}, {\"name\": \"tomaarsen/Qwen3-Reranker-8B-seq-cls\", \"link\": \"https://huggingface.co/tomaarsen/Qwen3-Reranker-8B-seq-cls\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 16\"}, {\"name\": \"dengcao/Qwen3-Reranker-0.6B\", \"link\": \"https://huggingface.co/dengcao/Qwen3-Reranker-0.6B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"dengcao/Qwen3-Reranker-4B\", \"link\": \"https://huggingface.co/dengcao/Qwen3-Reranker-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"dengcao/Qwen3-Reranker-8B\", \"link\": \"https://huggingface.co/dengcao/Qwen3-Reranker-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"dengcao/Qwen3-Embedding-0.6B-GGUF\", \"link\": \"https://huggingface.co/dengcao/Qwen3-Embedding-0.6B-GGUF\", \"task\": \"\", \"likes\": \"43\", \"downloads\": \"\", \"updated\": \"Jun 21\"}, {\"name\": \"yourleige/test_model_upload\", \"link\": \"https://huggingface.co/yourleige/test_model_upload\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"DefaultDF/Qwen3-Embedding-0.6B-GGUF\", \"link\": \"https://huggingface.co/DefaultDF/Qwen3-Embedding-0.6B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"MrDragonFox/Qwen3\", \"link\": \"https://huggingface.co/MrDragonFox/Qwen3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 27\"}, {\"name\": \"dengcao/Qwen3-Reranker-4B-seq-cls\", \"link\": \"https://huggingface.co/dengcao/Qwen3-Reranker-4B-seq-cls\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"dengcao/Qwen3-Embedding-8B\", \"link\": \"https://huggingface.co/dengcao/Qwen3-Embedding-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"dengcao/Qwen3-Embedding-4B\", \"link\": \"https://huggingface.co/dengcao/Qwen3-Embedding-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"dengcao/Qwen3-Embedding-0.6B\", \"link\": \"https://huggingface.co/dengcao/Qwen3-Embedding-0.6B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"Muhammed164/embedding\", \"link\": \"https://huggingface.co/Muhammed164/embedding\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"Tnt3o5/Qwen3-Embedding-0.6B-base\", \"link\": \"https://huggingface.co/Tnt3o5/Qwen3-Embedding-0.6B-base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"LeoMandtler/Qwen3-Embedding-8B-bnb-4bit\", \"link\": \"https://huggingface.co/LeoMandtler/Qwen3-Embedding-8B-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 21\"}, {\"name\": \"Tnt3o5/Qwen3-Reranker-0.6B-legal\", \"link\": \"https://huggingface.co/Tnt3o5/Qwen3-Reranker-0.6B-legal\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 22\"}, {\"name\": \"heyanzhuo/Qwen3-Embedding-0.6B-Base-Mod\", \"link\": \"https://huggingface.co/heyanzhuo/Qwen3-Embedding-0.6B-Base-Mod\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"FukayaTakashi/Qwen3-Reranker-8B\", \"link\": \"https://huggingface.co/FukayaTakashi/Qwen3-Reranker-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"alexisriot/insight-qwen3-reranker\", \"link\": \"https://huggingface.co/alexisriot/insight-qwen3-reranker\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 19\"}, {\"name\": \"alexisriot/qwen3-06b\", \"link\": \"https://huggingface.co/alexisriot/qwen3-06b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 19\"}, {\"name\": \"talentguide/qwen06-base\", \"link\": \"https://huggingface.co/talentguide/qwen06-base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 19\"}, {\"name\": \"anyidea/Qwen3-Embedding-8B\", \"link\": \"https://huggingface.co/anyidea/Qwen3-Embedding-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 25\"}, {\"name\": \"AXERA-TECH/Qwen3-Embedding-0.6B-GPTQ-Int8\", \"link\": \"https://huggingface.co/AXERA-TECH/Qwen3-Embedding-0.6B-GPTQ-Int8\", \"task\": \"\", \"likes\": \"144\", \"downloads\": \"\", \"updated\": \"Sep 26\"}, {\"name\": \"socrate-tech/Qwen3-Embedding-0.6B-GGUF\", \"link\": \"https://huggingface.co/socrate-tech/Qwen3-Embedding-0.6B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 26\"}, {\"name\": \"zumainternal/Qwen3-Reranker-0.6B\", \"link\": \"https://huggingface.co/zumainternal/Qwen3-Reranker-0.6B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 27\"}, {\"name\": \"leoikaichen/MyOwn_Qwen3-Embedding-0.8B\", \"link\": \"https://huggingface.co/leoikaichen/MyOwn_Qwen3-Embedding-0.8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 7\"}, {\"name\": \"tonywng/Qwen3-Embedding-0.6B\", \"link\": \"https://huggingface.co/tonywng/Qwen3-Embedding-0.6B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"Echo9Zulu/Qwen3-Embedding-0.6B-int8_asym-ov\", \"link\": \"https://huggingface.co/Echo9Zulu/Qwen3-Embedding-0.6B-int8_asym-ov\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 12\"}, {\"name\": \"Casual-Autopsy/Qwen3-Embedding-4B-GGUFs\", \"link\": \"https://huggingface.co/Casual-Autopsy/Qwen3-Embedding-4B-GGUFs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\"}, {\"name\": \"ManiKumarAdapala/Qwen3-Reranker-0.6B-Q8_0-Safetensors\", \"link\": \"https://huggingface.co/ManiKumarAdapala/Qwen3-Reranker-0.6B-Q8_0-Safetensors\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"techAInewb/Qwen3-Embedding-0.6B-INT8\", \"link\": \"https://huggingface.co/techAInewb/Qwen3-Embedding-0.6B-INT8\", \"task\": \"\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"22 days ago\"}, {\"name\": \"ZanderM98/Qwen3-Embedding-8B-bnb-4bit\", \"link\": \"https://huggingface.co/ZanderM98/Qwen3-Embedding-8B-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"akahana/qwen3-4b-text-embedding-4bit\", \"link\": \"https://huggingface.co/akahana/qwen3-4b-text-embedding-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"homesome/qwen3-reranker-4b-homesome\", \"link\": \"https://huggingface.co/homesome/qwen3-reranker-4b-homesome\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"Casual-Autopsy/Qwen3-Embedding-0.6B-GGUFs\", \"link\": \"https://huggingface.co/Casual-Autopsy/Qwen3-Embedding-0.6B-GGUFs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"DocDBrown/Qwen3-rerank-8B-onnx\", \"link\": \"https://huggingface.co/DocDBrown/Qwen3-rerank-8B-onnx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"ManiKumarAdapala/Qwen3-Embedding-0.6B-Q8_0-Safetensors\", \"link\": \"https://huggingface.co/ManiKumarAdapala/Qwen3-Embedding-0.6B-Q8_0-Safetensors\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"10 days ago\"}, {\"name\": \"bflhc/MoD-Embedding\", \"link\": \"https://huggingface.co/bflhc/MoD-Embedding\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "SINAI/ALIA-legal-administrative-cqa",
    "datasets_links": "https://huggingface.co/datasets/SINAI/ALIA-legal-administrative-cqa",
    "datasets_detailed": "[{\"name\": \"SINAI/ALIA-legal-administrative-cqa\", \"link\": \"https://huggingface.co/datasets/SINAI/ALIA-legal-administrative-cqa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.05209",
    "first_seen_date": "2025-06-06",
    "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed Text",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.05209The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly\n  Licensed TextPublished on Jun 5\u00b7Submitted byStefan Schweteron Jun 6#3 Paper of the dayUpvote59+51Authors:Nikhil Kandpal,Brian Lester,Colin Raffel,Sebastian Majstorovic,Stella Biderman,Baber Abbasi,Luca Soldaini,Enrico Shippole,A. Feder Cooper,Aviya Skowron,John Kirchenbauer,Shayne Longpre,Lintang Sutawika,Alon Albalak,Zhenlin Xu,Guilherme Penedo,Loubna Ben Allal,Elie Bakouch,John David Pressman,Honglu Fan,Dashiell Stander,Guangyu Song+5 authorsAbstractThe Common Pile v0.1 dataset, an 8-terabyte collection of openly licensed text, is used to train competitive 7 billion parameter LLMs.AI-generated summaryLarge language models(LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. TrainingLLMsonopenly licensed textpresents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performantLLMs. To address this gap, we collect, curate, and release\ntheCommon Pile v0.1, an eight terabyte collection ofopenly licensed textdesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameterLLMson text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance toLLMstrained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing theCommon Pile v0.1itself, we also release the\ncode used in its creation as well as thetraining mixtureandcheckpointsfor\nthe Comma v0.1 mode",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/r-three/common-pile",
    "hf_paper_url": "https://huggingface.co/papers/2506.05209",
    "arxiv_url": "https://arxiv.org/abs/2506.05209",
    "num_models": 2,
    "models_list": "common-pile/comma-v0.1-2t, common-pile/comma-v0.1-1t",
    "models_links": "https://huggingface.co/common-pile/comma-v0.1-2t, https://huggingface.co/common-pile/comma-v0.1-1t",
    "models_detailed": "[{\"name\": \"common-pile/comma-v0.1-2t\", \"link\": \"https://huggingface.co/common-pile/comma-v0.1-2t\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\"}, {\"name\": \"common-pile/comma-v0.1-1t\", \"link\": \"https://huggingface.co/common-pile/comma-v0.1-1t\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Jun 6\"}]",
    "num_datasets": 63,
    "datasets_list": "common-pile/comma_v0.1_training_dataset, common-pile/caselaw_access_project, common-pile/raw_v0.1_parquet, common-pile/stackv2, common-pile/library_of_congress, common-pile/stackexchange, common-pile/ubuntu_irc, common-pile/project_gutenberg, common-pile/pubmed, common-pile/news, common-pile/public_domain_review, common-pile/biodiversity_heritage_library, common-pile/regulations, common-pile/pre_1929_books, common-pile/youtube, common-pile/arxiv_abstracts, common-pile/arxiv_papers, common-pile/usgpo, common-pile/foodista, common-pile/python_enhancement_proposals, common-pile/peS2o, common-pile/libretexts, common-pile/oercommons, common-pile/pressbooks, common-pile/github_archive, common-pile/doab, common-pile/data_provenance_initiative, common-pile/uspto, common-pile/uk_hansard, common-pile/wikiteam, common-pile/wikimedia, common-pile/arxiv_abstracts_filtered, common-pile/arxiv_papers_filtered, common-pile/biodiversity_heritage_library_filtered, common-pile/caselaw_access_project_filtered, common-pile/cccc_filtered, common-pile/data_provenance_initiative_filtered, common-pile/doab_filtered, common-pile/foodista_filtered, common-pile/github_archive_filtered, common-pile/library_of_congress_filtered, common-pile/libretexts_filtered, common-pile/news_filtered, common-pile/oercommons_filtered, common-pile/peS2o_filtered, common-pile/pre_1929_books_filtered, common-pile/pressbooks_filtered, common-pile/project_gutenberg_filtered, common-pile/public_domain_review_filtered, common-pile/pubmed_filtered, common-pile/python_enhancement_proposals_filtered, common-pile/regulations_filtered, common-pile/stackexchange_filtered, common-pile/stackv2_edu_filtered, common-pile/ubuntu_irc_filtered, common-pile/uk_hansard_filtered, common-pile/usgpo_filtered, common-pile/uspto_filtered, common-pile/wikimedia_filtered, common-pile/wikiteam_filtered, common-pile/youtube_filtered, common-pile/cccc, pykeio/topazolite",
    "datasets_links": "https://huggingface.co/datasets/common-pile/comma_v0.1_training_dataset, https://huggingface.co/datasets/common-pile/caselaw_access_project, https://huggingface.co/datasets/common-pile/raw_v0.1_parquet, https://huggingface.co/datasets/common-pile/stackv2, https://huggingface.co/datasets/common-pile/library_of_congress, https://huggingface.co/datasets/common-pile/stackexchange, https://huggingface.co/datasets/common-pile/ubuntu_irc, https://huggingface.co/datasets/common-pile/project_gutenberg, https://huggingface.co/datasets/common-pile/pubmed, https://huggingface.co/datasets/common-pile/news, https://huggingface.co/datasets/common-pile/public_domain_review, https://huggingface.co/datasets/common-pile/biodiversity_heritage_library, https://huggingface.co/datasets/common-pile/regulations, https://huggingface.co/datasets/common-pile/pre_1929_books, https://huggingface.co/datasets/common-pile/youtube, https://huggingface.co/datasets/common-pile/arxiv_abstracts, https://huggingface.co/datasets/common-pile/arxiv_papers, https://huggingface.co/datasets/common-pile/usgpo, https://huggingface.co/datasets/common-pile/foodista, https://huggingface.co/datasets/common-pile/python_enhancement_proposals, https://huggingface.co/datasets/common-pile/peS2o, https://huggingface.co/datasets/common-pile/libretexts, https://huggingface.co/datasets/common-pile/oercommons, https://huggingface.co/datasets/common-pile/pressbooks, https://huggingface.co/datasets/common-pile/github_archive, https://huggingface.co/datasets/common-pile/doab, https://huggingface.co/datasets/common-pile/data_provenance_initiative, https://huggingface.co/datasets/common-pile/uspto, https://huggingface.co/datasets/common-pile/uk_hansard, https://huggingface.co/datasets/common-pile/wikiteam, https://huggingface.co/datasets/common-pile/wikimedia, https://huggingface.co/datasets/common-pile/arxiv_abstracts_filtered, https://huggingface.co/datasets/common-pile/arxiv_papers_filtered, https://huggingface.co/datasets/common-pile/biodiversity_heritage_library_filtered, https://huggingface.co/datasets/common-pile/caselaw_access_project_filtered, https://huggingface.co/datasets/common-pile/cccc_filtered, https://huggingface.co/datasets/common-pile/data_provenance_initiative_filtered, https://huggingface.co/datasets/common-pile/doab_filtered, https://huggingface.co/datasets/common-pile/foodista_filtered, https://huggingface.co/datasets/common-pile/github_archive_filtered, https://huggingface.co/datasets/common-pile/library_of_congress_filtered, https://huggingface.co/datasets/common-pile/libretexts_filtered, https://huggingface.co/datasets/common-pile/news_filtered, https://huggingface.co/datasets/common-pile/oercommons_filtered, https://huggingface.co/datasets/common-pile/peS2o_filtered, https://huggingface.co/datasets/common-pile/pre_1929_books_filtered, https://huggingface.co/datasets/common-pile/pressbooks_filtered, https://huggingface.co/datasets/common-pile/project_gutenberg_filtered, https://huggingface.co/datasets/common-pile/public_domain_review_filtered, https://huggingface.co/datasets/common-pile/pubmed_filtered, https://huggingface.co/datasets/common-pile/python_enhancement_proposals_filtered, https://huggingface.co/datasets/common-pile/regulations_filtered, https://huggingface.co/datasets/common-pile/stackexchange_filtered, https://huggingface.co/datasets/common-pile/stackv2_edu_filtered, https://huggingface.co/datasets/common-pile/ubuntu_irc_filtered, https://huggingface.co/datasets/common-pile/uk_hansard_filtered, https://huggingface.co/datasets/common-pile/usgpo_filtered, https://huggingface.co/datasets/common-pile/uspto_filtered, https://huggingface.co/datasets/common-pile/wikimedia_filtered, https://huggingface.co/datasets/common-pile/wikiteam_filtered, https://huggingface.co/datasets/common-pile/youtube_filtered, https://huggingface.co/datasets/common-pile/cccc, https://huggingface.co/datasets/pykeio/topazolite",
    "datasets_detailed": "[{\"name\": \"common-pile/comma_v0.1_training_dataset\", \"link\": \"https://huggingface.co/datasets/common-pile/comma_v0.1_training_dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/caselaw_access_project\", \"link\": \"https://huggingface.co/datasets/common-pile/caselaw_access_project\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/raw_v0.1_parquet\", \"link\": \"https://huggingface.co/datasets/common-pile/raw_v0.1_parquet\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\", \"size\": \"\"}, {\"name\": \"common-pile/stackv2\", \"link\": \"https://huggingface.co/datasets/common-pile/stackv2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/library_of_congress\", \"link\": \"https://huggingface.co/datasets/common-pile/library_of_congress\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\", \"size\": \"\"}, {\"name\": \"common-pile/stackexchange\", \"link\": \"https://huggingface.co/datasets/common-pile/stackexchange\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/ubuntu_irc\", \"link\": \"https://huggingface.co/datasets/common-pile/ubuntu_irc\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/project_gutenberg\", \"link\": \"https://huggingface.co/datasets/common-pile/project_gutenberg\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/pubmed\", \"link\": \"https://huggingface.co/datasets/common-pile/pubmed\", \"task\": \"\", \"likes\": \"368\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/news\", \"link\": \"https://huggingface.co/datasets/common-pile/news\", \"task\": \"\", \"likes\": \"425\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/public_domain_review\", \"link\": \"https://huggingface.co/datasets/common-pile/public_domain_review\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/biodiversity_heritage_library\", \"link\": \"https://huggingface.co/datasets/common-pile/biodiversity_heritage_library\", \"task\": \"\", \"likes\": \"362\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/regulations\", \"link\": \"https://huggingface.co/datasets/common-pile/regulations\", \"task\": \"\", \"likes\": \"138\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/pre_1929_books\", \"link\": \"https://huggingface.co/datasets/common-pile/pre_1929_books\", \"task\": \"\", \"likes\": \"398\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/youtube\", \"link\": \"https://huggingface.co/datasets/common-pile/youtube\", \"task\": \"\", \"likes\": \"404\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/arxiv_abstracts\", \"link\": \"https://huggingface.co/datasets/common-pile/arxiv_abstracts\", \"task\": \"\", \"likes\": \"250\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/arxiv_papers\", \"link\": \"https://huggingface.co/datasets/common-pile/arxiv_papers\", \"task\": \"\", \"likes\": \"383\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/usgpo\", \"link\": \"https://huggingface.co/datasets/common-pile/usgpo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/foodista\", \"link\": \"https://huggingface.co/datasets/common-pile/foodista\", \"task\": \"\", \"likes\": \"148\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/python_enhancement_proposals\", \"link\": \"https://huggingface.co/datasets/common-pile/python_enhancement_proposals\", \"task\": \"\", \"likes\": \"656\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/peS2o\", \"link\": \"https://huggingface.co/datasets/common-pile/peS2o\", \"task\": \"\", \"likes\": \"744\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/libretexts\", \"link\": \"https://huggingface.co/datasets/common-pile/libretexts\", \"task\": \"\", \"likes\": \"57\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/oercommons\", \"link\": \"https://huggingface.co/datasets/common-pile/oercommons\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/pressbooks\", \"link\": \"https://huggingface.co/datasets/common-pile/pressbooks\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/github_archive\", \"link\": \"https://huggingface.co/datasets/common-pile/github_archive\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/doab\", \"link\": \"https://huggingface.co/datasets/common-pile/doab\", \"task\": \"\", \"likes\": \"165\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/data_provenance_initiative\", \"link\": \"https://huggingface.co/datasets/common-pile/data_provenance_initiative\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/uspto\", \"link\": \"https://huggingface.co/datasets/common-pile/uspto\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/uk_hansard\", \"link\": \"https://huggingface.co/datasets/common-pile/uk_hansard\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/wikiteam\", \"link\": \"https://huggingface.co/datasets/common-pile/wikiteam\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/wikimedia\", \"link\": \"https://huggingface.co/datasets/common-pile/wikimedia\", \"task\": \"\", \"likes\": \"864\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/arxiv_abstracts_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/arxiv_abstracts_filtered\", \"task\": \"\", \"likes\": \"237\", \"downloads\": \"\", \"updated\": \"Nov 19\", \"size\": \"\"}, {\"name\": \"common-pile/arxiv_papers_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/arxiv_papers_filtered\", \"task\": \"\", \"likes\": \"334\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/biodiversity_heritage_library_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/biodiversity_heritage_library_filtered\", \"task\": \"\", \"likes\": \"333\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/caselaw_access_project_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/caselaw_access_project_filtered\", \"task\": \"\", \"likes\": \"820\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/cccc_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/cccc_filtered\", \"task\": \"\", \"likes\": \"267\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/data_provenance_initiative_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/data_provenance_initiative_filtered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/doab_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/doab_filtered\", \"task\": \"\", \"likes\": \"119\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/foodista_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/foodista_filtered\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/github_archive_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/github_archive_filtered\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/library_of_congress_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/library_of_congress_filtered\", \"task\": \"\", \"likes\": \"194\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/libretexts_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/libretexts_filtered\", \"task\": \"\", \"likes\": \"99\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/news_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/news_filtered\", \"task\": \"\", \"likes\": \"83\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/oercommons_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/oercommons_filtered\", \"task\": \"\", \"likes\": \"57\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/peS2o_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/peS2o_filtered\", \"task\": \"\", \"likes\": \"264\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/pre_1929_books_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/pre_1929_books_filtered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/pressbooks_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/pressbooks_filtered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/project_gutenberg_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/project_gutenberg_filtered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/public_domain_review_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/public_domain_review_filtered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/pubmed_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/pubmed_filtered\", \"task\": \"\", \"likes\": \"188\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/python_enhancement_proposals_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/python_enhancement_proposals_filtered\", \"task\": \"\", \"likes\": \"655\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/regulations_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/regulations_filtered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/stackexchange_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/stackexchange_filtered\", \"task\": \"\", \"likes\": \"457\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/stackv2_edu_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/stackv2_edu_filtered\", \"task\": \"\", \"likes\": \"464\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/ubuntu_irc_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/ubuntu_irc_filtered\", \"task\": \"\", \"likes\": \"83\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/uk_hansard_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/uk_hansard_filtered\", \"task\": \"\", \"likes\": \"282\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/usgpo_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/usgpo_filtered\", \"task\": \"\", \"likes\": \"189\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/uspto_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/uspto_filtered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/wikimedia_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/wikimedia_filtered\", \"task\": \"\", \"likes\": \"281\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/wikiteam_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/wikiteam_filtered\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/youtube_filtered\", \"link\": \"https://huggingface.co/datasets/common-pile/youtube_filtered\", \"task\": \"\", \"likes\": \"175\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"common-pile/cccc\", \"link\": \"https://huggingface.co/datasets/common-pile/cccc\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\", \"size\": \"\"}, {\"name\": \"pykeio/topazolite\", \"link\": \"https://huggingface.co/datasets/pykeio/topazolite\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.05301",
    "first_seen_date": "2025-06-06",
    "title": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-Training",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.05301SeedVR2: One-Step Video Restoration via Diffusion Adversarial\n  Post-TrainingPublished on Jun 5\u00b7Submitted byJIANYI WANGon Jun 6\u00b7ByteDance SeedUpvote58+50Authors:Jianyi Wang,Shanchuan Lin,Zhijie Lin,Yuxi Ren,Meng Wei,Zongsheng Yue,Shangchen Zhou,Hao Chen,Yang Zhao,Ceyuan Yang,Xuefeng Xiao,Chen Change Loy,Lu JiangAbstractSeedVR2 is a one-step diffusion-based video restoration model that uses adaptive window attention and feature matching loss to achieve high visual quality with reduced computational cost compared to existing methods.AI-generated summaryRecent advances indiffusion-based video restoration(VR) demonstrate\nsignificant improvement in visual quality, yet yield a prohibitive\ncomputational cost during inference. While several distillation-based\napproaches have exhibited the potential of one-step image restoration,\nextending existing approaches toVRremains challenging and underexplored,\nparticularly when dealing with high-resolution video in real-world settings. In\nthis work, we propose a one-step diffusion-basedVRmodel, termed as SeedVR2,\nwhich performsadversarial VR trainingagainst real data. To handle the\nchallenging high-resolutionVRwithin a single step, we introduce several\nenhancements to both model architecture and training procedures. Specifically,\nanadaptive window attentionmechanism is proposed, where the window size is\ndynamically adjusted to fit the output resolutions, avoiding window\ninconsistency observed under high-resolutionVRusing window attention with a\npredefined window size. To stabilize and improve the adversarial post-training\ntowardsVR, we further verify the effectiveness of a series of losses,\nincluding a proposedfeature matching losswithout significantly sacrificing\ntraining efficiency. Extensive experiments show that SeedVR2 can achieve\ncomparable or even better performance compared with existingVRapproaches in a\nsingle step.View arXiv pag",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/ByteDance-Seed/SeedVR",
    "hf_paper_url": "https://huggingface.co/papers/2506.05301",
    "arxiv_url": "https://arxiv.org/abs/2506.05301",
    "num_models": 7,
    "models_list": "ByteDance-Seed/SeedVR2-7B, ByteDance-Seed/SeedVR2-3B, ByteDance-Seed/SeedVR-7B, ByteDance-Seed/SeedVR-3B, SassyDiffusion/SeedVR2-7B_FP32, SassyDiffusion/SeedVR2-7B_BF16, ussoewwin/SeedVR2-fp16-GGUF",
    "models_links": "https://huggingface.co/ByteDance-Seed/SeedVR2-7B, https://huggingface.co/ByteDance-Seed/SeedVR2-3B, https://huggingface.co/ByteDance-Seed/SeedVR-7B, https://huggingface.co/ByteDance-Seed/SeedVR-3B, https://huggingface.co/SassyDiffusion/SeedVR2-7B_FP32, https://huggingface.co/SassyDiffusion/SeedVR2-7B_BF16, https://huggingface.co/ussoewwin/SeedVR2-fp16-GGUF",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/SeedVR2-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/SeedVR2-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"ByteDance-Seed/SeedVR2-3B\", \"link\": \"https://huggingface.co/ByteDance-Seed/SeedVR2-3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 22\"}, {\"name\": \"ByteDance-Seed/SeedVR-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/SeedVR-7B\", \"task\": \"\", \"likes\": \"128\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"ByteDance-Seed/SeedVR-3B\", \"link\": \"https://huggingface.co/ByteDance-Seed/SeedVR-3B\", \"task\": \"\", \"likes\": \"78\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"SassyDiffusion/SeedVR2-7B_FP32\", \"link\": \"https://huggingface.co/SassyDiffusion/SeedVR2-7B_FP32\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"SassyDiffusion/SeedVR2-7B_BF16\", \"link\": \"https://huggingface.co/SassyDiffusion/SeedVR2-7B_BF16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"ussoewwin/SeedVR2-fp16-GGUF\", \"link\": \"https://huggingface.co/ussoewwin/SeedVR2-fp16-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"23 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "Iceclear/SeedVR_VideoDemos",
    "datasets_links": "https://huggingface.co/datasets/Iceclear/SeedVR_VideoDemos",
    "datasets_detailed": "[{\"name\": \"Iceclear/SeedVR_VideoDemos\", \"link\": \"https://huggingface.co/datasets/Iceclear/SeedVR_VideoDemos\", \"task\": \"\", \"likes\": \"104\", \"downloads\": \"\", \"updated\": \"Jun 19\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.02863",
    "first_seen_date": "2025-06-05",
    "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned\n  Text-to-Speech",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.02863CapSpeech: Enabling Downstream Applications in Style-Captioned\n  Text-to-SpeechPublished on Jun 3\u00b7Submitted byHelin Wangon Jun 5Upvote8Authors:Helin Wang,Jiarui Hai,Dading Chong,Karan Thakkar,Tiantian Feng,Dongchao Yang,Junhyeok Lee,Laureano Moro Velazquez,Jesus Villalba,Zengyi Qin,Shrikanth Narayanan,Mounya Elhiali,Najim DehakAbstractCapSpeech introduces a large benchmark dataset for various captioned text-to-speech tasks, facilitating advancements in style, accent, emotion, and chat-agent synthesis.AI-generated summaryRecent advancements in generative artificial intelligence have significantly\ntransformed the field of style-captioned text-to-speech synthesis (CapTTS).\nHowever, adaptingCapTTSto real-world applications remains challenging due to\nthe lack of standardized, comprehensive datasets and limited research on\ndownstream tasks built uponCapTTS. To address these gaps, we introduce\nCapSpeech, a new benchmark designed for a series ofCapTTS-related tasks,\nincluding style-captioned text-to-speech synthesis with sound events\n(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS\n(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech\ncomprises over 10 millionmachine-annotatedaudio-caption pairs and nearly 0.36\nmillionhuman-annotatedaudio-caption pairs. In addition, we introduce two new\ndatasets collected and recorded by a professional voice actor and experienced\naudio engineers, specifically for theAgentTTSandCapTTS-SEtasks. Alongside\nthe datasets, we conduct comprehensive experiments using both autoregressive\nandnon-autoregressive modelson CapSpeech. Our results demonstrate\nhigh-fidelity and highly intelligible speech synthesis across a diverse range\nof speaking styles. To the best of our knowledge, CapSpeech is the largest\navailable dataset offering comprehensive annotations forCapTTS-related tasks.\nThe experiments and findings fur",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/WangHelin1997/CapSpeech",
    "hf_paper_url": "https://huggingface.co/papers/2506.02863",
    "arxiv_url": "https://arxiv.org/abs/2506.02863",
    "num_models": 1,
    "models_list": "OpenSound/CapSpeech-models",
    "models_links": "https://huggingface.co/OpenSound/CapSpeech-models",
    "models_detailed": "[{\"name\": \"OpenSound/CapSpeech-models\", \"link\": \"https://huggingface.co/OpenSound/CapSpeech-models\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 27\"}]",
    "num_datasets": 15,
    "datasets_list": "OpenSound/CapSpeech, OpenSound/CapTTS-SFT, OpenSound/CapSpeech-PT-SEDB-Audio, OpenSound/CapSpeech-PT, OpenSound/CapSpeech-CommonVoice, OpenSound/CapSpeech_GigaSpeech, OpenSound/CapSpeech-MLS, OpenSound/CapSpeech_Emilia, OpenSound/CapSpeech-PT-SEDB-HQ, OpenSound/CapSpeech-PT-SEDB-HQ-Audio, OpenSound/CapTTS-SFT-Audio, OpenSound/CapSpeech-AgentDB, OpenSound/CapSpeech-SEDB, OpenSound/CapSpeech-SEDB-test, OpenSound/CapSpeech-SEDB-Audio",
    "datasets_links": "https://huggingface.co/datasets/OpenSound/CapSpeech, https://huggingface.co/datasets/OpenSound/CapTTS-SFT, https://huggingface.co/datasets/OpenSound/CapSpeech-PT-SEDB-Audio, https://huggingface.co/datasets/OpenSound/CapSpeech-PT, https://huggingface.co/datasets/OpenSound/CapSpeech-CommonVoice, https://huggingface.co/datasets/OpenSound/CapSpeech_GigaSpeech, https://huggingface.co/datasets/OpenSound/CapSpeech-MLS, https://huggingface.co/datasets/OpenSound/CapSpeech_Emilia, https://huggingface.co/datasets/OpenSound/CapSpeech-PT-SEDB-HQ, https://huggingface.co/datasets/OpenSound/CapSpeech-PT-SEDB-HQ-Audio, https://huggingface.co/datasets/OpenSound/CapTTS-SFT-Audio, https://huggingface.co/datasets/OpenSound/CapSpeech-AgentDB, https://huggingface.co/datasets/OpenSound/CapSpeech-SEDB, https://huggingface.co/datasets/OpenSound/CapSpeech-SEDB-test, https://huggingface.co/datasets/OpenSound/CapSpeech-SEDB-Audio",
    "datasets_detailed": "[{\"name\": \"OpenSound/CapSpeech\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech\", \"task\": \"\", \"likes\": \"725\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapTTS-SFT\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapTTS-SFT\", \"task\": \"\", \"likes\": \"393\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-PT-SEDB-Audio\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-PT-SEDB-Audio\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-PT\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-PT\", \"task\": \"\", \"likes\": \"228\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-CommonVoice\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-CommonVoice\", \"task\": \"\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech_GigaSpeech\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech_GigaSpeech\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-MLS\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-MLS\", \"task\": \"\", \"likes\": \"60\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech_Emilia\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech_Emilia\", \"task\": \"\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-PT-SEDB-HQ\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-PT-SEDB-HQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-PT-SEDB-HQ-Audio\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-PT-SEDB-HQ-Audio\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapTTS-SFT-Audio\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapTTS-SFT-Audio\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-AgentDB\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-AgentDB\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-SEDB\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-SEDB\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-SEDB-test\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-SEDB-test\", \"task\": \"\", \"likes\": \"496\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}, {\"name\": \"OpenSound/CapSpeech-SEDB-Audio\", \"link\": \"https://huggingface.co/datasets/OpenSound/CapSpeech-SEDB-Audio\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.03295",
    "first_seen_date": "2025-06-05",
    "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.03295Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One ProblemPublished on Jun 3\u00b7Submitted byyuboon Jun 5Upvote17+9Authors:Yubo Wang,Ping Nie,Kai Zou,Lijun Wu,Wenhu ChenAbstractCritique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.AI-generated summaryWe have witnessed that strong LLMs likeQwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate thatCritique Fine-Tuning(CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and usingteacher LLMsto\nprovide detailed critiques. We fine-tune Qwen andLlama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significantperformance gainsacross diversereasoning tasks. For example, with just 5 GPU\nhours of training,Qwen-Math-7B-CFT show an average improvement of 15% on sixmath benchmarksand 16% on threelogic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness ofone-shot CFTacross different prompt\nproblems. These results highlightone-shot CFTas a simple, general, and\ncompute-efficient appro",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/TIGER-AI-Lab/One-Shot-CFT",
    "hf_paper_url": "https://huggingface.co/papers/2506.03295",
    "arxiv_url": "https://arxiv.org/abs/2506.03295",
    "num_models": 3,
    "models_list": "TIGER-Lab/One-Shot-CFT-Math-Qwen-7B, TIGER-Lab/One-Shot-CFT-Math-Qwen-14B, TIGER-Lab/One-Shot-CFT-Math-Qwen-1.5B",
    "models_links": "https://huggingface.co/TIGER-Lab/One-Shot-CFT-Math-Qwen-7B, https://huggingface.co/TIGER-Lab/One-Shot-CFT-Math-Qwen-14B, https://huggingface.co/TIGER-Lab/One-Shot-CFT-Math-Qwen-1.5B",
    "models_detailed": "[{\"name\": \"TIGER-Lab/One-Shot-CFT-Math-Qwen-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/One-Shot-CFT-Math-Qwen-7B\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"TIGER-Lab/One-Shot-CFT-Math-Qwen-14B\", \"link\": \"https://huggingface.co/TIGER-Lab/One-Shot-CFT-Math-Qwen-14B\", \"task\": \"Text Generation\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"TIGER-Lab/One-Shot-CFT-Math-Qwen-1.5B\", \"link\": \"https://huggingface.co/TIGER-Lab/One-Shot-CFT-Math-Qwen-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/One-Shot-CFT-Data",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/One-Shot-CFT-Data",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/One-Shot-CFT-Data\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/One-Shot-CFT-Data\", \"task\": \"\", \"likes\": \"196\", \"downloads\": \"\", \"updated\": \"Jun 9\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.03930",
    "first_seen_date": "2025-06-05",
    "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.03930VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  GenerationPublished on Jun 4\u00b7Submitted byYuansheng Nion Jun 5Upvote26+18Authors:Yuansheng Ni,Ping Nie,Kai Zou,Xiang Yue,Wenhu ChenAbstractVisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.AI-generated summaryLarge language models(LLMs) often struggle withvisualization taskslike\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support foriterative code correction, resulting\nin fragile and unreliableplot generation. We presentVisCode-200K, a\nlarge-scale instruction tuning dataset forPython-based visualizationand\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions andrendered plots; and (2) 45K multi-turncorrection dialoguesfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tuneQwen2.5-Coder-InstructonVisCode-200Kto createVisCoder, and evaluate it onPandasPlotBench.VisCodersignificantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt aself-debug evaluationprotocol to assess iterative repair, demonstrating the benefits offeedback-driven learningfor executable, visually accurate code generation.View arXiv pageView PDFProject pageGitHub16Add to collectionCommunityyuanshengniPaper authorPaper submitterJun 5https://tiger-ai-lab.github.io/VisCoder/Replylibrarian-botJun 6This is an automated message from theLibrarian Bot. I found the following papers similar to this p",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/TIGER-AI-Lab/VisCoder",
    "hf_paper_url": "https://huggingface.co/papers/2506.03930",
    "arxiv_url": "https://arxiv.org/abs/2506.03930",
    "num_models": 6,
    "models_list": "TIGER-Lab/VisCoder-7B, TIGER-Lab/VisCoder-3B, TIGER-Lab/VisCoder2-3B, TIGER-Lab/VisCoder2-7B, TIGER-Lab/VisCoder2-14B, TIGER-Lab/VisCoder2-32B",
    "models_links": "https://huggingface.co/TIGER-Lab/VisCoder-7B, https://huggingface.co/TIGER-Lab/VisCoder-3B, https://huggingface.co/TIGER-Lab/VisCoder2-3B, https://huggingface.co/TIGER-Lab/VisCoder2-7B, https://huggingface.co/TIGER-Lab/VisCoder2-14B, https://huggingface.co/TIGER-Lab/VisCoder2-32B",
    "models_detailed": "[{\"name\": \"TIGER-Lab/VisCoder-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder-7B\", \"task\": \"Text Generation\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Jun 8\"}, {\"name\": \"TIGER-Lab/VisCoder-3B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder-3B\", \"task\": \"Text Generation\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"Jun 8\"}, {\"name\": \"TIGER-Lab/VisCoder2-3B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder2-3B\", \"task\": \"Text Generation\", \"likes\": \"49\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"TIGER-Lab/VisCoder2-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder2-7B\", \"task\": \"\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"TIGER-Lab/VisCoder2-14B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder2-14B\", \"task\": \"\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"TIGER-Lab/VisCoder2-32B\", \"link\": \"https://huggingface.co/TIGER-Lab/VisCoder2-32B\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Nov 3\"}]",
    "num_datasets": 3,
    "datasets_list": "TIGER-Lab/VisCode-Multi-679K, TIGER-Lab/VisPlotBench, TIGER-Lab/VisCode-200K",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/VisCode-Multi-679K, https://huggingface.co/datasets/TIGER-Lab/VisPlotBench, https://huggingface.co/datasets/TIGER-Lab/VisCode-200K",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/VisCode-Multi-679K\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisCode-Multi-679K\", \"task\": \"\", \"likes\": \"219\", \"downloads\": \"\", \"updated\": \"Nov 3\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/VisPlotBench\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisPlotBench\", \"task\": \"\", \"likes\": \"888\", \"downloads\": \"\", \"updated\": \"Nov 3\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/VisCode-200K\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisCode-200K\", \"task\": \"\", \"likes\": \"129\", \"downloads\": \"\", \"updated\": \"Jun 8\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.04178",
    "first_seen_date": "2025-06-05",
    "title": "OpenThoughts: Data Recipes for Reasoning Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.04178OpenThoughts: Data Recipes for Reasoning ModelsPublished on Jun 4\u00b7Submitted byEtash Guhaon Jun 5#3 Paper of the dayUpvote48+40Authors:Etash Guha,Ryan Marten,Sedrick Keh,Negin Raoof,Georgios Smyrnis,Hritik Bansal,Marianna Nezhurina,Jean Mercat,Trung Vu,Zayne Sprague,Ashima Suvarna,Benjamin Feuer,Liangyu Chen,Zaid Khan,Eric Frankel,Sachin Grover,Caroline Choi,Niklas Muennighoff,Shiye Su,Wanjia Zhao,John Yang,Shreyas Pimpalgaonkar+28 authorsAbstractThe OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.AI-generated summaryReasoning modelshave made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best\ntraining recipes for reasoning since state-of-the-art models often rely on\nproprietary datasets with little to no public information available. To address\nthis, the goal of theOpenThoughts projectis to create open-source datasets\nfor trainingreasoning models. After initial explorations, ourOpenThoughts2-1Mdataset led toOpenThinker2-32B, the first model trained on public reasoning\ndata to matchDeepSeek-R1-Distill-32Bonstandard reasoning benchmarkssuch asAIMEandLiveCodeBench. We then improve our dataset further by systematically\ninvestigating each step of our data generation pipeline with 1,000+ controlled\nexperiments, which led toOpenThoughts3. Scaling the pipeline to 1.2M examples\nand using QwQ-32B as teacher yields ourOpenThinker3-7Bmodel, which achieves\nstate-of-the-art results: 53% onAIME2025, 51% onLiveCodeBench06/24-01/25,\nand 54% onGPQA Diamond. All of our datasets and models are available on\nhttps://openthoughts.ai.View arXiv pageView PDFProject pageGitHub2.17kautoAdd to collectionCommunityEtashGuhaPaper submitterJun 5Reasoning models have made rapid progress on many benchmarks involving math, code, ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/open-thoughts/open-thoughts",
    "hf_paper_url": "https://huggingface.co/papers/2506.04178",
    "arxiv_url": "https://arxiv.org/abs/2506.04178",
    "num_models": 13,
    "models_list": "open-thoughts/OpenThinker-32B, open-thoughts/OpenThinker-7B, open-thoughts/OpenThinker3-7B, open-thoughts/OpenThinker2-32B, QuantFactory/OpenThinker-7B-GGUF, open-thoughts/OpenThinker2-7B, lmstudio-community/OpenThinker3-7B-GGUF, Mungert/OpenThinker3-7B-GGUF, QuantFactory/OpenThinker3-7B-GGUF, QuantFactory/OpenThinker2-7B-GGUF, open-thoughts/OpenThinker3-1.5B, EpistemeAI/Hercules1-8B-E4B-it, KenSensei/gemma-3-270m-openthoughts",
    "models_links": "https://huggingface.co/open-thoughts/OpenThinker-32B, https://huggingface.co/open-thoughts/OpenThinker-7B, https://huggingface.co/open-thoughts/OpenThinker3-7B, https://huggingface.co/open-thoughts/OpenThinker2-32B, https://huggingface.co/QuantFactory/OpenThinker-7B-GGUF, https://huggingface.co/open-thoughts/OpenThinker2-7B, https://huggingface.co/lmstudio-community/OpenThinker3-7B-GGUF, https://huggingface.co/Mungert/OpenThinker3-7B-GGUF, https://huggingface.co/QuantFactory/OpenThinker3-7B-GGUF, https://huggingface.co/QuantFactory/OpenThinker2-7B-GGUF, https://huggingface.co/open-thoughts/OpenThinker3-1.5B, https://huggingface.co/EpistemeAI/Hercules1-8B-E4B-it, https://huggingface.co/KenSensei/gemma-3-270m-openthoughts",
    "models_detailed": "[{\"name\": \"open-thoughts/OpenThinker-32B\", \"link\": \"https://huggingface.co/open-thoughts/OpenThinker-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"open-thoughts/OpenThinker-7B\", \"link\": \"https://huggingface.co/open-thoughts/OpenThinker-7B\", \"task\": \"Text Generation\", \"likes\": \"600\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"open-thoughts/OpenThinker3-7B\", \"link\": \"https://huggingface.co/open-thoughts/OpenThinker3-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"open-thoughts/OpenThinker2-32B\", \"link\": \"https://huggingface.co/open-thoughts/OpenThinker2-32B\", \"task\": \"Text Generation\", \"likes\": \"165\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"QuantFactory/OpenThinker-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/OpenThinker-7B-GGUF\", \"task\": \"\", \"likes\": \"245\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"open-thoughts/OpenThinker2-7B\", \"link\": \"https://huggingface.co/open-thoughts/OpenThinker2-7B\", \"task\": \"Text Generation\", \"likes\": \"122\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"lmstudio-community/OpenThinker3-7B-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/OpenThinker3-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"119\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"Mungert/OpenThinker3-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenThinker3-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"QuantFactory/OpenThinker3-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/OpenThinker3-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"507\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"QuantFactory/OpenThinker2-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/OpenThinker2-7B-GGUF\", \"task\": \"\", \"likes\": \"578\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"open-thoughts/OpenThinker3-1.5B\", \"link\": \"https://huggingface.co/open-thoughts/OpenThinker3-1.5B\", \"task\": \"Text Generation\", \"likes\": \"324\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"EpistemeAI/Hercules1-8B-E4B-it\", \"link\": \"https://huggingface.co/EpistemeAI/Hercules1-8B-E4B-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"KenSensei/gemma-3-270m-openthoughts\", \"link\": \"https://huggingface.co/KenSensei/gemma-3-270m-openthoughts\", \"task\": \"\", \"likes\": \"27\", \"downloads\": \"\", \"updated\": \"Oct 28\"}]",
    "num_datasets": 8,
    "datasets_list": "open-thoughts/OpenThoughts-114k, ryanmarten/OpenThoughts-1k-sample, open-thoughts/OpenThoughts3-1.2M, open-thoughts/OpenThoughts2-1M, NewstaR/CoTton-64k-6725-Collective, NewstaR/CoTton-67k-6725-Collective, ppq177/OpenThoughts-114k, Sidsidney/OpenThoughts-114k",
    "datasets_links": "https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k, https://huggingface.co/datasets/ryanmarten/OpenThoughts-1k-sample, https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M, https://huggingface.co/datasets/open-thoughts/OpenThoughts2-1M, https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective, https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective, https://huggingface.co/datasets/ppq177/OpenThoughts-114k, https://huggingface.co/datasets/Sidsidney/OpenThoughts-114k",
    "datasets_detailed": "[{\"name\": \"open-thoughts/OpenThoughts-114k\", \"link\": \"https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 31\", \"size\": \"\"}, {\"name\": \"ryanmarten/OpenThoughts-1k-sample\", \"link\": \"https://huggingface.co/datasets/ryanmarten/OpenThoughts-1k-sample\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 31\", \"size\": \"\"}, {\"name\": \"open-thoughts/OpenThoughts3-1.2M\", \"link\": \"https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\", \"size\": \"\"}, {\"name\": \"open-thoughts/OpenThoughts2-1M\", \"link\": \"https://huggingface.co/datasets/open-thoughts/OpenThoughts2-1M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-64k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Jun 8\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-67k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Jun 7\", \"size\": \"\"}, {\"name\": \"ppq177/OpenThoughts-114k\", \"link\": \"https://huggingface.co/datasets/ppq177/OpenThoughts-114k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\", \"size\": \"\"}, {\"name\": \"Sidsidney/OpenThoughts-114k\", \"link\": \"https://huggingface.co/datasets/Sidsidney/OpenThoughts-114k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 12\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.16968",
    "first_seen_date": "2025-06-05",
    "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.16968CASS: Nvidia to AMD Transpilation with Data, Models, and BenchmarkPublished on May 22\u00b7Submitted byAhmed Heaklon Jun 5Upvote40+32Authors:Ahmed Heakl,Sarim Hashmi,Gustavo Bertolo Stahl,Seung Hun Eddie Han,Salman Khan,Abdulrahman MahmoudAbstractCASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.AI-generated summaryWe introduceCASS, the first large-scale dataset and model suite forcross-architecture GPU code transpilation, targeting both source-level (CUDAleftrightarrowHIP) and assembly-level (Nvidia SASSleftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train theCASSfamily of domain-specific language\nmodels, achieving 95%source translation accuracyand 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, andHipify. Our generated code matchesnative performancein\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduceCASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress inGPU compiler tooling, binary\ncompatibility, andLLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.View arXiv pageView PDFProject pageGitHub32Add to collectionCommunityahmedheaklPaper authorPaper submitterMay 28We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA \u2194 HIP) and assembly-level (Nvidia SASS \u2194 A",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/GustavoStahl/CASS",
    "hf_paper_url": "https://huggingface.co/papers/2505.16968",
    "arxiv_url": "https://arxiv.org/abs/2505.16968",
    "num_models": 7,
    "models_list": "ahmedheakl/cass-src-7B, ahmedheakl/cass-smA100-1.5b, ahmedheakl/cass-smA100-7b, ahmedheakl/cass-src-3b, ahmedheakl/cass-src-1.5b, ahmedheakl/cass-sm4090-3b, ahmedheakl/cass-sm4090-1.5b",
    "models_links": "https://huggingface.co/ahmedheakl/cass-src-7B, https://huggingface.co/ahmedheakl/cass-smA100-1.5b, https://huggingface.co/ahmedheakl/cass-smA100-7b, https://huggingface.co/ahmedheakl/cass-src-3b, https://huggingface.co/ahmedheakl/cass-src-1.5b, https://huggingface.co/ahmedheakl/cass-sm4090-3b, https://huggingface.co/ahmedheakl/cass-sm4090-1.5b",
    "models_detailed": "[{\"name\": \"ahmedheakl/cass-src-7B\", \"link\": \"https://huggingface.co/ahmedheakl/cass-src-7B\", \"task\": \"Text Generation\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"ahmedheakl/cass-smA100-1.5b\", \"link\": \"https://huggingface.co/ahmedheakl/cass-smA100-1.5b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"ahmedheakl/cass-smA100-7b\", \"link\": \"https://huggingface.co/ahmedheakl/cass-smA100-7b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"ahmedheakl/cass-src-3b\", \"link\": \"https://huggingface.co/ahmedheakl/cass-src-3b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 20\"}, {\"name\": \"ahmedheakl/cass-src-1.5b\", \"link\": \"https://huggingface.co/ahmedheakl/cass-src-1.5b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"ahmedheakl/cass-sm4090-3b\", \"link\": \"https://huggingface.co/ahmedheakl/cass-sm4090-3b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"ahmedheakl/cass-sm4090-1.5b\", \"link\": \"https://huggingface.co/ahmedheakl/cass-sm4090-1.5b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}]",
    "num_datasets": 1,
    "datasets_list": "MBZUAI/cass",
    "datasets_links": "https://huggingface.co/datasets/MBZUAI/cass",
    "datasets_detailed": "[{\"name\": \"MBZUAI/cass\", \"link\": \"https://huggingface.co/datasets/MBZUAI/cass\", \"task\": \"\", \"likes\": \"503\", \"downloads\": \"\", \"updated\": \"May 28\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.03107",
    "first_seen_date": "2025-06-04",
    "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid\n  Motions",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.03107ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid\n  MotionsPublished on Jun 3\u00b7Submitted byDi Changon Jun 4Upvote2Authors:Di Chang,Mingdeng Cao,Yichun Shi,Bo Liu,Shengqu Cai,Shijie Zhou,Weilin Huang,Gordon Wetzstein,Mohammad Soleymani,Peng WangAbstractByteMorph, a framework using the Diffusion Transformer, addresses non-rigid motion in image editing with a large-scale dataset and comprehensive evaluation.AI-generated summaryEditing images with instructions to reflect non-rigid motions, camera\nviewpoint shifts, object deformations, human articulations, and complex\ninteractions, poses a challenging yet underexplored problem in computer vision.\nExisting approaches and datasets predominantly focus on static scenes or rigid\ntransformations, limiting their capacity to handle expressive edits involving\ndynamic motion. To address this gap, we introduceByteMorph, a comprehensive\nframework forinstruction-based image editingwith an emphasis on non-rigid\nmotions.ByteMorphcomprises a large-scale dataset,ByteMorph-6M, and a strong\nbaseline model built upon theDiffusion Transformer(DiT), namedByteMorpher.ByteMorph-6Mincludes over 6 million high-resolution image editing pairs for\ntraining, along with a carefully curated evaluation benchmarkByteMorph-Bench.\nBoth capture a wide variety of non-rigid motion types across diverse\nenvironments, human figures, and object categories. The dataset is constructed\nusingmotion-guided data generation,layered compositingtechniques, andautomated captioningto ensure diversity, realism, and semantic coherence. We\nfurther conduct a comprehensive evaluation of recent instruction-based image\nediting methods from both academic and commercial domains.View arXiv pageView PDFProject pageGitHub43Add to collectionCommunityBoese0601Paper authorPaper submitterJun 5\u2022edited Jun 5We introduce ByteMorph, a comprehensive framework for instruction-bas",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/ByteDance-Seed/BM-code",
    "hf_paper_url": "https://huggingface.co/papers/2506.03107",
    "arxiv_url": "https://arxiv.org/abs/2506.03107",
    "num_models": 1,
    "models_list": "ByteDance-Seed/BM-Model",
    "models_links": "https://huggingface.co/ByteDance-Seed/BM-Model",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/BM-Model\", \"link\": \"https://huggingface.co/ByteDance-Seed/BM-Model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}]",
    "num_datasets": 3,
    "datasets_list": "ByteDance-Seed/BM-6M, ByteDance-Seed/BM-6M-Demo, ByteDance-Seed/BM-Bench",
    "datasets_links": "https://huggingface.co/datasets/ByteDance-Seed/BM-6M, https://huggingface.co/datasets/ByteDance-Seed/BM-6M-Demo, https://huggingface.co/datasets/ByteDance-Seed/BM-Bench",
    "datasets_detailed": "[{\"name\": \"ByteDance-Seed/BM-6M\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/BM-6M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\", \"size\": \"\"}, {\"name\": \"ByteDance-Seed/BM-6M-Demo\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/BM-6M-Demo\", \"task\": \"\", \"likes\": \"514\", \"downloads\": \"\", \"updated\": \"Sep 20\", \"size\": \"\"}, {\"name\": \"ByteDance-Seed/BM-Bench\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/BM-Bench\", \"task\": \"\", \"likes\": \"613\", \"downloads\": \"\", \"updated\": \"Jun 11\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.03136",
    "first_seen_date": "2025-06-04",
    "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.03136Co-Evolving LLM Coder and Unit Tester via Reinforcement LearningPublished on Jun 3\u00b7Submitted byLing Yangon Jun 4Upvote25+17Authors:Yinjie Wang,Ling Yang,Ye Tian,Ke Shen,Mengdi WangAbstractCURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.AI-generated summaryWe propose CURE, a novelreinforcement learningframework with a dedicatedreward designthat co-evolvescodingandunit test generationcapabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derivedReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization onQwen2.5-Instructmodels,\noutperforming similarly sizedQwen-Coder,DeepSeek-Coder, andSeed-Coder. They\nnaturally extend to downstream tasks such astest-time scalingand agenticcoding-achieving a 8.1% improvement over the base model. For thelong-CoTmodel, ourReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8%inference efficiencyinunit test generation. Notably, we also\nfind that our model can serve as an effectivereward modelfor reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CUREView arXiv pageView PDFProject pageGitHub144Add to collectionCommunityLingaaaaaaaPaper authorPaper submitterJun 4Github:https://github.com/Gen-Verse/CUREModel:https://huggingface.co/collections/Gen-Verse/reasonflux-coder-6833109ed9300c62deb32c6bSee translationReplylibrarian-botJun 5This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIIterative Self-training for Code Generati",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/Gen-Verse/CURE",
    "hf_paper_url": "https://huggingface.co/papers/2506.03136",
    "arxiv_url": "https://arxiv.org/abs/2506.03136",
    "num_models": 3,
    "models_list": "Gen-Verse/ReasonFlux-Coder-14B, Gen-Verse/ReasonFlux-Coder-7B, Gen-Verse/ReasonFlux-Coder-4B",
    "models_links": "https://huggingface.co/Gen-Verse/ReasonFlux-Coder-14B, https://huggingface.co/Gen-Verse/ReasonFlux-Coder-7B, https://huggingface.co/Gen-Verse/ReasonFlux-Coder-4B",
    "models_detailed": "[{\"name\": \"Gen-Verse/ReasonFlux-Coder-14B\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-Coder-14B\", \"task\": \"Text Generation\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"Gen-Verse/ReasonFlux-Coder-7B\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-Coder-7B\", \"task\": \"Text Generation\", \"likes\": \"279\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"Gen-Verse/ReasonFlux-Coder-4B\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-Coder-4B\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Jun 4\"}]",
    "num_datasets": 6,
    "datasets_list": "Gen-Verse/LiveBench-ReasonFlux, Gen-Verse/CodeForces, Gen-Verse/CodeContests_train, Gen-Verse/CodeContests, Gen-Verse/MBPP-ReasonFlux, Gen-Verse/LiveCodeBench-ReasonFlux",
    "datasets_links": "https://huggingface.co/datasets/Gen-Verse/LiveBench-ReasonFlux, https://huggingface.co/datasets/Gen-Verse/CodeForces, https://huggingface.co/datasets/Gen-Verse/CodeContests_train, https://huggingface.co/datasets/Gen-Verse/CodeContests, https://huggingface.co/datasets/Gen-Verse/MBPP-ReasonFlux, https://huggingface.co/datasets/Gen-Verse/LiveCodeBench-ReasonFlux",
    "datasets_detailed": "[{\"name\": \"Gen-Verse/LiveBench-ReasonFlux\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/LiveBench-ReasonFlux\", \"task\": \"\", \"likes\": \"128\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"Gen-Verse/CodeForces\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/CodeForces\", \"task\": \"\", \"likes\": \"467\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"Gen-Verse/CodeContests_train\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/CodeContests_train\", \"task\": \"\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"Gen-Verse/CodeContests\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/CodeContests\", \"task\": \"\", \"likes\": \"239\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"Gen-Verse/MBPP-ReasonFlux\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/MBPP-ReasonFlux\", \"task\": \"\", \"likes\": \"221\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"Gen-Verse/LiveCodeBench-ReasonFlux\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/LiveCodeBench-ReasonFlux\", \"task\": \"\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2506.00338",
    "first_seen_date": "2025-06-03",
    "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.00338OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  CleaningPublished on May 31\u00b7Submitted byYifan Pengon Jun 3Upvote10+2Authors:Yifan Peng,Shakeel Muhammad,Yui Sudo,William Chen,Jinchuan Tian,Chyi-Jiunn Lin,Shinji WatanabeAbstractThe OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.AI-generated summaryThe OpenWhisper-style Speech Models (OWSM) project has developed a series of\nfully open speech foundation models using academic-scale resources, but their\ntraining data remains insufficient. This work enhances OWSM by integratingYODAS, a large-scale web-crawled dataset with a Creative Commons license.\nHowever, incorporatingYODASis nontrivial due to its wild nature, which\nintroduces challenges such as incorrect language labels and audio-text\nmisalignments. To address this, we develop a scalabledata-cleaning pipelineusing public toolkits, yielding a dataset with 166,000 hours of speech across\n75 languages. Our new series of OWSM v4 models, trained on this curated dataset\nalongside existing OWSM data, significantly outperform previous versions onmultilingual benchmarks. Our models even match or surpass frontier industrial\nmodels likeWhisperandMMSin multiple scenarios. We will publicly release the\ncleanedYODASdata, pre-trained models, and all associated scripts via theESPnet toolkit.View arXiv pageView PDFProject pageGitHub9.65kAdd to collectionCommunitypyf98Paper authorPaper submitterJun 3The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/espnet/espnet",
    "hf_paper_url": "https://huggingface.co/papers/2506.00338",
    "arxiv_url": "https://arxiv.org/abs/2506.00338",
    "num_models": 2,
    "models_list": "espnet/owsm_v4_small_370M, espnet/owsm_v4_medium_1B",
    "models_links": "https://huggingface.co/espnet/owsm_v4_small_370M, https://huggingface.co/espnet/owsm_v4_medium_1B",
    "models_detailed": "[{\"name\": \"espnet/owsm_v4_small_370M\", \"link\": \"https://huggingface.co/espnet/owsm_v4_small_370M\", \"task\": \"\", \"likes\": \"43\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"espnet/owsm_v4_medium_1B\", \"link\": \"https://huggingface.co/espnet/owsm_v4_medium_1B\", \"task\": \"\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"Aug 30\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2506.01844",
    "first_seen_date": "2025-06-03",
    "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2506.01844SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  RoboticsPublished on Jun 2\u00b7Submitted byAndres Marafiotion Jun 3#2 Paper of the dayUpvote144+136Authors:Mustafa Shukor,Dana Aubakirova,Francesco Capuano,Pepijn Kooijmans,Steven Palma,Adil Zouitine,Michel Aractingi,Caroline Pascal,Martino Russi,Andres Marafioti,Simon Alibert,Matthieu Cord,Thomas Wolf,Remi CadeneAbstractSmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.AI-generated summaryVision-language models(VLMs) pretrained on large-scalemultimodal datasetsencode rich visual and linguistic knowledge, making them a strong foundation\nfor robotics. Rather than trainingrobotic policiesfrom scratch, recent\napproaches adapt VLMs into vision-language-action (VLA) models that enablenatural language-driven perceptionand control. However, existing VLAs are\ntypically massive--often with billions of parameters--leading to high training\ncosts and limited real-world deployability. Moreover, they rely on academic and\nindustrial datasets, overlooking the growing availability of\ncommunity-collected data from affordable robotic platforms. In this work, we\npresent SmolVLA, a small, efficient, and community-driven VLA that drastically\nreduces both training and inference costs, while retaining competitive\nperformance. SmolVLA is designed to be trained on a single GPU and deployed on\nconsumer-grade GPUs or even CPUs. To further improve responsiveness, we\nintroduce anasynchronous inferencestack decoupling perception and action\nprediction fromaction execution, allowing higher control rates with chunked\naction generation. Despite its compact size, SmolVLA achieves performance\ncomparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both\nsimulated as well as real-world robotic benchma",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2506,
    "github_repo": "https://github.com/huggingface/lerobot",
    "hf_paper_url": "https://huggingface.co/papers/2506.01844",
    "arxiv_url": "https://arxiv.org/abs/2506.01844",
    "num_models": 302,
    "models_list": "lerobot/smolvla_base, HuggingFaceVLA/smolvla_libero, ezzzeee/my_smolvla, jadechoghari/smolvla_metaworld, Beilinghamburger/smolvla_so100_vla, pepijn223/mobile_so100_test, rancheng222/smolvla_so101_tie_bag, jccj/smolvla_pickup_home_45k, jccj/smolvla_pickup_cube_full_res, jccj/smolvla_pickup_cube_full_res_last, jccj/smolvla_pickup_cube_resized_last, pepijn223/my_smolvla, juni3227/smolvla_practice_005_2500step, jccj/smolvla_shape_match5k_no_aug, jccj/smolvla_shape_match10k_no_aug, rtsmc/smolvla_box_in_bin_so101_test, danielkorth/smolvla-whiteboard-and-bike-light, jccj/match_shape2_5k_smolvla, jccj/match_shape2_10k_smolvla, kaku/smolvla-candy-pick, kaku/smolvla-candy-pick_trainigng_state, juni3227/smolvla_shell_20k, jccj/match_shape2_15k_smolvla, jccj/match_shape2_5k_smolvla_data_aug, jccj/match_shape2_20k_smolvla, jccj/match_shape3_15k_smolvla, jccj/match_shape3_10k_smolvla, jccj/match_shape3_30k_smolvla, jccj/match_shape3_25k_smolvla, jccj/match_shape4_5k_smolvla_from_25k_pretrained, jccj/match_shape4_10k_smolvla_from_25k_pretrained, hannanasko/smolvla_so_100_garlic, jccj/match_shape4_15k_smolvla_from_25k_pretrained, jccj/match_shape4_20k_smolvla_from_25k_pretrained, jccj/match_shape4_30k_smolvla_from_25k_pretrained, Mauro-Abidal-Carrer/10k_camTop_finetuned_smolVLA, ezzzeee/my_smolvla2, iellis02/smolvla_test, neilslt/smolvla_coffee_cleanup, oxhatestrading/my_smolvla1, reach-vb/smolvla-test-model, JipJ/smolvla_dice_project010000, pepijn223/my_policy21, pepijn223/my_policy22, pepijn223/my_policy23, pepijn223/my_policy25, pepijn223/my_policy28, pepijn223/my_policy29, pepijn223/my_policy30, JipJ/smolvla_dice_project, pepijn223/my_policy40, pepijn223/my_policy44, pepijn223/my_policy50, karimnihal/smolvla_panda_pick_cube, Damin3927/smolvla_pickplace_ft, HelloCephalopod/my_smolvla_1, taiobot/smolvla_so101_test8_trained, sleepingdrew/my_smolvla, yutaro-kimura-acs/so101_pp_blue_cube_policy, ncnynl/smolvla_model_repo, zakaria121212/smolvla-test, Infatoshi/smolvlav2, spesrobotics/wire_pick_place_multi_view_smolvla, Askel1419/so101_smolvla_cube_in_cup, yutaro-kimura-acs/so101_pp_blue_box_policy_01, pratapram/argolis_smolvla, pratapram/argolis_smolvla_2, astroyat/smolvla_paperball5, gribok201/smolvla_45k, sleepingdrew/smolvla_lerobot_lego_brick, TensorTemplar/smolvla-singlecam2, tak0325/two-object-vla-policy, T-Y-N/niryo_smolvla_model, ovo-bot/so101-arms, un1c0rnio/smolvla_so101_eraser_mat1, astroyat/smolvla_paperball2, fracapuano/smolvla_base, ub216/pickplace_8loc, zonglin11/lekiwi_pickup_tape_to_basket, incyvinvy/smolvla-grasp-test, yangfengzzz/smolvla_policy_20000, SSatoya/smolvla_glod_cube_grab, jian001/smolvla_so101_GRB1, incyvinvy/smolvla-colab-wandb, TobinH/foo_smolvla, Shirish24/smolvla_run_trossen_run4, DrainpipeAI/my_smolvla, zonglin11/lekiwi_pickup_tape_to_basket_2, ub216/stack_pickplace_chk_all_layers, junho232/my_record_test_policy, Shirish24/smolvla_run_trossen_run5, Edautel/my_smolvla_model, fracapuano/smolvla_async, incyvinvy/smolvla_colabtrained_wandb, lumal/move_pawn_forward, yangfengzzz/red_block_grasp_static_smolvla, cchenalds17/custom_aria_1dof_point_glasses, duys0304/local_training_model, zhengod/my_smolvla_train-test, ub216/so101_two_task, ases200q2/PandaPickCubeSpacemouseRandom2_smolvla, ncnynl/my_policy, junho232/orange_cube_black_box_policy, SSatoya/smolvla_pen_pick_and_place, helper2424/smolvla_check_async, chirag1701/smolvla_can_sandwich, shuohsuan/svla_collect_colors_r, gregor-pr/pick-up-bear, camilasfeijoo/my_smolvla_model, xybc/smalvla_test, jian001/smolvla_so101_GRB1orNo, yangfengzzz/yellow_block_grasp_vary_smolvla, taiobot/svla_socks, sean112/smolvla_robot_placing_policy, shuohsuan/svlab_collect_colors_rg, shuohsuan/svlab_collect_colors, shuohsuan/svlab_tune_colors, Zxlee1811/TestLuna, shuohsuan/svlab_collect_colors_rgb, YDY0427/smolvla_so101_test, zhenxuan/finetuned_smolvla_base, dinglx/smolvla_stacking_model, arclabmit/lx7r_smolvla_pickup_model, shuohsuan/svlab_collect_colors_rgbb, qownscks/testing1, helper2424/smolval_movet_to_blue_rtx, shuohsuan/svlab_collect_colors_rgbbs, tshiamor/my-smolvla-model, jian001/smolvla_so101_GRB1orDNT1m, aiden-li/so101-smolvla-picktape, lumal/move_pawn_overlay, camilasfeijoo/my_smolvla_model_sort, yestucan/smolvla_burger, abhifoo/tape_to_basket_1, oordonez/my_smolvla_policy, abhifoo/tape_to_basket_2, jhou/smolvla_pickplace, abhifoo/tape_to_basket_3, jian001/smolvla_so101_GRorGB1orDNT1mR, Shirish24/smolvla_run_trossen_run1.2, qownscks/test_start, pepijn223/smol_vla_special, bicmol/smolvla-libero, pepijn223/smol_vla_special1, abhifoo/tape_to_basket_new_embodiment, ygjlddsrx/my_smolvla, abhifoo/tape_to_basket_new_embodiment2k2, girardijp/sam_smolvla, dleon23/smolVLA_dual_so101, Deepkar/smolvla-test-redblock-5ksteps, abhifoo/tape_to_basket_new_embodiment3k_2, 1zsk/SO-101_Cube_pick_place_smolvla_model, pepijn223/smol_vla_special5, abhifoo/tape_to_basket_new_embodiment7k_3, nik658/xarm_smolvla, mvnagakishan/Smolvla_pick_place, nik658/xarm_smolvla_19jul, mvnagakishan/SMVLAPP, abhifoo/tape_to_basket_new_embodiment20k_4, Hartvi/smolvla, arpitg1304/smolvla_place_cylinder, Deepkar/smolvla-test-redblock-10ksteps, arminfg/test123, 9mxxb/test1, omkarmayekar555/smolvla_finetunning20july_nolookbrainthrow20july, wangguobin/my_smolvla_model, chasedreaminf/smolvla_so101_3cam-run2, nik658/ur10e_50ep_smol_pick, omkarmayekar555/smolvla_finetunning21july_marker_pickip_put_in_box, mbalabanski/mysmolvla, shuohsuan/svla_multi_collect_colors, shuohsuan/svla_multi_grasp_pos, leesangoh/smolvla_pickplace, Tank-123/tank-123, Amanpatel81/SMVLABK, hariharanr1799/smolvla_hebi_pick_and_drop, camilasfeijoo/my_policy, SuchangC/smolvla_719, masato-ka/SO100_generalize_pick_pos_smolvla, Sakits/smolvla_so101_pickplace_020000, sk1700/my_policy, qownscks/smolvla_first, attilczuk/bin_sim_smolvla_1, Askel1419/so101_smolvla_blue_cube, attilczuk/bin_sim_smolvla_2, SuchangC/smolvla_723, camilasfeijoo/my_smolvla_sortingblocks, ahad-j/smolvla_chess_pawn, jhou/smolvla_pickeraser_2, karimnihal/smolvla_so101_pen_cup_base, j1y2003/j1y2003, kjydb/lerobot_test_001, NLTuan/smolvla_pol, asyk454/smolvla_testFinetune_40k, Askel1419/act_so101_rtx_3_cube_no2, cchenalds17/svla_glasses_test0, asyk454/smolvla_throwaway, nik658/ur10e_50ep_smol_2cam, arpitg1304/smolvla_s101_stack_lego, yutaro-kimura-acs/so101_pp_blue_and_red_policy, masato-ka/SO100_act_generalize_pick_pos_smolvla_40k, cijerezg/pol1, cijerezg/push_policy, vivibruce/trained_model, asyk454/svla_glasses_20k, asyk454/svla_glasses_30k, sancov/smolvla_so101_test, cijerezg/push_policyv2, cijerezg/push_policyv3, bindels03/block_cup_smolvla1.0, easonjcc/my_biscuit_smolvla_model, observabot/smolvla_so101_die_mat4_b64_lr5e-4_cs100_nas100_robo, girardijp/sam_smolvla_better, Sahana16/my_smolvla, kjydb/lerobot_test_024, observabot/smolvla_so101_die_mat4_b64_lr5e-4_cs200_nas200_robo, kjydb/lerobot_test_025, norips/smolvla_lekiwi_orange_cube_box2, Askel1419/smolvla_so101_PLB_dataset, mbalabanski/smolvla, nikka-140/my_smolvla-by_box, shauryam75/smolVLA-2touch, LBST/t01_pick_and_place, Askel1419/smolvla_so101_100EP, zacapa/SO101_chess_policy4, camilasfeijoo/my_smolvla_sorter, LBST/t06_pick_and_place, nik658/ur10e_150ep_smol_2cam, easonjcc/my_blackobj_smolvla_model, oretti/so101_dice_5_smolvla_policy1, starf5/testrec7, shuohsuan/svla_multi_grasp_red_put_center, kjydb/lerobot_test_026, yestucan/smolvla_burger_v2, cHemingway/smolvla_move_purple_tape, jadechoghari/smolvla-libero-ckpts, kjydb/lerobot_test_028, starf5/so101PickPinkChoco_policy, norips/smolvla_lekiwi_orange_green_dual, Lakesenberg/None, ymatari/smolvla_so101_test, camilasfeijoo/my_smolvla_pressplace_sorted, TrossenRoboticsCommunity/bimanual_widowxai_handover_cube_smolvla, rhecker/my_smol_policy, Philmat/picko-v7_smolvla-0, observabot/smolvla_so101_cloth_folding1_b64_lr5e-4_cs100_nas100_robo, LBST/t08_pick_and_place_smolvla_10k, wuc1/ur5_test, ymatari/smolvla_so101_test2, kjydb/lerobot_test_035, hangxiangchen1/my_smolvla_experiment, YouXiangyu/SmolVLA, norips/smolvla_lekiwi_socks, camilasfeijoo/my_smolvla_pensort, adungus/smolVLA_200k, girardijp/testso101, cchenalds17/svla_3cam_place_glasses_in_hand_64_20k, DozenDucc/SmolVLAv2, qownscks/firstvla_bimanual, wuc1/ur5_test_v2, asyk454/svla_3cams_128_20k, cijerezg/push_policy_v101, bicmol/smolvla-libero-combined, ymatari/smolvla_so101_grab_dibble, asyk454/svla_3cam_64_40k, j1y2003/libero_10_25k, cchenalds17/svla_2cam_place_glasses_in_hand_64_20k, tsp12/test, girardijp/testso101_3, DozenDucc/smolvla3, LBST/t08_pick_and_place_20k, cijerezg/pick-up-policy-v2, abhifoo/tape_to_basket_new_embodiment20k_v3_1smolvla, asyk454/svla_2cam_128_40k, cchenalds17/svla_2cam_place_glasses_in_hand_64_40k, cchenalds17/svla_2cam_place_glasses_in_hand_128_20k, camilasfeijoo/my_smolvla_redblock, asyk454/svla_3cam_128_40k, crislmfroes/svla-panda-open-base-cabinet-sim, zacapa/SO101_chess_policy7, abhifoo/tape_to_basket_new_embodiment20k_v3_2smolvla, kjydb/lerobot_test_074, mvnagakishan/smolvla_Bxarm, kjydb/lerobot_test_082, pepijn223/thanos_picking_power_gem_migrated, crislmfroes/svla-panda-open-base-cabinet-sim-v2, attilczuk/sponge_colab, attilczuk/sponge1, crislmfroes/svla-panda-open-base-cabinet-sim-v3, qownscks/eggplant_bimanual, attilczuk/sponge_colab_20k, sk1700/my_policy03, kjydb/lerobot_model_001, annyi/my_smolvla_policy, Benxiaogu/my_smolvla, camilasfeijoo/my_smolvla_tape, crislmfroes/svla-panda-open-base-cabinet-sim-v4, koyomi1/smolvla, attilczuk/sponge_colab6, attilczuk/sponge_colab2_20k, Deepkar/redblock-pick-place-50ep-smolvla, crislmfroes/svla-panda-open-base-cabinet-sim-v5",
    "models_links": "https://huggingface.co/lerobot/smolvla_base, https://huggingface.co/HuggingFaceVLA/smolvla_libero, https://huggingface.co/ezzzeee/my_smolvla, https://huggingface.co/jadechoghari/smolvla_metaworld, https://huggingface.co/Beilinghamburger/smolvla_so100_vla, https://huggingface.co/pepijn223/mobile_so100_test, https://huggingface.co/rancheng222/smolvla_so101_tie_bag, https://huggingface.co/jccj/smolvla_pickup_home_45k, https://huggingface.co/jccj/smolvla_pickup_cube_full_res, https://huggingface.co/jccj/smolvla_pickup_cube_full_res_last, https://huggingface.co/jccj/smolvla_pickup_cube_resized_last, https://huggingface.co/pepijn223/my_smolvla, https://huggingface.co/juni3227/smolvla_practice_005_2500step, https://huggingface.co/jccj/smolvla_shape_match5k_no_aug, https://huggingface.co/jccj/smolvla_shape_match10k_no_aug, https://huggingface.co/rtsmc/smolvla_box_in_bin_so101_test, https://huggingface.co/danielkorth/smolvla-whiteboard-and-bike-light, https://huggingface.co/jccj/match_shape2_5k_smolvla, https://huggingface.co/jccj/match_shape2_10k_smolvla, https://huggingface.co/kaku/smolvla-candy-pick, https://huggingface.co/kaku/smolvla-candy-pick_trainigng_state, https://huggingface.co/juni3227/smolvla_shell_20k, https://huggingface.co/jccj/match_shape2_15k_smolvla, https://huggingface.co/jccj/match_shape2_5k_smolvla_data_aug, https://huggingface.co/jccj/match_shape2_20k_smolvla, https://huggingface.co/jccj/match_shape3_15k_smolvla, https://huggingface.co/jccj/match_shape3_10k_smolvla, https://huggingface.co/jccj/match_shape3_30k_smolvla, https://huggingface.co/jccj/match_shape3_25k_smolvla, https://huggingface.co/jccj/match_shape4_5k_smolvla_from_25k_pretrained, https://huggingface.co/jccj/match_shape4_10k_smolvla_from_25k_pretrained, https://huggingface.co/hannanasko/smolvla_so_100_garlic, https://huggingface.co/jccj/match_shape4_15k_smolvla_from_25k_pretrained, https://huggingface.co/jccj/match_shape4_20k_smolvla_from_25k_pretrained, https://huggingface.co/jccj/match_shape4_30k_smolvla_from_25k_pretrained, https://huggingface.co/Mauro-Abidal-Carrer/10k_camTop_finetuned_smolVLA, https://huggingface.co/ezzzeee/my_smolvla2, https://huggingface.co/iellis02/smolvla_test, https://huggingface.co/neilslt/smolvla_coffee_cleanup, https://huggingface.co/oxhatestrading/my_smolvla1, https://huggingface.co/reach-vb/smolvla-test-model, https://huggingface.co/JipJ/smolvla_dice_project010000, https://huggingface.co/pepijn223/my_policy21, https://huggingface.co/pepijn223/my_policy22, https://huggingface.co/pepijn223/my_policy23, https://huggingface.co/pepijn223/my_policy25, https://huggingface.co/pepijn223/my_policy28, https://huggingface.co/pepijn223/my_policy29, https://huggingface.co/pepijn223/my_policy30, https://huggingface.co/JipJ/smolvla_dice_project, https://huggingface.co/pepijn223/my_policy40, https://huggingface.co/pepijn223/my_policy44, https://huggingface.co/pepijn223/my_policy50, https://huggingface.co/karimnihal/smolvla_panda_pick_cube, https://huggingface.co/Damin3927/smolvla_pickplace_ft, https://huggingface.co/HelloCephalopod/my_smolvla_1, https://huggingface.co/taiobot/smolvla_so101_test8_trained, https://huggingface.co/sleepingdrew/my_smolvla, https://huggingface.co/yutaro-kimura-acs/so101_pp_blue_cube_policy, https://huggingface.co/ncnynl/smolvla_model_repo, https://huggingface.co/zakaria121212/smolvla-test, https://huggingface.co/Infatoshi/smolvlav2, https://huggingface.co/spesrobotics/wire_pick_place_multi_view_smolvla, https://huggingface.co/Askel1419/so101_smolvla_cube_in_cup, https://huggingface.co/yutaro-kimura-acs/so101_pp_blue_box_policy_01, https://huggingface.co/pratapram/argolis_smolvla, https://huggingface.co/pratapram/argolis_smolvla_2, https://huggingface.co/astroyat/smolvla_paperball5, https://huggingface.co/gribok201/smolvla_45k, https://huggingface.co/sleepingdrew/smolvla_lerobot_lego_brick, https://huggingface.co/TensorTemplar/smolvla-singlecam2, https://huggingface.co/tak0325/two-object-vla-policy, https://huggingface.co/T-Y-N/niryo_smolvla_model, https://huggingface.co/ovo-bot/so101-arms, https://huggingface.co/un1c0rnio/smolvla_so101_eraser_mat1, https://huggingface.co/astroyat/smolvla_paperball2, https://huggingface.co/fracapuano/smolvla_base, https://huggingface.co/ub216/pickplace_8loc, https://huggingface.co/zonglin11/lekiwi_pickup_tape_to_basket, https://huggingface.co/incyvinvy/smolvla-grasp-test, https://huggingface.co/yangfengzzz/smolvla_policy_20000, https://huggingface.co/SSatoya/smolvla_glod_cube_grab, https://huggingface.co/jian001/smolvla_so101_GRB1, https://huggingface.co/incyvinvy/smolvla-colab-wandb, https://huggingface.co/TobinH/foo_smolvla, https://huggingface.co/Shirish24/smolvla_run_trossen_run4, https://huggingface.co/DrainpipeAI/my_smolvla, https://huggingface.co/zonglin11/lekiwi_pickup_tape_to_basket_2, https://huggingface.co/ub216/stack_pickplace_chk_all_layers, https://huggingface.co/junho232/my_record_test_policy, https://huggingface.co/Shirish24/smolvla_run_trossen_run5, https://huggingface.co/Edautel/my_smolvla_model, https://huggingface.co/fracapuano/smolvla_async, https://huggingface.co/incyvinvy/smolvla_colabtrained_wandb, https://huggingface.co/lumal/move_pawn_forward, https://huggingface.co/yangfengzzz/red_block_grasp_static_smolvla, https://huggingface.co/cchenalds17/custom_aria_1dof_point_glasses, https://huggingface.co/duys0304/local_training_model, https://huggingface.co/zhengod/my_smolvla_train-test, https://huggingface.co/ub216/so101_two_task, https://huggingface.co/ases200q2/PandaPickCubeSpacemouseRandom2_smolvla, https://huggingface.co/ncnynl/my_policy, https://huggingface.co/junho232/orange_cube_black_box_policy, https://huggingface.co/SSatoya/smolvla_pen_pick_and_place, https://huggingface.co/helper2424/smolvla_check_async, https://huggingface.co/chirag1701/smolvla_can_sandwich, https://huggingface.co/shuohsuan/svla_collect_colors_r, https://huggingface.co/gregor-pr/pick-up-bear, https://huggingface.co/camilasfeijoo/my_smolvla_model, https://huggingface.co/xybc/smalvla_test, https://huggingface.co/jian001/smolvla_so101_GRB1orNo, https://huggingface.co/yangfengzzz/yellow_block_grasp_vary_smolvla, https://huggingface.co/taiobot/svla_socks, https://huggingface.co/sean112/smolvla_robot_placing_policy, https://huggingface.co/shuohsuan/svlab_collect_colors_rg, https://huggingface.co/shuohsuan/svlab_collect_colors, https://huggingface.co/shuohsuan/svlab_tune_colors, https://huggingface.co/Zxlee1811/TestLuna, https://huggingface.co/shuohsuan/svlab_collect_colors_rgb, https://huggingface.co/YDY0427/smolvla_so101_test, https://huggingface.co/zhenxuan/finetuned_smolvla_base, https://huggingface.co/dinglx/smolvla_stacking_model, https://huggingface.co/arclabmit/lx7r_smolvla_pickup_model, https://huggingface.co/shuohsuan/svlab_collect_colors_rgbb, https://huggingface.co/qownscks/testing1, https://huggingface.co/helper2424/smolval_movet_to_blue_rtx, https://huggingface.co/shuohsuan/svlab_collect_colors_rgbbs, https://huggingface.co/tshiamor/my-smolvla-model, https://huggingface.co/jian001/smolvla_so101_GRB1orDNT1m, https://huggingface.co/aiden-li/so101-smolvla-picktape, https://huggingface.co/lumal/move_pawn_overlay, https://huggingface.co/camilasfeijoo/my_smolvla_model_sort, https://huggingface.co/yestucan/smolvla_burger, https://huggingface.co/abhifoo/tape_to_basket_1, https://huggingface.co/oordonez/my_smolvla_policy, https://huggingface.co/abhifoo/tape_to_basket_2, https://huggingface.co/jhou/smolvla_pickplace, https://huggingface.co/abhifoo/tape_to_basket_3, https://huggingface.co/jian001/smolvla_so101_GRorGB1orDNT1mR, https://huggingface.co/Shirish24/smolvla_run_trossen_run1.2, https://huggingface.co/qownscks/test_start, https://huggingface.co/pepijn223/smol_vla_special, https://huggingface.co/bicmol/smolvla-libero, https://huggingface.co/pepijn223/smol_vla_special1, https://huggingface.co/abhifoo/tape_to_basket_new_embodiment, https://huggingface.co/ygjlddsrx/my_smolvla, https://huggingface.co/abhifoo/tape_to_basket_new_embodiment2k2, https://huggingface.co/girardijp/sam_smolvla, https://huggingface.co/dleon23/smolVLA_dual_so101, https://huggingface.co/Deepkar/smolvla-test-redblock-5ksteps, https://huggingface.co/abhifoo/tape_to_basket_new_embodiment3k_2, https://huggingface.co/1zsk/SO-101_Cube_pick_place_smolvla_model, https://huggingface.co/pepijn223/smol_vla_special5, https://huggingface.co/abhifoo/tape_to_basket_new_embodiment7k_3, https://huggingface.co/nik658/xarm_smolvla, https://huggingface.co/mvnagakishan/Smolvla_pick_place, https://huggingface.co/nik658/xarm_smolvla_19jul, https://huggingface.co/mvnagakishan/SMVLAPP, https://huggingface.co/abhifoo/tape_to_basket_new_embodiment20k_4, https://huggingface.co/Hartvi/smolvla, https://huggingface.co/arpitg1304/smolvla_place_cylinder, https://huggingface.co/Deepkar/smolvla-test-redblock-10ksteps, https://huggingface.co/arminfg/test123, https://huggingface.co/9mxxb/test1, https://huggingface.co/omkarmayekar555/smolvla_finetunning20july_nolookbrainthrow20july, https://huggingface.co/wangguobin/my_smolvla_model, https://huggingface.co/chasedreaminf/smolvla_so101_3cam-run2, https://huggingface.co/nik658/ur10e_50ep_smol_pick, https://huggingface.co/omkarmayekar555/smolvla_finetunning21july_marker_pickip_put_in_box, https://huggingface.co/mbalabanski/mysmolvla, https://huggingface.co/shuohsuan/svla_multi_collect_colors, https://huggingface.co/shuohsuan/svla_multi_grasp_pos, https://huggingface.co/leesangoh/smolvla_pickplace, https://huggingface.co/Tank-123/tank-123, https://huggingface.co/Amanpatel81/SMVLABK, https://huggingface.co/hariharanr1799/smolvla_hebi_pick_and_drop, https://huggingface.co/camilasfeijoo/my_policy, https://huggingface.co/SuchangC/smolvla_719, https://huggingface.co/masato-ka/SO100_generalize_pick_pos_smolvla, https://huggingface.co/Sakits/smolvla_so101_pickplace_020000, https://huggingface.co/sk1700/my_policy, https://huggingface.co/qownscks/smolvla_first, https://huggingface.co/attilczuk/bin_sim_smolvla_1, https://huggingface.co/Askel1419/so101_smolvla_blue_cube, https://huggingface.co/attilczuk/bin_sim_smolvla_2, https://huggingface.co/SuchangC/smolvla_723, https://huggingface.co/camilasfeijoo/my_smolvla_sortingblocks, https://huggingface.co/ahad-j/smolvla_chess_pawn, https://huggingface.co/jhou/smolvla_pickeraser_2, https://huggingface.co/karimnihal/smolvla_so101_pen_cup_base, https://huggingface.co/j1y2003/j1y2003, https://huggingface.co/kjydb/lerobot_test_001, https://huggingface.co/NLTuan/smolvla_pol, https://huggingface.co/asyk454/smolvla_testFinetune_40k, https://huggingface.co/Askel1419/act_so101_rtx_3_cube_no2, https://huggingface.co/cchenalds17/svla_glasses_test0, https://huggingface.co/asyk454/smolvla_throwaway, https://huggingface.co/nik658/ur10e_50ep_smol_2cam, https://huggingface.co/arpitg1304/smolvla_s101_stack_lego, https://huggingface.co/yutaro-kimura-acs/so101_pp_blue_and_red_policy, https://huggingface.co/masato-ka/SO100_act_generalize_pick_pos_smolvla_40k, https://huggingface.co/cijerezg/pol1, https://huggingface.co/cijerezg/push_policy, https://huggingface.co/vivibruce/trained_model, https://huggingface.co/asyk454/svla_glasses_20k, https://huggingface.co/asyk454/svla_glasses_30k, https://huggingface.co/sancov/smolvla_so101_test, https://huggingface.co/cijerezg/push_policyv2, https://huggingface.co/cijerezg/push_policyv3, https://huggingface.co/bindels03/block_cup_smolvla1.0, https://huggingface.co/easonjcc/my_biscuit_smolvla_model, https://huggingface.co/observabot/smolvla_so101_die_mat4_b64_lr5e-4_cs100_nas100_robo, https://huggingface.co/girardijp/sam_smolvla_better, https://huggingface.co/Sahana16/my_smolvla, https://huggingface.co/kjydb/lerobot_test_024, https://huggingface.co/observabot/smolvla_so101_die_mat4_b64_lr5e-4_cs200_nas200_robo, https://huggingface.co/kjydb/lerobot_test_025, https://huggingface.co/norips/smolvla_lekiwi_orange_cube_box2, https://huggingface.co/Askel1419/smolvla_so101_PLB_dataset, https://huggingface.co/mbalabanski/smolvla, https://huggingface.co/nikka-140/my_smolvla-by_box, https://huggingface.co/shauryam75/smolVLA-2touch, https://huggingface.co/LBST/t01_pick_and_place, https://huggingface.co/Askel1419/smolvla_so101_100EP, https://huggingface.co/zacapa/SO101_chess_policy4, https://huggingface.co/camilasfeijoo/my_smolvla_sorter, https://huggingface.co/LBST/t06_pick_and_place, https://huggingface.co/nik658/ur10e_150ep_smol_2cam, https://huggingface.co/easonjcc/my_blackobj_smolvla_model, https://huggingface.co/oretti/so101_dice_5_smolvla_policy1, https://huggingface.co/starf5/testrec7, https://huggingface.co/shuohsuan/svla_multi_grasp_red_put_center, https://huggingface.co/kjydb/lerobot_test_026, https://huggingface.co/yestucan/smolvla_burger_v2, https://huggingface.co/cHemingway/smolvla_move_purple_tape, https://huggingface.co/jadechoghari/smolvla-libero-ckpts, https://huggingface.co/kjydb/lerobot_test_028, https://huggingface.co/starf5/so101PickPinkChoco_policy, https://huggingface.co/norips/smolvla_lekiwi_orange_green_dual, https://huggingface.co/Lakesenberg/None, https://huggingface.co/ymatari/smolvla_so101_test, https://huggingface.co/camilasfeijoo/my_smolvla_pressplace_sorted, https://huggingface.co/TrossenRoboticsCommunity/bimanual_widowxai_handover_cube_smolvla, https://huggingface.co/rhecker/my_smol_policy, https://huggingface.co/Philmat/picko-v7_smolvla-0, https://huggingface.co/observabot/smolvla_so101_cloth_folding1_b64_lr5e-4_cs100_nas100_robo, https://huggingface.co/LBST/t08_pick_and_place_smolvla_10k, https://huggingface.co/wuc1/ur5_test, https://huggingface.co/ymatari/smolvla_so101_test2, https://huggingface.co/kjydb/lerobot_test_035, https://huggingface.co/hangxiangchen1/my_smolvla_experiment, https://huggingface.co/YouXiangyu/SmolVLA, https://huggingface.co/norips/smolvla_lekiwi_socks, https://huggingface.co/camilasfeijoo/my_smolvla_pensort, https://huggingface.co/adungus/smolVLA_200k, https://huggingface.co/girardijp/testso101, https://huggingface.co/cchenalds17/svla_3cam_place_glasses_in_hand_64_20k, https://huggingface.co/DozenDucc/SmolVLAv2, https://huggingface.co/qownscks/firstvla_bimanual, https://huggingface.co/wuc1/ur5_test_v2, https://huggingface.co/asyk454/svla_3cams_128_20k, https://huggingface.co/cijerezg/push_policy_v101, https://huggingface.co/bicmol/smolvla-libero-combined, https://huggingface.co/ymatari/smolvla_so101_grab_dibble, https://huggingface.co/asyk454/svla_3cam_64_40k, https://huggingface.co/j1y2003/libero_10_25k, https://huggingface.co/cchenalds17/svla_2cam_place_glasses_in_hand_64_20k, https://huggingface.co/tsp12/test, https://huggingface.co/girardijp/testso101_3, https://huggingface.co/DozenDucc/smolvla3, https://huggingface.co/LBST/t08_pick_and_place_20k, https://huggingface.co/cijerezg/pick-up-policy-v2, https://huggingface.co/abhifoo/tape_to_basket_new_embodiment20k_v3_1smolvla, https://huggingface.co/asyk454/svla_2cam_128_40k, https://huggingface.co/cchenalds17/svla_2cam_place_glasses_in_hand_64_40k, https://huggingface.co/cchenalds17/svla_2cam_place_glasses_in_hand_128_20k, https://huggingface.co/camilasfeijoo/my_smolvla_redblock, https://huggingface.co/asyk454/svla_3cam_128_40k, https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim, https://huggingface.co/zacapa/SO101_chess_policy7, https://huggingface.co/abhifoo/tape_to_basket_new_embodiment20k_v3_2smolvla, https://huggingface.co/kjydb/lerobot_test_074, https://huggingface.co/mvnagakishan/smolvla_Bxarm, https://huggingface.co/kjydb/lerobot_test_082, https://huggingface.co/pepijn223/thanos_picking_power_gem_migrated, https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim-v2, https://huggingface.co/attilczuk/sponge_colab, https://huggingface.co/attilczuk/sponge1, https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim-v3, https://huggingface.co/qownscks/eggplant_bimanual, https://huggingface.co/attilczuk/sponge_colab_20k, https://huggingface.co/sk1700/my_policy03, https://huggingface.co/kjydb/lerobot_model_001, https://huggingface.co/annyi/my_smolvla_policy, https://huggingface.co/Benxiaogu/my_smolvla, https://huggingface.co/camilasfeijoo/my_smolvla_tape, https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim-v4, https://huggingface.co/koyomi1/smolvla, https://huggingface.co/attilczuk/sponge_colab6, https://huggingface.co/attilczuk/sponge_colab2_20k, https://huggingface.co/Deepkar/redblock-pick-place-50ep-smolvla, https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim-v5",
    "models_detailed": "[{\"name\": \"lerobot/smolvla_base\", \"link\": \"https://huggingface.co/lerobot/smolvla_base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 10\"}, {\"name\": \"HuggingFaceVLA/smolvla_libero\", \"link\": \"https://huggingface.co/HuggingFaceVLA/smolvla_libero\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"ezzzeee/my_smolvla\", \"link\": \"https://huggingface.co/ezzzeee/my_smolvla\", \"task\": \"\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"jadechoghari/smolvla_metaworld\", \"link\": \"https://huggingface.co/jadechoghari/smolvla_metaworld\", \"task\": \"\", \"likes\": \"393\", \"downloads\": \"\", \"updated\": \"Oct 5\"}, {\"name\": \"Beilinghamburger/smolvla_so100_vla\", \"link\": \"https://huggingface.co/Beilinghamburger/smolvla_so100_vla\", \"task\": \"\", \"likes\": \"78\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"pepijn223/mobile_so100_test\", \"link\": \"https://huggingface.co/pepijn223/mobile_so100_test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"rancheng222/smolvla_so101_tie_bag\", \"link\": \"https://huggingface.co/rancheng222/smolvla_so101_tie_bag\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"jccj/smolvla_pickup_home_45k\", \"link\": \"https://huggingface.co/jccj/smolvla_pickup_home_45k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/smolvla_pickup_cube_full_res\", \"link\": \"https://huggingface.co/jccj/smolvla_pickup_cube_full_res\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/smolvla_pickup_cube_full_res_last\", \"link\": \"https://huggingface.co/jccj/smolvla_pickup_cube_full_res_last\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/smolvla_pickup_cube_resized_last\", \"link\": \"https://huggingface.co/jccj/smolvla_pickup_cube_resized_last\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"pepijn223/my_smolvla\", \"link\": \"https://huggingface.co/pepijn223/my_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"juni3227/smolvla_practice_005_2500step\", \"link\": \"https://huggingface.co/juni3227/smolvla_practice_005_2500step\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"jccj/smolvla_shape_match5k_no_aug\", \"link\": \"https://huggingface.co/jccj/smolvla_shape_match5k_no_aug\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/smolvla_shape_match10k_no_aug\", \"link\": \"https://huggingface.co/jccj/smolvla_shape_match10k_no_aug\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"rtsmc/smolvla_box_in_bin_so101_test\", \"link\": \"https://huggingface.co/rtsmc/smolvla_box_in_bin_so101_test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"danielkorth/smolvla-whiteboard-and-bike-light\", \"link\": \"https://huggingface.co/danielkorth/smolvla-whiteboard-and-bike-light\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"jccj/match_shape2_5k_smolvla\", \"link\": \"https://huggingface.co/jccj/match_shape2_5k_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape2_10k_smolvla\", \"link\": \"https://huggingface.co/jccj/match_shape2_10k_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"kaku/smolvla-candy-pick\", \"link\": \"https://huggingface.co/kaku/smolvla-candy-pick\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"kaku/smolvla-candy-pick_trainigng_state\", \"link\": \"https://huggingface.co/kaku/smolvla-candy-pick_trainigng_state\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"juni3227/smolvla_shell_20k\", \"link\": \"https://huggingface.co/juni3227/smolvla_shell_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"jccj/match_shape2_15k_smolvla\", \"link\": \"https://huggingface.co/jccj/match_shape2_15k_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape2_5k_smolvla_data_aug\", \"link\": \"https://huggingface.co/jccj/match_shape2_5k_smolvla_data_aug\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape2_20k_smolvla\", \"link\": \"https://huggingface.co/jccj/match_shape2_20k_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape3_15k_smolvla\", \"link\": \"https://huggingface.co/jccj/match_shape3_15k_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape3_10k_smolvla\", \"link\": \"https://huggingface.co/jccj/match_shape3_10k_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape3_30k_smolvla\", \"link\": \"https://huggingface.co/jccj/match_shape3_30k_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape3_25k_smolvla\", \"link\": \"https://huggingface.co/jccj/match_shape3_25k_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape4_5k_smolvla_from_25k_pretrained\", \"link\": \"https://huggingface.co/jccj/match_shape4_5k_smolvla_from_25k_pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape4_10k_smolvla_from_25k_pretrained\", \"link\": \"https://huggingface.co/jccj/match_shape4_10k_smolvla_from_25k_pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"hannanasko/smolvla_so_100_garlic\", \"link\": \"https://huggingface.co/hannanasko/smolvla_so_100_garlic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"jccj/match_shape4_15k_smolvla_from_25k_pretrained\", \"link\": \"https://huggingface.co/jccj/match_shape4_15k_smolvla_from_25k_pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape4_20k_smolvla_from_25k_pretrained\", \"link\": \"https://huggingface.co/jccj/match_shape4_20k_smolvla_from_25k_pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"jccj/match_shape4_30k_smolvla_from_25k_pretrained\", \"link\": \"https://huggingface.co/jccj/match_shape4_30k_smolvla_from_25k_pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"Mauro-Abidal-Carrer/10k_camTop_finetuned_smolVLA\", \"link\": \"https://huggingface.co/Mauro-Abidal-Carrer/10k_camTop_finetuned_smolVLA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"ezzzeee/my_smolvla2\", \"link\": \"https://huggingface.co/ezzzeee/my_smolvla2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"iellis02/smolvla_test\", \"link\": \"https://huggingface.co/iellis02/smolvla_test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"neilslt/smolvla_coffee_cleanup\", \"link\": \"https://huggingface.co/neilslt/smolvla_coffee_cleanup\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 16\"}, {\"name\": \"oxhatestrading/my_smolvla1\", \"link\": \"https://huggingface.co/oxhatestrading/my_smolvla1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"reach-vb/smolvla-test-model\", \"link\": \"https://huggingface.co/reach-vb/smolvla-test-model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 23\"}, {\"name\": \"JipJ/smolvla_dice_project010000\", \"link\": \"https://huggingface.co/JipJ/smolvla_dice_project010000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy21\", \"link\": \"https://huggingface.co/pepijn223/my_policy21\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy22\", \"link\": \"https://huggingface.co/pepijn223/my_policy22\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy23\", \"link\": \"https://huggingface.co/pepijn223/my_policy23\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy25\", \"link\": \"https://huggingface.co/pepijn223/my_policy25\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy28\", \"link\": \"https://huggingface.co/pepijn223/my_policy28\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy29\", \"link\": \"https://huggingface.co/pepijn223/my_policy29\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy30\", \"link\": \"https://huggingface.co/pepijn223/my_policy30\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"JipJ/smolvla_dice_project\", \"link\": \"https://huggingface.co/JipJ/smolvla_dice_project\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy40\", \"link\": \"https://huggingface.co/pepijn223/my_policy40\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy44\", \"link\": \"https://huggingface.co/pepijn223/my_policy44\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"pepijn223/my_policy50\", \"link\": \"https://huggingface.co/pepijn223/my_policy50\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"karimnihal/smolvla_panda_pick_cube\", \"link\": \"https://huggingface.co/karimnihal/smolvla_panda_pick_cube\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 27\"}, {\"name\": \"Damin3927/smolvla_pickplace_ft\", \"link\": \"https://huggingface.co/Damin3927/smolvla_pickplace_ft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"HelloCephalopod/my_smolvla_1\", \"link\": \"https://huggingface.co/HelloCephalopod/my_smolvla_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"taiobot/smolvla_so101_test8_trained\", \"link\": \"https://huggingface.co/taiobot/smolvla_so101_test8_trained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 29\"}, {\"name\": \"sleepingdrew/my_smolvla\", \"link\": \"https://huggingface.co/sleepingdrew/my_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 29\"}, {\"name\": \"yutaro-kimura-acs/so101_pp_blue_cube_policy\", \"link\": \"https://huggingface.co/yutaro-kimura-acs/so101_pp_blue_cube_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"ncnynl/smolvla_model_repo\", \"link\": \"https://huggingface.co/ncnynl/smolvla_model_repo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"zakaria121212/smolvla-test\", \"link\": \"https://huggingface.co/zakaria121212/smolvla-test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 4\"}, {\"name\": \"Infatoshi/smolvlav2\", \"link\": \"https://huggingface.co/Infatoshi/smolvlav2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 3\"}, {\"name\": \"spesrobotics/wire_pick_place_multi_view_smolvla\", \"link\": \"https://huggingface.co/spesrobotics/wire_pick_place_multi_view_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 3\"}, {\"name\": \"Askel1419/so101_smolvla_cube_in_cup\", \"link\": \"https://huggingface.co/Askel1419/so101_smolvla_cube_in_cup\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"yutaro-kimura-acs/so101_pp_blue_box_policy_01\", \"link\": \"https://huggingface.co/yutaro-kimura-acs/so101_pp_blue_box_policy_01\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 5\"}, {\"name\": \"pratapram/argolis_smolvla\", \"link\": \"https://huggingface.co/pratapram/argolis_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 5\"}, {\"name\": \"pratapram/argolis_smolvla_2\", \"link\": \"https://huggingface.co/pratapram/argolis_smolvla_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 5\"}, {\"name\": \"astroyat/smolvla_paperball5\", \"link\": \"https://huggingface.co/astroyat/smolvla_paperball5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\"}, {\"name\": \"gribok201/smolvla_45k\", \"link\": \"https://huggingface.co/gribok201/smolvla_45k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\"}, {\"name\": \"sleepingdrew/smolvla_lerobot_lego_brick\", \"link\": \"https://huggingface.co/sleepingdrew/smolvla_lerobot_lego_brick\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\"}, {\"name\": \"TensorTemplar/smolvla-singlecam2\", \"link\": \"https://huggingface.co/TensorTemplar/smolvla-singlecam2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\"}, {\"name\": \"tak0325/two-object-vla-policy\", \"link\": \"https://huggingface.co/tak0325/two-object-vla-policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\"}, {\"name\": \"T-Y-N/niryo_smolvla_model\", \"link\": \"https://huggingface.co/T-Y-N/niryo_smolvla_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"ovo-bot/so101-arms\", \"link\": \"https://huggingface.co/ovo-bot/so101-arms\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\"}, {\"name\": \"un1c0rnio/smolvla_so101_eraser_mat1\", \"link\": \"https://huggingface.co/un1c0rnio/smolvla_so101_eraser_mat1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"astroyat/smolvla_paperball2\", \"link\": \"https://huggingface.co/astroyat/smolvla_paperball2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"fracapuano/smolvla_base\", \"link\": \"https://huggingface.co/fracapuano/smolvla_base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"ub216/pickplace_8loc\", \"link\": \"https://huggingface.co/ub216/pickplace_8loc\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"zonglin11/lekiwi_pickup_tape_to_basket\", \"link\": \"https://huggingface.co/zonglin11/lekiwi_pickup_tape_to_basket\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"incyvinvy/smolvla-grasp-test\", \"link\": \"https://huggingface.co/incyvinvy/smolvla-grasp-test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"yangfengzzz/smolvla_policy_20000\", \"link\": \"https://huggingface.co/yangfengzzz/smolvla_policy_20000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"SSatoya/smolvla_glod_cube_grab\", \"link\": \"https://huggingface.co/SSatoya/smolvla_glod_cube_grab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"jian001/smolvla_so101_GRB1\", \"link\": \"https://huggingface.co/jian001/smolvla_so101_GRB1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"incyvinvy/smolvla-colab-wandb\", \"link\": \"https://huggingface.co/incyvinvy/smolvla-colab-wandb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"TobinH/foo_smolvla\", \"link\": \"https://huggingface.co/TobinH/foo_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"Shirish24/smolvla_run_trossen_run4\", \"link\": \"https://huggingface.co/Shirish24/smolvla_run_trossen_run4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"DrainpipeAI/my_smolvla\", \"link\": \"https://huggingface.co/DrainpipeAI/my_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"zonglin11/lekiwi_pickup_tape_to_basket_2\", \"link\": \"https://huggingface.co/zonglin11/lekiwi_pickup_tape_to_basket_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"ub216/stack_pickplace_chk_all_layers\", \"link\": \"https://huggingface.co/ub216/stack_pickplace_chk_all_layers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"junho232/my_record_test_policy\", \"link\": \"https://huggingface.co/junho232/my_record_test_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"Shirish24/smolvla_run_trossen_run5\", \"link\": \"https://huggingface.co/Shirish24/smolvla_run_trossen_run5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"Edautel/my_smolvla_model\", \"link\": \"https://huggingface.co/Edautel/my_smolvla_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"fracapuano/smolvla_async\", \"link\": \"https://huggingface.co/fracapuano/smolvla_async\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 17\"}, {\"name\": \"incyvinvy/smolvla_colabtrained_wandb\", \"link\": \"https://huggingface.co/incyvinvy/smolvla_colabtrained_wandb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"lumal/move_pawn_forward\", \"link\": \"https://huggingface.co/lumal/move_pawn_forward\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"yangfengzzz/red_block_grasp_static_smolvla\", \"link\": \"https://huggingface.co/yangfengzzz/red_block_grasp_static_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"cchenalds17/custom_aria_1dof_point_glasses\", \"link\": \"https://huggingface.co/cchenalds17/custom_aria_1dof_point_glasses\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"duys0304/local_training_model\", \"link\": \"https://huggingface.co/duys0304/local_training_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"zhengod/my_smolvla_train-test\", \"link\": \"https://huggingface.co/zhengod/my_smolvla_train-test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"ub216/so101_two_task\", \"link\": \"https://huggingface.co/ub216/so101_two_task\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"ases200q2/PandaPickCubeSpacemouseRandom2_smolvla\", \"link\": \"https://huggingface.co/ases200q2/PandaPickCubeSpacemouseRandom2_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"ncnynl/my_policy\", \"link\": \"https://huggingface.co/ncnynl/my_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"junho232/orange_cube_black_box_policy\", \"link\": \"https://huggingface.co/junho232/orange_cube_black_box_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"SSatoya/smolvla_pen_pick_and_place\", \"link\": \"https://huggingface.co/SSatoya/smolvla_pen_pick_and_place\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"helper2424/smolvla_check_async\", \"link\": \"https://huggingface.co/helper2424/smolvla_check_async\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"chirag1701/smolvla_can_sandwich\", \"link\": \"https://huggingface.co/chirag1701/smolvla_can_sandwich\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"shuohsuan/svla_collect_colors_r\", \"link\": \"https://huggingface.co/shuohsuan/svla_collect_colors_r\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"gregor-pr/pick-up-bear\", \"link\": \"https://huggingface.co/gregor-pr/pick-up-bear\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"camilasfeijoo/my_smolvla_model\", \"link\": \"https://huggingface.co/camilasfeijoo/my_smolvla_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"xybc/smalvla_test\", \"link\": \"https://huggingface.co/xybc/smalvla_test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"jian001/smolvla_so101_GRB1orNo\", \"link\": \"https://huggingface.co/jian001/smolvla_so101_GRB1orNo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"yangfengzzz/yellow_block_grasp_vary_smolvla\", \"link\": \"https://huggingface.co/yangfengzzz/yellow_block_grasp_vary_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"taiobot/svla_socks\", \"link\": \"https://huggingface.co/taiobot/svla_socks\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"sean112/smolvla_robot_placing_policy\", \"link\": \"https://huggingface.co/sean112/smolvla_robot_placing_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"shuohsuan/svlab_collect_colors_rg\", \"link\": \"https://huggingface.co/shuohsuan/svlab_collect_colors_rg\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"shuohsuan/svlab_collect_colors\", \"link\": \"https://huggingface.co/shuohsuan/svlab_collect_colors\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"shuohsuan/svlab_tune_colors\", \"link\": \"https://huggingface.co/shuohsuan/svlab_tune_colors\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"Zxlee1811/TestLuna\", \"link\": \"https://huggingface.co/Zxlee1811/TestLuna\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"shuohsuan/svlab_collect_colors_rgb\", \"link\": \"https://huggingface.co/shuohsuan/svlab_collect_colors_rgb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"YDY0427/smolvla_so101_test\", \"link\": \"https://huggingface.co/YDY0427/smolvla_so101_test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"zhenxuan/finetuned_smolvla_base\", \"link\": \"https://huggingface.co/zhenxuan/finetuned_smolvla_base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"dinglx/smolvla_stacking_model\", \"link\": \"https://huggingface.co/dinglx/smolvla_stacking_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"arclabmit/lx7r_smolvla_pickup_model\", \"link\": \"https://huggingface.co/arclabmit/lx7r_smolvla_pickup_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 9\"}, {\"name\": \"shuohsuan/svlab_collect_colors_rgbb\", \"link\": \"https://huggingface.co/shuohsuan/svlab_collect_colors_rgbb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"qownscks/testing1\", \"link\": \"https://huggingface.co/qownscks/testing1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"helper2424/smolval_movet_to_blue_rtx\", \"link\": \"https://huggingface.co/helper2424/smolval_movet_to_blue_rtx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"shuohsuan/svlab_collect_colors_rgbbs\", \"link\": \"https://huggingface.co/shuohsuan/svlab_collect_colors_rgbbs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"tshiamor/my-smolvla-model\", \"link\": \"https://huggingface.co/tshiamor/my-smolvla-model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"jian001/smolvla_so101_GRB1orDNT1m\", \"link\": \"https://huggingface.co/jian001/smolvla_so101_GRB1orDNT1m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"aiden-li/so101-smolvla-picktape\", \"link\": \"https://huggingface.co/aiden-li/so101-smolvla-picktape\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"lumal/move_pawn_overlay\", \"link\": \"https://huggingface.co/lumal/move_pawn_overlay\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"camilasfeijoo/my_smolvla_model_sort\", \"link\": \"https://huggingface.co/camilasfeijoo/my_smolvla_model_sort\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"yestucan/smolvla_burger\", \"link\": \"https://huggingface.co/yestucan/smolvla_burger\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"abhifoo/tape_to_basket_1\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"oordonez/my_smolvla_policy\", \"link\": \"https://huggingface.co/oordonez/my_smolvla_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"abhifoo/tape_to_basket_2\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"jhou/smolvla_pickplace\", \"link\": \"https://huggingface.co/jhou/smolvla_pickplace\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"abhifoo/tape_to_basket_3\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"jian001/smolvla_so101_GRorGB1orDNT1mR\", \"link\": \"https://huggingface.co/jian001/smolvla_so101_GRorGB1orDNT1mR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"Shirish24/smolvla_run_trossen_run1.2\", \"link\": \"https://huggingface.co/Shirish24/smolvla_run_trossen_run1.2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"qownscks/test_start\", \"link\": \"https://huggingface.co/qownscks/test_start\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"pepijn223/smol_vla_special\", \"link\": \"https://huggingface.co/pepijn223/smol_vla_special\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"bicmol/smolvla-libero\", \"link\": \"https://huggingface.co/bicmol/smolvla-libero\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"pepijn223/smol_vla_special1\", \"link\": \"https://huggingface.co/pepijn223/smol_vla_special1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"abhifoo/tape_to_basket_new_embodiment\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_new_embodiment\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"ygjlddsrx/my_smolvla\", \"link\": \"https://huggingface.co/ygjlddsrx/my_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"abhifoo/tape_to_basket_new_embodiment2k2\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_new_embodiment2k2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"girardijp/sam_smolvla\", \"link\": \"https://huggingface.co/girardijp/sam_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"dleon23/smolVLA_dual_so101\", \"link\": \"https://huggingface.co/dleon23/smolVLA_dual_so101\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"Deepkar/smolvla-test-redblock-5ksteps\", \"link\": \"https://huggingface.co/Deepkar/smolvla-test-redblock-5ksteps\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"abhifoo/tape_to_basket_new_embodiment3k_2\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_new_embodiment3k_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"1zsk/SO-101_Cube_pick_place_smolvla_model\", \"link\": \"https://huggingface.co/1zsk/SO-101_Cube_pick_place_smolvla_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"pepijn223/smol_vla_special5\", \"link\": \"https://huggingface.co/pepijn223/smol_vla_special5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"abhifoo/tape_to_basket_new_embodiment7k_3\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_new_embodiment7k_3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"nik658/xarm_smolvla\", \"link\": \"https://huggingface.co/nik658/xarm_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"mvnagakishan/Smolvla_pick_place\", \"link\": \"https://huggingface.co/mvnagakishan/Smolvla_pick_place\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"nik658/xarm_smolvla_19jul\", \"link\": \"https://huggingface.co/nik658/xarm_smolvla_19jul\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"mvnagakishan/SMVLAPP\", \"link\": \"https://huggingface.co/mvnagakishan/SMVLAPP\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"abhifoo/tape_to_basket_new_embodiment20k_4\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_new_embodiment20k_4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"Hartvi/smolvla\", \"link\": \"https://huggingface.co/Hartvi/smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 20\"}, {\"name\": \"arpitg1304/smolvla_place_cylinder\", \"link\": \"https://huggingface.co/arpitg1304/smolvla_place_cylinder\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 20\"}, {\"name\": \"Deepkar/smolvla-test-redblock-10ksteps\", \"link\": \"https://huggingface.co/Deepkar/smolvla-test-redblock-10ksteps\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 20\"}, {\"name\": \"arminfg/test123\", \"link\": \"https://huggingface.co/arminfg/test123\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"9mxxb/test1\", \"link\": \"https://huggingface.co/9mxxb/test1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"omkarmayekar555/smolvla_finetunning20july_nolookbrainthrow20july\", \"link\": \"https://huggingface.co/omkarmayekar555/smolvla_finetunning20july_nolookbrainthrow20july\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"wangguobin/my_smolvla_model\", \"link\": \"https://huggingface.co/wangguobin/my_smolvla_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"chasedreaminf/smolvla_so101_3cam-run2\", \"link\": \"https://huggingface.co/chasedreaminf/smolvla_so101_3cam-run2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"nik658/ur10e_50ep_smol_pick\", \"link\": \"https://huggingface.co/nik658/ur10e_50ep_smol_pick\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"omkarmayekar555/smolvla_finetunning21july_marker_pickip_put_in_box\", \"link\": \"https://huggingface.co/omkarmayekar555/smolvla_finetunning21july_marker_pickip_put_in_box\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 17\"}, {\"name\": \"mbalabanski/mysmolvla\", \"link\": \"https://huggingface.co/mbalabanski/mysmolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"shuohsuan/svla_multi_collect_colors\", \"link\": \"https://huggingface.co/shuohsuan/svla_multi_collect_colors\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"shuohsuan/svla_multi_grasp_pos\", \"link\": \"https://huggingface.co/shuohsuan/svla_multi_grasp_pos\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"leesangoh/smolvla_pickplace\", \"link\": \"https://huggingface.co/leesangoh/smolvla_pickplace\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"Tank-123/tank-123\", \"link\": \"https://huggingface.co/Tank-123/tank-123\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"Amanpatel81/SMVLABK\", \"link\": \"https://huggingface.co/Amanpatel81/SMVLABK\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"hariharanr1799/smolvla_hebi_pick_and_drop\", \"link\": \"https://huggingface.co/hariharanr1799/smolvla_hebi_pick_and_drop\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"camilasfeijoo/my_policy\", \"link\": \"https://huggingface.co/camilasfeijoo/my_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"SuchangC/smolvla_719\", \"link\": \"https://huggingface.co/SuchangC/smolvla_719\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"masato-ka/SO100_generalize_pick_pos_smolvla\", \"link\": \"https://huggingface.co/masato-ka/SO100_generalize_pick_pos_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"Sakits/smolvla_so101_pickplace_020000\", \"link\": \"https://huggingface.co/Sakits/smolvla_so101_pickplace_020000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"sk1700/my_policy\", \"link\": \"https://huggingface.co/sk1700/my_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"qownscks/smolvla_first\", \"link\": \"https://huggingface.co/qownscks/smolvla_first\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"attilczuk/bin_sim_smolvla_1\", \"link\": \"https://huggingface.co/attilczuk/bin_sim_smolvla_1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"Askel1419/so101_smolvla_blue_cube\", \"link\": \"https://huggingface.co/Askel1419/so101_smolvla_blue_cube\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"attilczuk/bin_sim_smolvla_2\", \"link\": \"https://huggingface.co/attilczuk/bin_sim_smolvla_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"SuchangC/smolvla_723\", \"link\": \"https://huggingface.co/SuchangC/smolvla_723\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"camilasfeijoo/my_smolvla_sortingblocks\", \"link\": \"https://huggingface.co/camilasfeijoo/my_smolvla_sortingblocks\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"ahad-j/smolvla_chess_pawn\", \"link\": \"https://huggingface.co/ahad-j/smolvla_chess_pawn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"jhou/smolvla_pickeraser_2\", \"link\": \"https://huggingface.co/jhou/smolvla_pickeraser_2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"karimnihal/smolvla_so101_pen_cup_base\", \"link\": \"https://huggingface.co/karimnihal/smolvla_so101_pen_cup_base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"j1y2003/j1y2003\", \"link\": \"https://huggingface.co/j1y2003/j1y2003\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"kjydb/lerobot_test_001\", \"link\": \"https://huggingface.co/kjydb/lerobot_test_001\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"NLTuan/smolvla_pol\", \"link\": \"https://huggingface.co/NLTuan/smolvla_pol\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"asyk454/smolvla_testFinetune_40k\", \"link\": \"https://huggingface.co/asyk454/smolvla_testFinetune_40k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"Askel1419/act_so101_rtx_3_cube_no2\", \"link\": \"https://huggingface.co/Askel1419/act_so101_rtx_3_cube_no2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"cchenalds17/svla_glasses_test0\", \"link\": \"https://huggingface.co/cchenalds17/svla_glasses_test0\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"asyk454/smolvla_throwaway\", \"link\": \"https://huggingface.co/asyk454/smolvla_throwaway\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"nik658/ur10e_50ep_smol_2cam\", \"link\": \"https://huggingface.co/nik658/ur10e_50ep_smol_2cam\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"arpitg1304/smolvla_s101_stack_lego\", \"link\": \"https://huggingface.co/arpitg1304/smolvla_s101_stack_lego\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"yutaro-kimura-acs/so101_pp_blue_and_red_policy\", \"link\": \"https://huggingface.co/yutaro-kimura-acs/so101_pp_blue_and_red_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"masato-ka/SO100_act_generalize_pick_pos_smolvla_40k\", \"link\": \"https://huggingface.co/masato-ka/SO100_act_generalize_pick_pos_smolvla_40k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"cijerezg/pol1\", \"link\": \"https://huggingface.co/cijerezg/pol1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"cijerezg/push_policy\", \"link\": \"https://huggingface.co/cijerezg/push_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"vivibruce/trained_model\", \"link\": \"https://huggingface.co/vivibruce/trained_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"asyk454/svla_glasses_20k\", \"link\": \"https://huggingface.co/asyk454/svla_glasses_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"asyk454/svla_glasses_30k\", \"link\": \"https://huggingface.co/asyk454/svla_glasses_30k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"sancov/smolvla_so101_test\", \"link\": \"https://huggingface.co/sancov/smolvla_so101_test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"cijerezg/push_policyv2\", \"link\": \"https://huggingface.co/cijerezg/push_policyv2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"cijerezg/push_policyv3\", \"link\": \"https://huggingface.co/cijerezg/push_policyv3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"bindels03/block_cup_smolvla1.0\", \"link\": \"https://huggingface.co/bindels03/block_cup_smolvla1.0\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"easonjcc/my_biscuit_smolvla_model\", \"link\": \"https://huggingface.co/easonjcc/my_biscuit_smolvla_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"observabot/smolvla_so101_die_mat4_b64_lr5e-4_cs100_nas100_robo\", \"link\": \"https://huggingface.co/observabot/smolvla_so101_die_mat4_b64_lr5e-4_cs100_nas100_robo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"girardijp/sam_smolvla_better\", \"link\": \"https://huggingface.co/girardijp/sam_smolvla_better\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"Sahana16/my_smolvla\", \"link\": \"https://huggingface.co/Sahana16/my_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"kjydb/lerobot_test_024\", \"link\": \"https://huggingface.co/kjydb/lerobot_test_024\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"observabot/smolvla_so101_die_mat4_b64_lr5e-4_cs200_nas200_robo\", \"link\": \"https://huggingface.co/observabot/smolvla_so101_die_mat4_b64_lr5e-4_cs200_nas200_robo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"kjydb/lerobot_test_025\", \"link\": \"https://huggingface.co/kjydb/lerobot_test_025\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"norips/smolvla_lekiwi_orange_cube_box2\", \"link\": \"https://huggingface.co/norips/smolvla_lekiwi_orange_cube_box2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"Askel1419/smolvla_so101_PLB_dataset\", \"link\": \"https://huggingface.co/Askel1419/smolvla_so101_PLB_dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"mbalabanski/smolvla\", \"link\": \"https://huggingface.co/mbalabanski/smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"nikka-140/my_smolvla-by_box\", \"link\": \"https://huggingface.co/nikka-140/my_smolvla-by_box\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\"}, {\"name\": \"shauryam75/smolVLA-2touch\", \"link\": \"https://huggingface.co/shauryam75/smolVLA-2touch\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"LBST/t01_pick_and_place\", \"link\": \"https://huggingface.co/LBST/t01_pick_and_place\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"Askel1419/smolvla_so101_100EP\", \"link\": \"https://huggingface.co/Askel1419/smolvla_so101_100EP\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"zacapa/SO101_chess_policy4\", \"link\": \"https://huggingface.co/zacapa/SO101_chess_policy4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"camilasfeijoo/my_smolvla_sorter\", \"link\": \"https://huggingface.co/camilasfeijoo/my_smolvla_sorter\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"LBST/t06_pick_and_place\", \"link\": \"https://huggingface.co/LBST/t06_pick_and_place\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nik658/ur10e_150ep_smol_2cam\", \"link\": \"https://huggingface.co/nik658/ur10e_150ep_smol_2cam\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"easonjcc/my_blackobj_smolvla_model\", \"link\": \"https://huggingface.co/easonjcc/my_blackobj_smolvla_model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"oretti/so101_dice_5_smolvla_policy1\", \"link\": \"https://huggingface.co/oretti/so101_dice_5_smolvla_policy1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"starf5/testrec7\", \"link\": \"https://huggingface.co/starf5/testrec7\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"shuohsuan/svla_multi_grasp_red_put_center\", \"link\": \"https://huggingface.co/shuohsuan/svla_multi_grasp_red_put_center\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"kjydb/lerobot_test_026\", \"link\": \"https://huggingface.co/kjydb/lerobot_test_026\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"yestucan/smolvla_burger_v2\", \"link\": \"https://huggingface.co/yestucan/smolvla_burger_v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"cHemingway/smolvla_move_purple_tape\", \"link\": \"https://huggingface.co/cHemingway/smolvla_move_purple_tape\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 10\"}, {\"name\": \"jadechoghari/smolvla-libero-ckpts\", \"link\": \"https://huggingface.co/jadechoghari/smolvla-libero-ckpts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 10\"}, {\"name\": \"kjydb/lerobot_test_028\", \"link\": \"https://huggingface.co/kjydb/lerobot_test_028\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"starf5/so101PickPinkChoco_policy\", \"link\": \"https://huggingface.co/starf5/so101PickPinkChoco_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"norips/smolvla_lekiwi_orange_green_dual\", \"link\": \"https://huggingface.co/norips/smolvla_lekiwi_orange_green_dual\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"Lakesenberg/None\", \"link\": \"https://huggingface.co/Lakesenberg/None\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"ymatari/smolvla_so101_test\", \"link\": \"https://huggingface.co/ymatari/smolvla_so101_test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"camilasfeijoo/my_smolvla_pressplace_sorted\", \"link\": \"https://huggingface.co/camilasfeijoo/my_smolvla_pressplace_sorted\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"TrossenRoboticsCommunity/bimanual_widowxai_handover_cube_smolvla\", \"link\": \"https://huggingface.co/TrossenRoboticsCommunity/bimanual_widowxai_handover_cube_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"rhecker/my_smol_policy\", \"link\": \"https://huggingface.co/rhecker/my_smol_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"Philmat/picko-v7_smolvla-0\", \"link\": \"https://huggingface.co/Philmat/picko-v7_smolvla-0\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"observabot/smolvla_so101_cloth_folding1_b64_lr5e-4_cs100_nas100_robo\", \"link\": \"https://huggingface.co/observabot/smolvla_so101_cloth_folding1_b64_lr5e-4_cs100_nas100_robo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"LBST/t08_pick_and_place_smolvla_10k\", \"link\": \"https://huggingface.co/LBST/t08_pick_and_place_smolvla_10k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"wuc1/ur5_test\", \"link\": \"https://huggingface.co/wuc1/ur5_test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"ymatari/smolvla_so101_test2\", \"link\": \"https://huggingface.co/ymatari/smolvla_so101_test2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"kjydb/lerobot_test_035\", \"link\": \"https://huggingface.co/kjydb/lerobot_test_035\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"hangxiangchen1/my_smolvla_experiment\", \"link\": \"https://huggingface.co/hangxiangchen1/my_smolvla_experiment\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"YouXiangyu/SmolVLA\", \"link\": \"https://huggingface.co/YouXiangyu/SmolVLA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"norips/smolvla_lekiwi_socks\", \"link\": \"https://huggingface.co/norips/smolvla_lekiwi_socks\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"camilasfeijoo/my_smolvla_pensort\", \"link\": \"https://huggingface.co/camilasfeijoo/my_smolvla_pensort\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"adungus/smolVLA_200k\", \"link\": \"https://huggingface.co/adungus/smolVLA_200k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}, {\"name\": \"girardijp/testso101\", \"link\": \"https://huggingface.co/girardijp/testso101\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"cchenalds17/svla_3cam_place_glasses_in_hand_64_20k\", \"link\": \"https://huggingface.co/cchenalds17/svla_3cam_place_glasses_in_hand_64_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"DozenDucc/SmolVLAv2\", \"link\": \"https://huggingface.co/DozenDucc/SmolVLAv2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"qownscks/firstvla_bimanual\", \"link\": \"https://huggingface.co/qownscks/firstvla_bimanual\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"wuc1/ur5_test_v2\", \"link\": \"https://huggingface.co/wuc1/ur5_test_v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"asyk454/svla_3cams_128_20k\", \"link\": \"https://huggingface.co/asyk454/svla_3cams_128_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"cijerezg/push_policy_v101\", \"link\": \"https://huggingface.co/cijerezg/push_policy_v101\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"bicmol/smolvla-libero-combined\", \"link\": \"https://huggingface.co/bicmol/smolvla-libero-combined\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"ymatari/smolvla_so101_grab_dibble\", \"link\": \"https://huggingface.co/ymatari/smolvla_so101_grab_dibble\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"asyk454/svla_3cam_64_40k\", \"link\": \"https://huggingface.co/asyk454/svla_3cam_64_40k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"j1y2003/libero_10_25k\", \"link\": \"https://huggingface.co/j1y2003/libero_10_25k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"cchenalds17/svla_2cam_place_glasses_in_hand_64_20k\", \"link\": \"https://huggingface.co/cchenalds17/svla_2cam_place_glasses_in_hand_64_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"tsp12/test\", \"link\": \"https://huggingface.co/tsp12/test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"girardijp/testso101_3\", \"link\": \"https://huggingface.co/girardijp/testso101_3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"DozenDucc/smolvla3\", \"link\": \"https://huggingface.co/DozenDucc/smolvla3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"LBST/t08_pick_and_place_20k\", \"link\": \"https://huggingface.co/LBST/t08_pick_and_place_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"cijerezg/pick-up-policy-v2\", \"link\": \"https://huggingface.co/cijerezg/pick-up-policy-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"abhifoo/tape_to_basket_new_embodiment20k_v3_1smolvla\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_new_embodiment20k_v3_1smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"asyk454/svla_2cam_128_40k\", \"link\": \"https://huggingface.co/asyk454/svla_2cam_128_40k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"cchenalds17/svla_2cam_place_glasses_in_hand_64_40k\", \"link\": \"https://huggingface.co/cchenalds17/svla_2cam_place_glasses_in_hand_64_40k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"cchenalds17/svla_2cam_place_glasses_in_hand_128_20k\", \"link\": \"https://huggingface.co/cchenalds17/svla_2cam_place_glasses_in_hand_128_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"camilasfeijoo/my_smolvla_redblock\", \"link\": \"https://huggingface.co/camilasfeijoo/my_smolvla_redblock\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"asyk454/svla_3cam_128_40k\", \"link\": \"https://huggingface.co/asyk454/svla_3cam_128_40k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"crislmfroes/svla-panda-open-base-cabinet-sim\", \"link\": \"https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"zacapa/SO101_chess_policy7\", \"link\": \"https://huggingface.co/zacapa/SO101_chess_policy7\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"abhifoo/tape_to_basket_new_embodiment20k_v3_2smolvla\", \"link\": \"https://huggingface.co/abhifoo/tape_to_basket_new_embodiment20k_v3_2smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"kjydb/lerobot_test_074\", \"link\": \"https://huggingface.co/kjydb/lerobot_test_074\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"mvnagakishan/smolvla_Bxarm\", \"link\": \"https://huggingface.co/mvnagakishan/smolvla_Bxarm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"kjydb/lerobot_test_082\", \"link\": \"https://huggingface.co/kjydb/lerobot_test_082\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"pepijn223/thanos_picking_power_gem_migrated\", \"link\": \"https://huggingface.co/pepijn223/thanos_picking_power_gem_migrated\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"crislmfroes/svla-panda-open-base-cabinet-sim-v2\", \"link\": \"https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"attilczuk/sponge_colab\", \"link\": \"https://huggingface.co/attilczuk/sponge_colab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"attilczuk/sponge1\", \"link\": \"https://huggingface.co/attilczuk/sponge1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"crislmfroes/svla-panda-open-base-cabinet-sim-v3\", \"link\": \"https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim-v3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"qownscks/eggplant_bimanual\", \"link\": \"https://huggingface.co/qownscks/eggplant_bimanual\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"attilczuk/sponge_colab_20k\", \"link\": \"https://huggingface.co/attilczuk/sponge_colab_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"sk1700/my_policy03\", \"link\": \"https://huggingface.co/sk1700/my_policy03\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"kjydb/lerobot_model_001\", \"link\": \"https://huggingface.co/kjydb/lerobot_model_001\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 8\"}, {\"name\": \"annyi/my_smolvla_policy\", \"link\": \"https://huggingface.co/annyi/my_smolvla_policy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"Benxiaogu/my_smolvla\", \"link\": \"https://huggingface.co/Benxiaogu/my_smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"camilasfeijoo/my_smolvla_tape\", \"link\": \"https://huggingface.co/camilasfeijoo/my_smolvla_tape\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"crislmfroes/svla-panda-open-base-cabinet-sim-v4\", \"link\": \"https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim-v4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"koyomi1/smolvla\", \"link\": \"https://huggingface.co/koyomi1/smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 24\"}, {\"name\": \"attilczuk/sponge_colab6\", \"link\": \"https://huggingface.co/attilczuk/sponge_colab6\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"attilczuk/sponge_colab2_20k\", \"link\": \"https://huggingface.co/attilczuk/sponge_colab2_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"Deepkar/redblock-pick-place-50ep-smolvla\", \"link\": \"https://huggingface.co/Deepkar/redblock-pick-place-50ep-smolvla\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"crislmfroes/svla-panda-open-base-cabinet-sim-v5\", \"link\": \"https://huggingface.co/crislmfroes/svla-panda-open-base-cabinet-sim-v5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}]",
    "num_datasets": 9,
    "datasets_list": "HuggingFaceVLA/community_dataset_v1, HuggingFaceVLA/community_dataset_v3, HuggingFaceVLA/community_dataset_v2, aleenatron/sample_test_aleena, SiliconVandals/lerobot, liamlau/svla_so100_sorting, Anteid11/hil, paulagb/hackathon-dataset_caramelos, Johnbosco20/community_dataset_v3",
    "datasets_links": "https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v1, https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v3, https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v2, https://huggingface.co/datasets/aleenatron/sample_test_aleena, https://huggingface.co/datasets/SiliconVandals/lerobot, https://huggingface.co/datasets/liamlau/svla_so100_sorting, https://huggingface.co/datasets/Anteid11/hil, https://huggingface.co/datasets/paulagb/hackathon-dataset_caramelos, https://huggingface.co/datasets/Johnbosco20/community_dataset_v3",
    "datasets_detailed": "[{\"name\": \"HuggingFaceVLA/community_dataset_v1\", \"link\": \"https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\", \"size\": \"\"}, {\"name\": \"HuggingFaceVLA/community_dataset_v3\", \"link\": \"https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}, {\"name\": \"HuggingFaceVLA/community_dataset_v2\", \"link\": \"https://huggingface.co/datasets/HuggingFaceVLA/community_dataset_v2\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Nov 13\", \"size\": \"\"}, {\"name\": \"aleenatron/sample_test_aleena\", \"link\": \"https://huggingface.co/datasets/aleenatron/sample_test_aleena\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\", \"size\": \"\"}, {\"name\": \"SiliconVandals/lerobot\", \"link\": \"https://huggingface.co/datasets/SiliconVandals/lerobot\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 14\", \"size\": \"\"}, {\"name\": \"liamlau/svla_so100_sorting\", \"link\": \"https://huggingface.co/datasets/liamlau/svla_so100_sorting\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 26\", \"size\": \"\"}, {\"name\": \"Anteid11/hil\", \"link\": \"https://huggingface.co/datasets/Anteid11/hil\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\", \"size\": \"\"}, {\"name\": \"paulagb/hackathon-dataset_caramelos\", \"link\": \"https://huggingface.co/datasets/paulagb/hackathon-dataset_caramelos\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"30 days ago\", \"size\": \"\"}, {\"name\": \"Johnbosco20/community_dataset_v3\", \"link\": \"https://huggingface.co/datasets/Johnbosco20/community_dataset_v3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"9 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.23754",
    "first_seen_date": "2025-05-30",
    "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.23754DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement LearningPublished on May 29\u00b7Submitted byJiahao Xuon May 30Upvote15+7Authors:Ziyin Zhang,Jiahao Xu,Zhiwei He,Tian Liang,Qiuzhi Liu,Yansi Li,Linfeng Song,Zhengwen Liang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong YuAbstractDeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.AI-generated summaryTheorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal,natural languageknowledge acquired during pre-training. In this work, we proposeDeepTheorem, a\ncomprehensiveinformal theorem-provingframework exploitingnatural languageto\nenhance LLM mathematical reasoning.DeepTheoremincludes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging theverified theorem variantsto incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examiningproof correctnessand the quality of reasoning\nsteps. Extensive experimental analyses demonstrateDeepTheoremsignificantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy andreasoning qua",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/Jiahao004/DeepTheorem",
    "hf_paper_url": "https://huggingface.co/papers/2505.23754",
    "arxiv_url": "https://arxiv.org/abs/2505.23754",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "m-a-p/FineLeanCorpus, Jiahao004/DeepTheorem",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/FineLeanCorpus, https://huggingface.co/datasets/Jiahao004/DeepTheorem",
    "datasets_detailed": "[{\"name\": \"m-a-p/FineLeanCorpus\", \"link\": \"https://huggingface.co/datasets/m-a-p/FineLeanCorpus\", \"task\": \"\", \"likes\": \"481\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}, {\"name\": \"Jiahao004/DeepTheorem\", \"link\": \"https://huggingface.co/datasets/Jiahao004/DeepTheorem\", \"task\": \"\", \"likes\": \"380\", \"downloads\": \"\", \"updated\": \"Jul 3\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.23762",
    "first_seen_date": "2025-05-30",
    "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.23762ZeroGUI: Automating Online GUI Learning at Zero Human CostPublished on May 29\u00b7Submitted byChenyu Yangon May 30Upvote45+37Authors:Chenyu Yang,Shiqian Su,Shi Liu,Xuan Dong,Yue Yu,Weijie Su,Xuehui Wang,Zhaoyang Liu,Jinguo Zhu,Hao Li,Wenhai Wang,Yu Qiao,Xizhou Zhu,Jifeng DaiAbstractZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.AI-generated summaryThe rapid advancement of largeVision-Language Models(VLMs) has propelled\nthe development of pure-vision-basedGUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt anoffline learningframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations forelement groundingandaction supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-basedautomatic task generationto produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage onlinereinforcement learningto continuously interact with\nand learn from GUI environments. Experiments on two advancedGUI Agents(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacrossOSWorldandAndroidLabenvironments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.View arXiv pageView PDFGitHub103Add to collectionCommunitycyyang822Paper authorPaper submitterMay 30ZeroGUI, a fully automated online reinforcement learning framework that enables GUI agents to tr",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/OpenGVLab/ZeroGUI",
    "hf_paper_url": "https://huggingface.co/papers/2505.23762",
    "arxiv_url": "https://arxiv.org/abs/2505.23762",
    "num_models": 2,
    "models_list": "OpenGVLab/ZeroGUI-OSWorld-7B, OpenGVLab/ZeroGUI-AndroidLab-7B",
    "models_links": "https://huggingface.co/OpenGVLab/ZeroGUI-OSWorld-7B, https://huggingface.co/OpenGVLab/ZeroGUI-AndroidLab-7B",
    "models_detailed": "[{\"name\": \"OpenGVLab/ZeroGUI-OSWorld-7B\", \"link\": \"https://huggingface.co/OpenGVLab/ZeroGUI-OSWorld-7B\", \"task\": \"\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"OpenGVLab/ZeroGUI-AndroidLab-7B\", \"link\": \"https://huggingface.co/OpenGVLab/ZeroGUI-AndroidLab-7B\", \"task\": \"\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"May 30\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.22648",
    "first_seen_date": "2025-05-29",
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.22648WebDancer: Towards Autonomous Information Seeking AgencyPublished on May 28\u00b7Submitted byJialong Wuon May 29Upvote33+25Authors:Jialong Wu,Baixuan Li,Runnan Fang,Wenbiao Yin,Liwen Zhang,Zhengwei Tao,Dingchu Zhang,Zekun Xi,Yong Jiang,Pengjun Xie,Fei Huang,Jingren ZhouAbstractThe paper proposes a framework for building end-to-end agentic information seeking agents through a combination of data construction, trajectory sampling, supervised fine-tuning, and reinforcement learning, showcasing its effectiveness on information seeking benchmarks.AI-generated summaryAddressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)browsing data construction, (2)trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4)reinforcement learningfor\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct,WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks,GAIAandWebWalkerQA, demonstrate the strong performance ofWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.View arXiv pageView PDFGitHub17.7kAdd to collectionCommunitycallanwuPaper authorPaper submitterMay 29Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Rece",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/Alibaba-NLP/WebAgent",
    "hf_paper_url": "https://huggingface.co/papers/2505.22648",
    "arxiv_url": "https://arxiv.org/abs/2505.22648",
    "num_models": 1,
    "models_list": "Alibaba-NLP/WebDancer-32B",
    "models_links": "https://huggingface.co/Alibaba-NLP/WebDancer-32B",
    "models_detailed": "[{\"name\": \"Alibaba-NLP/WebDancer-32B\", \"link\": \"https://huggingface.co/Alibaba-NLP/WebDancer-32B\", \"task\": \"Text Generation\", \"likes\": \"229\", \"downloads\": \"\", \"updated\": \"Jun 26\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.19314",
    "first_seen_date": "2025-05-28",
    "title": "SoloSpeech: Enhancing Intelligibility and Quality in Target Speech\n  Extraction through a Cascaded Generative Pipeline",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.19314SoloSpeech: Enhancing Intelligibility and Quality in Target Speech\n  Extraction through a Cascaded Generative PipelinePublished on May 25\u00b7Submitted byHelin Wangon May 28Upvote4Authors:Helin Wang,Jiarui Hai,Dongchao Yang,Chen Chen,Kai Li,Junyi Peng,Thomas Thebaud,Laureano Moro Velazquez,Jesus Villalba,Najim DehakAbstractSoloSpeech, a cascaded generative pipeline, improves target speech extraction and speech separation by addressing artifact introduction, naturalness reduction, and environment mismatches, achieving state-of-the-art intelligibility and quality.AI-generated summaryTarget Speech Extraction(TSE) aims to isolate a target speaker's voice from\na mixture of multiple speakers by leveraging speaker-specific cues, typically\nprovided as auxiliary audio (a.k.a. cue audio). Although recent advancements in\nTSE have primarily employeddiscriminative modelsthat offer high perceptual\nquality, these models often introduce unwanted artifacts, reduce naturalness,\nand are sensitive to discrepancies between training and testing environments.\nOn the other hand,generative modelsfor TSE lag in perceptual quality and\nintelligibility. To address these challenges, we present SoloSpeech, a novel\ncascaded generative pipeline that integrates compression, extraction,\nreconstruction, and correction processes. SoloSpeech features aspeaker-embedding-free target extractorthat utilizes conditional information\nfrom the cue audio'slatent space, aligning it with the mixture audio's latent\nspace to prevent mismatches. Evaluated on the widely-usedLibri2Mix dataset,\nSoloSpeech achieves the new state-of-the-art intelligibility and quality intarget speech extractionandspeech separationtasks while demonstrating\nexceptional generalization on out-of-domain data and real-world scenarios.View arXiv pageView PDFProject pageGitHub290Add to collectionCommunitywestbrookPaper authorPaper submitterMay 28See trans",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/WangHelin1997/SoloSpeech",
    "hf_paper_url": "https://huggingface.co/papers/2505.19314",
    "arxiv_url": "https://arxiv.org/abs/2505.19314",
    "num_models": 1,
    "models_list": "OpenSound/SoloSpeech-models",
    "models_links": "https://huggingface.co/OpenSound/SoloSpeech-models",
    "models_detailed": "[{\"name\": \"OpenSound/SoloSpeech-models\", \"link\": \"https://huggingface.co/OpenSound/SoloSpeech-models\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 28\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.16984",
    "first_seen_date": "2025-05-27",
    "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.16984UFT: Unifying Supervised and Reinforcement Fine-TuningPublished on May 22\u00b7Submitted byMingyang Liuon May 27Upvote3Authors:Mingyang Liu,Gabriele Farina,Asuman OzdaglarAbstractA new post-training method, Unified Fine-Tuning (UFT), improves upon supervised and reinforcement fine-tuning for large language models by combining their benefits, achieving better generalization and faster convergence.AI-generated summaryPost-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized intosupervised fine-tuning(SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we proposeUnified Fine-Tuning(UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponentialsample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\nonlong-horizon reasoning tasks.View arXiv pageView PDFGitHub20Add to collectionCommunityliumy2010Paper authorPaper submitterMay 26\u2022edited May 26Code:https://github.com/liumy2010/UFTSee translationReplyliumy2010Paper authorPaper submitterMay 27TL;DR: We propose a novel fine-tuning algorithm, UFT, that unifies supervised & reinforcement fine-tuning, and outpe",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/liumy2010/UFT",
    "hf_paper_url": "https://huggingface.co/papers/2505.16984",
    "arxiv_url": "https://arxiv.org/abs/2505.16984",
    "num_models": 75,
    "models_list": "liumy2010/Llama-3.2-3B-math-SFT, liumy2010/Llama-3.2-3B-countdown-R3, liumy2010/Qwen2.5-0.5B-countdown-SFT, liumy2010/Qwen2.5-1.5B-countdown-SFT, liumy2010/Qwen2.5-3B-countdown-SFT, liumy2010/Qwen2.5-0.5B-math-SFT, liumy2010/Qwen2.5-1.5B-math-SFT, liumy2010/Qwen2.5-3B-math-SFT, liumy2010/Qwen2.5-0.5B-kk_logic-SFT, liumy2010/Qwen2.5-1.5B-kk_logic-SFT, liumy2010/Qwen2.5-3B-kk_logic-SFT, liumy2010/Qwen2.5-0.5B-countdown-RFT, liumy2010/Qwen2.5-0.5B-countdown-SFT-RFT, liumy2010/Qwen2.5-0.5B-countdown-UFT, liumy2010/Qwen2.5-0.5B-countdown-R3, liumy2010/Qwen2.5-1.5B-countdown-UFT, liumy2010/Qwen2.5-1.5B-countdown-RFT, liumy2010/Qwen2.5-1.5B-countdown-SFT-RFT, liumy2010/Qwen2.5-1.5B-countdown-R3, liumy2010/Qwen2.5-3B-countdown-UFT, liumy2010/Qwen2.5-3B-countdown-SFT-RFT, liumy2010/Qwen2.5-3B-countdown-RFT, liumy2010/Qwen2.5-0.5B-math-UFT, liumy2010/Qwen2.5-0.5B-math-RFT, liumy2010/Qwen2.5-0.5B-math-R3, liumy2010/Qwen2.5-0.5B-math-SFT-RFT, liumy2010/Qwen2.5-1.5B-math-UFT, liumy2010/Qwen2.5-1.5B-math-RFT, liumy2010/Qwen2.5-1.5B-math-SFT-RFT, liumy2010/Qwen2.5-1.5B-math-R3, liumy2010/Qwen2.5-3B-math-UFT, liumy2010/Qwen2.5-3B-math-SFT-RFT, liumy2010/Qwen2.5-3B-math-RFT, liumy2010/Qwen2.5-0.5B-kk_logic-UFT, liumy2010/Qwen2.5-0.5B-kk_logic-SFT-RFT, liumy2010/Qwen2.5-0.5B-kk_logic-RFT, liumy2010/Qwen2.5-0.5B-kk_logic-R3, liumy2010/Qwen2.5-3B-kk_logic-UFT, liumy2010/Qwen2.5-3B-math-R3, liumy2010/Qwen2.5-1.5B-kk_logic-UFT, liumy2010/Qwen2.5-1.5B-kk_logic-RFT, liumy2010/Qwen2.5-1.5B-kk_logic-SFT-RFT, liumy2010/Qwen2.5-1.5B-kk_logic-R3, liumy2010/Qwen2.5-3B-kk_logic-RFT, liumy2010/Qwen2.5-3B-countdown-R3, liumy2010/Qwen2.5-3B-kk_logic-R3, liumy2010/Qwen2.5-3B-kk_logic-SFT-RFT, liumy2010/Llama-3.2-1B-countdown-SFT, liumy2010/Llama-3.2-3B-countdown-SFT, liumy2010/Llama-3.2-1B-math-SFT, liumy2010/Llama-3.2-1B-kk_logic-SFT, liumy2010/Llama-3.2-3B-kk_logic-SFT, liumy2010/Llama-3.2-1B-countdown-UFT, liumy2010/Llama-3.2-1B-countdown-SFT-RFT, liumy2010/Llama-3.2-1B-countdown-R3, liumy2010/Llama-3.2-1B-countdown-RFT, liumy2010/Llama-3.2-1B-math-UFT, liumy2010/Llama-3.2-1B-math-SFT-RFT, liumy2010/Llama-3.2-1B-math-RFT, liumy2010/Llama-3.2-1B-math-R3, liumy2010/Llama-3.2-1B-kk_logic-UFT, liumy2010/Llama-3.2-1B-kk_logic-SFT-RFT, liumy2010/Llama-3.2-1B-kk_logic-RFT, liumy2010/Llama-3.2-1B-kk_logic-R3, liumy2010/Llama-3.2-3B-countdown-UFT, liumy2010/Llama-3.2-3B-countdown-SFT-RFT, liumy2010/Llama-3.2-3B-countdown-RFT, liumy2010/Llama-3.2-3B-math-UFT, liumy2010/Llama-3.2-3B-math-RFT, liumy2010/Llama-3.2-3B-math-SFT-RFT, liumy2010/Llama-3.2-3B-math-R3, liumy2010/Llama-3.2-3B-kk_logic-UFT, liumy2010/Llama-3.2-3B-kk_logic-RFT, liumy2010/Llama-3.2-3B-kk_logic-SFT-RFT, liumy2010/Llama-3.2-3B-kk_logic-R3",
    "models_links": "https://huggingface.co/liumy2010/Llama-3.2-3B-math-SFT, https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-R3, https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-SFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-SFT, https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-SFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-SFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-SFT, https://huggingface.co/liumy2010/Qwen2.5-3B-math-SFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-SFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-SFT, https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-SFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-RFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-SFT-RFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-UFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-R3, https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-UFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-RFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-SFT-RFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-R3, https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-UFT, https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-SFT-RFT, https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-RFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-UFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-RFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-R3, https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-SFT-RFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-UFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-RFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-SFT-RFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-R3, https://huggingface.co/liumy2010/Qwen2.5-3B-math-UFT, https://huggingface.co/liumy2010/Qwen2.5-3B-math-SFT-RFT, https://huggingface.co/liumy2010/Qwen2.5-3B-math-RFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-UFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-SFT-RFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-RFT, https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-R3, https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-UFT, https://huggingface.co/liumy2010/Qwen2.5-3B-math-R3, https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-UFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-RFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-SFT-RFT, https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-R3, https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-RFT, https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-R3, https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-R3, https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-SFT-RFT, https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-SFT, https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-SFT, https://huggingface.co/liumy2010/Llama-3.2-1B-math-SFT, https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-SFT, https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-SFT, https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-UFT, https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-SFT-RFT, https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-R3, https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-RFT, https://huggingface.co/liumy2010/Llama-3.2-1B-math-UFT, https://huggingface.co/liumy2010/Llama-3.2-1B-math-SFT-RFT, https://huggingface.co/liumy2010/Llama-3.2-1B-math-RFT, https://huggingface.co/liumy2010/Llama-3.2-1B-math-R3, https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-UFT, https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-SFT-RFT, https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-RFT, https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-R3, https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-UFT, https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-SFT-RFT, https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-RFT, https://huggingface.co/liumy2010/Llama-3.2-3B-math-UFT, https://huggingface.co/liumy2010/Llama-3.2-3B-math-RFT, https://huggingface.co/liumy2010/Llama-3.2-3B-math-SFT-RFT, https://huggingface.co/liumy2010/Llama-3.2-3B-math-R3, https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-UFT, https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-RFT, https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-SFT-RFT, https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-R3",
    "models_detailed": "[{\"name\": \"liumy2010/Llama-3.2-3B-math-SFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-math-SFT\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-countdown-R3\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-R3\", \"task\": \"Text Generation\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-countdown-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-countdown-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-countdown-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-math-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-math-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-math-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-math-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-kk_logic-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-kk_logic-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-kk_logic-SFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-countdown-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-countdown-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-countdown-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-countdown-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-countdown-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-countdown-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-countdown-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-countdown-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-countdown-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-countdown-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-countdown-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-countdown-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-countdown-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-math-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-math-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-math-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-math-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-math-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-math-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-math-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-math-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-math-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-math-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-math-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-math-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-math-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-math-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-math-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-math-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-kk_logic-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-kk_logic-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-kk_logic-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-0.5B-kk_logic-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-0.5B-kk_logic-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-kk_logic-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-math-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-math-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-kk_logic-UFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-kk_logic-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-kk_logic-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-1.5B-kk_logic-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-1.5B-kk_logic-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-kk_logic-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-countdown-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-countdown-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-kk_logic-R3\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Qwen2.5-3B-kk_logic-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Qwen2.5-3B-kk_logic-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-countdown-SFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-countdown-SFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-math-SFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-math-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-kk_logic-SFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-kk_logic-SFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-countdown-UFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-countdown-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-countdown-R3\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-countdown-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-countdown-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-math-UFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-math-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-math-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-math-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-math-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-math-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-math-R3\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-math-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-kk_logic-UFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-kk_logic-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-kk_logic-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-1B-kk_logic-R3\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-1B-kk_logic-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-countdown-UFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-countdown-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-countdown-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-countdown-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-math-UFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-math-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-math-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-math-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-math-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-math-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-math-R3\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-math-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-kk_logic-UFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-UFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-kk_logic-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-kk_logic-SFT-RFT\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-SFT-RFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"liumy2010/Llama-3.2-3B-kk_logic-R3\", \"link\": \"https://huggingface.co/liumy2010/Llama-3.2-3B-kk_logic-R3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.18116",
    "first_seen_date": "2025-05-27",
    "title": "Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.18116Bridging Supervised Learning and Reinforcement Learning in Math\n  ReasoningPublished on May 23\u00b7Submitted byHaoxiang Wangon May 27Upvote4Authors:Huayu Chen,Kaiwen Zheng,Qinsheng Zhang,Ganqu Cui,Yin Cui,Haotian Ye,Tsung-Yi Lin,Ming-Yu Liu,Jun Zhu,Haoxiang WangAbstractNegative-aware Fine-Tuning (NFT) enhances LLMs' math abilities using supervised learning with negative feedback, achieving performance comparable to RL methods.AI-generated summaryReinforcement Learning (RL)has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast,Supervised Learning (SL)is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and proposeNegative-aware Fine-Tuning (NFT)-- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generatednegative answers,\nNFT constructs animplicit negative policyto model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling directpolicy optimizationon all LLMs' generations. We conduct\nexperiments on 7B and 32B models inmath reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines likeRejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms likeGRPOandDAPO.\nFurthermore, we demonstrate that NFT andGRPOare actually equivalent instrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL method",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/BytedTsinghua-SIA/DAPO",
    "hf_paper_url": "https://huggingface.co/papers/2505.18116",
    "arxiv_url": "https://arxiv.org/abs/2505.18116",
    "num_models": 4,
    "models_list": "nvidia/NFT-32B, nvidia/NFT-7B, gabriellarson/NFT-32B-GGUF, gabriellarson/NFT-7B-GGUF",
    "models_links": "https://huggingface.co/nvidia/NFT-32B, https://huggingface.co/nvidia/NFT-7B, https://huggingface.co/gabriellarson/NFT-32B-GGUF, https://huggingface.co/gabriellarson/NFT-7B-GGUF",
    "models_detailed": "[{\"name\": \"nvidia/NFT-32B\", \"link\": \"https://huggingface.co/nvidia/NFT-32B\", \"task\": \"Text Generation\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"nvidia/NFT-7B\", \"link\": \"https://huggingface.co/nvidia/NFT-7B\", \"task\": \"Text Generation\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"gabriellarson/NFT-32B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/NFT-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"gabriellarson/NFT-7B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/NFT-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.20139",
    "first_seen_date": "2025-05-27",
    "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural\n  Outputs",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.20139StructEval: Benchmarking LLMs' Capabilities to Generate Structural\n  OutputsPublished on May 26\u00b7Submitted byDongfu Jiangon May 27Upvote19+11Authors:Jialin Yang,Dongfu Jiang,Lipeng He,Sherman Siu,Yuxuan Zhang,Disen Liao,Zhuofeng Li,Huaye Zeng,Yiming Jia,Haozhe Wang,Benjamin Schneider,Chi Ruan,Wentao Ma,Zhiheng Lyu,Yifei Wang,Yi Lu,Quy Duc Do,Ziyan Jiang,Ping Nie,Wenhu ChenAbstractStructEval benchmarks Large Language Models for generating and converting structured outputs, highlighting performance gaps and challenges in producing visual content.AI-generated summaryAsLarge Language Models (LLMs)become integral to software development\nworkflows, their ability to generatestructured outputshas become critically\nimportant. We introduceStructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON,YAML,CSV) and\nrenderable (HTML,React,SVG) structured formats. Unlike prior benchmarks,StructEvalsystematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1)generation tasks, producing structured output from\nnatural language prompts, and 2)conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics forformat adherenceandstructural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We findgeneration tasksmore challenging thanconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.View arXiv pageView PDFProject pageGitHub15Add to collectionCommunityDongfuJiangPaper authorPaper submitterMay 27see our website athttps://tiger-ai-lab.github.io/StructEval/See translationReplyEditPreviewUpload images, audio, and videos by dragging in the te",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/TIGER-AI-Lab/StructEval",
    "hf_paper_url": "https://huggingface.co/papers/2505.20139",
    "arxiv_url": "https://arxiv.org/abs/2505.20139",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/StructEval",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/StructEval",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/StructEval\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/StructEval\", \"task\": \"\", \"likes\": \"412\", \"downloads\": \"\", \"updated\": \"Sep 23\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.17667",
    "first_seen_date": "2025-05-26",
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.17667QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement LearningPublished on May 23\u00b7Submitted byFanqi Wanon May 26#2 Paper of the dayUpvote88+80Authors:Fanqi Wan,Weizhou Shen,Shengyi Liao,Yingcheng Shi,Chenliang Li,Ziyi Yang,Ji Zhang,Fei Huang,Jingren Zhou,Ming YanAbstractA framework called QwenLong-L1 enhances large reasoning models for long-context reasoning through reinforcement learning, achieving leading performance on document question-answering benchmarks.AI-generated summaryRecent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities throughreinforcement learning(RL). These improvements have\nprimarily been observed within theshort-context reasoningtasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm oflong-context reasoningRL, and identify key challenges in\nsuboptimaltraining efficiencyand unstableoptimization process. To address\nthese issues, we proposeQwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios viaprogressive context scaling. Specifically,\nwe utilize a warm-upsupervised fine-tuning(SFT) stage to establish a robust\ninitial policy, followed by acurriculum-guided phased RLtechnique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-contextdocument question-answering benchmarksdemonstrate thatQwenLong-L1-32B outperforms flagship LRMs likeOpenAI-o3-miniandQwen3-235B-A22B, achieving performance on par withClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.View a",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/Tongyi-Zhiwen/QwenLong-L1",
    "hf_paper_url": "https://huggingface.co/papers/2505.17667",
    "arxiv_url": "https://arxiv.org/abs/2505.17667",
    "num_models": 4,
    "models_list": "Tongyi-Zhiwen/QwenLong-L1-32B, Tongyi-Zhiwen/QwenLong-L1-32B-AWQ, Mungert/QwenLong-L1-32B-GGUF, kmouratidis/QwenLong-L1-32B-4.25bpw",
    "models_links": "https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B, https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B-AWQ, https://huggingface.co/Mungert/QwenLong-L1-32B-GGUF, https://huggingface.co/kmouratidis/QwenLong-L1-32B-4.25bpw",
    "models_detailed": "[{\"name\": \"Tongyi-Zhiwen/QwenLong-L1-32B\", \"link\": \"https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B\", \"task\": \"Text Generation\", \"likes\": \"127\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"Tongyi-Zhiwen/QwenLong-L1-32B-AWQ\", \"link\": \"https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B-AWQ\", \"task\": \"\", \"likes\": \"68\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"Mungert/QwenLong-L1-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/QwenLong-L1-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"894\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"kmouratidis/QwenLong-L1-32B-4.25bpw\", \"link\": \"https://huggingface.co/kmouratidis/QwenLong-L1-32B-4.25bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.18092",
    "first_seen_date": "2025-05-26",
    "title": "QwenLong-CPRS: Towards infty-LLMs with Dynamic Context Optimization",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.18092QwenLong-CPRS: Towards infty-LLMs with Dynamic Context OptimizationPublished on May 23\u00b7Submitted byWeizhou Shenon May 26Upvote43+35Authors:Weizhou Shen,Chenliang Li,Fanqi Wan,Shengyi Liao,Shaopeng Lai,Bo Zhang,Yingcheng Shi,Yuning Wu,Gang Fu,Zhansheng Li,Bin Yang,Ji Zhang,Fei Huang,Jingren Zhou,Ming YanAbstractQwenLong-CPRS enhances large language models with multi-granularity context compression, dynamic optimization guided by natural language, and efficient bidirectional reasoning and parallel inference, achieving superior performance and context management.AI-generated summaryThis technical report presentsQwenLong-CPRS, acontext compressionframework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation oflarge language models(LLMs) during long sequence\nprocessing. Implemented through a noveldynamic context optimizationmechanism,QwenLong-CPRS enables multi-granularitycontext compressionguided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from theQwenarchitecture series,QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)Bidirectional reasoning layersfor enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrateQwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods likeRAGandsparse attentionin both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nandQwen2.5-max, achieves 21.59timescontext compressionalongside\n19.15-point average performance gains; (3) Deploy",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/Tongyi-Zhiwen/QwenLong-CPRS",
    "hf_paper_url": "https://huggingface.co/papers/2505.18092",
    "arxiv_url": "https://arxiv.org/abs/2505.18092",
    "num_models": 1,
    "models_list": "Tongyi-Zhiwen/QwenLong-CPRS-7B",
    "models_links": "https://huggingface.co/Tongyi-Zhiwen/QwenLong-CPRS-7B",
    "models_detailed": "[{\"name\": \"Tongyi-Zhiwen/QwenLong-CPRS-7B\", \"link\": \"https://huggingface.co/Tongyi-Zhiwen/QwenLong-CPRS-7B\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"May 28\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.15966",
    "first_seen_date": "2025-05-23",
    "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.15966Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement LearningPublished on May 21\u00b7Submitted byWenhu Chenon May 23Upvote53+45Authors:Alex Su,Haozhe Wang,Weimin Ren,Fangzhen Lin,Wenhu ChenAbstractIntroducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.AI-generated summaryChain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework,Vision-Language Models(VLMs) are equipped with a suite of visual\nreasoning operations, such aszoom-inandselect-frame. These operations enableVLMsto directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities inVLMspresents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, areinforcement learning(RL) phase leverages acuriosity-driven reward schemeto balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations,VLMscan\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reason",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/TIGER-AI-Lab/Pixel-Reasoner",
    "hf_paper_url": "https://huggingface.co/papers/2505.15966",
    "arxiv_url": "https://arxiv.org/abs/2505.15966",
    "num_models": 1,
    "models_list": "TIGER-Lab/PixelReasoner-RL-v1",
    "models_links": "https://huggingface.co/TIGER-Lab/PixelReasoner-RL-v1",
    "models_detailed": "[{\"name\": \"TIGER-Lab/PixelReasoner-RL-v1\", \"link\": \"https://huggingface.co/TIGER-Lab/PixelReasoner-RL-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 11\"}]",
    "num_datasets": 2,
    "datasets_list": "TIGER-Lab/PixelReasoner-SFT-Data, TIGER-Lab/PixelReasoner-RL-Data",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data, https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-RL-Data",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/PixelReasoner-SFT-Data\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-SFT-Data\", \"task\": \"\", \"likes\": \"326\", \"downloads\": \"\", \"updated\": \"Aug 13\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/PixelReasoner-RL-Data\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/PixelReasoner-RL-Data\", \"task\": \"\", \"likes\": \"187\", \"downloads\": \"\", \"updated\": \"Jun 3\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.16400",
    "first_seen_date": "2025-05-23",
    "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.16400AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement LearningPublished on May 22\u00b7Submitted byYang Chenon May 23Upvote35+27Authors:Yang Chen,Zhuolin Yang,Zihan Liu,Chankyu Lee,Peng Xu,Mohammad Shoeybi,Bryan Catanzaro,Wei PingAbstractLarge-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.AI-generated summaryDespite recent progress in large-scalereinforcement learning(RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such asDeepSeek-R1, includingdata curationstrategies andRLtraining recipe, are\noften omitted. Moreover, recent research indicatesdistillationremains more\neffective thanRLfor smaller models. In this work, we demonstrate that\nlarge-scaleRLcan significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-artdistillation-based models. We systematically study theRLtraining process through extensive ablations and propose a simple yet effective\napproach: first training onmath-only prompts, then oncode-only prompts.\nNotably, we find that math-onlyRLnot only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% onAIME 2025for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% onLiveCodeBenchfor the 7B / 14B models). In addition,\nextended code-onlyRLiterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-basedRLacross both domains.\nFinally, we identify key experime",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2505.16400",
    "arxiv_url": "https://arxiv.org/abs/2505.16400",
    "num_models": 12,
    "models_list": "nvidia/AceReason-Nemotron-14B, nvidia/AceReason-Nemotron-7B, unsloth/AceReason-Nemotron-14B-GGUF, lmstudio-community/AceReason-Nemotron-14B-GGUF, unsloth/AceReason-Nemotron-14B, lucyknada/nvidia_AceReason-Nemotron-14B-exl3, lmstudio-community/AceReason-Nemotron-7B-GGUF, QuantFactory/AceReason-Nemotron-7B-GGUF, QuantFactory/AceReason-Nemotron-14B-GGUF, Prince-1/AceReason-Nemotron-14B-Onnx, onnx-community/AceReason-Nemotron-14B-Onnx, Mungert/AceReason-Nemotron-7B-GGUF",
    "models_links": "https://huggingface.co/nvidia/AceReason-Nemotron-14B, https://huggingface.co/nvidia/AceReason-Nemotron-7B, https://huggingface.co/unsloth/AceReason-Nemotron-14B-GGUF, https://huggingface.co/lmstudio-community/AceReason-Nemotron-14B-GGUF, https://huggingface.co/unsloth/AceReason-Nemotron-14B, https://huggingface.co/lucyknada/nvidia_AceReason-Nemotron-14B-exl3, https://huggingface.co/lmstudio-community/AceReason-Nemotron-7B-GGUF, https://huggingface.co/QuantFactory/AceReason-Nemotron-7B-GGUF, https://huggingface.co/QuantFactory/AceReason-Nemotron-14B-GGUF, https://huggingface.co/Prince-1/AceReason-Nemotron-14B-Onnx, https://huggingface.co/onnx-community/AceReason-Nemotron-14B-Onnx, https://huggingface.co/Mungert/AceReason-Nemotron-7B-GGUF",
    "models_detailed": "[{\"name\": \"nvidia/AceReason-Nemotron-14B\", \"link\": \"https://huggingface.co/nvidia/AceReason-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"nvidia/AceReason-Nemotron-7B\", \"link\": \"https://huggingface.co/nvidia/AceReason-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"unsloth/AceReason-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/unsloth/AceReason-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"553\", \"downloads\": \"\", \"updated\": \"May 23\"}, {\"name\": \"lmstudio-community/AceReason-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/AceReason-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"74\", \"downloads\": \"\", \"updated\": \"May 23\"}, {\"name\": \"unsloth/AceReason-Nemotron-14B\", \"link\": \"https://huggingface.co/unsloth/AceReason-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 23\"}, {\"name\": \"lucyknada/nvidia_AceReason-Nemotron-14B-exl3\", \"link\": \"https://huggingface.co/lucyknada/nvidia_AceReason-Nemotron-14B-exl3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 23\"}, {\"name\": \"lmstudio-community/AceReason-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/AceReason-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"May 26\"}, {\"name\": \"QuantFactory/AceReason-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/AceReason-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"48\", \"downloads\": \"\", \"updated\": \"Jun 13\"}, {\"name\": \"QuantFactory/AceReason-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/AceReason-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"Prince-1/AceReason-Nemotron-14B-Onnx\", \"link\": \"https://huggingface.co/Prince-1/AceReason-Nemotron-14B-Onnx\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 23\"}, {\"name\": \"onnx-community/AceReason-Nemotron-14B-Onnx\", \"link\": \"https://huggingface.co/onnx-community/AceReason-Nemotron-14B-Onnx\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"Mungert/AceReason-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/AceReason-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"216\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 5,
    "datasets_list": "allenai/Dolci-Think-RL-7B, nvidia/AceReason-Math, allenai/Dolci-Think-RL-32B, allenai/Dolci-Think-RL-7B-Completions-SFT, allenai/Dolci-Think-RL-7B-Completions-DPO",
    "datasets_links": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B, https://huggingface.co/datasets/nvidia/AceReason-Math, https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO",
    "datasets_detailed": "[{\"name\": \"allenai/Dolci-Think-RL-7B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"nvidia/AceReason-Math\", \"link\": \"https://huggingface.co/datasets/nvidia/AceReason-Math\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-32B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-SFT\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT\", \"task\": \"\", \"likes\": \"413\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-DPO\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO\", \"task\": \"\", \"likes\": \"332\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.15809",
    "first_seen_date": "2025-05-22",
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.15809MMaDA: Multimodal Large Diffusion Language ModelsPublished on May 21\u00b7Submitted byLing Yangon May 22Upvote97+89Authors:Ling Yang,Ye Tian,Bowen Li,Xinchen Zhang,Ke Shen,Yunhai Tong,Mengdi WangAbstractMMaDA, a multimodal diffusion foundation model, achieves superior performance through a unified architecture, mixed long chain-of-thought fine-tuning, and a unified policy-gradient-based RL algorithm.AI-generated summaryWe introduce MMaDA, a novel class ofmultimodal diffusion foundation modelsdesigned to achieve superior performance across diverse domains such as textual\nreasoning,multimodal understanding, andtext-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitatescold-start trainingfor the finalreinforcement learning(RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We proposeUniGRPO, a unifiedpolicy-gradient-based RL algorithmspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling,UniGRPOunifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits stronggeneralization capabilitiesas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B intextual reasoning, outperforms Show-o and SEED-X inmultimodal understanding, and excels over SDXL and Jan",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/Gen-Verse/MMaDA",
    "hf_paper_url": "https://huggingface.co/papers/2505.15809",
    "arxiv_url": "https://arxiv.org/abs/2505.15809",
    "num_models": 2,
    "models_list": "Gen-Verse/MMaDA-8B-Base, Gen-Verse/MMaDA-8B-MixCoT",
    "models_links": "https://huggingface.co/Gen-Verse/MMaDA-8B-Base, https://huggingface.co/Gen-Verse/MMaDA-8B-MixCoT",
    "models_detailed": "[{\"name\": \"Gen-Verse/MMaDA-8B-Base\", \"link\": \"https://huggingface.co/Gen-Verse/MMaDA-8B-Base\", \"task\": \"\", \"likes\": \"891\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"Gen-Verse/MMaDA-8B-MixCoT\", \"link\": \"https://huggingface.co/Gen-Verse/MMaDA-8B-MixCoT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 15\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.14640",
    "first_seen_date": "2025-05-21",
    "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.14640VideoEval-Pro: Robust and Realistic Long Video Understanding EvaluationPublished on May 20\u00b7Submitted byWeiming Renon May 21Upvote16+8Authors:Wentao Ma,Weiming Ren,Yiming Jia,Zhuofeng Li,Ping Nie,Ge Zhang,Wenhu ChenAbstractVideoEval-Pro, a benchmark using open-ended questions, provides a more accurate measure of long video understanding compared to existing multiple-choice question benchmarks.AI-generated summaryLarge multimodal models(LMMs) have recently emerged as a powerful tool forlong video understanding(LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existingLVU benchmarks. First, most existing\nbenchmarks rely heavily onmultiple-choice questions(MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video onVideo-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we proposeVideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video.VideoEval-Proassesses both\nsegment-level andfull-video understandingthrough perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance (>25\\%) drops on\nopen-ended questions compared w",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/TIGER-AI-Lab/VideoEval-Pro",
    "hf_paper_url": "https://huggingface.co/papers/2505.14640",
    "arxiv_url": "https://arxiv.org/abs/2505.14640",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/VideoEval-Pro",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/VideoEval-Pro\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro\", \"task\": \"\", \"likes\": \"371\", \"downloads\": \"\", \"updated\": \"May 30\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.14652",
    "first_seen_date": "2025-05-21",
    "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.14652General-Reasoner: Advancing LLM Reasoning Across All DomainsPublished on May 20\u00b7Submitted byXueguang Maon May 21\u00b7University of WaterlooUpvote24+16Authors:Xueguang Ma,Qian Liu,Dongfu Jiang,Ge Zhang,Zejun Ma,Wenhu ChenAbstractGeneral-Reasoner enhances large language model reasoning across diverse domains by using a large-scale dataset and generative model-based answer verification, outperforming existing methods.AI-generated summaryReinforcement learning(RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities oflarge language models(LLMs).\nParticularly, the \"Zero\"reinforcement learningintroduced byDeepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing agenerative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability ofchain-of-thoughtandcontext-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outpe",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/TIGER-AI-Lab/General-Reasoner",
    "hf_paper_url": "https://huggingface.co/papers/2505.14652",
    "arxiv_url": "https://arxiv.org/abs/2505.14652",
    "num_models": 4,
    "models_list": "TIGER-Lab/General-Reasoner-Qwen3-14B, TIGER-Lab/General-Reasoner-Qwen2.5-14B, TIGER-Lab/General-Reasoner-Qwen2.5-7B, TIGER-Lab/General-Reasoner-Qwen3-4B",
    "models_links": "https://huggingface.co/TIGER-Lab/General-Reasoner-Qwen3-14B, https://huggingface.co/TIGER-Lab/General-Reasoner-Qwen2.5-14B, https://huggingface.co/TIGER-Lab/General-Reasoner-Qwen2.5-7B, https://huggingface.co/TIGER-Lab/General-Reasoner-Qwen3-4B",
    "models_detailed": "[{\"name\": \"TIGER-Lab/General-Reasoner-Qwen3-14B\", \"link\": \"https://huggingface.co/TIGER-Lab/General-Reasoner-Qwen3-14B\", \"task\": \"Question Answering\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"TIGER-Lab/General-Reasoner-Qwen2.5-14B\", \"link\": \"https://huggingface.co/TIGER-Lab/General-Reasoner-Qwen2.5-14B\", \"task\": \"Text Generation\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"TIGER-Lab/General-Reasoner-Qwen2.5-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/General-Reasoner-Qwen2.5-7B\", \"task\": \"\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"TIGER-Lab/General-Reasoner-Qwen3-4B\", \"link\": \"https://huggingface.co/TIGER-Lab/General-Reasoner-Qwen3-4B\", \"task\": \"Question Answering\", \"likes\": \"53\", \"downloads\": \"\", \"updated\": \"May 21\"}]",
    "num_datasets": 5,
    "datasets_list": "TIGER-Lab/WebInstruct-verified, RLAIF-V/RLPR-Benchmarks, openbmb/RLPR-Evaluation, TIGER-Lab/WebInstruct-verified-unfiltered, RLAIF-V/RLPR-Train-Dataset",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-verified, https://huggingface.co/datasets/RLAIF-V/RLPR-Benchmarks, https://huggingface.co/datasets/openbmb/RLPR-Evaluation, https://huggingface.co/datasets/TIGER-Lab/WebInstruct-verified-unfiltered, https://huggingface.co/datasets/RLAIF-V/RLPR-Train-Dataset",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/WebInstruct-verified\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/WebInstruct-verified\", \"task\": \"\", \"likes\": \"584\", \"downloads\": \"\", \"updated\": \"24 days ago\", \"size\": \"\"}, {\"name\": \"RLAIF-V/RLPR-Benchmarks\", \"link\": \"https://huggingface.co/datasets/RLAIF-V/RLPR-Benchmarks\", \"task\": \"\", \"likes\": \"638\", \"downloads\": \"\", \"updated\": \"Jun 22\", \"size\": \"\"}, {\"name\": \"openbmb/RLPR-Evaluation\", \"link\": \"https://huggingface.co/datasets/openbmb/RLPR-Evaluation\", \"task\": \"\", \"likes\": \"638\", \"downloads\": \"\", \"updated\": \"Jul 11\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/WebInstruct-verified-unfiltered\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/WebInstruct-verified-unfiltered\", \"task\": \"\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"Jul 1\", \"size\": \"\"}, {\"name\": \"RLAIF-V/RLPR-Train-Dataset\", \"link\": \"https://huggingface.co/datasets/RLAIF-V/RLPR-Train-Dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 22\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.14683",
    "first_seen_date": "2025-05-21",
    "title": "Emerging Properties in Unified Multimodal Pretraining",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.14683Emerging Properties in Unified Multimodal PretrainingPublished on May 20\u00b7Submitted byKunchang Lion May 21Upvote134+126Authors:Chaorui Deng,Deyao Zhu,Kunchang Li,Chenhui Gou,Feng Li,Zeyu Wang,Shu Zhong,Weihao Yu,Xiaonan Nie,Ziang Song,Guang Shi,Haoqi FanAbstractBAGEL, an open-source foundational model trained on diverse multimodal data, significantly outperforms existing models in both generation and understanding tasks.AI-generated summaryUnifyingmultimodal understandingand generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0sourcefoundational modelthat natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\nontrillions of tokenscurated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities incomplex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such asfree-form image manipulation,future frame prediction,3D manipulation, andworld navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/View arXiv pageView PDFProject pageAdd to collectionCommunityAndy1621Paper authorPaper submitterMay 21Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open-source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder-only model pre",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2505.14683",
    "arxiv_url": "https://arxiv.org/abs/2505.14683",
    "num_models": 12,
    "models_list": "ByteDance-Seed/BAGEL-7B-MoT, Gapeleon/bytedance_BAGEL-7B-MoT-INT8, JiaxinGe/Diffusers-BAGEL, Ziruibest/SafeUMM, RedbeardNZ/BAGEL-7B-MoT, zanchat-ai/fast-bagel, joshmiao/bagel_mvot, Zillis/ByteDance_Seed_BAGEL_7B_MoT, exptest2/s1_onlyt2i_14kema, HappyCorpse/Bagel_harmful_medical, HappyCorpse/Bagel_harmful_medical_text, princepride/BAGEL-7B-MoT",
    "models_links": "https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT, https://huggingface.co/Gapeleon/bytedance_BAGEL-7B-MoT-INT8, https://huggingface.co/JiaxinGe/Diffusers-BAGEL, https://huggingface.co/Ziruibest/SafeUMM, https://huggingface.co/RedbeardNZ/BAGEL-7B-MoT, https://huggingface.co/zanchat-ai/fast-bagel, https://huggingface.co/joshmiao/bagel_mvot, https://huggingface.co/Zillis/ByteDance_Seed_BAGEL_7B_MoT, https://huggingface.co/exptest2/s1_onlyt2i_14kema, https://huggingface.co/HappyCorpse/Bagel_harmful_medical, https://huggingface.co/HappyCorpse/Bagel_harmful_medical_text, https://huggingface.co/princepride/BAGEL-7B-MoT",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/BAGEL-7B-MoT\", \"link\": \"https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT\", \"task\": \"\", \"likes\": \"959\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"Gapeleon/bytedance_BAGEL-7B-MoT-INT8\", \"link\": \"https://huggingface.co/Gapeleon/bytedance_BAGEL-7B-MoT-INT8\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"JiaxinGe/Diffusers-BAGEL\", \"link\": \"https://huggingface.co/JiaxinGe/Diffusers-BAGEL\", \"task\": \"\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"Ziruibest/SafeUMM\", \"link\": \"https://huggingface.co/Ziruibest/SafeUMM\", \"task\": \"\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"RedbeardNZ/BAGEL-7B-MoT\", \"link\": \"https://huggingface.co/RedbeardNZ/BAGEL-7B-MoT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"zanchat-ai/fast-bagel\", \"link\": \"https://huggingface.co/zanchat-ai/fast-bagel\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"joshmiao/bagel_mvot\", \"link\": \"https://huggingface.co/joshmiao/bagel_mvot\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"Zillis/ByteDance_Seed_BAGEL_7B_MoT\", \"link\": \"https://huggingface.co/Zillis/ByteDance_Seed_BAGEL_7B_MoT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"exptest2/s1_onlyt2i_14kema\", \"link\": \"https://huggingface.co/exptest2/s1_onlyt2i_14kema\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"HappyCorpse/Bagel_harmful_medical\", \"link\": \"https://huggingface.co/HappyCorpse/Bagel_harmful_medical\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"HappyCorpse/Bagel_harmful_medical_text\", \"link\": \"https://huggingface.co/HappyCorpse/Bagel_harmful_medical_text\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"princepride/BAGEL-7B-MoT\", \"link\": \"https://huggingface.co/princepride/BAGEL-7B-MoT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "same899/codebase",
    "datasets_links": "https://huggingface.co/datasets/same899/codebase",
    "datasets_detailed": "[{\"name\": \"same899/codebase\", \"link\": \"https://huggingface.co/datasets/same899/codebase\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.11475",
    "first_seen_date": "2025-05-20",
    "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and Languages",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.11475HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and LanguagesPublished on May 16\u00b7Submitted byZhilin Wangon May 20Upvote4Authors:Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Hoo-Chang Shin,Felipe Soares,Alexander Bukharin,Ellie Evans,Yi Dong,Oleksii KuchaievAbstractHelpSteer3-Preference, a high-quality human-annotated dataset, enhances Reward Models for Reinforcement Learning from Human Feedback, achieving top performance on RM-Bench and JudgeBench.AI-generated summaryPreference datasetsare essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we trainReward Models (RMs)that\nachieve top performance onRM-Bench(82.4%) andJudgeBench(73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to trainGenerative RMsand how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preferenceView arXiv pageView PDFProject pageAdd to collectionCommunityzhilinwPaper authorPaper submitterMay 20Data:https://huggingface.co/datasets/nvidia/HelpSteer3#preferenceSee translationReplylibrarian-botMay 21This is an automated message from theLibrarian Bot. I fou",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2505.11475",
    "arxiv_url": "https://arxiv.org/abs/2505.11475",
    "num_models": 14,
    "models_list": "nvidia/Qwen3-Nemotron-32B-RLBFF, nvidia/Llama-3_3-Nemotron-Super-49B-GenRM, nvidia/Qwen-3-Nemotron-32B-Reward, nvidia/Qwen3-Nemotron-235B-A22B-GenRM, nvidia/Qwen3-Nemotron-32B-GenRM-Principle, nvidia/Llama-3_3-Nemotron-Super-49B-GenRM-Multilingual, nvidia/Llama-3.3-Nemotron-70B-Reward, nvidia/Llama-3.3-Nemotron-70B-Reward-Multilingual, nvidia/Qwen-2.5-Nemotron-32B-Reward, Bifrost-AI/Qwen-3-Nemotron-32B-Reward-F16, nvidia/Llama-3.3-Nemotron-70B-Reward-Principle, cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit, cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit, ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx",
    "models_links": "https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-GenRM, https://huggingface.co/nvidia/Qwen-3-Nemotron-32B-Reward, https://huggingface.co/nvidia/Qwen3-Nemotron-235B-A22B-GenRM, https://huggingface.co/nvidia/Qwen3-Nemotron-32B-GenRM-Principle, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-GenRM-Multilingual, https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward, https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward-Multilingual, https://huggingface.co/nvidia/Qwen-2.5-Nemotron-32B-Reward, https://huggingface.co/Bifrost-AI/Qwen-3-Nemotron-32B-Reward-F16, https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward-Principle, https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit, https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit, https://huggingface.co/ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx",
    "models_detailed": "[{\"name\": \"nvidia/Qwen3-Nemotron-32B-RLBFF\", \"link\": \"https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF\", \"task\": \"Text Generation\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-GenRM\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-GenRM\", \"task\": \"Text Generation\", \"likes\": \"152\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"nvidia/Qwen-3-Nemotron-32B-Reward\", \"link\": \"https://huggingface.co/nvidia/Qwen-3-Nemotron-32B-Reward\", \"task\": \"\", \"likes\": \"106\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"nvidia/Qwen3-Nemotron-235B-A22B-GenRM\", \"link\": \"https://huggingface.co/nvidia/Qwen3-Nemotron-235B-A22B-GenRM\", \"task\": \"Text Generation\", \"likes\": \"122\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"nvidia/Qwen3-Nemotron-32B-GenRM-Principle\", \"link\": \"https://huggingface.co/nvidia/Qwen3-Nemotron-32B-GenRM-Principle\", \"task\": \"Text Generation\", \"likes\": \"667\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-GenRM-Multilingual\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-GenRM-Multilingual\", \"task\": \"Text Generation\", \"likes\": \"130\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"nvidia/Llama-3.3-Nemotron-70B-Reward\", \"link\": \"https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward\", \"task\": \"Text Generation\", \"likes\": \"551\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"nvidia/Llama-3.3-Nemotron-70B-Reward-Multilingual\", \"link\": \"https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward-Multilingual\", \"task\": \"Text Generation\", \"likes\": \"111\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"nvidia/Qwen-2.5-Nemotron-32B-Reward\", \"link\": \"https://huggingface.co/nvidia/Qwen-2.5-Nemotron-32B-Reward\", \"task\": \"\", \"likes\": \"82\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"Bifrost-AI/Qwen-3-Nemotron-32B-Reward-F16\", \"link\": \"https://huggingface.co/Bifrost-AI/Qwen-3-Nemotron-32B-Reward-F16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"nvidia/Llama-3.3-Nemotron-70B-Reward-Principle\", \"link\": \"https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward-Principle\", \"task\": \"Text Generation\", \"likes\": \"126\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx\", \"link\": \"https://huggingface.co/ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}]",
    "num_datasets": 3,
    "datasets_list": "nvidia/HelpSteer3, RizhongLin/MNLP_M2_dpo_dataset_HelpSteer3, Jennny/h3_pairs",
    "datasets_links": "https://huggingface.co/datasets/nvidia/HelpSteer3, https://huggingface.co/datasets/RizhongLin/MNLP_M2_dpo_dataset_HelpSteer3, https://huggingface.co/datasets/Jennny/h3_pairs",
    "datasets_detailed": "[{\"name\": \"nvidia/HelpSteer3\", \"link\": \"https://huggingface.co/datasets/nvidia/HelpSteer3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\", \"size\": \"\"}, {\"name\": \"RizhongLin/MNLP_M2_dpo_dataset_HelpSteer3\", \"link\": \"https://huggingface.co/datasets/RizhongLin/MNLP_M2_dpo_dataset_HelpSteer3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 25\", \"size\": \"\"}, {\"name\": \"Jennny/h3_pairs\", \"link\": \"https://huggingface.co/datasets/Jennny/h3_pairs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.09388",
    "first_seen_date": "2025-05-19",
    "title": "Qwen3 Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.09388Qwen3 Technical ReportPublished on May 14\u00b7Submitted byChujie Zhengon May 19#1 Paper of the day\u00b7QwenUpvote318+310Authors:An Yang,Anfeng Li,Baosong Yang,Beichen Zhang,Binyuan Hui,Bo Zheng,Bowen Yu,Chang Gao,Chengen Huang,Chenxu Lv,Chujie Zheng,Dayiheng Liu,Fan Zhou,Fei Huang,Feng Hu,Hao Ge,Haoran Wei,Huan Lin,Jialong Tang,Jian Yang,Jianhong Tu,Jianwei Zhang+38 authorsAbstractQwen3, a unified series of large language models, integrates thinking and non-thinking modes, reduces computational resources, and achieves state-of-the-art performance across various tasks and languages.AI-generated summaryIn this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series oflarge language models(LLMs) designed to advance\nperformance, efficiency, andmultilingual capabilities. The Qwen3 series\nincludes models of both dense andMixture-of-Expert(MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration ofthinking mode(for complex, multi-step reasoning) andnon-thinking mode(for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces athinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks incode g",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/QwenLM/Qwen3",
    "hf_paper_url": "https://huggingface.co/papers/2505.09388",
    "arxiv_url": "https://arxiv.org/abs/2505.09388",
    "num_models": 299,
    "models_list": "Qwen/Qwen3-Coder-480B-A35B-Instruct, Qwen/Qwen3-235B-A22B, Qwen/Qwen3-Next-80B-A3B-Instruct, Qwen/Qwen3-0.6B, Qwen/Qwen3-VL-8B-Instruct, Qwen/Qwen3-4B-Instruct-2507, Qwen/Qwen3-Coder-30B-A3B-Instruct, Qwen/Qwen3-8B, Qwen/Qwen3-VL-30B-A3B-Instruct, Qwen/Qwen3-30B-A3B-Instruct-2507, Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF, Qwen/Qwen3-4B-Thinking-2507, unsloth/Qwen3-4B-Instruct-2507-GGUF, Qwen/Qwen3-1.7B, Qwen/Qwen3-4B, unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF, Qwen/Qwen3-VL-8B-Thinking, Qwen/Qwen3-VL-30B-A3B-Thinking, Qwen/Qwen3-VL-235B-A22B-Instruct, Qwen/Qwen3-VL-4B-Instruct, Qwen/Qwen3-VL-8B-Instruct-GGUF, Qwen/Qwen3-14B, Qwen/Qwen3-32B, Qwen/Qwen3-235B-A22B-Instruct-2507, Qwen/Qwen3-235B-A22B-Thinking-2507-FP8, Qwen/Qwen3-30B-A3B-Instruct-2507-FP8, unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF, Qwen/Qwen3-30B-A3B-Thinking-2507, Qwen/Qwen3-30B-A3B-Thinking-2507-FP8, Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8, Qwen/Qwen3-Next-80B-A3B-Thinking-FP8, Qwen/Qwen3-VL-235B-A22B-Thinking, Qwen/Qwen3-VL-32B-Instruct, Qwen/Qwen3-VL-32B-Thinking, Qwen/Qwen3-VL-2B-Instruct, Qwen/Qwen3-VL-4B-Instruct-GGUF, unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF, unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF, Qwen/Qwen3-30B-A3B, Qwen/Qwen3-4B-Instruct-2507-FP8, unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit, Qwen/Qwen3-Next-80B-A3B-Instruct-FP8, unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF, p-e-w/Qwen3-4B-Instruct-2507-heretic, Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF, coder3101/Qwen3-VL-32B-Instruct-heretic-v2, Qwen/Qwen3-32B-AWQ, Qwen/Qwen3-4B-AWQ, unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF, Qwen/Qwen3-235B-A22B-Thinking-2507, cpatonn/Qwen3-4B-Thinking-2507-AWQ-4bit, unsloth/Qwen3-4B-Thinking-2507-GGUF, Qwen/Qwen3-VL-8B-Instruct-FP8, unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit, unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF, unsloth/Qwen3-VL-8B-Thinking-1M-GGUF, unsloth/Qwen3-VL-32B-Thinking-1M-GGUF, coder3101/Qwen3-VL-4B-Instruct-heretic, Qwen/Qwen3-0.6B-FP8, Qwen/Qwen3-32B-FP8, Qwen/Qwen3-14B-AWQ, Qwen/Qwen3-8B-AWQ, Qwen/Qwen3-4B-GGUF, Qwen/Qwen3-30B-A3B-GPTQ-Int4, Qwen/Qwen3-0.6B-MLX-4bit, Qwen/Qwen3-4B-MLX-4bit, Qwen/Qwen3-235B-A22B-Instruct-2507-FP8, unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF, QuantTrio/Qwen3-235B-A22B-Thinking-2507-GPTQ-Int4-Int8Mix, cpatonn/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit, unsloth/Qwen3-Coder-30B-A3B-Instruct, cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit, Qwen/Qwen3-4B-Thinking-2507-FP8, cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit, unsloth/Qwen3-4B-Instruct-2507-bnb-4bit, Qwen/Qwen3-Next-80B-A3B-Thinking, cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit, unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF, Qwen/Qwen3-VL-235B-A22B-Instruct-FP8, Qwen/Qwen3-VL-30B-A3B-Instruct-FP8, Qwen/Qwen3-VL-30B-A3B-Thinking-FP8, QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ, Qwen/Qwen3-VL-4B-Thinking, Qwen/Qwen3-VL-4B-Instruct-FP8, unsloth/Qwen3-VL-8B-Thinking, cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit, cpatonn/Qwen3-VL-4B-Instruct-AWQ-8bit, Qwen/Qwen3-VL-2B-Instruct-FP8, Qwen/Qwen3-VL-2B-Thinking, unsloth/Qwen3-VL-4B-Instruct-GGUF, unsloth/Qwen3-VL-32B-Instruct-GGUF, Qwen/Qwen3-VL-2B-Instruct-GGUF, Qwen/Qwen3-VL-8B-Thinking-GGUF, Qwen/Qwen3-VL-4B-Thinking-GGUF, Qwen/Qwen3-VL-30B-A3B-Instruct-GGUF, Qwen/Qwen3-VL-235B-A22B-Instruct-GGUF, unsloth/Qwen3-VL-8B-Instruct-1M-GGUF, svjack/Qwen3-VL-4B-Instruct-heretic-7refusal, IIEleven11/Qwen3-4B-abliterated_dark, ChuGyouk/Qwen3-14B-Base, coder3101/Qwen3-VL-32B-Thinking-heretic-v2, Qwen/Qwen3-30B-A3B-Base, Qwen/Qwen3-8B-Base, Qwen/Qwen3-4B-Base, Qwen/Qwen3-1.7B-Base, Qwen/Qwen3-14B-Base, Qwen/Qwen3-0.6B-Base, Qwen/Qwen3-1.7B-FP8, Qwen/Qwen3-4B-FP8, Qwen/Qwen3-8B-FP8, unsloth/Qwen3-0.6B-Base, Qwen/Qwen3-14B-FP8, unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit, unsloth/Qwen3-0.6B-Base-bnb-4bit, unsloth/Qwen3-8B-Base, unsloth/Qwen3-8B-Base-unsloth-bnb-4bit, unsloth/Qwen3-8B-Base-bnb-4bit, unsloth/Qwen3-14B-Base, unsloth/Qwen3-14B-Base-unsloth-bnb-4bit, unsloth/Qwen3-14B-Base-bnb-4bit, Qwen/Qwen3-30B-A3B-FP8, Qwen/Qwen3-235B-A22B-FP8, JunHowie/Qwen3-0.6B-GPTQ-Int4, JunHowie/Qwen3-0.6B-GPTQ-Int8, JunHowie/Qwen3-1.7B-GPTQ-Int4, JunHowie/Qwen3-1.7B-GPTQ-Int8, JunHowie/Qwen3-32B-GPTQ-Int4, JunHowie/Qwen3-32B-GPTQ-Int8, JunHowie/Qwen3-30B-A3B-GPTQ-Int4, JunHowie/Qwen3-14B-GPTQ-Int8, JunHowie/Qwen3-14B-GPTQ-Int4, JunHowie/Qwen3-8B-GPTQ-Int8, JunHowie/Qwen3-8B-GPTQ-Int4, JunHowie/Qwen3-4B-GPTQ-Int4, JunHowie/Qwen3-4B-GPTQ-Int8, Qwen/Qwen3-8B-GGUF, Qwen/Qwen3-30B-A3B-GGUF, ThomasTheMaker/Qwen3_0.6B_v.1.2.0, Qwen/Qwen3-1.7B-GPTQ-Int8, Qwen/Qwen3-0.6B-GPTQ-Int8, Qwen/Qwen3-235B-A22B-GPTQ-Int4, Qwen/Qwen3-235B-A22B-GGUF, JunHowie/Qwen3-30B-A3B-GPTQ-Int8, QuantFactory/Qwen3-0.6B-GGUF, QuantFactory/Qwen3-1.7B-GGUF, 94insane/Qwen3-14b-lora-codeAlpaca_20k, picklerick42069/Qwen3-235B-A22B-EXL3-3.0bpw, Mungert/Qwen3-1.7B-GGUF, Qwen/Qwen3-0.6B-MLX-6bit, Qwen/Qwen3-0.6B-MLX-bf16, Qwen/Qwen3-0.6B-MLX-8bit, Qwen/Qwen3-1.7B-MLX-bf16, Qwen/Qwen3-8B-MLX-6bit, Qwen/Qwen3-1.7B-MLX-6bit, Qwen/Qwen3-1.7B-MLX-8bit, Qwen/Qwen3-1.7B-MLX-4bit, Qwen/Qwen3-8B-MLX-4bit, Qwen/Qwen3-8B-MLX-8bit, Qwen/Qwen3-8B-MLX-bf16, Qwen/Qwen3-4B-MLX-bf16, Qwen/Qwen3-14B-MLX-8bit, Qwen/Qwen3-4B-MLX-8bit, Qwen/Qwen3-14B-MLX-6bit, Qwen/Qwen3-4B-MLX-6bit, Qwen/Qwen3-14B-MLX-4bit, smikulas/MNLP_M2_rag_model, ibrahimkettaneh/Qwen3-0.6B-abliterated, TachyHealth/Gazal-R1-32B-GRPO-preview, allura-forge/q3-8b-ft-ep2-merged, VityaVitalich/Qwen3-1.7B, VityaVitalich/Qwen3-4B, Vikhrmodels/QVikhr-3-1.7B-Instruction-noreasoning, clowman/Qwen3-32B-FP8, clowman/Qwen3-30B-A3B-FP8, qingy2024/GRMR-V3-Q1.7B, qingy2024/GRMR-V3-Q4B, ThomasTheMaker/Qwen3-1.7B-RKLLM-v1.2.0, Erland/Qwen3Softpick-8B-Base, Moeb96/Qwen3-14B, NewstaR/Newstar-Qwen3-0.6B, ThomasTheMaker/Qwen3-4B-RKLLM-v1.2.0, KhushRai78/qwen3-0.6B-GRPO-LORA, schmuell/Qwen3-1.7B, Qwen/Qwen3-32B-MLX-bf16, Qwen/Qwen3-32B-MLX-8bit, Qwen/Qwen3-32B-MLX-6bit, Qwen/Qwen3-32B-MLX-4bit, Qwen/Qwen3-30B-A3B-MLX-bf16, Qwen/Qwen3-30B-A3B-MLX-8bit, Qwen/Qwen3-30B-A3B-MLX-6bit, Qwen/Qwen3-30B-A3B-MLX-4bit, Qwen/Qwen3-235B-A22B-MLX-bf16, Qwen/Qwen3-235B-A22B-MLX-4bit, Qwen/Qwen3-235B-A22B-MLX-6bit, Qwen/Qwen3-235B-A22B-MLX-8bit, Qwen/Qwen3-14B-MLX-bf16, allura-forge/q3-8b-sft-take2-adpt-ep1-merged, allura-forge/q3-8b-sft-take2-adpt-ep2-merged, QuixiAI/Qwen3-72B-Embiggened, QuixiAI/Qwen3-58B-Embiggened, RoadToNowhere/Qwen3-0.6B-ao-float8wo, muzerai/qwen3-8b-aijoah-magic8, QuixiAI/Qwen3-58B-Embiggened-gguf, QuixiAI/Qwen3-72B-Embiggened-gguf, openentry/qwen3-8b-merge-openentry, openentry/qwen3-8b-merge-openentry-GGUF, Alphatao/Affine-2883486, Alphatao/Affine-3590802, Alphatao/Affine-6043407, Alphatao/Affine-2922728, Alphatao/Affine-9459823, Alphatao/Affine-6692834, Alphatao/Affine-5878053, Alphatao/Affine-9801198, Alphatao/Affine-2501551, Alphatao/Affine-1901852, Alphatao/Affine-1855255, Alphatao/Affine-5956831, Alphatao/Affine-9711767, Alphatao/Affine-7341712, Alphatao/Affine-6817055, Alphatao/Affine-1710883, Alphatao/Affine-7470548, Alphatao/Affine-2333827, kldzj/Qwen3-235B-A22B-bnb-8bit, aarnphm/qwen3-235b-a22b-fp8-sharded-tp4, aarnphm/qwen3-30b-a3b-sharded-tp2, NVFP4/Qwen3-32B-FP4, IntervitensInc/Qwen3-235B-A22B-tt-ckpt, codys12/Qwen3-235B-A22B, networkstatic/qwen3-0-6b-this-is-an-extraordinarily-long-model-name-created-to-reproduce-llm-d-bug-20250626, Vikhrmodels/QVikhr-3-4B-Instruction, Alphatao/Affine-5246433, Vikhrmodels/QVikhr-3-4B-Instruction-GGUF, marketeam/Qwen-Marketing, abhaygupta/Qwen3-14B, abhaygupta/Qwen3-32B, Alphatao/Affine-3784603, tf919/qwen3-8b-test, codys12/Qwen3-8B-BitNet, PF94/Qwen3-8B-4.0bpw-exl2, PF94/Qwen3-8B-6.0bpw-exl2, Bot2025/Qwen3-14B-with-Yarn, MichiganNLP/TAMA-QWen2.5, MichiganNLP/TAMA-QWen3, huggit0000/Qwen3-0.6B-GGUF-FP32, float-trip/qwen-drama, float-trip/qwen-3-14b-drama, ramblingpolymath/Qwen3-32B-W8A8, ramblingpolymath/Qwen3-14B-W8A8, ramblingpolymath/Qwen3-8B-W8A8, ramblingpolymath/Qwen3-4B-W8A8, ramblingpolymath/qwen3-30B-A3B-w8a8, ramblingpolymath/Qwen3-0.6B-W8A8, justinj92/qwen3-32b-malMlym, FastFlowLM/Qwen3-4B-NPU2, justinj92/qwen3-32b-malayalam-v1, justinj92/qwen3-32b-malayalam-v1-GGUF, nickosn/qwen30, abheekga/Qwen3-8B-bnb-4bit, JetLM/SDAR-1.7B-Chat, unsloth/Qwen3-235B-A22B-Instruct-2507-FP8, unsloth/Qwen3-235B-A22B-Instruct-2507, chriswritescode/Qwen3-235B-A22B-Instruct-2507-INT4-W4A16, Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8, unsloth/Qwen3-Coder-480B-A35B-Instruct, unsloth/Qwen3-Coder-480B-A35B-Instruct-FP8, unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF, unsloth/Qwen3-Coder-480B-A35B-Instruct-1M, unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF, koushd/Qwen3-235B-A22B-Instruct-2507-AWQ, NVFP4/Qwen3-235B-A22B-Instruct-2507-FP4, NVFP4/Qwen3-Coder-480B-A35B-Instruct-FP4, bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-4.0bpw, codys12/Qwen3-8B, QuantTrio/Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix, TechxGenus/Qwen3-Coder-480B-A35B-Instruct-AWQ, QuantTrio/Qwen3-235B-A22B-Instruct-2507-AWQ, bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-5.0bpw, bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-3.5bpw, bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-3.0bpw, Kwai-Klear/Klear-Qwen3-Thinking-Preview, Rorical/qode-30b, unsloth/Qwen3-235B-A22B-Thinking-2507, unsloth/Qwen3-235B-A22B-Thinking-2507-FP8, Rorical/qode-14b, QuantTrio/Qwen3-Coder-480B-A35B-Instruct-AWQ, QuantTrio/Qwen3-Coder-480B-A35B-Instruct-GPTQ-Int4-Int8Mix, QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ, NVFP4/Qwen3-235B-A22B-Thinking-2507-FP4, chriswritescode/Qwen3-235B-A22B-Instruct-2507-AWQ-Swift, IntervitensInc/Qwen3-235B-A22B-Instruct-2507-tt-ckpt, bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-4.0bpw, bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-3.5bpw, whaoyang/Qwen3-1.7B-rk3588-1.2.1-hybrid-0.25, bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-3.0bpw, unsloth/Qwen3-30B-A3B-Instruct-2507, unsloth/Qwen3-30B-A3B-Instruct-2507-FP8, Rorical/qode-4b",
    "models_links": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct, https://huggingface.co/Qwen/Qwen3-235B-A22B, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct, https://huggingface.co/Qwen/Qwen3-0.6B, https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct, https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507, https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct, https://huggingface.co/Qwen/Qwen3-8B, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct, https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507, https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF, https://huggingface.co/Qwen/Qwen3-1.7B, https://huggingface.co/Qwen/Qwen3-4B, https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-14B, https://huggingface.co/Qwen/Qwen3-32B, https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507, https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507-FP8, https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8, https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF, https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507, https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507-FP8, https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-FP8, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-30B-A3B, https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507-FP8, https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF, https://huggingface.co/p-e-w/Qwen3-4B-Instruct-2507-heretic, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF, https://huggingface.co/coder3101/Qwen3-VL-32B-Instruct-heretic-v2, https://huggingface.co/Qwen/Qwen3-32B-AWQ, https://huggingface.co/Qwen/Qwen3-4B-AWQ, https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF, https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507, https://huggingface.co/cpatonn/Qwen3-4B-Thinking-2507-AWQ-4bit, https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF, https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF, https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-1M-GGUF, https://huggingface.co/coder3101/Qwen3-VL-4B-Instruct-heretic, https://huggingface.co/Qwen/Qwen3-0.6B-FP8, https://huggingface.co/Qwen/Qwen3-32B-FP8, https://huggingface.co/Qwen/Qwen3-14B-AWQ, https://huggingface.co/Qwen/Qwen3-8B-AWQ, https://huggingface.co/Qwen/Qwen3-4B-GGUF, https://huggingface.co/Qwen/Qwen3-30B-A3B-GPTQ-Int4, https://huggingface.co/Qwen/Qwen3-0.6B-MLX-4bit, https://huggingface.co/Qwen/Qwen3-4B-MLX-4bit, https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8, https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF, https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Thinking-2507-GPTQ-Int4-Int8Mix, https://huggingface.co/cpatonn/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit, https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct, https://huggingface.co/cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit, https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507-FP8, https://huggingface.co/cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit, https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-bnb-4bit, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking, https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct-FP8, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct-FP8, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking-FP8, https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ, https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking, https://huggingface.co/cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit, https://huggingface.co/cpatonn/Qwen3-VL-4B-Instruct-AWQ-8bit, https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-FP8, https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking, https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-1M-GGUF, https://huggingface.co/svjack/Qwen3-VL-4B-Instruct-heretic-7refusal, https://huggingface.co/IIEleven11/Qwen3-4B-abliterated_dark, https://huggingface.co/ChuGyouk/Qwen3-14B-Base, https://huggingface.co/coder3101/Qwen3-VL-32B-Thinking-heretic-v2, https://huggingface.co/Qwen/Qwen3-30B-A3B-Base, https://huggingface.co/Qwen/Qwen3-8B-Base, https://huggingface.co/Qwen/Qwen3-4B-Base, https://huggingface.co/Qwen/Qwen3-1.7B-Base, https://huggingface.co/Qwen/Qwen3-14B-Base, https://huggingface.co/Qwen/Qwen3-0.6B-Base, https://huggingface.co/Qwen/Qwen3-1.7B-FP8, https://huggingface.co/Qwen/Qwen3-4B-FP8, https://huggingface.co/Qwen/Qwen3-8B-FP8, https://huggingface.co/unsloth/Qwen3-0.6B-Base, https://huggingface.co/Qwen/Qwen3-14B-FP8, https://huggingface.co/unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-0.6B-Base-bnb-4bit, https://huggingface.co/unsloth/Qwen3-8B-Base, https://huggingface.co/unsloth/Qwen3-8B-Base-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-8B-Base-bnb-4bit, https://huggingface.co/unsloth/Qwen3-14B-Base, https://huggingface.co/unsloth/Qwen3-14B-Base-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-14B-Base-bnb-4bit, https://huggingface.co/Qwen/Qwen3-30B-A3B-FP8, https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8, https://huggingface.co/JunHowie/Qwen3-0.6B-GPTQ-Int4, https://huggingface.co/JunHowie/Qwen3-0.6B-GPTQ-Int8, https://huggingface.co/JunHowie/Qwen3-1.7B-GPTQ-Int4, https://huggingface.co/JunHowie/Qwen3-1.7B-GPTQ-Int8, https://huggingface.co/JunHowie/Qwen3-32B-GPTQ-Int4, https://huggingface.co/JunHowie/Qwen3-32B-GPTQ-Int8, https://huggingface.co/JunHowie/Qwen3-30B-A3B-GPTQ-Int4, https://huggingface.co/JunHowie/Qwen3-14B-GPTQ-Int8, https://huggingface.co/JunHowie/Qwen3-14B-GPTQ-Int4, https://huggingface.co/JunHowie/Qwen3-8B-GPTQ-Int8, https://huggingface.co/JunHowie/Qwen3-8B-GPTQ-Int4, https://huggingface.co/JunHowie/Qwen3-4B-GPTQ-Int4, https://huggingface.co/JunHowie/Qwen3-4B-GPTQ-Int8, https://huggingface.co/Qwen/Qwen3-8B-GGUF, https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF, https://huggingface.co/ThomasTheMaker/Qwen3_0.6B_v.1.2.0, https://huggingface.co/Qwen/Qwen3-1.7B-GPTQ-Int8, https://huggingface.co/Qwen/Qwen3-0.6B-GPTQ-Int8, https://huggingface.co/Qwen/Qwen3-235B-A22B-GPTQ-Int4, https://huggingface.co/Qwen/Qwen3-235B-A22B-GGUF, https://huggingface.co/JunHowie/Qwen3-30B-A3B-GPTQ-Int8, https://huggingface.co/QuantFactory/Qwen3-0.6B-GGUF, https://huggingface.co/QuantFactory/Qwen3-1.7B-GGUF, https://huggingface.co/94insane/Qwen3-14b-lora-codeAlpaca_20k, https://huggingface.co/picklerick42069/Qwen3-235B-A22B-EXL3-3.0bpw, https://huggingface.co/Mungert/Qwen3-1.7B-GGUF, https://huggingface.co/Qwen/Qwen3-0.6B-MLX-6bit, https://huggingface.co/Qwen/Qwen3-0.6B-MLX-bf16, https://huggingface.co/Qwen/Qwen3-0.6B-MLX-8bit, https://huggingface.co/Qwen/Qwen3-1.7B-MLX-bf16, https://huggingface.co/Qwen/Qwen3-8B-MLX-6bit, https://huggingface.co/Qwen/Qwen3-1.7B-MLX-6bit, https://huggingface.co/Qwen/Qwen3-1.7B-MLX-8bit, https://huggingface.co/Qwen/Qwen3-1.7B-MLX-4bit, https://huggingface.co/Qwen/Qwen3-8B-MLX-4bit, https://huggingface.co/Qwen/Qwen3-8B-MLX-8bit, https://huggingface.co/Qwen/Qwen3-8B-MLX-bf16, https://huggingface.co/Qwen/Qwen3-4B-MLX-bf16, https://huggingface.co/Qwen/Qwen3-14B-MLX-8bit, https://huggingface.co/Qwen/Qwen3-4B-MLX-8bit, https://huggingface.co/Qwen/Qwen3-14B-MLX-6bit, https://huggingface.co/Qwen/Qwen3-4B-MLX-6bit, https://huggingface.co/Qwen/Qwen3-14B-MLX-4bit, https://huggingface.co/smikulas/MNLP_M2_rag_model, https://huggingface.co/ibrahimkettaneh/Qwen3-0.6B-abliterated, https://huggingface.co/TachyHealth/Gazal-R1-32B-GRPO-preview, https://huggingface.co/allura-forge/q3-8b-ft-ep2-merged, https://huggingface.co/VityaVitalich/Qwen3-1.7B, https://huggingface.co/VityaVitalich/Qwen3-4B, https://huggingface.co/Vikhrmodels/QVikhr-3-1.7B-Instruction-noreasoning, https://huggingface.co/clowman/Qwen3-32B-FP8, https://huggingface.co/clowman/Qwen3-30B-A3B-FP8, https://huggingface.co/qingy2024/GRMR-V3-Q1.7B, https://huggingface.co/qingy2024/GRMR-V3-Q4B, https://huggingface.co/ThomasTheMaker/Qwen3-1.7B-RKLLM-v1.2.0, https://huggingface.co/Erland/Qwen3Softpick-8B-Base, https://huggingface.co/Moeb96/Qwen3-14B, https://huggingface.co/NewstaR/Newstar-Qwen3-0.6B, https://huggingface.co/ThomasTheMaker/Qwen3-4B-RKLLM-v1.2.0, https://huggingface.co/KhushRai78/qwen3-0.6B-GRPO-LORA, https://huggingface.co/schmuell/Qwen3-1.7B, https://huggingface.co/Qwen/Qwen3-32B-MLX-bf16, https://huggingface.co/Qwen/Qwen3-32B-MLX-8bit, https://huggingface.co/Qwen/Qwen3-32B-MLX-6bit, https://huggingface.co/Qwen/Qwen3-32B-MLX-4bit, https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-bf16, https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-8bit, https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-6bit, https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit, https://huggingface.co/Qwen/Qwen3-235B-A22B-MLX-bf16, https://huggingface.co/Qwen/Qwen3-235B-A22B-MLX-4bit, https://huggingface.co/Qwen/Qwen3-235B-A22B-MLX-6bit, https://huggingface.co/Qwen/Qwen3-235B-A22B-MLX-8bit, https://huggingface.co/Qwen/Qwen3-14B-MLX-bf16, https://huggingface.co/allura-forge/q3-8b-sft-take2-adpt-ep1-merged, https://huggingface.co/allura-forge/q3-8b-sft-take2-adpt-ep2-merged, https://huggingface.co/QuixiAI/Qwen3-72B-Embiggened, https://huggingface.co/QuixiAI/Qwen3-58B-Embiggened, https://huggingface.co/RoadToNowhere/Qwen3-0.6B-ao-float8wo, https://huggingface.co/muzerai/qwen3-8b-aijoah-magic8, https://huggingface.co/QuixiAI/Qwen3-58B-Embiggened-gguf, https://huggingface.co/QuixiAI/Qwen3-72B-Embiggened-gguf, https://huggingface.co/openentry/qwen3-8b-merge-openentry, https://huggingface.co/openentry/qwen3-8b-merge-openentry-GGUF, https://huggingface.co/Alphatao/Affine-2883486, https://huggingface.co/Alphatao/Affine-3590802, https://huggingface.co/Alphatao/Affine-6043407, https://huggingface.co/Alphatao/Affine-2922728, https://huggingface.co/Alphatao/Affine-9459823, https://huggingface.co/Alphatao/Affine-6692834, https://huggingface.co/Alphatao/Affine-5878053, https://huggingface.co/Alphatao/Affine-9801198, https://huggingface.co/Alphatao/Affine-2501551, https://huggingface.co/Alphatao/Affine-1901852, https://huggingface.co/Alphatao/Affine-1855255, https://huggingface.co/Alphatao/Affine-5956831, https://huggingface.co/Alphatao/Affine-9711767, https://huggingface.co/Alphatao/Affine-7341712, https://huggingface.co/Alphatao/Affine-6817055, https://huggingface.co/Alphatao/Affine-1710883, https://huggingface.co/Alphatao/Affine-7470548, https://huggingface.co/Alphatao/Affine-2333827, https://huggingface.co/kldzj/Qwen3-235B-A22B-bnb-8bit, https://huggingface.co/aarnphm/qwen3-235b-a22b-fp8-sharded-tp4, https://huggingface.co/aarnphm/qwen3-30b-a3b-sharded-tp2, https://huggingface.co/NVFP4/Qwen3-32B-FP4, https://huggingface.co/IntervitensInc/Qwen3-235B-A22B-tt-ckpt, https://huggingface.co/codys12/Qwen3-235B-A22B, https://huggingface.co/networkstatic/qwen3-0-6b-this-is-an-extraordinarily-long-model-name-created-to-reproduce-llm-d-bug-20250626, https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction, https://huggingface.co/Alphatao/Affine-5246433, https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF, https://huggingface.co/marketeam/Qwen-Marketing, https://huggingface.co/abhaygupta/Qwen3-14B, https://huggingface.co/abhaygupta/Qwen3-32B, https://huggingface.co/Alphatao/Affine-3784603, https://huggingface.co/tf919/qwen3-8b-test, https://huggingface.co/codys12/Qwen3-8B-BitNet, https://huggingface.co/PF94/Qwen3-8B-4.0bpw-exl2, https://huggingface.co/PF94/Qwen3-8B-6.0bpw-exl2, https://huggingface.co/Bot2025/Qwen3-14B-with-Yarn, https://huggingface.co/MichiganNLP/TAMA-QWen2.5, https://huggingface.co/MichiganNLP/TAMA-QWen3, https://huggingface.co/huggit0000/Qwen3-0.6B-GGUF-FP32, https://huggingface.co/float-trip/qwen-drama, https://huggingface.co/float-trip/qwen-3-14b-drama, https://huggingface.co/ramblingpolymath/Qwen3-32B-W8A8, https://huggingface.co/ramblingpolymath/Qwen3-14B-W8A8, https://huggingface.co/ramblingpolymath/Qwen3-8B-W8A8, https://huggingface.co/ramblingpolymath/Qwen3-4B-W8A8, https://huggingface.co/ramblingpolymath/qwen3-30B-A3B-w8a8, https://huggingface.co/ramblingpolymath/Qwen3-0.6B-W8A8, https://huggingface.co/justinj92/qwen3-32b-malMlym, https://huggingface.co/FastFlowLM/Qwen3-4B-NPU2, https://huggingface.co/justinj92/qwen3-32b-malayalam-v1, https://huggingface.co/justinj92/qwen3-32b-malayalam-v1-GGUF, https://huggingface.co/nickosn/qwen30, https://huggingface.co/abheekga/Qwen3-8B-bnb-4bit, https://huggingface.co/JetLM/SDAR-1.7B-Chat, https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-FP8, https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507, https://huggingface.co/chriswritescode/Qwen3-235B-A22B-Instruct-2507-INT4-W4A16, https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct, https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M, https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF, https://huggingface.co/koushd/Qwen3-235B-A22B-Instruct-2507-AWQ, https://huggingface.co/NVFP4/Qwen3-235B-A22B-Instruct-2507-FP4, https://huggingface.co/NVFP4/Qwen3-Coder-480B-A35B-Instruct-FP4, https://huggingface.co/bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-4.0bpw, https://huggingface.co/codys12/Qwen3-8B, https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix, https://huggingface.co/TechxGenus/Qwen3-Coder-480B-A35B-Instruct-AWQ, https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Instruct-2507-AWQ, https://huggingface.co/bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-5.0bpw, https://huggingface.co/bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-3.5bpw, https://huggingface.co/bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-3.0bpw, https://huggingface.co/Kwai-Klear/Klear-Qwen3-Thinking-Preview, https://huggingface.co/Rorical/qode-30b, https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507, https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507-FP8, https://huggingface.co/Rorical/qode-14b, https://huggingface.co/QuantTrio/Qwen3-Coder-480B-A35B-Instruct-AWQ, https://huggingface.co/QuantTrio/Qwen3-Coder-480B-A35B-Instruct-GPTQ-Int4-Int8Mix, https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ, https://huggingface.co/NVFP4/Qwen3-235B-A22B-Thinking-2507-FP4, https://huggingface.co/chriswritescode/Qwen3-235B-A22B-Instruct-2507-AWQ-Swift, https://huggingface.co/IntervitensInc/Qwen3-235B-A22B-Instruct-2507-tt-ckpt, https://huggingface.co/bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-4.0bpw, https://huggingface.co/bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-3.5bpw, https://huggingface.co/whaoyang/Qwen3-1.7B-rk3588-1.2.1-hybrid-0.25, https://huggingface.co/bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-3.0bpw, https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507, https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-FP8, https://huggingface.co/Rorical/qode-4b",
    "models_detailed": "[{\"name\": \"Qwen/Qwen3-Coder-480B-A35B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 21\"}, {\"name\": \"Qwen/Qwen3-235B-A22B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Qwen/Qwen3-0.6B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-0.6B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Qwen/Qwen3-4B-Instruct-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Qwen/Qwen3-Coder-30B-A3B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"Qwen/Qwen3-8B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-Instruct-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"Qwen/Qwen3-4B-Thinking-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"unsloth/Qwen3-4B-Instruct-2507-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"Qwen/Qwen3-1.7B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-1.7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-4B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 8\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-14B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-32B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-Instruct-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-Thinking-2507-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-Thinking-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 17\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-Thinking-2507-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-32B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"Qwen/Qwen3-VL-32B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"10 days ago\"}, {\"name\": \"unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"Qwen/Qwen3-30B-A3B\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-4B-Instruct-2507-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 13\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"p-e-w/Qwen3-4B-Instruct-2507-heretic\", \"link\": \"https://huggingface.co/p-e-w/Qwen3-4B-Instruct-2507-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"coder3101/Qwen3-VL-32B-Instruct-heretic-v2\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-32B-Instruct-heretic-v2\", \"task\": \"\", \"likes\": \"181\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"Qwen/Qwen3-32B-AWQ\", \"link\": \"https://huggingface.co/Qwen/Qwen3-32B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-4B-AWQ\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-Thinking-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 17\"}, {\"name\": \"cpatonn/Qwen3-4B-Thinking-2507-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-4B-Thinking-2507-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"902\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"unsloth/Qwen3-4B-Thinking-2507-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Thinking-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Thinking-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"coder3101/Qwen3-VL-4B-Instruct-heretic\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-4B-Instruct-heretic\", \"task\": \"\", \"likes\": \"262\", \"downloads\": \"\", \"updated\": \"29 days ago\"}, {\"name\": \"Qwen/Qwen3-0.6B-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-0.6B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-32B-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-32B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-14B-AWQ\", \"link\": \"https://huggingface.co/Qwen/Qwen3-14B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-8B-AWQ\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-4B-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-GPTQ-Int4\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-0.6B-MLX-4bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-0.6B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"Qwen/Qwen3-4B-MLX-4bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"QuantTrio/Qwen3-235B-A22B-Thinking-2507-GPTQ-Int4-Int8Mix\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Thinking-2507-GPTQ-Int4-Int8Mix\", \"task\": \"Text Generation\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"cpatonn/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"unsloth/Qwen3-Coder-30B-A3B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-Coder-30B-A3B-Instruct-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"Qwen/Qwen3-4B-Thinking-2507-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"unsloth/Qwen3-4B-Instruct-2507-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 15\"}, {\"name\": \"cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Thinking\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"cpatonn/Qwen3-VL-4B-Instruct-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-4B-Instruct-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"svjack/Qwen3-VL-4B-Instruct-heretic-7refusal\", \"link\": \"https://huggingface.co/svjack/Qwen3-VL-4B-Instruct-heretic-7refusal\", \"task\": \"\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"about 1\"}, {\"name\": \"IIEleven11/Qwen3-4B-abliterated_dark\", \"link\": \"https://huggingface.co/IIEleven11/Qwen3-4B-abliterated_dark\", \"task\": \"Text Generation\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"ChuGyouk/Qwen3-14B-Base\", \"link\": \"https://huggingface.co/ChuGyouk/Qwen3-14B-Base\", \"task\": \"Text Generation\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"coder3101/Qwen3-VL-32B-Thinking-heretic-v2\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-32B-Thinking-heretic-v2\", \"task\": \"\", \"likes\": \"99\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-Base\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-8B-Base\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-4B-Base\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-1.7B-Base\", \"link\": \"https://huggingface.co/Qwen/Qwen3-1.7B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-14B-Base\", \"link\": \"https://huggingface.co/Qwen/Qwen3-14B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-0.6B-Base\", \"link\": \"https://huggingface.co/Qwen/Qwen3-0.6B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-1.7B-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-1.7B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-4B-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-8B-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"unsloth/Qwen3-0.6B-Base\", \"link\": \"https://huggingface.co/unsloth/Qwen3-0.6B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"Qwen/Qwen3-14B-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-14B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"unsloth/Qwen3-0.6B-Base-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-0.6B-Base-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"unsloth/Qwen3-8B-Base\", \"link\": \"https://huggingface.co/unsloth/Qwen3-8B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"unsloth/Qwen3-8B-Base-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-8B-Base-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"unsloth/Qwen3-8B-Base-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-8B-Base-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"772\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"unsloth/Qwen3-14B-Base\", \"link\": \"https://huggingface.co/unsloth/Qwen3-14B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"unsloth/Qwen3-14B-Base-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-14B-Base-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"unsloth/Qwen3-14B-Base-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-14B-Base-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"JunHowie/Qwen3-0.6B-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-0.6B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"321\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"JunHowie/Qwen3-0.6B-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-0.6B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"JunHowie/Qwen3-1.7B-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-1.7B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"69\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"JunHowie/Qwen3-1.7B-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-1.7B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"JunHowie/Qwen3-32B-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-32B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"635\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"JunHowie/Qwen3-32B-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-32B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"JunHowie/Qwen3-30B-A3B-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-30B-A3B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"122\", \"downloads\": \"\", \"updated\": \"Sep 6\"}, {\"name\": \"JunHowie/Qwen3-14B-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-14B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"142\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"JunHowie/Qwen3-14B-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-14B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"622\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"JunHowie/Qwen3-8B-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-8B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 4\"}, {\"name\": \"JunHowie/Qwen3-8B-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-8B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 4\"}, {\"name\": \"JunHowie/Qwen3-4B-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-4B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"421\", \"downloads\": \"\", \"updated\": \"Sep 4\"}, {\"name\": \"JunHowie/Qwen3-4B-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-4B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 4\"}, {\"name\": \"Qwen/Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"ThomasTheMaker/Qwen3_0.6B_v.1.2.0\", \"link\": \"https://huggingface.co/ThomasTheMaker/Qwen3_0.6B_v.1.2.0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"Qwen/Qwen3-1.7B-GPTQ-Int8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-1.7B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-0.6B-GPTQ-Int8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-0.6B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-GPTQ-Int4\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"531\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"JunHowie/Qwen3-30B-A3B-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-30B-A3B-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 6\"}, {\"name\": \"QuantFactory/Qwen3-0.6B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/Qwen3-0.6B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"366\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"QuantFactory/Qwen3-1.7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/Qwen3-1.7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"422\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"94insane/Qwen3-14b-lora-codeAlpaca_20k\", \"link\": \"https://huggingface.co/94insane/Qwen3-14b-lora-codeAlpaca_20k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 23\"}, {\"name\": \"picklerick42069/Qwen3-235B-A22B-EXL3-3.0bpw\", \"link\": \"https://huggingface.co/picklerick42069/Qwen3-235B-A22B-EXL3-3.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"Mungert/Qwen3-1.7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-1.7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Qwen/Qwen3-0.6B-MLX-6bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-0.6B-MLX-6bit\", \"task\": \"Text Generation\", \"likes\": \"84\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-0.6B-MLX-bf16\", \"link\": \"https://huggingface.co/Qwen/Qwen3-0.6B-MLX-bf16\", \"task\": \"Text Generation\", \"likes\": \"426\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-0.6B-MLX-8bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-0.6B-MLX-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-1.7B-MLX-bf16\", \"link\": \"https://huggingface.co/Qwen/Qwen3-1.7B-MLX-bf16\", \"task\": \"Text Generation\", \"likes\": \"151\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-8B-MLX-6bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B-MLX-6bit\", \"task\": \"Text Generation\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-1.7B-MLX-6bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-1.7B-MLX-6bit\", \"task\": \"Text Generation\", \"likes\": \"99\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-1.7B-MLX-8bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-1.7B-MLX-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-1.7B-MLX-4bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-1.7B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-8B-MLX-4bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"486\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-8B-MLX-8bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B-MLX-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-8B-MLX-bf16\", \"link\": \"https://huggingface.co/Qwen/Qwen3-8B-MLX-bf16\", \"task\": \"Text Generation\", \"likes\": \"207\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-4B-MLX-bf16\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-MLX-bf16\", \"task\": \"Text Generation\", \"likes\": \"87\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-14B-MLX-8bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-14B-MLX-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-4B-MLX-8bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-MLX-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-14B-MLX-6bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-14B-MLX-6bit\", \"task\": \"Text Generation\", \"likes\": \"79\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-4B-MLX-6bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-4B-MLX-6bit\", \"task\": \"Text Generation\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-14B-MLX-4bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-14B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"378\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"smikulas/MNLP_M2_rag_model\", \"link\": \"https://huggingface.co/smikulas/MNLP_M2_rag_model\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 27\"}, {\"name\": \"ibrahimkettaneh/Qwen3-0.6B-abliterated\", \"link\": \"https://huggingface.co/ibrahimkettaneh/Qwen3-0.6B-abliterated\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 26\"}, {\"name\": \"TachyHealth/Gazal-R1-32B-GRPO-preview\", \"link\": \"https://huggingface.co/TachyHealth/Gazal-R1-32B-GRPO-preview\", \"task\": \"Text Generation\", \"likes\": \"543\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"allura-forge/q3-8b-ft-ep2-merged\", \"link\": \"https://huggingface.co/allura-forge/q3-8b-ft-ep2-merged\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 27\"}, {\"name\": \"VityaVitalich/Qwen3-1.7B\", \"link\": \"https://huggingface.co/VityaVitalich/Qwen3-1.7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"VityaVitalich/Qwen3-4B\", \"link\": \"https://huggingface.co/VityaVitalich/Qwen3-4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"Vikhrmodels/QVikhr-3-1.7B-Instruction-noreasoning\", \"link\": \"https://huggingface.co/Vikhrmodels/QVikhr-3-1.7B-Instruction-noreasoning\", \"task\": \"Text Generation\", \"likes\": \"205\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"clowman/Qwen3-32B-FP8\", \"link\": \"https://huggingface.co/clowman/Qwen3-32B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"clowman/Qwen3-30B-A3B-FP8\", \"link\": \"https://huggingface.co/clowman/Qwen3-30B-A3B-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"qingy2024/GRMR-V3-Q1.7B\", \"link\": \"https://huggingface.co/qingy2024/GRMR-V3-Q1.7B\", \"task\": \"Text Generation\", \"likes\": \"85\", \"downloads\": \"\", \"updated\": \"Jun 3\"}, {\"name\": \"qingy2024/GRMR-V3-Q4B\", \"link\": \"https://huggingface.co/qingy2024/GRMR-V3-Q4B\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"ThomasTheMaker/Qwen3-1.7B-RKLLM-v1.2.0\", \"link\": \"https://huggingface.co/ThomasTheMaker/Qwen3-1.7B-RKLLM-v1.2.0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 3\"}, {\"name\": \"Erland/Qwen3Softpick-8B-Base\", \"link\": \"https://huggingface.co/Erland/Qwen3Softpick-8B-Base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"Moeb96/Qwen3-14B\", \"link\": \"https://huggingface.co/Moeb96/Qwen3-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"NewstaR/Newstar-Qwen3-0.6B\", \"link\": \"https://huggingface.co/NewstaR/Newstar-Qwen3-0.6B\", \"task\": \"Text Generation\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Jun 7\"}, {\"name\": \"ThomasTheMaker/Qwen3-4B-RKLLM-v1.2.0\", \"link\": \"https://huggingface.co/ThomasTheMaker/Qwen3-4B-RKLLM-v1.2.0\", \"task\": \"Text Generation\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Jun 8\"}, {\"name\": \"KhushRai78/qwen3-0.6B-GRPO-LORA\", \"link\": \"https://huggingface.co/KhushRai78/qwen3-0.6B-GRPO-LORA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"schmuell/Qwen3-1.7B\", \"link\": \"https://huggingface.co/schmuell/Qwen3-1.7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"Qwen/Qwen3-32B-MLX-bf16\", \"link\": \"https://huggingface.co/Qwen/Qwen3-32B-MLX-bf16\", \"task\": \"Text Generation\", \"likes\": \"268\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-32B-MLX-8bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-32B-MLX-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-32B-MLX-6bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-32B-MLX-6bit\", \"task\": \"Text Generation\", \"likes\": \"78\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-32B-MLX-4bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-32B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"151\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-MLX-bf16\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-bf16\", \"task\": \"Text Generation\", \"likes\": \"110\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-MLX-8bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-8bit\", \"task\": \"Text Generation\", \"likes\": \"752\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-MLX-6bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-6bit\", \"task\": \"Text Generation\", \"likes\": \"62\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-MLX-4bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"566\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-MLX-bf16\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-MLX-bf16\", \"task\": \"Text Generation\", \"likes\": \"86\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-MLX-4bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"172\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-MLX-6bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-MLX-6bit\", \"task\": \"Text Generation\", \"likes\": \"60\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-MLX-8bit\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-MLX-8bit\", \"task\": \"Text Generation\", \"likes\": \"133\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"Qwen/Qwen3-14B-MLX-bf16\", \"link\": \"https://huggingface.co/Qwen/Qwen3-14B-MLX-bf16\", \"task\": \"Text Generation\", \"likes\": \"65\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"allura-forge/q3-8b-sft-take2-adpt-ep1-merged\", \"link\": \"https://huggingface.co/allura-forge/q3-8b-sft-take2-adpt-ep1-merged\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 12\"}, {\"name\": \"allura-forge/q3-8b-sft-take2-adpt-ep2-merged\", \"link\": \"https://huggingface.co/allura-forge/q3-8b-sft-take2-adpt-ep2-merged\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 13\"}, {\"name\": \"QuixiAI/Qwen3-72B-Embiggened\", \"link\": \"https://huggingface.co/QuixiAI/Qwen3-72B-Embiggened\", \"task\": \"\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"QuixiAI/Qwen3-58B-Embiggened\", \"link\": \"https://huggingface.co/QuixiAI/Qwen3-58B-Embiggened\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"RoadToNowhere/Qwen3-0.6B-ao-float8wo\", \"link\": \"https://huggingface.co/RoadToNowhere/Qwen3-0.6B-ao-float8wo\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 13\"}, {\"name\": \"muzerai/qwen3-8b-aijoah-magic8\", \"link\": \"https://huggingface.co/muzerai/qwen3-8b-aijoah-magic8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"QuixiAI/Qwen3-58B-Embiggened-gguf\", \"link\": \"https://huggingface.co/QuixiAI/Qwen3-58B-Embiggened-gguf\", \"task\": \"\", \"likes\": \"205\", \"downloads\": \"\", \"updated\": \"Jun 13\"}, {\"name\": \"QuixiAI/Qwen3-72B-Embiggened-gguf\", \"link\": \"https://huggingface.co/QuixiAI/Qwen3-72B-Embiggened-gguf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 13\"}, {\"name\": \"openentry/qwen3-8b-merge-openentry\", \"link\": \"https://huggingface.co/openentry/qwen3-8b-merge-openentry\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"openentry/qwen3-8b-merge-openentry-GGUF\", \"link\": \"https://huggingface.co/openentry/qwen3-8b-merge-openentry-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"Alphatao/Affine-2883486\", \"link\": \"https://huggingface.co/Alphatao/Affine-2883486\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"Alphatao/Affine-3590802\", \"link\": \"https://huggingface.co/Alphatao/Affine-3590802\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"Alphatao/Affine-6043407\", \"link\": \"https://huggingface.co/Alphatao/Affine-6043407\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"Alphatao/Affine-2922728\", \"link\": \"https://huggingface.co/Alphatao/Affine-2922728\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"Alphatao/Affine-9459823\", \"link\": \"https://huggingface.co/Alphatao/Affine-9459823\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"Alphatao/Affine-6692834\", \"link\": \"https://huggingface.co/Alphatao/Affine-6692834\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"Alphatao/Affine-5878053\", \"link\": \"https://huggingface.co/Alphatao/Affine-5878053\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"Alphatao/Affine-9801198\", \"link\": \"https://huggingface.co/Alphatao/Affine-9801198\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"Alphatao/Affine-2501551\", \"link\": \"https://huggingface.co/Alphatao/Affine-2501551\", \"task\": \"Text Generation\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"Alphatao/Affine-1901852\", \"link\": \"https://huggingface.co/Alphatao/Affine-1901852\", \"task\": \"Text Generation\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Alphatao/Affine-1855255\", \"link\": \"https://huggingface.co/Alphatao/Affine-1855255\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Alphatao/Affine-5956831\", \"link\": \"https://huggingface.co/Alphatao/Affine-5956831\", \"task\": \"Text Generation\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Alphatao/Affine-9711767\", \"link\": \"https://huggingface.co/Alphatao/Affine-9711767\", \"task\": \"Text Generation\", \"likes\": \"27\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Alphatao/Affine-7341712\", \"link\": \"https://huggingface.co/Alphatao/Affine-7341712\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Alphatao/Affine-6817055\", \"link\": \"https://huggingface.co/Alphatao/Affine-6817055\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Alphatao/Affine-1710883\", \"link\": \"https://huggingface.co/Alphatao/Affine-1710883\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"Alphatao/Affine-7470548\", \"link\": \"https://huggingface.co/Alphatao/Affine-7470548\", \"task\": \"Text Generation\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Jun 21\"}, {\"name\": \"Alphatao/Affine-2333827\", \"link\": \"https://huggingface.co/Alphatao/Affine-2333827\", \"task\": \"Text Generation\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Jun 21\"}, {\"name\": \"kldzj/Qwen3-235B-A22B-bnb-8bit\", \"link\": \"https://huggingface.co/kldzj/Qwen3-235B-A22B-bnb-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 22\"}, {\"name\": \"aarnphm/qwen3-235b-a22b-fp8-sharded-tp4\", \"link\": \"https://huggingface.co/aarnphm/qwen3-235b-a22b-fp8-sharded-tp4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 23\"}, {\"name\": \"aarnphm/qwen3-30b-a3b-sharded-tp2\", \"link\": \"https://huggingface.co/aarnphm/qwen3-30b-a3b-sharded-tp2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 23\"}, {\"name\": \"NVFP4/Qwen3-32B-FP4\", \"link\": \"https://huggingface.co/NVFP4/Qwen3-32B-FP4\", \"task\": \"Text Generation\", \"likes\": \"406\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"IntervitensInc/Qwen3-235B-A22B-tt-ckpt\", \"link\": \"https://huggingface.co/IntervitensInc/Qwen3-235B-A22B-tt-ckpt\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"codys12/Qwen3-235B-A22B\", \"link\": \"https://huggingface.co/codys12/Qwen3-235B-A22B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 27\"}, {\"name\": \"networkstatic/qwen3-0-6b-this-is-an-extraordinarily-long-model-name-created-to-reproduce-llm-d-bug-20250626\", \"link\": \"https://huggingface.co/networkstatic/qwen3-0-6b-this-is-an-extraordinarily-long-model-name-created-to-reproduce-llm-d-bug-20250626\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Vikhrmodels/QVikhr-3-4B-Instruction\", \"link\": \"https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction\", \"task\": \"Text Generation\", \"likes\": \"561\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"Alphatao/Affine-5246433\", \"link\": \"https://huggingface.co/Alphatao/Affine-5246433\", \"task\": \"Text Generation\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"Vikhrmodels/QVikhr-3-4B-Instruction-GGUF\", \"link\": \"https://huggingface.co/Vikhrmodels/QVikhr-3-4B-Instruction-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"marketeam/Qwen-Marketing\", \"link\": \"https://huggingface.co/marketeam/Qwen-Marketing\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"abhaygupta/Qwen3-14B\", \"link\": \"https://huggingface.co/abhaygupta/Qwen3-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"abhaygupta/Qwen3-32B\", \"link\": \"https://huggingface.co/abhaygupta/Qwen3-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"Alphatao/Affine-3784603\", \"link\": \"https://huggingface.co/Alphatao/Affine-3784603\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"tf919/qwen3-8b-test\", \"link\": \"https://huggingface.co/tf919/qwen3-8b-test\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"codys12/Qwen3-8B-BitNet\", \"link\": \"https://huggingface.co/codys12/Qwen3-8B-BitNet\", \"task\": \"Text Generation\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"Jul 7\"}, {\"name\": \"PF94/Qwen3-8B-4.0bpw-exl2\", \"link\": \"https://huggingface.co/PF94/Qwen3-8B-4.0bpw-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"PF94/Qwen3-8B-6.0bpw-exl2\", \"link\": \"https://huggingface.co/PF94/Qwen3-8B-6.0bpw-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"Bot2025/Qwen3-14B-with-Yarn\", \"link\": \"https://huggingface.co/Bot2025/Qwen3-14B-with-Yarn\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 3\"}, {\"name\": \"MichiganNLP/TAMA-QWen2.5\", \"link\": \"https://huggingface.co/MichiganNLP/TAMA-QWen2.5\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Aug 22\"}, {\"name\": \"MichiganNLP/TAMA-QWen3\", \"link\": \"https://huggingface.co/MichiganNLP/TAMA-QWen3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 22\"}, {\"name\": \"huggit0000/Qwen3-0.6B-GGUF-FP32\", \"link\": \"https://huggingface.co/huggit0000/Qwen3-0.6B-GGUF-FP32\", \"task\": \"Text Generation\", \"likes\": \"35\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"float-trip/qwen-drama\", \"link\": \"https://huggingface.co/float-trip/qwen-drama\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"float-trip/qwen-3-14b-drama\", \"link\": \"https://huggingface.co/float-trip/qwen-3-14b-drama\", \"task\": \"Text Generation\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"ramblingpolymath/Qwen3-32B-W8A8\", \"link\": \"https://huggingface.co/ramblingpolymath/Qwen3-32B-W8A8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"ramblingpolymath/Qwen3-14B-W8A8\", \"link\": \"https://huggingface.co/ramblingpolymath/Qwen3-14B-W8A8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"ramblingpolymath/Qwen3-8B-W8A8\", \"link\": \"https://huggingface.co/ramblingpolymath/Qwen3-8B-W8A8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"ramblingpolymath/Qwen3-4B-W8A8\", \"link\": \"https://huggingface.co/ramblingpolymath/Qwen3-4B-W8A8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"ramblingpolymath/qwen3-30B-A3B-w8a8\", \"link\": \"https://huggingface.co/ramblingpolymath/qwen3-30B-A3B-w8a8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"ramblingpolymath/Qwen3-0.6B-W8A8\", \"link\": \"https://huggingface.co/ramblingpolymath/Qwen3-0.6B-W8A8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 3\"}, {\"name\": \"justinj92/qwen3-32b-malMlym\", \"link\": \"https://huggingface.co/justinj92/qwen3-32b-malMlym\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"FastFlowLM/Qwen3-4B-NPU2\", \"link\": \"https://huggingface.co/FastFlowLM/Qwen3-4B-NPU2\", \"task\": \"Text Generation\", \"likes\": \"97\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"justinj92/qwen3-32b-malayalam-v1\", \"link\": \"https://huggingface.co/justinj92/qwen3-32b-malayalam-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"justinj92/qwen3-32b-malayalam-v1-GGUF\", \"link\": \"https://huggingface.co/justinj92/qwen3-32b-malayalam-v1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}, {\"name\": \"nickosn/qwen30\", \"link\": \"https://huggingface.co/nickosn/qwen30\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"abheekga/Qwen3-8B-bnb-4bit\", \"link\": \"https://huggingface.co/abheekga/Qwen3-8B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"JetLM/SDAR-1.7B-Chat\", \"link\": \"https://huggingface.co/JetLM/SDAR-1.7B-Chat\", \"task\": \"Text Generation\", \"likes\": \"562\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"unsloth/Qwen3-235B-A22B-Instruct-2507-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"94\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"unsloth/Qwen3-235B-A22B-Instruct-2507\", \"link\": \"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507\", \"task\": \"Text Generation\", \"likes\": \"86\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"chriswritescode/Qwen3-235B-A22B-Instruct-2507-INT4-W4A16\", \"link\": \"https://huggingface.co/chriswritescode/Qwen3-235B-A22B-Instruct-2507-INT4-W4A16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 21\"}, {\"name\": \"unsloth/Qwen3-Coder-480B-A35B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"unsloth/Qwen3-Coder-480B-A35B-Instruct-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-FP8\", \"task\": \"Text Generation\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"unsloth/Qwen3-Coder-480B-A35B-Instruct-1M\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M\", \"task\": \"Text Generation\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"koushd/Qwen3-235B-A22B-Instruct-2507-AWQ\", \"link\": \"https://huggingface.co/koushd/Qwen3-235B-A22B-Instruct-2507-AWQ\", \"task\": \"Text Generation\", \"likes\": \"70\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"NVFP4/Qwen3-235B-A22B-Instruct-2507-FP4\", \"link\": \"https://huggingface.co/NVFP4/Qwen3-235B-A22B-Instruct-2507-FP4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"NVFP4/Qwen3-Coder-480B-A35B-Instruct-FP4\", \"link\": \"https://huggingface.co/NVFP4/Qwen3-Coder-480B-A35B-Instruct-FP4\", \"task\": \"Text Generation\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-4.0bpw\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-4.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"codys12/Qwen3-8B\", \"link\": \"https://huggingface.co/codys12/Qwen3-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"QuantTrio/Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Instruct-2507-GPTQ-Int4-Int8Mix\", \"task\": \"Text Generation\", \"likes\": \"349\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"TechxGenus/Qwen3-Coder-480B-A35B-Instruct-AWQ\", \"link\": \"https://huggingface.co/TechxGenus/Qwen3-Coder-480B-A35B-Instruct-AWQ\", \"task\": \"Text Generation\", \"likes\": \"47\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"QuantTrio/Qwen3-235B-A22B-Instruct-2507-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Instruct-2507-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-5.0bpw\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-5.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-3.5bpw\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-3.5bpw\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-3.0bpw\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-235B-A22B-Instruct-2507-exl3-3.0bpw\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"Kwai-Klear/Klear-Qwen3-Thinking-Preview\", \"link\": \"https://huggingface.co/Kwai-Klear/Klear-Qwen3-Thinking-Preview\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Rorical/qode-30b\", \"link\": \"https://huggingface.co/Rorical/qode-30b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"unsloth/Qwen3-235B-A22B-Thinking-2507\", \"link\": \"https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507\", \"task\": \"Text Generation\", \"likes\": \"101\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"unsloth/Qwen3-235B-A22B-Thinking-2507-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-235B-A22B-Thinking-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"Rorical/qode-14b\", \"link\": \"https://huggingface.co/Rorical/qode-14b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"QuantTrio/Qwen3-Coder-480B-A35B-Instruct-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-Coder-480B-A35B-Instruct-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"QuantTrio/Qwen3-Coder-480B-A35B-Instruct-GPTQ-Int4-Int8Mix\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-Coder-480B-A35B-Instruct-GPTQ-Int4-Int8Mix\", \"task\": \"Text Generation\", \"likes\": \"293\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-235B-A22B-Thinking-2507-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"NVFP4/Qwen3-235B-A22B-Thinking-2507-FP4\", \"link\": \"https://huggingface.co/NVFP4/Qwen3-235B-A22B-Thinking-2507-FP4\", \"task\": \"Text Generation\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"chriswritescode/Qwen3-235B-A22B-Instruct-2507-AWQ-Swift\", \"link\": \"https://huggingface.co/chriswritescode/Qwen3-235B-A22B-Instruct-2507-AWQ-Swift\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"IntervitensInc/Qwen3-235B-A22B-Instruct-2507-tt-ckpt\", \"link\": \"https://huggingface.co/IntervitensInc/Qwen3-235B-A22B-Instruct-2507-tt-ckpt\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-4.0bpw\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-4.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-3.5bpw\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-3.5bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"whaoyang/Qwen3-1.7B-rk3588-1.2.1-hybrid-0.25\", \"link\": \"https://huggingface.co/whaoyang/Qwen3-1.7B-rk3588-1.2.1-hybrid-0.25\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-3.0bpw\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-235B-A22B-Thinking-2507-exl3-3.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"unsloth/Qwen3-30B-A3B-Instruct-2507\", \"link\": \"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"unsloth/Qwen3-30B-A3B-Instruct-2507-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"Rorical/qode-4b\", \"link\": \"https://huggingface.co/Rorical/qode-4b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.10475",
    "first_seen_date": "2025-05-16",
    "title": "Parallel Scaling Law for Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.10475Parallel Scaling Law for Language ModelsPublished on May 15\u00b7Submitted byNiels Roggeon May 16#2 Paper of the dayUpvote83+75Authors:Mouxiang Chen,Binyuan Hui,Zeyu Cui,Jiaxi Yang,Dayiheng Liu,Jianling Sun,Junyang Lin,Zhongxin LiuAbstractParallel scaling (ParScale) improves inference efficiency by reusing existing parameters and executing multiple transformations in parallel, offering superior performance with reduced memory and latency compared to parameter scaling.AI-generated summaryIt is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply P diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the P outputs. This method, namelyparallel scaling(ParScale), scalesparallel computationby reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a newscaling lawand validate it\nthrough large-scale pre-training, which shows that a model with P parallel\nstreams is similar to scaling the parameters by O(log P) while showing\nsuperiorinference efficiency. For example,ParScalecan use up to 22times\nlessmemory increaseand 6times lesslatency increasecompared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one bypost-trainingon a small amount of tokens, further reducing the training budget. The newscaling lawwe discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspectiv",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/QwenLM/ParScale",
    "hf_paper_url": "https://huggingface.co/papers/2505.10475",
    "arxiv_url": "https://arxiv.org/abs/2505.10475",
    "num_models": 67,
    "models_list": "ParScale/ParScale-1.8B-P8, ParScale/ParScale-1.8B-P8-Inst, ParScale/ParScale-1.8B-P1, ParScale/ParScale-1.8B-P4, ParScale/ParScale-0.7B-P1-Pile, ParScale/ParScale-0.7B-P1-Python, ParScale/ParScale-0.7B-P2-Pile, ParScale/ParScale-0.7B-P2-Python, ParScale/ParScale-0.7B-P4-Pile, ParScale/ParScale-0.7B-P4-Python, ParScale/ParScale-0.7B-P8-Pile, ParScale/ParScale-0.7B-P8-Python, ParScale/ParScale-0.9B-P1-Python, ParScale/ParScale-0.9B-P2-Python, ParScale/ParScale-0.9B-P4-Python, ParScale/ParScale-0.9B-P8-Python, ParScale/ParScale-0.9B-P8-Pile, ParScale/ParScale-0.9B-P4-Pile, ParScale/ParScale-0.9B-P2-Pile, ParScale/ParScale-0.9B-P1-Pile, ParScale/ParScale-1.3B-P8-Python, ParScale/ParScale-1.3B-P4-Python, ParScale/ParScale-1.3B-P2-Python, ParScale/ParScale-1.3B-P1-Python, ParScale/ParScale-1.3B-P1-Pile, ParScale/ParScale-1.3B-P2-Pile, ParScale/ParScale-1.3B-P4-Pile, ParScale/ParScale-1.3B-P8-Pile, ParScale/ParScale-1.8B-P8-Pile, ParScale/ParScale-1.8B-P4-Pile, ParScale/ParScale-1.8B-P2-Pile, ParScale/ParScale-1.8B-P1-Pile, ParScale/ParScale-1.8B-P2, ParScale/ParScale-1.8B-P4-Inst, ParScale/ParScale-1.8B-P2-Inst, ParScale/ParScale-1.8B-P1-Inst, ParScale/ParScale-1.8B-P1-Python, ParScale/ParScale-1.8B-P2-Python, ParScale/ParScale-1.8B-P4-Python, ParScale/ParScale-1.8B-P8-Python, ParScale/ParScale-4.7B-P8-Python, ParScale/ParScale-4.7B-P4-Python, ParScale/ParScale-4.7B-P2-Python, ParScale/ParScale-4.7B-P1-Python, ParScale/ParScale-4.7B-P1-Pile, ParScale/ParScale-4.7B-P2-Pile, ParScale/ParScale-4.7B-P4-Pile, ParScale/ParScale-4.7B-P8-Pile, ParScale/ParScale-3B-P1-Pile, ParScale/ParScale-3B-P1-Python, ParScale/ParScale-3B-P2-Pile, ParScale/ParScale-3B-P2-Python, ParScale/ParScale-3B-P4-Pile, ParScale/ParScale-3B-P4-Python, ParScale/ParScale-3B-P8-Pile, ParScale/ParScale-3B-P8-Python, ParScale/ParScale-QwenInit-3B-P1-Pile, ParScale/ParScale-QwenInit-3B-P1-Python, ParScale/ParScale-QwenInit-3B-P2-Pile, ParScale/ParScale-QwenInit-3B-P2-Python, ParScale/ParScale-QwenInit-3B-P4-Pile, ParScale/ParScale-QwenInit-3B-P4-Python, ParScale/ParScale-QwenInit-3B-P8-Pile, ParScale/ParScale-QwenInit-3B-P8-Python, ParScale/ParScale-Qwen-3B-P2-Python, ParScale/ParScale-Qwen-3B-P4-Python, ParScale/ParScale-Qwen-3B-P8-Python",
    "models_links": "https://huggingface.co/ParScale/ParScale-1.8B-P8, https://huggingface.co/ParScale/ParScale-1.8B-P8-Inst, https://huggingface.co/ParScale/ParScale-1.8B-P1, https://huggingface.co/ParScale/ParScale-1.8B-P4, https://huggingface.co/ParScale/ParScale-0.7B-P1-Pile, https://huggingface.co/ParScale/ParScale-0.7B-P1-Python, https://huggingface.co/ParScale/ParScale-0.7B-P2-Pile, https://huggingface.co/ParScale/ParScale-0.7B-P2-Python, https://huggingface.co/ParScale/ParScale-0.7B-P4-Pile, https://huggingface.co/ParScale/ParScale-0.7B-P4-Python, https://huggingface.co/ParScale/ParScale-0.7B-P8-Pile, https://huggingface.co/ParScale/ParScale-0.7B-P8-Python, https://huggingface.co/ParScale/ParScale-0.9B-P1-Python, https://huggingface.co/ParScale/ParScale-0.9B-P2-Python, https://huggingface.co/ParScale/ParScale-0.9B-P4-Python, https://huggingface.co/ParScale/ParScale-0.9B-P8-Python, https://huggingface.co/ParScale/ParScale-0.9B-P8-Pile, https://huggingface.co/ParScale/ParScale-0.9B-P4-Pile, https://huggingface.co/ParScale/ParScale-0.9B-P2-Pile, https://huggingface.co/ParScale/ParScale-0.9B-P1-Pile, https://huggingface.co/ParScale/ParScale-1.3B-P8-Python, https://huggingface.co/ParScale/ParScale-1.3B-P4-Python, https://huggingface.co/ParScale/ParScale-1.3B-P2-Python, https://huggingface.co/ParScale/ParScale-1.3B-P1-Python, https://huggingface.co/ParScale/ParScale-1.3B-P1-Pile, https://huggingface.co/ParScale/ParScale-1.3B-P2-Pile, https://huggingface.co/ParScale/ParScale-1.3B-P4-Pile, https://huggingface.co/ParScale/ParScale-1.3B-P8-Pile, https://huggingface.co/ParScale/ParScale-1.8B-P8-Pile, https://huggingface.co/ParScale/ParScale-1.8B-P4-Pile, https://huggingface.co/ParScale/ParScale-1.8B-P2-Pile, https://huggingface.co/ParScale/ParScale-1.8B-P1-Pile, https://huggingface.co/ParScale/ParScale-1.8B-P2, https://huggingface.co/ParScale/ParScale-1.8B-P4-Inst, https://huggingface.co/ParScale/ParScale-1.8B-P2-Inst, https://huggingface.co/ParScale/ParScale-1.8B-P1-Inst, https://huggingface.co/ParScale/ParScale-1.8B-P1-Python, https://huggingface.co/ParScale/ParScale-1.8B-P2-Python, https://huggingface.co/ParScale/ParScale-1.8B-P4-Python, https://huggingface.co/ParScale/ParScale-1.8B-P8-Python, https://huggingface.co/ParScale/ParScale-4.7B-P8-Python, https://huggingface.co/ParScale/ParScale-4.7B-P4-Python, https://huggingface.co/ParScale/ParScale-4.7B-P2-Python, https://huggingface.co/ParScale/ParScale-4.7B-P1-Python, https://huggingface.co/ParScale/ParScale-4.7B-P1-Pile, https://huggingface.co/ParScale/ParScale-4.7B-P2-Pile, https://huggingface.co/ParScale/ParScale-4.7B-P4-Pile, https://huggingface.co/ParScale/ParScale-4.7B-P8-Pile, https://huggingface.co/ParScale/ParScale-3B-P1-Pile, https://huggingface.co/ParScale/ParScale-3B-P1-Python, https://huggingface.co/ParScale/ParScale-3B-P2-Pile, https://huggingface.co/ParScale/ParScale-3B-P2-Python, https://huggingface.co/ParScale/ParScale-3B-P4-Pile, https://huggingface.co/ParScale/ParScale-3B-P4-Python, https://huggingface.co/ParScale/ParScale-3B-P8-Pile, https://huggingface.co/ParScale/ParScale-3B-P8-Python, https://huggingface.co/ParScale/ParScale-QwenInit-3B-P1-Pile, https://huggingface.co/ParScale/ParScale-QwenInit-3B-P1-Python, https://huggingface.co/ParScale/ParScale-QwenInit-3B-P2-Pile, https://huggingface.co/ParScale/ParScale-QwenInit-3B-P2-Python, https://huggingface.co/ParScale/ParScale-QwenInit-3B-P4-Pile, https://huggingface.co/ParScale/ParScale-QwenInit-3B-P4-Python, https://huggingface.co/ParScale/ParScale-QwenInit-3B-P8-Pile, https://huggingface.co/ParScale/ParScale-QwenInit-3B-P8-Python, https://huggingface.co/ParScale/ParScale-Qwen-3B-P2-Python, https://huggingface.co/ParScale/ParScale-Qwen-3B-P4-Python, https://huggingface.co/ParScale/ParScale-Qwen-3B-P8-Python",
    "models_detailed": "[{\"name\": \"ParScale/ParScale-1.8B-P8\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P8\", \"task\": \"Text Generation\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P8-Inst\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P8-Inst\", \"task\": \"Text Generation\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P1\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P1\", \"task\": \"Text Generation\", \"likes\": \"113\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P4\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P4\", \"task\": \"Text Generation\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.7B-P1-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.7B-P1-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.7B-P1-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.7B-P1-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.7B-P2-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.7B-P2-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.7B-P2-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.7B-P2-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.7B-P4-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.7B-P4-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.7B-P4-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.7B-P4-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.7B-P8-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.7B-P8-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.7B-P8-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.7B-P8-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.9B-P1-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.9B-P1-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.9B-P2-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.9B-P2-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.9B-P4-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.9B-P4-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.9B-P8-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.9B-P8-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.9B-P8-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.9B-P8-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.9B-P4-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.9B-P4-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.9B-P2-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.9B-P2-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-0.9B-P1-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-0.9B-P1-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.3B-P8-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.3B-P8-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.3B-P4-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.3B-P4-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.3B-P2-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.3B-P2-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.3B-P1-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.3B-P1-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.3B-P1-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.3B-P1-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.3B-P2-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.3B-P2-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.3B-P4-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.3B-P4-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.3B-P8-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.3B-P8-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P8-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P8-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P4-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P4-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P2-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P2-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P1-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P1-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P2\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P4-Inst\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P4-Inst\", \"task\": \"Text Generation\", \"likes\": \"29\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P2-Inst\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P2-Inst\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P1-Inst\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P1-Inst\", \"task\": \"Text Generation\", \"likes\": \"92\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P1-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P1-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P2-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P2-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P4-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P4-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-1.8B-P8-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-1.8B-P8-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-4.7B-P8-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-4.7B-P8-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-4.7B-P4-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-4.7B-P4-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-4.7B-P2-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-4.7B-P2-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-4.7B-P1-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-4.7B-P1-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-4.7B-P1-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-4.7B-P1-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-4.7B-P2-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-4.7B-P2-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-4.7B-P4-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-4.7B-P4-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-4.7B-P8-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-4.7B-P8-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-3B-P1-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-3B-P1-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-3B-P1-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-3B-P1-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-3B-P2-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-3B-P2-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-3B-P2-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-3B-P2-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-3B-P4-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-3B-P4-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-3B-P4-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-3B-P4-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-3B-P8-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-3B-P8-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-3B-P8-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-3B-P8-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-QwenInit-3B-P1-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-QwenInit-3B-P1-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-QwenInit-3B-P1-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-QwenInit-3B-P1-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-QwenInit-3B-P2-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-QwenInit-3B-P2-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-QwenInit-3B-P2-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-QwenInit-3B-P2-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-QwenInit-3B-P4-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-QwenInit-3B-P4-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-QwenInit-3B-P4-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-QwenInit-3B-P4-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-QwenInit-3B-P8-Pile\", \"link\": \"https://huggingface.co/ParScale/ParScale-QwenInit-3B-P8-Pile\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-QwenInit-3B-P8-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-QwenInit-3B-P8-Python\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-Qwen-3B-P2-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-Qwen-3B-P2-Python\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-Qwen-3B-P4-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-Qwen-3B-P4-Python\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"ParScale/ParScale-Qwen-3B-P8-Python\", \"link\": \"https://huggingface.co/ParScale/ParScale-Qwen-3B-P8-Python\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.10527",
    "first_seen_date": "2025-05-16",
    "title": "WorldPM: Scaling Human Preference Modeling",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.10527WorldPM: Scaling Human Preference ModelingPublished on May 15\u00b7Submitted byBowen Yuon May 16Upvote34+26Authors:Binghai Wang,Runji Lin,Keming Lu,Le Yu,Zhenru Zhang,Fei Huang,Chujie Zheng,Kai Dang,Yang Fan,Xingzhang Ren,An Yang,Binyuan Hui,Dayiheng Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Bowen Yu,Jingren Zhou,Junyang LinAbstractWorld Preference Modeling (WorldPM) enhances preference fine-tuning through large-scale preference data, showing scalability benefits in adversarial and objective metrics, and improving generalization across various human preference datasets.AI-generated summaryMotivated by scaling laws in language modeling that demonstrate how test loss\nscales as a power law with model and dataset sizes, we find that similar laws\nexist in preference modeling. We proposeWorld Preference Modeling$ (WorldPM)\nto emphasize this scaling potential, where World Preference embodies a unified\nrepresentation of human preferences. In this paper, we collectpreference datafrom public forums covering diverse user communities, and conduct extensive\ntraining using 15M-scale data across models ranging from 1.5B to 72B\nparameters. We observe distinct patterns across different evaluation metrics:\n(1)Adversarial metrics(ability to identify deceptive features) consistently\nscale up with increased training data and base model size; (2) Objective\nmetrics (objective knowledge with well-defined answers) show emergent behavior\nin larger language models, highlighting WorldPM's scalability potential; (3)Subjective metrics(subjective preferences from a limited number of humans or\nAI) do not demonstrate scaling trends. Further experiments validate the\neffectiveness of WorldPM as a foundation forpreference fine-tuning. Through\nevaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly\nimproves thegeneralization performanceacross humanpreference datasets of\nvarying sizes (",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/qwenlm/worldpm",
    "hf_paper_url": "https://huggingface.co/papers/2505.10527",
    "arxiv_url": "https://arxiv.org/abs/2505.10527",
    "num_models": 5,
    "models_list": "Qwen/WorldPM-72B, Qwen/WorldPM-72B-HelpSteer2, Qwen/WorldPM-72B-RLHFLow, Qwen/WorldPM-72B-UltraFeedback, KnutJaegersberg/WorldPM-72B-4bit",
    "models_links": "https://huggingface.co/Qwen/WorldPM-72B, https://huggingface.co/Qwen/WorldPM-72B-HelpSteer2, https://huggingface.co/Qwen/WorldPM-72B-RLHFLow, https://huggingface.co/Qwen/WorldPM-72B-UltraFeedback, https://huggingface.co/KnutJaegersberg/WorldPM-72B-4bit",
    "models_detailed": "[{\"name\": \"Qwen/WorldPM-72B\", \"link\": \"https://huggingface.co/Qwen/WorldPM-72B\", \"task\": \"\", \"likes\": \"111\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"Qwen/WorldPM-72B-HelpSteer2\", \"link\": \"https://huggingface.co/Qwen/WorldPM-72B-HelpSteer2\", \"task\": \"\", \"likes\": \"84\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"Qwen/WorldPM-72B-RLHFLow\", \"link\": \"https://huggingface.co/Qwen/WorldPM-72B-RLHFLow\", \"task\": \"\", \"likes\": \"75\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"Qwen/WorldPM-72B-UltraFeedback\", \"link\": \"https://huggingface.co/Qwen/WorldPM-72B-UltraFeedback\", \"task\": \"\", \"likes\": \"115\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"KnutJaegersberg/WorldPM-72B-4bit\", \"link\": \"https://huggingface.co/KnutJaegersberg/WorldPM-72B-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.08311",
    "first_seen_date": "2025-05-14",
    "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.08311AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B ScalePublished on May 13\u00b7Submitted byAKon May 14#3 Paper of the dayUpvote18+10Authors:Yunjie Ji,Xiaoyu Tian,Sitong Zhao,Haotian Wang,Shuaiting Chen,Yiping Peng,Han Zhao,Xiangang LiAbstractAM-Thinking-v1, a 32B dense language model, achieves state-of-the-art performance in mathematical and coding tasks by leveraging supervised fine-tuning and reinforcement learning, demonstrating the capabilities of mid-scale open-source models.AI-generated summaryWe present AM-Thinking-v1, a 32Bdense language modelthat advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leadingMixture-of-Experts(MoE) models likeQwen3-235B-A22BandSeed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 onAIME2024, 74.4 onAIME2025, and 70.3 onLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously craftedpost-training pipeline- combiningsupervised fine-tuningand reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\nhttps://huggingface.co/a-m-team/AM-Thinking-v1{Hugging Face}.View arXiv pageView PDFProject pageAdd to collectionCommunityakhaliqPaper submitterMay 14See translationReplylibrarian-botMay 15This i",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2505.08311",
    "arxiv_url": "https://arxiv.org/abs/2505.08311",
    "num_models": 7,
    "models_list": "a-m-team/AM-Thinking-v1, a-m-team/AM-Thinking-v1-gguf, Mungert/AM-Thinking-v1-GGUF, MetaphoricalCode/AM-Thinking-v1-exl3-4bpw-hb6, DanyDA/AM-Thinking-v1-exl3-3.0bpw, DanyDA/AM-Thinking-v1-exl3-2.5bpw, DanyDA/AM-Thinking-v1-exl3-2.0bpw",
    "models_links": "https://huggingface.co/a-m-team/AM-Thinking-v1, https://huggingface.co/a-m-team/AM-Thinking-v1-gguf, https://huggingface.co/Mungert/AM-Thinking-v1-GGUF, https://huggingface.co/MetaphoricalCode/AM-Thinking-v1-exl3-4bpw-hb6, https://huggingface.co/DanyDA/AM-Thinking-v1-exl3-3.0bpw, https://huggingface.co/DanyDA/AM-Thinking-v1-exl3-2.5bpw, https://huggingface.co/DanyDA/AM-Thinking-v1-exl3-2.0bpw",
    "models_detailed": "[{\"name\": \"a-m-team/AM-Thinking-v1\", \"link\": \"https://huggingface.co/a-m-team/AM-Thinking-v1\", \"task\": \"Text Generation\", \"likes\": \"681\", \"downloads\": \"\", \"updated\": \"May 14\"}, {\"name\": \"a-m-team/AM-Thinking-v1-gguf\", \"link\": \"https://huggingface.co/a-m-team/AM-Thinking-v1-gguf\", \"task\": \"\", \"likes\": \"981\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"Mungert/AM-Thinking-v1-GGUF\", \"link\": \"https://huggingface.co/Mungert/AM-Thinking-v1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"MetaphoricalCode/AM-Thinking-v1-exl3-4bpw-hb6\", \"link\": \"https://huggingface.co/MetaphoricalCode/AM-Thinking-v1-exl3-4bpw-hb6\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Jun 21\"}, {\"name\": \"DanyDA/AM-Thinking-v1-exl3-3.0bpw\", \"link\": \"https://huggingface.co/DanyDA/AM-Thinking-v1-exl3-3.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"DanyDA/AM-Thinking-v1-exl3-2.5bpw\", \"link\": \"https://huggingface.co/DanyDA/AM-Thinking-v1-exl3-2.5bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"DanyDA/AM-Thinking-v1-exl3-2.0bpw\", \"link\": \"https://huggingface.co/DanyDA/AM-Thinking-v1-exl3-2.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}]",
    "num_datasets": 2,
    "datasets_list": "ibndias/DeepSeek-Distilled-40M, a-m-team/AM-Thinking-v1-RL-Dataset",
    "datasets_links": "https://huggingface.co/datasets/ibndias/DeepSeek-Distilled-40M, https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-RL-Dataset",
    "datasets_detailed": "[{\"name\": \"ibndias/DeepSeek-Distilled-40M\", \"link\": \"https://huggingface.co/datasets/ibndias/DeepSeek-Distilled-40M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 8\", \"size\": \"\"}, {\"name\": \"a-m-team/AM-Thinking-v1-RL-Dataset\", \"link\": \"https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-RL-Dataset\", \"task\": \"\", \"likes\": \"264\", \"downloads\": \"\", \"updated\": \"May 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.07263",
    "first_seen_date": "2025-05-13",
    "title": "Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.07263Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and ReasoningPublished on May 12\u00b7Submitted bywangxiaokunon May 13\u00b7SkyworkUpvote30+22Authors:Xiaokun Wang,Chris,Jiangbo Pei,Wei Shen,Yi Peng,Yunzhuo Hao,Weijie Qiu,Ai Jian,Tianyidan Xie,Xuchen Song,Yang Liu,Yahui ZhouAbstractSkywork-VL Reward is a multimodal reward model that uses a large-scale preference dataset and Qwen2.5-VL-7B-Instruct architecture to achieve state-of-the-art performance in multimodal reasoning.AI-generated summaryWe propose Skywork-VL Reward, amultimodal reward modelthat provides reward\nsignals for bothmultimodal understandingand reasoning tasks. Our technical\napproach comprises two key components: First, we construct a large-scale\nmultimodal preference dataset that covers a wide range of tasks and scenarios,\nwith responses collected from both standard vision-language models (VLMs) and\nadvanced VLM reasoners. Second, we design a reward model architecture based onQwen2.5-VL-7B-Instruct, integrating areward headand applying multi-stage\nfine-tuning usingpairwise ranking losson pairwise preference data.\nExperimental evaluations show that Skywork-VL Reward achieves state-of-the-art\nresults onmultimodal VL-RewardBenchand exhibits competitive performance on\nthetext-only RewardBenchbenchmark. Furthermore, preference data constructed\nbased on our Skywork-VL Reward proves highly effective for training Mixed\nPreference Optimization (MPO), leading to significant improvements inmultimodal reasoning capabilities. Our results underscore Skywork-VL Reward as\na significant advancement toward general-purpose, reliable reward models formultimodal alignment. Our model has been publicly released to promote\ntransparency and reproducibility.View arXiv pageView PDFAdd to collectionCommunityshawn0wangPaper authorPaper submitterMay 13We have released a multimodal reward model that achieves state-of-th",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2505.07263",
    "arxiv_url": "https://arxiv.org/abs/2505.07263",
    "num_models": 2,
    "models_list": "Skywork/Skywork-VL-Reward-7B, Mungert/Skywork-VL-Reward-7B-GGUF",
    "models_links": "https://huggingface.co/Skywork/Skywork-VL-Reward-7B, https://huggingface.co/Mungert/Skywork-VL-Reward-7B-GGUF",
    "models_detailed": "[{\"name\": \"Skywork/Skywork-VL-Reward-7B\", \"link\": \"https://huggingface.co/Skywork/Skywork-VL-Reward-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"Mungert/Skywork-VL-Reward-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Skywork-VL-Reward-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2505.05071",
    "first_seen_date": "2025-05-09",
    "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.05071FG-CLIP: Fine-Grained Visual and Textual AlignmentPublished on May 8\u00b7Submitted byDavid Leonon May 9Upvote18+10Authors:Chunyu Xie,Bin Wang,Fanjing Kong,Jincheng Li,Dawei Liang,Gengshen Zhang,Dawei Leng,Yuhui YinAbstractFG-CLIP enhances fine-grained understanding in multimodal tasks by leveraging large multimodal models, a high-quality dataset with detailed captions, and hard fine-grained negative samples.AI-generated summaryContrastive Language-Image Pre-training (CLIP)excels in multimodal tasks\nsuch asimage-text retrievaland zero-shot classification but struggles withfine-grained understandingdue to its focus on coarse-grained short captions.\nTo address this, we proposeFine-Grained CLIP (FG-CLIP), which enhancesfine-grained understandingthrough three key innovations. First, we leverage\nlargemultimodal modelsto generate 1.6 billion longcaption-image pairsfor\ncapturingglobal-level semantic details. Second, ahigh-quality datasetis\nconstructed with 12 million images and 40 million region-specific bounding\nboxes aligned withdetailed captionsto ensure precise, context-rich\nrepresentations. Third, 10 million hardfine-grained negative samplesare\nincorporated to improve the model's ability to distinguish subtle semantic\ndifferences. Corresponding training methods are meticulously designed for these\ndata. Extensive experiments demonstrate that FG-CLIP outperforms the original\nCLIP and other state-of-the-art methods across various downstream tasks,\nincludingfine-grained understanding,open-vocabulary object detection,image-text retrieval, and generalmultimodal benchmarks. These results\nhighlight FG-CLIP's effectiveness in capturing fine-grained image details and\nimproving overall model performance. The related data, code, and models are\navailable at https://github.com/360CVGroup/FG-CLIP.View arXiv pageView PDFGitHub513Add to collectionCommunityDavidLeonPaper authorPaper submitterMay",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/360CVGroup/FG-CLIP",
    "hf_paper_url": "https://huggingface.co/papers/2505.05071",
    "arxiv_url": "https://arxiv.org/abs/2505.05071",
    "num_models": 5,
    "models_list": "qihoo360/fg-clip2-base, qihoo360/fg-clip-large, qihoo360/fg-clip-base, qihoo360/fg-clip2-large, qihoo360/fg-clip2-so400m",
    "models_links": "https://huggingface.co/qihoo360/fg-clip2-base, https://huggingface.co/qihoo360/fg-clip-large, https://huggingface.co/qihoo360/fg-clip-base, https://huggingface.co/qihoo360/fg-clip2-large, https://huggingface.co/qihoo360/fg-clip2-so400m",
    "models_detailed": "[{\"name\": \"qihoo360/fg-clip2-base\", \"link\": \"https://huggingface.co/qihoo360/fg-clip2-base\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 6\"}, {\"name\": \"qihoo360/fg-clip-large\", \"link\": \"https://huggingface.co/qihoo360/fg-clip-large\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 9\"}, {\"name\": \"qihoo360/fg-clip-base\", \"link\": \"https://huggingface.co/qihoo360/fg-clip-base\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 9\"}, {\"name\": \"qihoo360/fg-clip2-large\", \"link\": \"https://huggingface.co/qihoo360/fg-clip2-large\", \"task\": \"Image Classification\", \"likes\": \"407\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"qihoo360/fg-clip2-so400m\", \"link\": \"https://huggingface.co/qihoo360/fg-clip2-so400m\", \"task\": \"Image Classification\", \"likes\": \"753\", \"downloads\": \"\", \"updated\": \"Oct 20\"}]",
    "num_datasets": 5,
    "datasets_list": "qihoo360/FineHARD, qihoo360/DOCCI-CN, qihoo360/LIT-CN, qihoo360/DCI-CN, qihoo360/BoxClass-CN",
    "datasets_links": "https://huggingface.co/datasets/qihoo360/FineHARD, https://huggingface.co/datasets/qihoo360/DOCCI-CN, https://huggingface.co/datasets/qihoo360/LIT-CN, https://huggingface.co/datasets/qihoo360/DCI-CN, https://huggingface.co/datasets/qihoo360/BoxClass-CN",
    "datasets_detailed": "[{\"name\": \"qihoo360/FineHARD\", \"link\": \"https://huggingface.co/datasets/qihoo360/FineHARD\", \"task\": \"\", \"likes\": \"701\", \"downloads\": \"\", \"updated\": \"Oct 9\", \"size\": \"\"}, {\"name\": \"qihoo360/DOCCI-CN\", \"link\": \"https://huggingface.co/datasets/qihoo360/DOCCI-CN\", \"task\": \"\", \"likes\": \"89\", \"downloads\": \"\", \"updated\": \"Oct 20\", \"size\": \"\"}, {\"name\": \"qihoo360/LIT-CN\", \"link\": \"https://huggingface.co/datasets/qihoo360/LIT-CN\", \"task\": \"\", \"likes\": \"75\", \"downloads\": \"\", \"updated\": \"Oct 20\", \"size\": \"\"}, {\"name\": \"qihoo360/DCI-CN\", \"link\": \"https://huggingface.co/datasets/qihoo360/DCI-CN\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\", \"size\": \"\"}, {\"name\": \"qihoo360/BoxClass-CN\", \"link\": \"https://huggingface.co/datasets/qihoo360/BoxClass-CN\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Oct 15\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.03318",
    "first_seen_date": "2025-05-07",
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.03318Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-TuningPublished on May 6\u00b7Submitted bySII-Yibin Wangon May 7Upvote92+84Authors:Yibin Wang,Zhimin Li,Yuhang Zang,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi WangAbstractA unified multimodal reward model that incorporates long chains of thought reasoning improves reliability and accuracy in vision reward tasks through exploration-driven reinforcement learning.AI-generated summaryRecent advances inmultimodal Reward Models(RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallowreasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the rewardreasoning processcan significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposesUnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt anexploration-driven reinforcement fine-tuningapproach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill thereasoning processofGPT-4o,\nwhich is then used for the model'scold startto learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model'sreasoning processacross various vision\n",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "https://github.com/CodeGoat24/UnifiedReward",
    "hf_paper_url": "https://huggingface.co/papers/2505.03318",
    "arxiv_url": "https://arxiv.org/abs/2505.03318",
    "num_models": 6,
    "models_list": "CodeGoat24/UnifiedReward-Think-7b, CodeGoat24/UnifiedReward-Think-qwen-7b, CodeGoat24/UnifiedReward-Think-qwen3vl-8b, CodeGoat24/UnifiedReward-Think-qwen3vl-2b, CodeGoat24/UnifiedReward-Think-qwen3vl-4b, CodeGoat24/UnifiedReward-Think-qwen3vl-32b",
    "models_links": "https://huggingface.co/CodeGoat24/UnifiedReward-Think-7b, https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen-7b, https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen3vl-8b, https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen3vl-2b, https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen3vl-4b, https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen3vl-32b",
    "models_detailed": "[{\"name\": \"CodeGoat24/UnifiedReward-Think-7b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Think-7b\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/UnifiedReward-Think-qwen-7b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen-7b\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/UnifiedReward-Think-qwen3vl-8b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen3vl-8b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 17\"}, {\"name\": \"CodeGoat24/UnifiedReward-Think-qwen3vl-2b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen3vl-2b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"CodeGoat24/UnifiedReward-Think-qwen3vl-4b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen3vl-4b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"30 days ago\"}, {\"name\": \"CodeGoat24/UnifiedReward-Think-qwen3vl-32b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Think-qwen3vl-32b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}]",
    "num_datasets": 10,
    "datasets_list": "CodeGoat24/UnifiedReward-2.0-T2X-score-data, CodeGoat24/LLaVA-Critic-113k, CodeGoat24/VideoDPO, CodeGoat24/EvalMuse, CodeGoat24/HPD, CodeGoat24/LiFT-HRA, CodeGoat24/OIP, CodeGoat24/ShareGPTVideo-DPO, CodeGoat24/VideoFeedback, CodeGoat24/ImageGen-CoT-Reward-5K",
    "datasets_links": "https://huggingface.co/datasets/CodeGoat24/UnifiedReward-2.0-T2X-score-data, https://huggingface.co/datasets/CodeGoat24/LLaVA-Critic-113k, https://huggingface.co/datasets/CodeGoat24/VideoDPO, https://huggingface.co/datasets/CodeGoat24/EvalMuse, https://huggingface.co/datasets/CodeGoat24/HPD, https://huggingface.co/datasets/CodeGoat24/LiFT-HRA, https://huggingface.co/datasets/CodeGoat24/OIP, https://huggingface.co/datasets/CodeGoat24/ShareGPTVideo-DPO, https://huggingface.co/datasets/CodeGoat24/VideoFeedback, https://huggingface.co/datasets/CodeGoat24/ImageGen-CoT-Reward-5K",
    "datasets_detailed": "[{\"name\": \"CodeGoat24/UnifiedReward-2.0-T2X-score-data\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/UnifiedReward-2.0-T2X-score-data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 26\", \"size\": \"\"}, {\"name\": \"CodeGoat24/LLaVA-Critic-113k\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/LLaVA-Critic-113k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/VideoDPO\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/VideoDPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/EvalMuse\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/EvalMuse\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/HPD\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/HPD\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/LiFT-HRA\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/LiFT-HRA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\", \"size\": \"\"}, {\"name\": \"CodeGoat24/OIP\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/OIP\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/ShareGPTVideo-DPO\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/ShareGPTVideo-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\", \"size\": \"\"}, {\"name\": \"CodeGoat24/VideoFeedback\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/VideoFeedback\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/ImageGen-CoT-Reward-5K\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/ImageGen-CoT-Reward-5K\", \"task\": \"\", \"likes\": \"126\", \"downloads\": \"\", \"updated\": \"Aug 29\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.21798",
    "first_seen_date": "2025-05-07",
    "title": "SWE-smith: Scaling Data for Software Engineering Agents",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.21798SWE-smith: Scaling Data for Software Engineering AgentsPublished on Apr 30\u00b7Submitted byJohnon May 7Upvote11+3Authors:John Yang,Kilian Leret,Carlos E. Jimenez,Alexander Wettig,Kabir Khandpur,Yanzhe Zhang,Binyuan Hui,Ofir Press,Ludwig Schmidt,Diyi YangAbstractSWE-smith automates the generation of large-scale software engineering training data and improves the performance of language models on automated software engineering tasks.AI-generated summaryDespite recent progress inLanguage Models(LMs) forsoftware engineering,\ncollectingtraining dataremains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduceSWE-smith,\na novel pipeline for generatingsoftware engineeringtraining dataat scale.\nGiven any Python codebase,SWE-smithconstructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s oftask instancesthat break existing test(s) in the codebase. UsingSWE-smith, we create a\ndataset of 50k instances sourced from 128GitHub repositories, an order of\nmagnitude larger than all previous works. We trainSWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on theSWE-bench Verified benchmark, state of the art\namong open source models. We open sourceSWE-smith(collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems forautomated software engineering. All assets available at\nhttps://swesmith.com.View arXiv pageView PDFProject pageGitHub487autoAdd to collectionCommunityjohn-b-yangPaper authorPaper submitterMay 7Cracked 40% on SWE-bench verified (single",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/SWE-bench/SWE-smith",
    "hf_paper_url": "https://huggingface.co/papers/2504.21798",
    "arxiv_url": "https://arxiv.org/abs/2504.21798",
    "num_models": 5,
    "models_list": "SWE-bench/SWE-agent-LM-32B, Mungert/SWE-agent-LM-32B-GGUF, SWE-bench/SWE-agent-LM-7B, lucyknada/SWE-bench_SWE-agent-LM-32B-exl2, r2e-edits/SWE-agent-LM-32B",
    "models_links": "https://huggingface.co/SWE-bench/SWE-agent-LM-32B, https://huggingface.co/Mungert/SWE-agent-LM-32B-GGUF, https://huggingface.co/SWE-bench/SWE-agent-LM-7B, https://huggingface.co/lucyknada/SWE-bench_SWE-agent-LM-32B-exl2, https://huggingface.co/r2e-edits/SWE-agent-LM-32B",
    "models_detailed": "[{\"name\": \"SWE-bench/SWE-agent-LM-32B\", \"link\": \"https://huggingface.co/SWE-bench/SWE-agent-LM-32B\", \"task\": \"Text Generation\", \"likes\": \"674\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"Mungert/SWE-agent-LM-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/SWE-agent-LM-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"SWE-bench/SWE-agent-LM-7B\", \"link\": \"https://huggingface.co/SWE-bench/SWE-agent-LM-7B\", \"task\": \"Text Generation\", \"likes\": \"210\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"lucyknada/SWE-bench_SWE-agent-LM-32B-exl2\", \"link\": \"https://huggingface.co/lucyknada/SWE-bench_SWE-agent-LM-32B-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"r2e-edits/SWE-agent-LM-32B\", \"link\": \"https://huggingface.co/r2e-edits/SWE-agent-LM-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 9\"}]",
    "num_datasets": 5,
    "datasets_list": "SWE-bench/SWE-smith, SWE-bench/SWE-smith-trajectories, SWE-bench/SWE-smith-py, mteb/SWEbenchMultilingualRR, SWE-bench/SWE-smith-go",
    "datasets_links": "https://huggingface.co/datasets/SWE-bench/SWE-smith, https://huggingface.co/datasets/SWE-bench/SWE-smith-trajectories, https://huggingface.co/datasets/SWE-bench/SWE-smith-py, https://huggingface.co/datasets/mteb/SWEbenchMultilingualRR, https://huggingface.co/datasets/SWE-bench/SWE-smith-go",
    "datasets_detailed": "[{\"name\": \"SWE-bench/SWE-smith\", \"link\": \"https://huggingface.co/datasets/SWE-bench/SWE-smith\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"SWE-bench/SWE-smith-trajectories\", \"link\": \"https://huggingface.co/datasets/SWE-bench/SWE-smith-trajectories\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\", \"size\": \"\"}, {\"name\": \"SWE-bench/SWE-smith-py\", \"link\": \"https://huggingface.co/datasets/SWE-bench/SWE-smith-py\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}, {\"name\": \"mteb/SWEbenchMultilingualRR\", \"link\": \"https://huggingface.co/datasets/mteb/SWEbenchMultilingualRR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\", \"size\": \"\"}, {\"name\": \"SWE-bench/SWE-smith-go\", \"link\": \"https://huggingface.co/datasets/SWE-bench/SWE-smith-go\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2505.00949",
    "first_seen_date": "2025-05-05",
    "title": "Llama-Nemotron: Efficient Reasoning Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2505.00949Llama-Nemotron: Efficient Reasoning ModelsPublished on May 2\u00b7Submitted byAKon May 5#1 Paper of the dayUpvote42+34Authors:Akhiad Bercovich,Itay Levy,Izik Golan,Mohammad Dabbah,Ran El-Yaniv,Omri Puny,Ido Galil,Zach Moshe,Tomer Ronen,Najeeb Nabwani,Ido Shahaf,Oren Tropp,Ehud Karpas,Ran Zilberstein,Jiaqi Zeng,Soumye Singhal,Alexander Bukharin,Yian Zhang,Tugrul Konuk,Gerald Shen,Ameya Sunil Mahabaleshwarkar,Bilal Kartal+110 authorsAbstractLlama-Nemotron models offer exceptional reasoning capabilities, inference efficiency, and open licensing through neural architecture search, knowledge distillation, and reasoning-focused post-training, including supervised fine-tuning and reinforcement learning.AI-generated summaryWe introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts:supervised fine-tuningand\nlarge scalereinforcement learning. Llama-Nemotron models are the first\nopen-source models to support adynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commer",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2505,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2505.00949",
    "arxiv_url": "https://arxiv.org/abs/2505.00949",
    "num_models": 42,
    "models_list": "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1, nvidia/Llama-3_3-Nemotron-Super-49B-v1, nvidia/Llama-3_3-Nemotron-Super-49B-v1_5, nvidia/Llama-3.1-Nemotron-Nano-8B-v1, nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1, nvidia/Nemotron-H-47B-Reasoning-128K, nvidia/Llama-3_3-Nemotron-Super-49B-GenRM, nvidia/Nemotron-H-8B-Reasoning-128K, nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-NVFP4, Mungert/Llama-3.1-Nemotron-Nano-8B-v1-GGUF, unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF, nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8, unsloth/Llama-3.1-Nemotron-Nano-8B-v1, unsloth/Llama-3.1-Nemotron-Nano-8B-v1-GGUF, nvidia/Llama-3_3-Nemotron-Super-49B-v1-FP8, ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_3.0bpw_H6, kalbon/Llama-3_3-Nemotron-Super-49B-v1-FP8-32k, unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1, Mungert/Llama-3.1-Nemotron-Nano-4B-v1.1-GGUF, unsloth/Llama-3_3-Nemotron-Super-49B-v1, unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF, unsloth/Llama-3_1-Nemotron-Ultra-253B-v1, unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1-unsloth-bnb-4bit, unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1-bnb-4bit, nvidia/Llama-3_3-Nemotron-Super-49B-GenRM-Multilingual, naveenencipher1/trialup, nvidia/Nemotron-H-47B-Reasoning-128K-FP8, nvidia/Nemotron-H-8B-Reasoning-128K-FP8, kmouratidis/Llama-3_3-Nemotron-Super-49B-v1-exl3-8bpw, gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF, jncraton/Llama-3.1-Nemotron-Nano-4B-v1.1-ct2-int8, Mungert/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF, cpatonn/Llama-3_3-Nemotron-Super-49B-v1_5-AWQ-4bit, unsloth/Llama-3_3-Nemotron-Super-49B-v1_5, unsloth/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF, DeusImperator/Llama-3_3-Nemotron-Super-49B-v1_5_exl3_4.0bpw_H6, nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-FP8, shizhediao2/Llama-Nemotron-8B-v1-Prorl, etsien/Llama-3_3-Nemotron-Super-49B-v1_5-GPTQ-w4a8, DBMe/Llama-3_1-Nemotron-Ultra-253B-v1-exl3-2.7bpw, chengfu0118/llama31-8b-it-unroll, FriendliAI/Llama-3_3-Nemotron-Super-49B-v1_5",
    "models_links": "https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5, https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1, https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1, https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-GenRM, https://huggingface.co/nvidia/Nemotron-H-8B-Reasoning-128K, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-NVFP4, https://huggingface.co/Mungert/Llama-3.1-Nemotron-Nano-8B-v1-GGUF, https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF, https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8, https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-8B-v1, https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-8B-v1-GGUF, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1-FP8, https://huggingface.co/ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_3.0bpw_H6, https://huggingface.co/kalbon/Llama-3_3-Nemotron-Super-49B-v1-FP8-32k, https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1, https://huggingface.co/Mungert/Llama-3.1-Nemotron-Nano-4B-v1.1-GGUF, https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1, https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF, https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1, https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1-unsloth-bnb-4bit, https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1-bnb-4bit, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-GenRM-Multilingual, https://huggingface.co/naveenencipher1/trialup, https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K-FP8, https://huggingface.co/nvidia/Nemotron-H-8B-Reasoning-128K-FP8, https://huggingface.co/kmouratidis/Llama-3_3-Nemotron-Super-49B-v1-exl3-8bpw, https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF, https://huggingface.co/jncraton/Llama-3.1-Nemotron-Nano-4B-v1.1-ct2-int8, https://huggingface.co/Mungert/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF, https://huggingface.co/cpatonn/Llama-3_3-Nemotron-Super-49B-v1_5-AWQ-4bit, https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1_5, https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF, https://huggingface.co/DeusImperator/Llama-3_3-Nemotron-Super-49B-v1_5_exl3_4.0bpw_H6, https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-FP8, https://huggingface.co/shizhediao2/Llama-Nemotron-8B-v1-Prorl, https://huggingface.co/etsien/Llama-3_3-Nemotron-Super-49B-v1_5-GPTQ-w4a8, https://huggingface.co/DBMe/Llama-3_1-Nemotron-Ultra-253B-v1-exl3-2.7bpw, https://huggingface.co/chengfu0118/llama31-8b-it-unroll, https://huggingface.co/FriendliAI/Llama-3_3-Nemotron-Super-49B-v1_5",
    "models_detailed": "[{\"name\": \"nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\", \"link\": \"https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-v1\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\", \"link\": \"https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\", \"link\": \"https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"nvidia/Nemotron-H-47B-Reasoning-128K\", \"link\": \"https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K\", \"task\": \"Text Generation\", \"likes\": \"344\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-GenRM\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-GenRM\", \"task\": \"Text Generation\", \"likes\": \"152\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"nvidia/Nemotron-H-8B-Reasoning-128K\", \"link\": \"https://huggingface.co/nvidia/Nemotron-H-8B-Reasoning-128K\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-NVFP4\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-NVFP4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Mungert/Llama-3.1-Nemotron-Nano-8B-v1-GGUF\", \"link\": \"https://huggingface.co/Mungert/Llama-3.1-Nemotron-Nano-8B-v1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"307\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF\", \"link\": \"https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"805\", \"downloads\": \"\", \"updated\": \"May 26\"}, {\"name\": \"nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8\", \"link\": \"https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8\", \"task\": \"Text Generation\", \"likes\": \"297\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"unsloth/Llama-3.1-Nemotron-Nano-8B-v1\", \"link\": \"https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-8B-v1\", \"task\": \"Text Generation\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"unsloth/Llama-3.1-Nemotron-Nano-8B-v1-GGUF\", \"link\": \"https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-8B-v1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"765\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-v1-FP8\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1-FP8\", \"task\": \"Text Generation\", \"likes\": \"277\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_3.0bpw_H6\", \"link\": \"https://huggingface.co/ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_3.0bpw_H6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 14\"}, {\"name\": \"kalbon/Llama-3_3-Nemotron-Super-49B-v1-FP8-32k\", \"link\": \"https://huggingface.co/kalbon/Llama-3_3-Nemotron-Super-49B-v1-FP8-32k\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1\", \"link\": \"https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1\", \"task\": \"Text Generation\", \"likes\": \"73\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"Mungert/Llama-3.1-Nemotron-Nano-4B-v1.1-GGUF\", \"link\": \"https://huggingface.co/Mungert/Llama-3.1-Nemotron-Nano-4B-v1.1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"376\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"unsloth/Llama-3_3-Nemotron-Super-49B-v1\", \"link\": \"https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF\", \"link\": \"https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"581\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"unsloth/Llama-3_1-Nemotron-Ultra-253B-v1\", \"link\": \"https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 25\"}, {\"name\": \"unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Llama-3.1-Nemotron-Nano-4B-v1.1-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-GenRM-Multilingual\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-GenRM-Multilingual\", \"task\": \"Text Generation\", \"likes\": \"130\", \"downloads\": \"\", \"updated\": \"Jun 26\"}, {\"name\": \"naveenencipher1/trialup\", \"link\": \"https://huggingface.co/naveenencipher1/trialup\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 28\"}, {\"name\": \"nvidia/Nemotron-H-47B-Reasoning-128K-FP8\", \"link\": \"https://huggingface.co/nvidia/Nemotron-H-47B-Reasoning-128K-FP8\", \"task\": \"Text Generation\", \"likes\": \"64\", \"downloads\": \"\", \"updated\": \"Aug 21\"}, {\"name\": \"nvidia/Nemotron-H-8B-Reasoning-128K-FP8\", \"link\": \"https://huggingface.co/nvidia/Nemotron-H-8B-Reasoning-128K-FP8\", \"task\": \"Text Generation\", \"likes\": \"60\", \"downloads\": \"\", \"updated\": \"Aug 21\"}, {\"name\": \"kmouratidis/Llama-3_3-Nemotron-Super-49B-v1-exl3-8bpw\", \"link\": \"https://huggingface.co/kmouratidis/Llama-3_3-Nemotron-Super-49B-v1-exl3-8bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"jncraton/Llama-3.1-Nemotron-Nano-4B-v1.1-ct2-int8\", \"link\": \"https://huggingface.co/jncraton/Llama-3.1-Nemotron-Nano-4B-v1.1-ct2-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Mungert/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF\", \"link\": \"https://huggingface.co/Mungert/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF\", \"task\": \"Text Generation\", \"likes\": \"784\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"cpatonn/Llama-3_3-Nemotron-Super-49B-v1_5-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Llama-3_3-Nemotron-Super-49B-v1_5-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"410\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"unsloth/Llama-3_3-Nemotron-Super-49B-v1_5\", \"link\": \"https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1_5\", \"task\": \"Text Generation\", \"likes\": \"113\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"unsloth/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF\", \"link\": \"https://huggingface.co/unsloth/Llama-3_3-Nemotron-Super-49B-v1_5-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"DeusImperator/Llama-3_3-Nemotron-Super-49B-v1_5_exl3_4.0bpw_H6\", \"link\": \"https://huggingface.co/DeusImperator/Llama-3_3-Nemotron-Super-49B-v1_5_exl3_4.0bpw_H6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-FP8\", \"link\": \"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"shizhediao2/Llama-Nemotron-8B-v1-Prorl\", \"link\": \"https://huggingface.co/shizhediao2/Llama-Nemotron-8B-v1-Prorl\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 25\"}, {\"name\": \"etsien/Llama-3_3-Nemotron-Super-49B-v1_5-GPTQ-w4a8\", \"link\": \"https://huggingface.co/etsien/Llama-3_3-Nemotron-Super-49B-v1_5-GPTQ-w4a8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"DBMe/Llama-3_1-Nemotron-Ultra-253B-v1-exl3-2.7bpw\", \"link\": \"https://huggingface.co/DBMe/Llama-3_1-Nemotron-Ultra-253B-v1-exl3-2.7bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 6\"}, {\"name\": \"chengfu0118/llama31-8b-it-unroll\", \"link\": \"https://huggingface.co/chengfu0118/llama31-8b-it-unroll\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 10\"}, {\"name\": \"FriendliAI/Llama-3_3-Nemotron-Super-49B-v1_5\", \"link\": \"https://huggingface.co/FriendliAI/Llama-3_3-Nemotron-Super-49B-v1_5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}]",
    "num_datasets": 15,
    "datasets_list": "hiepp2/tvp4, nvidia/Nemotron-Post-Training-Dataset-v1, open-r1/Mixture-of-Thoughts, nvidia/Llama-Nemotron-Post-Training-Dataset, allenai/Dolci-Think-RL-32B, allenai/Dolci-Think-RL-7B, FlameF0X/Mixture-of-Thoughts-2048T, 1il7tw6n/makevideo, brandolorian/nemotron-post-training-samples, brandolorian/nemotron-post-training-samples-splits, yllkryeziu/gdr-Llama-Nemotron-Post-Training-Dataset, allenai/Dolci-Think-RL-7B-Completions-DPO, allenai/Dolci-Think-RL-7B-Completions-SFT, tulsatime37/Llama-Nemotron-Post-Training-Dataset, Sidsidney/Llama-Nemotron-Post-Training-Dataset",
    "datasets_links": "https://huggingface.co/datasets/hiepp2/tvp4, https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1, https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts, https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset, https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B, https://huggingface.co/datasets/FlameF0X/Mixture-of-Thoughts-2048T, https://huggingface.co/datasets/1il7tw6n/makevideo, https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples, https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples-splits, https://huggingface.co/datasets/yllkryeziu/gdr-Llama-Nemotron-Post-Training-Dataset, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT, https://huggingface.co/datasets/tulsatime37/Llama-Nemotron-Post-Training-Dataset, https://huggingface.co/datasets/Sidsidney/Llama-Nemotron-Post-Training-Dataset",
    "datasets_detailed": "[{\"name\": \"hiepp2/tvp4\", \"link\": \"https://huggingface.co/datasets/hiepp2/tvp4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Post-Training-Dataset-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 25\", \"size\": \"\"}, {\"name\": \"open-r1/Mixture-of-Thoughts\", \"link\": \"https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 26\", \"size\": \"\"}, {\"name\": \"nvidia/Llama-Nemotron-Post-Training-Dataset\", \"link\": \"https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-32B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"FlameF0X/Mixture-of-Thoughts-2048T\", \"link\": \"https://huggingface.co/datasets/FlameF0X/Mixture-of-Thoughts-2048T\", \"task\": \"\", \"likes\": \"145\", \"downloads\": \"\", \"updated\": \"Jun 29\", \"size\": \"\"}, {\"name\": \"1il7tw6n/makevideo\", \"link\": \"https://huggingface.co/datasets/1il7tw6n/makevideo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\", \"size\": \"\"}, {\"name\": \"brandolorian/nemotron-post-training-samples\", \"link\": \"https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 14\", \"size\": \"\"}, {\"name\": \"brandolorian/nemotron-post-training-samples-splits\", \"link\": \"https://huggingface.co/datasets/brandolorian/nemotron-post-training-samples-splits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 14\", \"size\": \"\"}, {\"name\": \"yllkryeziu/gdr-Llama-Nemotron-Post-Training-Dataset\", \"link\": \"https://huggingface.co/datasets/yllkryeziu/gdr-Llama-Nemotron-Post-Training-Dataset\", \"task\": \"\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Sep 22\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-DPO\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO\", \"task\": \"\", \"likes\": \"332\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-SFT\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT\", \"task\": \"\", \"likes\": \"413\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"tulsatime37/Llama-Nemotron-Post-Training-Dataset\", \"link\": \"https://huggingface.co/datasets/tulsatime37/Llama-Nemotron-Post-Training-Dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"8 days ago\", \"size\": \"\"}, {\"name\": \"Sidsidney/Llama-Nemotron-Post-Training-Dataset\", \"link\": \"https://huggingface.co/datasets/Sidsidney/Llama-Nemotron-Post-Training-Dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.21039",
    "first_seen_date": "2025-05-01",
    "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.21039Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical ReportPublished on Apr 28\u00b7Submitted byAman Priyanshuon May 1Upvote15+7Authors:Paul Kassianik,Baturay Saglam,Alexander Chen,Blaine Nelson,Anu Vellore,Massimo Aufiero,Fraser Burch,Dhruv Kedia,Avi Zohary,Sajana Weerawardhena,Aman Priyanshu,Adam Swanda,Amy Chang,Hyrum Anderson,Kojin Oshiba,Omar Santos,Yaron Singer,Amin KarbasiAbstractFoundation-Sec-8B, a LLM enhanced with cybersecurity-specific training, matches high-performance models in cybersecurity tasks and promotes AI adoption in the field.AI-generated summaryAstransformer-based large language models(LLMs) increasingly permeate\nsociety, they have revolutionized domains such as software engineering,\ncreative writing, and digital arts. However, their adoption in cybersecurity\nremains limited due to challenges like scarcity of specialized training data\nand complexity of representing cybersecurity-specific knowledge. To address\nthese gaps, we present Foundation-Sec-8B, acybersecurity-focused LLMbuilt on\ntheLlama 3.1architecture and enhanced throughcontinued pretrainingon a\ncarefully curatedcybersecurity corpus. We evaluate Foundation-Sec-8B across\nboth established and newcybersecurity benchmarks, showing that it matchesLlama 3.1-70B andGPT-4o-miniin certain cybersecurity-specific tasks. By\nreleasing our model to the public, we aim to accelerate progress and adoption\nof AI-driven tools in both public and private cybersecurity contexts.View arXiv pageView PDFProject pageAdd to collectionCommunityAmanPriyanshuPaper authorPaper submitterMay 1This paper introduces Foundation-Sec-8B, a cybersecurity-focused LLM based on Llama 3.1 architecture with continued pretraining on a specialized security corpus. Evaluation demonstrates comparable performance to larger models on security-specific tasks. The model is a publicly released open-weights model to support more AI adoption withi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.21039",
    "arxiv_url": "https://arxiv.org/abs/2504.21039",
    "num_models": 7,
    "models_list": "fdtn-ai/Foundation-Sec-8B, Mungert/Foundation-Sec-8B-GGUF, Mungert/Foundation-Sec-8B-Instruct-GGUF, 2p8xx/Foundation-Sec-8B-Instruct, goldenrye/FDTN-Sec-8B-GGUF, 2p8xx/Foundation-Sec-8B-Instruct-GGUF, QuantFactory/Foundation-Sec-8B-GGUF",
    "models_links": "https://huggingface.co/fdtn-ai/Foundation-Sec-8B, https://huggingface.co/Mungert/Foundation-Sec-8B-GGUF, https://huggingface.co/Mungert/Foundation-Sec-8B-Instruct-GGUF, https://huggingface.co/2p8xx/Foundation-Sec-8B-Instruct, https://huggingface.co/goldenrye/FDTN-Sec-8B-GGUF, https://huggingface.co/2p8xx/Foundation-Sec-8B-Instruct-GGUF, https://huggingface.co/QuantFactory/Foundation-Sec-8B-GGUF",
    "models_detailed": "[{\"name\": \"fdtn-ai/Foundation-Sec-8B\", \"link\": \"https://huggingface.co/fdtn-ai/Foundation-Sec-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Mungert/Foundation-Sec-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Foundation-Sec-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/Foundation-Sec-8B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/Foundation-Sec-8B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"2p8xx/Foundation-Sec-8B-Instruct\", \"link\": \"https://huggingface.co/2p8xx/Foundation-Sec-8B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"35\", \"downloads\": \"\", \"updated\": \"Jun 8\"}, {\"name\": \"goldenrye/FDTN-Sec-8B-GGUF\", \"link\": \"https://huggingface.co/goldenrye/FDTN-Sec-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\"}, {\"name\": \"2p8xx/Foundation-Sec-8B-Instruct-GGUF\", \"link\": \"https://huggingface.co/2p8xx/Foundation-Sec-8B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"846\", \"downloads\": \"\", \"updated\": \"Jun 8\"}, {\"name\": \"QuantFactory/Foundation-Sec-8B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/Foundation-Sec-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"820\", \"downloads\": \"\", \"updated\": \"Jun 18\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.21233",
    "first_seen_date": "2025-05-01",
    "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.21233Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in MathPublished on Apr 30\u00b7Submitted byAKon May 1Upvote49+41Authors:Haoran Xu,Baolin Peng,Hany Awadalla,Dongdong Chen,Yen-Chun Chen,Mei Gao,Young Jin Kim,Yunsheng Li,Liliang Ren,Yelong Shen,Shuohang Wang,Weijian Xu,Jianfeng Gao,Weizhu ChenAbstractA structured training method with high-quality Chain-of-Thought data improves reasoning abilities in small language models.AI-generated summaryChain-of-Thought(CoT) significantly enhances formal reasoning capabilities\ninLarge Language Models(LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning inSmall Language Models(SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates thatdistillationfrom LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1)large-scale mid-trainingon diverse distilled\nlong-CoT data, (2)supervised fine-tuningon high-quality long-CoT data, (3)Rollout DPOleveraging a carefully curated preference dataset, and (4)Reinforcement Learning(RL) withVerifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, onmath reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models.View arXiv pageView PDFAdd to collectionCommunityakhaliqPaper submitterMay ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.21233",
    "arxiv_url": "https://arxiv.org/abs/2504.21233",
    "num_models": 7,
    "models_list": "microsoft/Phi-4-mini-reasoning, Mungert/Phi-4-mini-reasoning-GGUF, Bifrost-AI/Phi-4-Mirage-sol-Reasoning-3.8B, lmstudio-community/Phi-4-mini-reasoning-GGUF, randomblock1/Phi-4-mini-reasoning-rk3588, askalgore/Phi-4-mini-reasoning-heretic, AiAsistent/Phi-4-mini-reasoning-heretic",
    "models_links": "https://huggingface.co/microsoft/Phi-4-mini-reasoning, https://huggingface.co/Mungert/Phi-4-mini-reasoning-GGUF, https://huggingface.co/Bifrost-AI/Phi-4-Mirage-sol-Reasoning-3.8B, https://huggingface.co/lmstudio-community/Phi-4-mini-reasoning-GGUF, https://huggingface.co/randomblock1/Phi-4-mini-reasoning-rk3588, https://huggingface.co/askalgore/Phi-4-mini-reasoning-heretic, https://huggingface.co/AiAsistent/Phi-4-mini-reasoning-heretic",
    "models_detailed": "[{\"name\": \"microsoft/Phi-4-mini-reasoning\", \"link\": \"https://huggingface.co/microsoft/Phi-4-mini-reasoning\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"Mungert/Phi-4-mini-reasoning-GGUF\", \"link\": \"https://huggingface.co/Mungert/Phi-4-mini-reasoning-GGUF\", \"task\": \"Text Generation\", \"likes\": \"374\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Bifrost-AI/Phi-4-Mirage-sol-Reasoning-3.8B\", \"link\": \"https://huggingface.co/Bifrost-AI/Phi-4-Mirage-sol-Reasoning-3.8B\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"lmstudio-community/Phi-4-mini-reasoning-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/Phi-4-mini-reasoning-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 1\"}, {\"name\": \"randomblock1/Phi-4-mini-reasoning-rk3588\", \"link\": \"https://huggingface.co/randomblock1/Phi-4-mini-reasoning-rk3588\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"askalgore/Phi-4-mini-reasoning-heretic\", \"link\": \"https://huggingface.co/askalgore/Phi-4-mini-reasoning-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 1\"}, {\"name\": \"AiAsistent/Phi-4-mini-reasoning-heretic\", \"link\": \"https://huggingface.co/AiAsistent/Phi-4-mini-reasoning-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.21318",
    "first_seen_date": "2025-05-01",
    "title": "Phi-4-reasoning Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.21318Phi-4-reasoning Technical ReportPublished on Apr 30\u00b7Submitted byAKon May 1Upvote53+45Authors:Marah Abdin,Sahaj Agarwal,Ahmed Awadallah,Vidhisha Balachandran,Harkirat Behl,Lingjiao Chen,Gustavo de Rosa,Suriya Gunasekar,Mojan Javaheripi,Neel Joshi,Piero Kauffmann,Yash Lara,Caio C\u00e9sar Teodoro Mendes,Arindam Mitra,Besmira Nushi,Dimitris Papailiopoulos,Olli Saarikivi,Shital Shah,Vaishnavi Shrivastava,Vibhav Vineet,Yue Wu,Safoora Yousefi+1 authorsAbstractPhi-4-reasoning, a 14-billion parameter model enhanced with supervised fine-tuning and reinforcement learning, outperforms larger models on complex reasoning tasks across various benchmarks.AI-generated summaryWe introducePhi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of\"teachable\" prompts-selected\nfor the right level of complexity and diversity-andreasoning demonstrationsgenerated usingo3-mini,Phi-4-reasoninggenerates detailed reasoning chains\nthat effectively leverage inference-time compute. We further developPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such asDeepSeek-R1-Distill-Llama-70Bmodel and approach the performance levels of fullDeepSeek-R1model. Our comprehensive evaluations span benchmarks in math andscientific reasoning,coding,algorithmic problem solving,planning, andspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements togeneral-purpose benchmarksas well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of carefuldata curationfor sup",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.21318",
    "arxiv_url": "https://arxiv.org/abs/2504.21318",
    "num_models": 7,
    "models_list": "microsoft/Phi-4-reasoning-plus, microsoft/Phi-4-reasoning, open-r1/OpenR1-Distill-7B, Jackrong/gpt-oss-120b-Distill-Phi-4-14B-GGUF, QuantFactory/OpenR1-Distill-7B-GGUF, aarnphm/phi-4-reasoning-plus-sharded-tp2, Jackrong/gpt-oss-120b-Distill-Phi-4-14B",
    "models_links": "https://huggingface.co/microsoft/Phi-4-reasoning-plus, https://huggingface.co/microsoft/Phi-4-reasoning, https://huggingface.co/open-r1/OpenR1-Distill-7B, https://huggingface.co/Jackrong/gpt-oss-120b-Distill-Phi-4-14B-GGUF, https://huggingface.co/QuantFactory/OpenR1-Distill-7B-GGUF, https://huggingface.co/aarnphm/phi-4-reasoning-plus-sharded-tp2, https://huggingface.co/Jackrong/gpt-oss-120b-Distill-Phi-4-14B",
    "models_detailed": "[{\"name\": \"microsoft/Phi-4-reasoning-plus\", \"link\": \"https://huggingface.co/microsoft/Phi-4-reasoning-plus\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"microsoft/Phi-4-reasoning\", \"link\": \"https://huggingface.co/microsoft/Phi-4-reasoning\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"open-r1/OpenR1-Distill-7B\", \"link\": \"https://huggingface.co/open-r1/OpenR1-Distill-7B\", \"task\": \"Text Generation\", \"likes\": \"140\", \"downloads\": \"\", \"updated\": \"May 26\"}, {\"name\": \"Jackrong/gpt-oss-120b-Distill-Phi-4-14B-GGUF\", \"link\": \"https://huggingface.co/Jackrong/gpt-oss-120b-Distill-Phi-4-14B-GGUF\", \"task\": \"Question Answering\", \"likes\": \"634\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"QuantFactory/OpenR1-Distill-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/OpenR1-Distill-7B-GGUF\", \"task\": \"\", \"likes\": \"149\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"aarnphm/phi-4-reasoning-plus-sharded-tp2\", \"link\": \"https://huggingface.co/aarnphm/phi-4-reasoning-plus-sharded-tp2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 23\"}, {\"name\": \"Jackrong/gpt-oss-120b-Distill-Phi-4-14B\", \"link\": \"https://huggingface.co/Jackrong/gpt-oss-120b-Distill-Phi-4-14B\", \"task\": \"Question Answering\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Oct 2\"}]",
    "num_datasets": 4,
    "datasets_list": "hiepp2/tvp4, open-r1/Mixture-of-Thoughts, microsoft/Eureka-Bench-Logs, 1il7tw6n/makevideo",
    "datasets_links": "https://huggingface.co/datasets/hiepp2/tvp4, https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts, https://huggingface.co/datasets/microsoft/Eureka-Bench-Logs, https://huggingface.co/datasets/1il7tw6n/makevideo",
    "datasets_detailed": "[{\"name\": \"hiepp2/tvp4\", \"link\": \"https://huggingface.co/datasets/hiepp2/tvp4\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}, {\"name\": \"open-r1/Mixture-of-Thoughts\", \"link\": \"https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 26\", \"size\": \"\"}, {\"name\": \"microsoft/Eureka-Bench-Logs\", \"link\": \"https://huggingface.co/datasets/microsoft/Eureka-Bench-Logs\", \"task\": \"\", \"likes\": \"717\", \"downloads\": \"\", \"updated\": \"May 28\", \"size\": \"\"}, {\"name\": \"1il7tw6n/makevideo\", \"link\": \"https://huggingface.co/datasets/1il7tw6n/makevideo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.17788",
    "first_seen_date": "2025-04-25",
    "title": "Dynamic Camera Poses and Where to Find Them",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.17788Dynamic Camera Poses and Where to Find ThemPublished on Apr 24\u00b7Submitted byChris Rockwellon Apr 25Upvote6Authors:Chris Rockwell,Joseph Tung,Tsung-Yi Lin,Ming-Yu Liu,David F. Fouhey,Chen-Hsuan LinAbstractDynPose-100K is a large-scale dataset of dynamic Internet videos with annotated camera poses, collected using advanced filtering and pose estimation techniques.AI-generated summaryAnnotatingcamera posesondynamic Internet videosat scale is critical for\nadvancing fields like realistic video generation and simulation. However,\ncollecting such a dataset is difficult, as most Internet videos are unsuitable\nfor pose estimation. Furthermore, annotatingdynamic Internet videospresent\nsignificant challenges even for state-of-theart methods. In this paper, we\nintroduce DynPose-100K, alarge-scale datasetofdynamic Internet videosannotated withcamera poses. Our collection pipeline addresses filtering using\na carefully combined set of task-specific and generalist models. For pose\nestimation, we combine the latest techniques ofpoint tracking, dynamic\nmasking, andstructure-from-motionto achieve improvements over the\nstate-of-the-art approaches. Our analysis and experiments demonstrate that\nDynPose-100K is both large-scale and diverse across several key attributes,\nopening up avenues for advancements in various downstream applications.View arXiv pageView PDFProject pageAdd to collectionCommunitycrockwellPaper submitterApr 25\u2022edited Apr 25Dataset download:https://huggingface.co/datasets/nvidia/dynpose-100kSee translationReplylibrarian-botApr 26This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIAnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos(2025)RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and Metric-scale Camera Movements(2025)Tow",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.17788",
    "arxiv_url": "https://arxiv.org/abs/2504.17788",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "nvidia/dynpose-100k",
    "datasets_links": "https://huggingface.co/datasets/nvidia/dynpose-100k",
    "datasets_detailed": "[{\"name\": \"nvidia/dynpose-100k\", \"link\": \"https://huggingface.co/datasets/nvidia/dynpose-100k\", \"task\": \"\", \"likes\": \"427\", \"downloads\": \"\", \"updated\": \"May 12\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.16891",
    "first_seen_date": "2025-04-24",
    "title": "AIMO-2 Winning Solution: Building State-of-the-Art Mathematical\n  Reasoning Models with OpenMathReasoning dataset",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.16891AIMO-2 Winning Solution: Building State-of-the-Art Mathematical\n  Reasoning Models with OpenMathReasoning datasetPublished on Apr 23\u00b7Submitted byIgor Gitmanon Apr 24Upvote25+17Authors:Ivan Moshkov,Darragh Hanley,Ivan Sorokin,Shubham Toshniwal,Christof Henkel,Benedikt Schifferer,Wei Du,Igor GitmanAbstractThis paper presents our winning submission to the AI Mathematical Olympiad -\nProgress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art\nmathematical reasoning models relies on three key pillars. First, we create a\nlarge-scale dataset comprising 540K unique high-quality math problems,\nincluding olympiad-level problems, and their 3.2M long-reasoning solutions.\nSecond, we develop a novel method to integrate code execution with long\nreasoning models through iterative training, generation, and quality filtering,\nresulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we\ncreate a pipeline to train models to select the most promising solution from\nmany candidates. We show that such generative solution selection (GenSelect)\ncan significantly improve upon majority voting baseline. Combining these ideas,\nwe train a series of models that achieve state-of-the-art results on\nmathematical reasoning benchmarks. To facilitate further research, we release\nour code, models, and the complete OpenMathReasoning dataset under a\ncommercially permissive license.View arXiv pageView PDFProject pageGitHub700Add to collectionCommunityigitmanPaper authorPaper submitterApr 24AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning datasetSee translationReplylibrarian-botApr 25This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIOpenCodeReasoning: Advancing Data Distillation for Competitive Coding(2025)P",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/NVIDIA/NeMo-Skills",
    "hf_paper_url": "https://huggingface.co/papers/2504.16891",
    "arxiv_url": "https://arxiv.org/abs/2504.16891",
    "num_models": 39,
    "models_list": "nvidia/OpenReasoning-Nemotron-32B, nvidia/OpenReasoning-Nemotron-1.5B, nvidia/OpenReasoning-Nemotron-7B, nvidia/OpenReasoning-Nemotron-14B, nvidia/OpenMath-Nemotron-1.5B, nvidia/OpenMath-Nemotron-7B, nvidia/OpenMath-Nemotron-14B, nvidia/OpenMath-Nemotron-32B, nvidia/OpenMath-Nemotron-14B-Kaggle, jobs-git/OpenMath-Nemotron-14B, jobs-git/OpenMath-Nemotron-32B, Mungert/OpenMath-Nemotron-32B-GGUF, Mungert/OpenMath-Nemotron-7B-GGUF, Mungert/OpenMath-Nemotron-1.5B-GGUF, prithivMLmods/Draconis-Qwen3_Math-4B-Preview, prithivMLmods/Hatshepsut-Qwen3_QWQ-LCoT-4B, prithivMLmods/Canum-Qwen3_R1-4B-iCoT, Mungert/OpenMath-Nemotron-14B-Kaggle-GGUF, gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF, gabriellarson/OpenReasoning-Nemotron-7B-GGUF, gabriellarson/OpenReasoning-Nemotron-14B-GGUF, gabriellarson/OpenReasoning-Nemotron-32B-GGUF, Bifrost-AI/Mirage-OpenReasoning-Nemotron-7B, Bifrost-AI/Mirage-OpenReasoning-Nemotron-7B-GGUF, unsloth/OpenReasoning-Nemotron-32B, unsloth/OpenReasoning-Nemotron-32B-GGUF, Mungert/OpenReasoning-Nemotron-32B-GGUF, codys12/OpenReasoning-Nemotron-32B, Mungert/OpenReasoning-Nemotron-7B-GGUF, Mungert/OpenReasoning-Nemotron-1.5B-GGUF, jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8, Mungert/OpenReasoning-Nemotron-14B-GGUF, Prince-1/OpenReasoning-Nemotron-7B, onnx-community/OpenReasoning-Nemotron-7B, onnx-community/OpenReasoning-Nemotron-1.5B, Prince-1/OpenReasoning-Nemotron-1.5B, pamanseau/OpenReasoning-Nemotron-32B, Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit, QuantFactory/OpenReasoning-Nemotron-7B-GGUF",
    "models_links": "https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B, https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B, https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B, https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B, https://huggingface.co/nvidia/OpenMath-Nemotron-1.5B, https://huggingface.co/nvidia/OpenMath-Nemotron-7B, https://huggingface.co/nvidia/OpenMath-Nemotron-14B, https://huggingface.co/nvidia/OpenMath-Nemotron-32B, https://huggingface.co/nvidia/OpenMath-Nemotron-14B-Kaggle, https://huggingface.co/jobs-git/OpenMath-Nemotron-14B, https://huggingface.co/jobs-git/OpenMath-Nemotron-32B, https://huggingface.co/Mungert/OpenMath-Nemotron-32B-GGUF, https://huggingface.co/Mungert/OpenMath-Nemotron-7B-GGUF, https://huggingface.co/Mungert/OpenMath-Nemotron-1.5B-GGUF, https://huggingface.co/prithivMLmods/Draconis-Qwen3_Math-4B-Preview, https://huggingface.co/prithivMLmods/Hatshepsut-Qwen3_QWQ-LCoT-4B, https://huggingface.co/prithivMLmods/Canum-Qwen3_R1-4B-iCoT, https://huggingface.co/Mungert/OpenMath-Nemotron-14B-Kaggle-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-7B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-14B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/Bifrost-AI/Mirage-OpenReasoning-Nemotron-7B, https://huggingface.co/Bifrost-AI/Mirage-OpenReasoning-Nemotron-7B-GGUF, https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B, https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/Mungert/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/codys12/OpenReasoning-Nemotron-32B, https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF, https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF, https://huggingface.co/jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8, https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF, https://huggingface.co/Prince-1/OpenReasoning-Nemotron-7B, https://huggingface.co/onnx-community/OpenReasoning-Nemotron-7B, https://huggingface.co/onnx-community/OpenReasoning-Nemotron-1.5B, https://huggingface.co/Prince-1/OpenReasoning-Nemotron-1.5B, https://huggingface.co/pamanseau/OpenReasoning-Nemotron-32B, https://huggingface.co/Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit, https://huggingface.co/QuantFactory/OpenReasoning-Nemotron-7B-GGUF",
    "models_detailed": "[{\"name\": \"nvidia/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"449\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"408\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-14B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"303\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenMath-Nemotron-1.5B\", \"link\": \"https://huggingface.co/nvidia/OpenMath-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"nvidia/OpenMath-Nemotron-7B\", \"link\": \"https://huggingface.co/nvidia/OpenMath-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"526\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"nvidia/OpenMath-Nemotron-14B\", \"link\": \"https://huggingface.co/nvidia/OpenMath-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"179\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"nvidia/OpenMath-Nemotron-32B\", \"link\": \"https://huggingface.co/nvidia/OpenMath-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"198\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"nvidia/OpenMath-Nemotron-14B-Kaggle\", \"link\": \"https://huggingface.co/nvidia/OpenMath-Nemotron-14B-Kaggle\", \"task\": \"Text Generation\", \"likes\": \"460\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"jobs-git/OpenMath-Nemotron-14B\", \"link\": \"https://huggingface.co/jobs-git/OpenMath-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"jobs-git/OpenMath-Nemotron-32B\", \"link\": \"https://huggingface.co/jobs-git/OpenMath-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 27\"}, {\"name\": \"Mungert/OpenMath-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenMath-Nemotron-32B-GGUF\", \"task\": \"\", \"likes\": \"232\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/OpenMath-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenMath-Nemotron-7B-GGUF\", \"task\": \"\", \"likes\": \"372\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/OpenMath-Nemotron-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenMath-Nemotron-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"prithivMLmods/Draconis-Qwen3_Math-4B-Preview\", \"link\": \"https://huggingface.co/prithivMLmods/Draconis-Qwen3_Math-4B-Preview\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 14\"}, {\"name\": \"prithivMLmods/Hatshepsut-Qwen3_QWQ-LCoT-4B\", \"link\": \"https://huggingface.co/prithivMLmods/Hatshepsut-Qwen3_QWQ-LCoT-4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 13\"}, {\"name\": \"prithivMLmods/Canum-Qwen3_R1-4B-iCoT\", \"link\": \"https://huggingface.co/prithivMLmods/Canum-Qwen3_R1-4B-iCoT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"Mungert/OpenMath-Nemotron-14B-Kaggle-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenMath-Nemotron-14B-Kaggle-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"182\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"145\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"Bifrost-AI/Mirage-OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/Bifrost-AI/Mirage-OpenReasoning-Nemotron-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"Bifrost-AI/Mirage-OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/Bifrost-AI/Mirage-OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\"}, {\"name\": \"unsloth/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"unsloth/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"610\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"codys12/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/codys12/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"273\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"187\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8\", \"link\": \"https://huggingface.co/jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Prince-1/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/Prince-1/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"onnx-community/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/onnx-community/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"onnx-community/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/onnx-community/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"Prince-1/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/Prince-1/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"pamanseau/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/pamanseau/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit\", \"link\": \"https://huggingface.co/Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"QuantFactory/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"135\", \"downloads\": \"\", \"updated\": \"Sep 1\"}]",
    "num_datasets": 11,
    "datasets_list": "nvidia/OpenMathReasoning, MaziyarPanahi/OpenMathReasoning_ShareGPT, jobs-git/OpenMathReasoning, Compumacy/NMT-openmath, nnmax/OpenMathReasoning, HradilSerhii/OpenMathReasoning, NewstaR/CoTton-64k-6725-Collective, NewstaR/CoTton-67k-6725-Collective, introvoyz041/OpenMathReasoning, xiongbubu/OpenMathReasoning, Sellopale/OpenMathReasoning",
    "datasets_links": "https://huggingface.co/datasets/nvidia/OpenMathReasoning, https://huggingface.co/datasets/MaziyarPanahi/OpenMathReasoning_ShareGPT, https://huggingface.co/datasets/jobs-git/OpenMathReasoning, https://huggingface.co/datasets/Compumacy/NMT-openmath, https://huggingface.co/datasets/nnmax/OpenMathReasoning, https://huggingface.co/datasets/HradilSerhii/OpenMathReasoning, https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective, https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective, https://huggingface.co/datasets/introvoyz041/OpenMathReasoning, https://huggingface.co/datasets/xiongbubu/OpenMathReasoning, https://huggingface.co/datasets/Sellopale/OpenMathReasoning",
    "datasets_detailed": "[{\"name\": \"nvidia/OpenMathReasoning\", \"link\": \"https://huggingface.co/datasets/nvidia/OpenMathReasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 27\", \"size\": \"\"}, {\"name\": \"MaziyarPanahi/OpenMathReasoning_ShareGPT\", \"link\": \"https://huggingface.co/datasets/MaziyarPanahi/OpenMathReasoning_ShareGPT\", \"task\": \"\", \"likes\": \"467\", \"downloads\": \"\", \"updated\": \"Apr 24\", \"size\": \"\"}, {\"name\": \"jobs-git/OpenMathReasoning\", \"link\": \"https://huggingface.co/datasets/jobs-git/OpenMathReasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\", \"size\": \"\"}, {\"name\": \"Compumacy/NMT-openmath\", \"link\": \"https://huggingface.co/datasets/Compumacy/NMT-openmath\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 11\", \"size\": \"\"}, {\"name\": \"nnmax/OpenMathReasoning\", \"link\": \"https://huggingface.co/datasets/nnmax/OpenMathReasoning\", \"task\": \"\", \"likes\": \"176\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"HradilSerhii/OpenMathReasoning\", \"link\": \"https://huggingface.co/datasets/HradilSerhii/OpenMathReasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-64k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Jun 8\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-67k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Jun 7\", \"size\": \"\"}, {\"name\": \"introvoyz041/OpenMathReasoning\", \"link\": \"https://huggingface.co/datasets/introvoyz041/OpenMathReasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"xiongbubu/OpenMathReasoning\", \"link\": \"https://huggingface.co/datasets/xiongbubu/OpenMathReasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\", \"size\": \"\"}, {\"name\": \"Sellopale/OpenMathReasoning\", \"link\": \"https://huggingface.co/datasets/Sellopale/OpenMathReasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.16915",
    "first_seen_date": "2025-04-24",
    "title": "DreamO: A Unified Framework for Image Customization",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.16915DreamO: A Unified Framework for Image CustomizationPublished on Apr 23\u00b7Submitted byYanze Wuon Apr 24Upvote24+16Authors:Chong Mou,Yanze Wu,Wenxu Wu,Zinan Guo,Pengze Zhang,Yufeng Cheng,Yiming Luo,Fei Ding,Shiwen Zhang,Xinghui Li,Mengtian Li,Songtao Zhao,Jian Zhang,Qian He,Xinglong WuAbstractDreamO, an image customization framework using diffusion transformers, supports various tasks and integrates multiple conditions through a unified approach.AI-generated summaryRecently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce afeature routing constraintto\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design aplaceholder strategythat associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ aprogressive training strategyconsisting of three stages: an initial stage\nfocused on simple tasks with limited data to establishbaseline consistency, afull-scale trainingstage to comprehensively enhance the customization\ncapabilities, and a finalquality alignment stageto correctquality biasesint",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/bytedance/DreamO",
    "hf_paper_url": "https://huggingface.co/papers/2504.16915",
    "arxiv_url": "https://arxiv.org/abs/2504.16915",
    "num_models": 2,
    "models_list": "ByteDance/DreamO, RedbeardNZ/DreamO",
    "models_links": "https://huggingface.co/ByteDance/DreamO, https://huggingface.co/RedbeardNZ/DreamO",
    "models_detailed": "[{\"name\": \"ByteDance/DreamO\", \"link\": \"https://huggingface.co/ByteDance/DreamO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"RedbeardNZ/DreamO\", \"link\": \"https://huggingface.co/RedbeardNZ/DreamO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.16030",
    "first_seen_date": "2025-04-23",
    "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.16030LiveCC: Learning Video LLM with Streaming Speech Transcription at ScalePublished on Apr 22\u00b7Submitted byJoya Chenon Apr 23Upvote36+28Authors:Joya Chen,Ziyun Zeng,Yiqi Lin,Wei Li,Zejun Ma,Mike Zheng ShouAbstractLarge-scale training for Video LLMs using ASR transcripts enables competitive video QA performance and real-time commentary capabilities.AI-generated summaryRecentvideo large language models(Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheapautomatic speech recognition(ASR)\ntranscripts. Specifically, we propose a novelstreaming trainingapproach that\ndensely interleaves theASRwords and video frames according to their\ntimestamps. Compared to previous studies invision-language representationwithASR, our method naturally fits the streaming characteristics ofASR, thus\nenabling the model to learntemporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and theirclosed captions(CC, same asASR),\nresulting inLive-CC-5Mdataset for pre-training andLive-WhisperX-526Kdataset\nfor high-qualitysupervised fine-tuning(SFT). Remarkably, even withoutSFT,\ntheASR-only pre-trainedLiveCC-7B-Basemodel demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a newLiveSports-3Kbenchmark, usingLLM-as-a-judgeto measure the free-form commentary.\nExperiments show our finalLiveCC-7B-Instructmodel can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct,LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/showlab/livecc",
    "hf_paper_url": "https://huggingface.co/papers/2504.16030",
    "arxiv_url": "https://arxiv.org/abs/2504.16030",
    "num_models": 3,
    "models_list": "chenjoya/LiveCC-7B-Instruct, chenjoya/LiveCC-7B-Base, Mungert/LiveCC-7B-Instruct-GGUF",
    "models_links": "https://huggingface.co/chenjoya/LiveCC-7B-Instruct, https://huggingface.co/chenjoya/LiveCC-7B-Base, https://huggingface.co/Mungert/LiveCC-7B-Instruct-GGUF",
    "models_detailed": "[{\"name\": \"chenjoya/LiveCC-7B-Instruct\", \"link\": \"https://huggingface.co/chenjoya/LiveCC-7B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"chenjoya/LiveCC-7B-Base\", \"link\": \"https://huggingface.co/chenjoya/LiveCC-7B-Base\", \"task\": \"\", \"likes\": \"703\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"Mungert/LiveCC-7B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/LiveCC-7B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"675\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 3,
    "datasets_list": "chenjoya/Live-WhisperX-526K, stdKonjac/LiveSports-3K, chenjoya/Live-CC-5M",
    "datasets_links": "https://huggingface.co/datasets/chenjoya/Live-WhisperX-526K, https://huggingface.co/datasets/stdKonjac/LiveSports-3K, https://huggingface.co/datasets/chenjoya/Live-CC-5M",
    "datasets_detailed": "[{\"name\": \"chenjoya/Live-WhisperX-526K\", \"link\": \"https://huggingface.co/datasets/chenjoya/Live-WhisperX-526K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\", \"size\": \"\"}, {\"name\": \"stdKonjac/LiveSports-3K\", \"link\": \"https://huggingface.co/datasets/stdKonjac/LiveSports-3K\", \"task\": \"\", \"likes\": \"723\", \"downloads\": \"\", \"updated\": \"May 20\", \"size\": \"\"}, {\"name\": \"chenjoya/Live-CC-5M\", \"link\": \"https://huggingface.co/datasets/chenjoya/Live-CC-5M\", \"task\": \"\", \"likes\": \"63\", \"downloads\": \"\", \"updated\": \"May 2\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.16072",
    "first_seen_date": "2025-04-23",
    "title": "Describe Anything: Detailed Localized Image and Video Captioning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.16072Describe Anything: Detailed Localized Image and Video CaptioningPublished on Apr 22\u00b7Submitted byLong(Tony) Lianon Apr 23Upvote63+55Authors:Long Lian,Yifan Ding,Yunhao Ge,Sifei Liu,Hanzi Mao,Boyi Li,Marco Pavone,Ming-Yu Liu,Trevor Darrell,Adam Yala,Yin CuiAbstractThe Describe Anything Model (DAM) leverages a focal prompt and localized vision backbone to achieve detailed localized captioning, outperforming existing models on various benchmarks through a semi-supervised data pipeline.AI-generated summaryGenerating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: afocal prompt, which ensures high-resolution\nencoding of targeted regions, and alocalized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose aSemi-supervised learning(SSL)-based Data\nPipeline (DLC-SDP).DLC-SDPstarts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduceDLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.View arXiv pageView PDFProject pageGitHub1.44kAdd to collectionCommunitylonglianPaper authorPaper submitterApr 23\u2022edited Apr 23We\u2019re excited to introduce the Describe Anything Model (DAM), a powerful MLLM that generates detailed descriptions for user-defined regions in images or videos using points, boxes, scribbles, or masks.Huggingface Demo (super cool):https://huggingface.co/spaces/nvidia/describe-anything-model-demoCo",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/NVlabs/describe-anything",
    "hf_paper_url": "https://huggingface.co/papers/2504.16072",
    "arxiv_url": "https://arxiv.org/abs/2504.16072",
    "num_models": 9,
    "models_list": "nvidia/DAM-3B, nvidia/DAM-3B-Video, nvidia/DAM-3B-Self-Contained, DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B, DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B, Alibaba-DAMO-Academy/PixelRefer-2B, Alibaba-DAMO-Academy/PixelRefer-7B, Alibaba-DAMO-Academy/PixelRefer-Lite-2B, Alibaba-DAMO-Academy/PixelRefer-Lite-7B",
    "models_links": "https://huggingface.co/nvidia/DAM-3B, https://huggingface.co/nvidia/DAM-3B-Video, https://huggingface.co/nvidia/DAM-3B-Self-Contained, https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B, https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B, https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-2B, https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-7B, https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-Lite-2B, https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-Lite-7B",
    "models_detailed": "[{\"name\": \"nvidia/DAM-3B\", \"link\": \"https://huggingface.co/nvidia/DAM-3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/DAM-3B-Video\", \"link\": \"https://huggingface.co/nvidia/DAM-3B-Video\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/DAM-3B-Self-Contained\", \"link\": \"https://huggingface.co/nvidia/DAM-3B-Self-Contained\", \"task\": \"\", \"likes\": \"644\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B\", \"task\": \"\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B\", \"task\": \"\", \"likes\": \"54\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"Alibaba-DAMO-Academy/PixelRefer-2B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-2B\", \"task\": \"\", \"likes\": \"324\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"Alibaba-DAMO-Academy/PixelRefer-7B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-7B\", \"task\": \"\", \"likes\": \"77\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"Alibaba-DAMO-Academy/PixelRefer-Lite-2B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-Lite-2B\", \"task\": \"\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"Alibaba-DAMO-Academy/PixelRefer-Lite-7B\", \"link\": \"https://huggingface.co/Alibaba-DAMO-Academy/PixelRefer-Lite-7B\", \"task\": \"\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Oct 28\"}]",
    "num_datasets": 2,
    "datasets_list": "nvidia/describe-anything-dataset, nvidia/DLC-Bench",
    "datasets_links": "https://huggingface.co/datasets/nvidia/describe-anything-dataset, https://huggingface.co/datasets/nvidia/DLC-Bench",
    "datasets_detailed": "[{\"name\": \"nvidia/describe-anything-dataset\", \"link\": \"https://huggingface.co/datasets/nvidia/describe-anything-dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\", \"size\": \"\"}, {\"name\": \"nvidia/DLC-Bench\", \"link\": \"https://huggingface.co/datasets/nvidia/DLC-Bench\", \"task\": \"\", \"likes\": \"77\", \"downloads\": \"\", \"updated\": \"Apr 24\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.13941",
    "first_seen_date": "2025-04-22",
    "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.13941NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math ReasoningPublished on Apr 15\u00b7Submitted bySyeda Nahida Akteron Apr 22Upvote11+3Authors:Syeda Nahida Akter,Shrimai Prabhumoye,Matvei Novikov,Seungju Han,Ying Lin,Evelina Bakhturi,Eric Nyberg,Yejin Choi,Mostofa Patwary,Mohammad Shoeybi,Bryan CatanzaroAbstractNEMOTRON-CROSSTHINK is a framework that incorporates diverse multi-domain data into RL training to enhance reasoning capabilities and efficiency of LLMs across various tasks and domains.AI-generated summaryLarge Language Models (LLMs) have shown strong reasoning capabilities,\nparticularly when enhanced throughReinforcement Learning(RL). While prior\nwork has successfully applied RL to mathematical reasoning -- where rules and\ncorrectness are well-defined -- generalizing these methods to broader reasoning\ndomains remains challenging due to limited data, the lack of verifiable reward\nstructures, and diverse task requirements. In this work, we proposeNEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain\ncorpora, including both synthetic and real-world question-answer pairs, into RL\ntraining to improve generalization across diverse reasoning tasks.NEMOTRON-CROSSTHINKaddresses key challenges by (1) incorporating data from\nvaried sources spanning STEM, humanities, social sciences, etc.; (2) applyingstructured templates(e.g., multiple-choice and open-ended) to control\nanswer-space complexity; (3) filtering for verifiable answers; and (4)\noptimizing data blending strategies that utilizes data from multiple sources\neffectively. Our approach enables scalable and verifiablereward modelingbeyond mathematics and demonstrates improved accuracies on both math (MATH-500:\n+30.1%,AMC23:+27.5%) and non-mathreasoning benchmarks(MMLU-PRO: +12.8%,GPQA-DIAMOND: +11.3%,AGIEVAL: +15.1%,SUPERGPQA: +3.8%). Moreover,NEMOTRON-CROSSTHINKexhibits significantly improved res",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.13941",
    "arxiv_url": "https://arxiv.org/abs/2504.13941",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "nvidia/Nemotron-CrossThink, Compumacy/NMT-Crossthinking",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Nemotron-CrossThink, https://huggingface.co/datasets/Compumacy/NMT-Crossthinking",
    "datasets_detailed": "[{\"name\": \"nvidia/Nemotron-CrossThink\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-CrossThink\", \"task\": \"\", \"likes\": \"254\", \"downloads\": \"\", \"updated\": \"May 1\", \"size\": \"\"}, {\"name\": \"Compumacy/NMT-Crossthinking\", \"link\": \"https://huggingface.co/datasets/Compumacy/NMT-Crossthinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 11\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.15271",
    "first_seen_date": "2025-04-22",
    "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.15271Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language ModelsPublished on Apr 21\u00b7Submitted byGuo Chenon Apr 22#2 Paper of the dayUpvote67+59Authors:Guo Chen,Zhiqi Li,Shihao Wang,Jindong Jiang,Yicheng Liu,Lidong Lu,De-An Huang,Wonmin Byeon,Matthieu Le,Tuomas Rintamaki,Tyler Poon,Max Ehrlich,Tuomas Rintamaki,Tyler Poon,Tong Lu,Limin Wang,Bryan Catanzaro,Jan Kautz,Andrew Tao,Zhiding Yu,Guilin LiuAbstractEagle 2.5 improves long-context multimodal learning through Automatic Degrade Sampling and Image Area Preservation techniques, enhancing VLMs for video and image understanding, and matches state-of-the-art performance on benchmarks.AI-generated summaryWe introduce Eagle 2.5, a family of frontiervision-language models(VLMs)\nforlong-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension andhigh-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporatesAutomatic Degrade SamplingandImage Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerousefficiency optimizationsin the pipeline forlong-context data training. Finally, we proposeEagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-contextmultimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% onVideo-MMEwith 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.View arXiv pageView PDFProject pageGitHub912Add to collectionCommunitycg1177Paper authorPaper submitterApr 22We introduce Eagle 2.5, a family of frontier vi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/NVlabs/EAGLE",
    "hf_paper_url": "https://huggingface.co/papers/2504.15271",
    "arxiv_url": "https://arxiv.org/abs/2504.15271",
    "num_models": 2,
    "models_list": "nvidia/Eagle2.5-8B, nvidia/GR00T-N1.6-3B",
    "models_links": "https://huggingface.co/nvidia/Eagle2.5-8B, https://huggingface.co/nvidia/GR00T-N1.6-3B",
    "models_detailed": "[{\"name\": \"nvidia/Eagle2.5-8B\", \"link\": \"https://huggingface.co/nvidia/Eagle2.5-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"23 days ago\"}, {\"name\": \"nvidia/GR00T-N1.6-3B\", \"link\": \"https://huggingface.co/nvidia/GR00T-N1.6-3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.13161",
    "first_seen_date": "2025-04-18",
    "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-training",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.13161CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for\n  Language Model Pre-trainingPublished on Apr 17\u00b7Submitted byShizhe Diaoon Apr 18#1 Paper of the dayUpvote93+85Authors:Shizhe Diao,Yu Yang,Yonggan Fu,Xin Dong,Dan Su,Markus Kliegl,Zijia Chen,Peter Belcak,Yoshi Suhara,Hongxu Yin,Mostofa Patwary,Yingyan,Lin,Jan Kautz,Pavlo MolchanovAbstractPre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically,CLIMBembeds and clusters large-scale\ndatasets in asemantic spaceand then iteratively searches for optimal mixtures\nusing a smallerproxy modeland apredictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduceClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, andClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/View arXiv pageView PDFProject pageAdd to collectionCommu",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.13161",
    "arxiv_url": "https://arxiv.org/abs/2504.13161",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 5,
    "datasets_list": "nvidia/Nemotron-ClimbLab, OptimalScale/ClimbLab, OptimalScale/ClimbMix, gvlassis/ClimbMix, nvidia/Nemotron-ClimbMix",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Nemotron-ClimbLab, https://huggingface.co/datasets/OptimalScale/ClimbLab, https://huggingface.co/datasets/OptimalScale/ClimbMix, https://huggingface.co/datasets/gvlassis/ClimbMix, https://huggingface.co/datasets/nvidia/Nemotron-ClimbMix",
    "datasets_detailed": "[{\"name\": \"nvidia/Nemotron-ClimbLab\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-ClimbLab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\", \"size\": \"\"}, {\"name\": \"OptimalScale/ClimbLab\", \"link\": \"https://huggingface.co/datasets/OptimalScale/ClimbLab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"OptimalScale/ClimbMix\", \"link\": \"https://huggingface.co/datasets/OptimalScale/ClimbMix\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"gvlassis/ClimbMix\", \"link\": \"https://huggingface.co/datasets/gvlassis/ClimbMix\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 11\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-ClimbMix\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-ClimbMix\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.13180",
    "first_seen_date": "2025-04-18",
    "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual\n  Understanding",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.13180PerceptionLM: Open-Access Data and Models for Detailed Visual\n  UnderstandingPublished on Apr 17\u00b7Submitted byJang Hyun (Vincent) Choon Apr 18Upvote19+11Authors:Jang Hyun Cho,Andrea Madotto,Effrosyni Mavroudi,Triantafyllos Afouras,Tushar Nagarajan,Muhammad Maaz,Yale Song,Tengyu Ma,Shuming Hu,Suyog Jain,Miguel Martin,Huiyu Wang,Hanoona Rasheed,Peize Sun,Po-Yao Huang,Daniel Bolya,Nikhila Ravi,Shashank Jain,Tammy Stark,Shane Moon,Babak Damavandi,Vivian Lee+7 authorsAbstractA fully transparent Perception Language Model (PLM) for image and video understanding is developed without relying on proprietary models, using large-scale synthetic and human-labeled data and introducing PLM-VideoBench for evaluation.AI-generated summaryVision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building aPerception Language Model(PLM) in a fully open and reproducible framework for\ntransparent research in image andvideo understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailedvideo understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grainedvideo question-answer pairsandspatio-temporally grounded video captions. Additionally, we introducePLM-VideoBench, a suite for evaluating challengingvideo understandingtasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/facebookresearch/perception_models",
    "hf_paper_url": "https://huggingface.co/papers/2504.13180",
    "arxiv_url": "https://arxiv.org/abs/2504.13180",
    "num_models": 9,
    "models_list": "facebook/Perception-LM-8B, facebook/Perception-LM-1B, facebook/Perception-LM-3B, facebook/PE-Lang-G14-448, facebook/PE-Lang-L14-448, timm/vit_pe_lang_gigantic_patch14_448.fb, timm/vit_pe_lang_large_patch14_448.fb, PIA-SPACE-LAB/Perception-LM-3B, PIA-SPACE-LAB/Perception-LM-1B",
    "models_links": "https://huggingface.co/facebook/Perception-LM-8B, https://huggingface.co/facebook/Perception-LM-1B, https://huggingface.co/facebook/Perception-LM-3B, https://huggingface.co/facebook/PE-Lang-G14-448, https://huggingface.co/facebook/PE-Lang-L14-448, https://huggingface.co/timm/vit_pe_lang_gigantic_patch14_448.fb, https://huggingface.co/timm/vit_pe_lang_large_patch14_448.fb, https://huggingface.co/PIA-SPACE-LAB/Perception-LM-3B, https://huggingface.co/PIA-SPACE-LAB/Perception-LM-1B",
    "models_detailed": "[{\"name\": \"facebook/Perception-LM-8B\", \"link\": \"https://huggingface.co/facebook/Perception-LM-8B\", \"task\": \"\", \"likes\": \"693\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"facebook/Perception-LM-1B\", \"link\": \"https://huggingface.co/facebook/Perception-LM-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 13\"}, {\"name\": \"facebook/Perception-LM-3B\", \"link\": \"https://huggingface.co/facebook/Perception-LM-3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 13\"}, {\"name\": \"facebook/PE-Lang-G14-448\", \"link\": \"https://huggingface.co/facebook/PE-Lang-G14-448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"facebook/PE-Lang-L14-448\", \"link\": \"https://huggingface.co/facebook/PE-Lang-L14-448\", \"task\": \"\", \"likes\": \"532\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"timm/vit_pe_lang_gigantic_patch14_448.fb\", \"link\": \"https://huggingface.co/timm/vit_pe_lang_gigantic_patch14_448.fb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"timm/vit_pe_lang_large_patch14_448.fb\", \"link\": \"https://huggingface.co/timm/vit_pe_lang_large_patch14_448.fb\", \"task\": \"\", \"likes\": \"43\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"PIA-SPACE-LAB/Perception-LM-3B\", \"link\": \"https://huggingface.co/PIA-SPACE-LAB/Perception-LM-3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"PIA-SPACE-LAB/Perception-LM-1B\", \"link\": \"https://huggingface.co/PIA-SPACE-LAB/Perception-LM-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}]",
    "num_datasets": 4,
    "datasets_list": "facebook/PLM-VideoBench, facebook/PLM-Image-Auto, facebook/PLM-Video-Human, facebook/PLM-Video-Auto",
    "datasets_links": "https://huggingface.co/datasets/facebook/PLM-VideoBench, https://huggingface.co/datasets/facebook/PLM-Image-Auto, https://huggingface.co/datasets/facebook/PLM-Video-Human, https://huggingface.co/datasets/facebook/PLM-Video-Auto",
    "datasets_detailed": "[{\"name\": \"facebook/PLM-VideoBench\", \"link\": \"https://huggingface.co/datasets/facebook/PLM-VideoBench\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\", \"size\": \"\"}, {\"name\": \"facebook/PLM-Image-Auto\", \"link\": \"https://huggingface.co/datasets/facebook/PLM-Image-Auto\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\", \"size\": \"\"}, {\"name\": \"facebook/PLM-Video-Human\", \"link\": \"https://huggingface.co/datasets/facebook/PLM-Video-Human\", \"task\": \"\", \"likes\": \"750\", \"downloads\": \"\", \"updated\": \"May 21\", \"size\": \"\"}, {\"name\": \"facebook/PLM-Video-Auto\", \"link\": \"https://huggingface.co/datasets/facebook/PLM-Video-Auto\", \"task\": \"\", \"likes\": \"354\", \"downloads\": \"\", \"updated\": \"Apr 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.13181",
    "first_seen_date": "2025-04-18",
    "title": "Perception Encoder: The best visual embeddings are not at the output of\n  the network",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.13181Perception Encoder: The best visual embeddings are not at the output of\n  the networkPublished on Apr 17\u00b7Submitted byNiels Roggeon Apr 18Upvote34+26Authors:Daniel Bolya,Po-Yao Huang,Peize Sun,Jang Hyun Cho,Andrea Madotto,Chen Wei,Tengyu Ma,Jiale Zhi,Jathushan Rajasegaran,Hanoona Rasheed,Junke Wang,Marco Monteiro,Hu Xu,Shiyu Dong,Nikhila Ravi,Daniel Li,Piotr Doll\u00e1r,Christoph FeichtenhoferAbstractPerception Encoder, trained via contrastive vision-language learning, achieves state-of-the-art performance across various image and video tasks using intermediate embeddings extracted through alignment methods.AI-generated summaryWe introducePerception Encoder(PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind thatcontrastive vision-language trainingalone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods,language alignmentfor\nmultimodal language modeling, andspatial alignmentfor dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, andvideo Q&A;\nand spatial tasks such asdetection,depth estimation, andtracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.View arXiv pageView PDFGitHub",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/facebookresearch/perception_models",
    "hf_paper_url": "https://huggingface.co/papers/2504.13181",
    "arxiv_url": "https://arxiv.org/abs/2504.13181",
    "num_models": 39,
    "models_list": "facebook/PE-Core-L14-336, facebook/PE-Spatial-G14-448, facebook/PE-Core-G14-448, facebook/PE-Detection, facebook/PE-Lang-L14-448, facebook/PE-Lang-G14-448, facebook/PE-Core-B16-224, facebook/pe_core_large_patch14_336_timm, facebook/pe_core_base_patch16_224_timm, facebook/pe_core_gigantic_patch14_448_timm, facebook/vit_pe_core_large_patch14_336_timm, facebook/vit_pe_core_base_patch16_224_timm, facebook/vit_pe_core_gigantic_patch14_448_timm, facebook/vit_pe_lang_large_patch14_448_timm, facebook/vit_pe_lang_gigantic_patch14_448_timm, facebook/vit_pe_spatial_gigantic_patch14_448_timm, timm/vit_pe_core_base_patch16_224.fb, timm/vit_pe_core_gigantic_patch14_448.fb, timm/vit_pe_core_large_patch14_336.fb, timm/vit_pe_lang_gigantic_patch14_448.fb, timm/vit_pe_lang_large_patch14_448.fb, timm/vit_pe_spatial_gigantic_patch14_448.fb, facebook/PE-Core-S16-384, facebook/PE-Core-T16-384, facebook/PE-Spatial-B16-512, facebook/PE-Spatial-L14-448, facebook/PE-Spatial-S16-512, facebook/PE-Spatial-T16-512, facebook/PE-Lang-G14-448-Tiling, facebook/PE-Lang-L14-448-Tiling, timm/PE-Core-T-16-384, timm/PE-Core-S-16-384, timm/PE-Core-B-16, timm/PE-Core-L-14-336, timm/PE-Core-bigG-14-448, birder-project/rope_i_vit_s16_pn_aps_c1_pe-core, birder-project/rope_i_vit_b16_pn_aps_c1_pe-core, birder-project/rope_i_vit_l14_pn_aps_c1_pe-core, birder-project/rope_i_vit_reg1_s16_pn_npn_avg_c1_pe-spatial",
    "models_links": "https://huggingface.co/facebook/PE-Core-L14-336, https://huggingface.co/facebook/PE-Spatial-G14-448, https://huggingface.co/facebook/PE-Core-G14-448, https://huggingface.co/facebook/PE-Detection, https://huggingface.co/facebook/PE-Lang-L14-448, https://huggingface.co/facebook/PE-Lang-G14-448, https://huggingface.co/facebook/PE-Core-B16-224, https://huggingface.co/facebook/pe_core_large_patch14_336_timm, https://huggingface.co/facebook/pe_core_base_patch16_224_timm, https://huggingface.co/facebook/pe_core_gigantic_patch14_448_timm, https://huggingface.co/facebook/vit_pe_core_large_patch14_336_timm, https://huggingface.co/facebook/vit_pe_core_base_patch16_224_timm, https://huggingface.co/facebook/vit_pe_core_gigantic_patch14_448_timm, https://huggingface.co/facebook/vit_pe_lang_large_patch14_448_timm, https://huggingface.co/facebook/vit_pe_lang_gigantic_patch14_448_timm, https://huggingface.co/facebook/vit_pe_spatial_gigantic_patch14_448_timm, https://huggingface.co/timm/vit_pe_core_base_patch16_224.fb, https://huggingface.co/timm/vit_pe_core_gigantic_patch14_448.fb, https://huggingface.co/timm/vit_pe_core_large_patch14_336.fb, https://huggingface.co/timm/vit_pe_lang_gigantic_patch14_448.fb, https://huggingface.co/timm/vit_pe_lang_large_patch14_448.fb, https://huggingface.co/timm/vit_pe_spatial_gigantic_patch14_448.fb, https://huggingface.co/facebook/PE-Core-S16-384, https://huggingface.co/facebook/PE-Core-T16-384, https://huggingface.co/facebook/PE-Spatial-B16-512, https://huggingface.co/facebook/PE-Spatial-L14-448, https://huggingface.co/facebook/PE-Spatial-S16-512, https://huggingface.co/facebook/PE-Spatial-T16-512, https://huggingface.co/facebook/PE-Lang-G14-448-Tiling, https://huggingface.co/facebook/PE-Lang-L14-448-Tiling, https://huggingface.co/timm/PE-Core-T-16-384, https://huggingface.co/timm/PE-Core-S-16-384, https://huggingface.co/timm/PE-Core-B-16, https://huggingface.co/timm/PE-Core-L-14-336, https://huggingface.co/timm/PE-Core-bigG-14-448, https://huggingface.co/birder-project/rope_i_vit_s16_pn_aps_c1_pe-core, https://huggingface.co/birder-project/rope_i_vit_b16_pn_aps_c1_pe-core, https://huggingface.co/birder-project/rope_i_vit_l14_pn_aps_c1_pe-core, https://huggingface.co/birder-project/rope_i_vit_reg1_s16_pn_npn_avg_c1_pe-spatial",
    "models_detailed": "[{\"name\": \"facebook/PE-Core-L14-336\", \"link\": \"https://huggingface.co/facebook/PE-Core-L14-336\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"facebook/PE-Spatial-G14-448\", \"link\": \"https://huggingface.co/facebook/PE-Spatial-G14-448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"facebook/PE-Core-G14-448\", \"link\": \"https://huggingface.co/facebook/PE-Core-G14-448\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"facebook/PE-Detection\", \"link\": \"https://huggingface.co/facebook/PE-Detection\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"facebook/PE-Lang-L14-448\", \"link\": \"https://huggingface.co/facebook/PE-Lang-L14-448\", \"task\": \"\", \"likes\": \"532\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"facebook/PE-Lang-G14-448\", \"link\": \"https://huggingface.co/facebook/PE-Lang-G14-448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"facebook/PE-Core-B16-224\", \"link\": \"https://huggingface.co/facebook/PE-Core-B16-224\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"facebook/pe_core_large_patch14_336_timm\", \"link\": \"https://huggingface.co/facebook/pe_core_large_patch14_336_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"facebook/pe_core_base_patch16_224_timm\", \"link\": \"https://huggingface.co/facebook/pe_core_base_patch16_224_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"facebook/pe_core_gigantic_patch14_448_timm\", \"link\": \"https://huggingface.co/facebook/pe_core_gigantic_patch14_448_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"facebook/vit_pe_core_large_patch14_336_timm\", \"link\": \"https://huggingface.co/facebook/vit_pe_core_large_patch14_336_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"facebook/vit_pe_core_base_patch16_224_timm\", \"link\": \"https://huggingface.co/facebook/vit_pe_core_base_patch16_224_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"facebook/vit_pe_core_gigantic_patch14_448_timm\", \"link\": \"https://huggingface.co/facebook/vit_pe_core_gigantic_patch14_448_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"facebook/vit_pe_lang_large_patch14_448_timm\", \"link\": \"https://huggingface.co/facebook/vit_pe_lang_large_patch14_448_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"facebook/vit_pe_lang_gigantic_patch14_448_timm\", \"link\": \"https://huggingface.co/facebook/vit_pe_lang_gigantic_patch14_448_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"facebook/vit_pe_spatial_gigantic_patch14_448_timm\", \"link\": \"https://huggingface.co/facebook/vit_pe_spatial_gigantic_patch14_448_timm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"timm/vit_pe_core_base_patch16_224.fb\", \"link\": \"https://huggingface.co/timm/vit_pe_core_base_patch16_224.fb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"timm/vit_pe_core_gigantic_patch14_448.fb\", \"link\": \"https://huggingface.co/timm/vit_pe_core_gigantic_patch14_448.fb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"timm/vit_pe_core_large_patch14_336.fb\", \"link\": \"https://huggingface.co/timm/vit_pe_core_large_patch14_336.fb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"timm/vit_pe_lang_gigantic_patch14_448.fb\", \"link\": \"https://huggingface.co/timm/vit_pe_lang_gigantic_patch14_448.fb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"timm/vit_pe_lang_large_patch14_448.fb\", \"link\": \"https://huggingface.co/timm/vit_pe_lang_large_patch14_448.fb\", \"task\": \"\", \"likes\": \"43\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"timm/vit_pe_spatial_gigantic_patch14_448.fb\", \"link\": \"https://huggingface.co/timm/vit_pe_spatial_gigantic_patch14_448.fb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"facebook/PE-Core-S16-384\", \"link\": \"https://huggingface.co/facebook/PE-Core-S16-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"facebook/PE-Core-T16-384\", \"link\": \"https://huggingface.co/facebook/PE-Core-T16-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"facebook/PE-Spatial-B16-512\", \"link\": \"https://huggingface.co/facebook/PE-Spatial-B16-512\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"facebook/PE-Spatial-L14-448\", \"link\": \"https://huggingface.co/facebook/PE-Spatial-L14-448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"facebook/PE-Spatial-S16-512\", \"link\": \"https://huggingface.co/facebook/PE-Spatial-S16-512\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"facebook/PE-Spatial-T16-512\", \"link\": \"https://huggingface.co/facebook/PE-Spatial-T16-512\", \"task\": \"\", \"likes\": \"415\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"facebook/PE-Lang-G14-448-Tiling\", \"link\": \"https://huggingface.co/facebook/PE-Lang-G14-448-Tiling\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"facebook/PE-Lang-L14-448-Tiling\", \"link\": \"https://huggingface.co/facebook/PE-Lang-L14-448-Tiling\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"timm/PE-Core-T-16-384\", \"link\": \"https://huggingface.co/timm/PE-Core-T-16-384\", \"task\": \"Image Classification\", \"likes\": \"272\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"timm/PE-Core-S-16-384\", \"link\": \"https://huggingface.co/timm/PE-Core-S-16-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"timm/PE-Core-B-16\", \"link\": \"https://huggingface.co/timm/PE-Core-B-16\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"timm/PE-Core-L-14-336\", \"link\": \"https://huggingface.co/timm/PE-Core-L-14-336\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"timm/PE-Core-bigG-14-448\", \"link\": \"https://huggingface.co/timm/PE-Core-bigG-14-448\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"birder-project/rope_i_vit_s16_pn_aps_c1_pe-core\", \"link\": \"https://huggingface.co/birder-project/rope_i_vit_s16_pn_aps_c1_pe-core\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 12\"}, {\"name\": \"birder-project/rope_i_vit_b16_pn_aps_c1_pe-core\", \"link\": \"https://huggingface.co/birder-project/rope_i_vit_b16_pn_aps_c1_pe-core\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 12\"}, {\"name\": \"birder-project/rope_i_vit_l14_pn_aps_c1_pe-core\", \"link\": \"https://huggingface.co/birder-project/rope_i_vit_l14_pn_aps_c1_pe-core\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 12\"}, {\"name\": \"birder-project/rope_i_vit_reg1_s16_pn_npn_avg_c1_pe-spatial\", \"link\": \"https://huggingface.co/birder-project/rope_i_vit_reg1_s16_pn_npn_avg_c1_pe-spatial\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\"}]",
    "num_datasets": 1,
    "datasets_list": "facebook/PE-Video",
    "datasets_links": "https://huggingface.co/datasets/facebook/PE-Video",
    "datasets_detailed": "[{\"name\": \"facebook/PE-Video\", \"link\": \"https://huggingface.co/datasets/facebook/PE-Video\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 18\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.10462",
    "first_seen_date": "2025-04-16",
    "title": "The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.10462The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single TransformerPublished on Apr 14\u00b7Submitted byXiangtai Lion Apr 16\u00b7ByteDanceUpvote15+7Authors:Weixian Lei,Jiacong Wang,Haochen Wang,Xiangtai Li,Jun Hao Liew,Jiashi Feng,Zilong HuangAbstractSAIL, a unified multimodal large language model, integrates raw pixel encoding and language decoding using mix-attention mechanisms and achieves performance comparable to modular MLLMs without a separate vision encoder.AI-generated summaryThis paper introduces SAIL, asingle transformerunified multimodal large\nlanguage model (MLLM) that integratesraw pixel encodingandlanguage decodingwithin a singular architecture. Unlike existing modular MLLMs, which rely on apre-trained vision transformer(ViT), SAIL eliminates the need for a separate\nvision encoder, presenting a more minimalist architecture design. Instead of\nintroducing novel architectural components, SAIL adapts mix-attention\nmechanisms andmultimodal positional encodingsto better align with the\ndistinct characteristics of visual and textual modalities. We systematically\ncompare SAIL's properties-includingscalability, cross-modal information flow\npatterns, andvisual representation capabilities-with those of modular MLLMs.\nBy scaling both training data and model size, SAIL achieves performance\ncomparable to modular MLLMs. Notably, the removal of pretrained ViT components\nenhances SAIL'sscalabilityand results in significantly different cross-modal\ninformation flow patterns. Moreover, SAIL demonstrates strong visual\nrepresentation capabilities, achieving results on par with ViT-22B in vision\ntasks such assemantic segmentation. Code and models are available at\nhttps://github.com/bytedance/SAIL.View arXiv pageView PDFGitHub76autoAdd to collectionCommunityLXTPaper authorPaper submitterApr 16This paper introduces SAIL, a single transformer unified mul",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/bytedance/sail",
    "hf_paper_url": "https://huggingface.co/papers/2504.10462",
    "arxiv_url": "https://arxiv.org/abs/2504.10462",
    "num_models": 1,
    "models_list": "ByteDance-Seed/SAIL-7B",
    "models_links": "https://huggingface.co/ByteDance-Seed/SAIL-7B",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/SAIL-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/SAIL-7B\", \"task\": \"\", \"likes\": \"85\", \"downloads\": \"\", \"updated\": \"May 7\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.11409",
    "first_seen_date": "2025-04-16",
    "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM\n  Pruning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.11409Efficient Hybrid Language Model Compression through Group-Aware SSM\n  PruningPublished on Apr 15\u00b7Submitted byAli Taghibakhshion Apr 16Upvote9+1Authors:Ali Taghibakhshi,Sharath Turuvekere Sreenivas,Saurav Muralidharan,Marcin Chochowski,Yashaswi Karnati,Raviraj Joshi,Ameya Sunil Mahabaleshwarkar,Zijia Chen,Yoshi Suhara,Oluwatobi Olabiyi,Daniel Korzekwa,Mostofa Patwary,Mohammad Shoeybi,Jan Kautz,Bryan Catanzaro,Ashwath Aithal,Nima Tajbakhsh,Pavlo MolchanovAbstractA novel group-aware pruning strategy is introduced to compress Hybrid LLM architectures, enhancing accuracy and inference speed while reducing training costs and parameters.AI-generated summaryHybrid LLM architecturesthat combineAttentionandState Space Models(SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation toAttention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity ofSSMblocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of suchSSMpruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombinesSSM,FFN,embedding dimension, andlayer pruning, followed byknowledge distillation-based retraining, similar to theMINITRONtechnique.\nUsing this approach, we compress theNemotron-H8B Hybrid model down to 4B\nparameters with up to 40x fewertraining tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.View arXiv pageView PDFAdd to collectionCommunityjrd971000Paper authorPaper submitterApr 16See translationReplylibrarian-botApr 17This is an ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.11409",
    "arxiv_url": "https://arxiv.org/abs/2504.11409",
    "num_models": 2,
    "models_list": "nvidia/Nemotron-H-4B-Instruct-128K, nvidia/Nemotron-H-4B-Base-8K",
    "models_links": "https://huggingface.co/nvidia/Nemotron-H-4B-Instruct-128K, https://huggingface.co/nvidia/Nemotron-H-4B-Base-8K",
    "models_detailed": "[{\"name\": \"nvidia/Nemotron-H-4B-Instruct-128K\", \"link\": \"https://huggingface.co/nvidia/Nemotron-H-4B-Instruct-128K\", \"task\": \"Text Generation\", \"likes\": \"427\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"nvidia/Nemotron-H-4B-Base-8K\", \"link\": \"https://huggingface.co/nvidia/Nemotron-H-4B-Base-8K\", \"task\": \"Text Generation\", \"likes\": \"468\", \"downloads\": \"\", \"updated\": \"Oct 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.08837",
    "first_seen_date": "2025-04-15",
    "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.08837VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models\n  with Reinforcement LearningPublished on Apr 10\u00b7Submitted byWenhu Chenon Apr 15Upvote43+35Authors:Haozhe Wang,Chao Qu,Zuming Huang,Wei Chu,Fangzhen Lin,Wenhu ChenAbstractVision-language models enhanced with reinforcement learning and Forced Rethinking achieve state-of-the-art performance on math and science benchmarks and approach the capabilities of slow-thinking systems.AI-generated summaryRecently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated\ngreat potential in solving challenging problems through explicit reflection.\nThey significantly outperform the best fast-thinking models, such as GPT-4o, on\nvarious math and science benchmarks. However, their multimodal reasoning\ncapabilities remain on par with fast-thinking models. For instance, GPT-o1's\nperformance on benchmarks likeMathVista,MathVerse, andMathVisionis similar\nto fast-thinking models. In this paper, we aim to enhance the slow-thinking\ncapabilities of vision-language models using reinforcement learning (without\nrelying on distillation) to advance the state of the art. First, we adapt theGRPO algorithmwith a novel technique calledSelective Sample Replay(SSR) to\naddress the vanishing advantages problem. While this approach yields strong\nperformance, the resulting RL-trained models exhibit limited self-reflection or\nself-verification. To further encourage slow-thinking, we introduce Forced\nRethinking, which appends a textual rethinking trigger to the end of initial\nrollouts in RL training, explicitly enforcing a self-reflection reasoning step.\nBy combining these two techniques, our model,VL-Rethinker, advances\nstate-of-the-art scores onMathVista,MathVerse, andMathVisionto achieve\n80.3%, 61.8%, and 43.9% respectively.VL-Rethinkeralso achieves open-source\nSoTA on multi-disciplinary benchmarks such asMMMU-Pro,EMMA, andMEGA-Be",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/TIGER-AI-Lab/VL-Rethinker",
    "hf_paper_url": "https://huggingface.co/papers/2504.08837",
    "arxiv_url": "https://arxiv.org/abs/2504.08837",
    "num_models": 4,
    "models_list": "TIGER-Lab/VL-Rethinker-7B, TIGER-Lab/VL-Rethinker-72B, TIGER-Lab/VL-Reasoner-72B, TIGER-Lab/VL-Reasoner-7B",
    "models_links": "https://huggingface.co/TIGER-Lab/VL-Rethinker-7B, https://huggingface.co/TIGER-Lab/VL-Rethinker-72B, https://huggingface.co/TIGER-Lab/VL-Reasoner-72B, https://huggingface.co/TIGER-Lab/VL-Reasoner-7B",
    "models_detailed": "[{\"name\": \"TIGER-Lab/VL-Rethinker-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/VL-Rethinker-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 5\"}, {\"name\": \"TIGER-Lab/VL-Rethinker-72B\", \"link\": \"https://huggingface.co/TIGER-Lab/VL-Rethinker-72B\", \"task\": \"Question Answering\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"May 5\"}, {\"name\": \"TIGER-Lab/VL-Reasoner-72B\", \"link\": \"https://huggingface.co/TIGER-Lab/VL-Reasoner-72B\", \"task\": \"Question Answering\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"TIGER-Lab/VL-Reasoner-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/VL-Reasoner-7B\", \"task\": \"Question Answering\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Apr 21\"}]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/ViRL39K",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/ViRL39K",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/ViRL39K\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/ViRL39K\", \"task\": \"\", \"likes\": \"383\", \"downloads\": \"\", \"updated\": \"Apr 23\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.10479",
    "first_seen_date": "2025-04-15",
    "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.10479InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal ModelsPublished on Apr 14\u00b7Submitted byWeiyun Wangon Apr 15#1 Paper of the dayUpvote306+298Authors:Jinguo Zhu,Weiyun Wang,Zhe Chen,Zhaoyang Liu,Shenglong Ye,Lixin Gu,Yuchen Duan,Hao Tian,Weijie Su,Jie Shao,Zhangwei Gao,Erfei Cui,Yue Cao,Yangzhou Liu,Weiye Xu,Hao Li,Jiahao Wang,Han Lv,Dengnian Chen,Songze Li,Yinan He,Tan Jiang+25 authorsAbstractInternVL3 is a multimodal pre-trained language model that jointly learns from both multimodal data and text, improving performance and scalability through advanced techniques and setting a new state-of-the-art in multimodal tasks.AI-generated summaryWe introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a nativemultimodal pre-trainingparadigm. Rather than adapting a\ntext-onlylarge language model(LLM) into amultimodal large language model(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such assupervised fine-tuning(SFT) and mixed\npreference optimization (MPO), and adoptstest-time scalingstrategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilit",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.10479",
    "arxiv_url": "https://arxiv.org/abs/2504.10479",
    "num_models": 192,
    "models_list": "OpenGVLab/InternVL3-78B, OpenGVLab/InternVL3_5-241B-A28B, OpenGVLab/InternVL3-8B, OpenGVLab/InternVL3_5-8B, OpenGVLab/InternVL3-14B, OpenGVLab/InternVL3-1B, OpenGVLab/InternVL3-1B-hf, OpenGVLab/InternVL3_5-38B, OpenGVLab/InternVL3-38B, OpenGVLab/InternVL3-9B, OpenGVLab/InternVL3-2B, OpenGVLab/InternVL3-78B-Instruct, OpenGVLab/InternVL3-1B-Instruct, OpenGVLab/InternVL3-2B-Instruct, OpenGVLab/InternVL3-38B-Instruct, OpenGVLab/InternVL3-14B-Instruct, OpenGVLab/InternVL3-8B-Instruct, OpenGVLab/InternVL3-9B-Instruct, OpenGVLab/InternVL3-1B-Pretrained, OpenGVLab/InternVL3-8B-Pretrained, OpenGVLab/InternVL3-2B-Pretrained, OpenGVLab/InternVL3-9B-Pretrained, OpenGVLab/InternVL3-14B-Pretrained, OpenGVLab/InternVL3-38B-Pretrained, OpenGVLab/InternVL3-78B-Pretrained, OpenGVLab/InternVL3-1B-AWQ, OpenGVLab/InternVL3-2B-AWQ, OpenGVLab/InternVL3-8B-AWQ, OpenGVLab/InternVL3-14B-AWQ, OpenGVLab/InternVL3-38B-AWQ, OpenGVLab/InternVL3-9B-AWQ, OpenGVLab/InternVL3-78B-AWQ, OpenGVLab/InternVL3-2B-hf, OpenGVLab/InternVL3-8B-hf, OpenGVLab/InternVL3-14B-hf, OpenGVLab/InternVL3-38B-hf, OpenGVLab/InternVL3-78B-hf, FriendliAI/InternVL3-9B-Instruct, FriendliAI/InternVL3-1B-Instruct, FriendliAI/InternVL3-2B-Instruct, FriendliAI/InternVL3-8B-Instruct, FriendliAI/InternVL3-14B-Instruct, FriendliAI/InternVL3-38B-Instruct, FriendliAI/InternVL3-78B-Instruct, unsloth/InternVL3-1B, unsloth/InternVL3-2B, unsloth/InternVL3-1B-GGUF, unsloth/InternVL3-2B-GGUF, unsloth/InternVL3-8B, unsloth/InternVL3-8B-GGUF, unsloth/InternVL3-14B-GGUF, unsloth/InternVL3-38B, unsloth/InternVL3-38B-GGUF, unsloth/InternVL3-78B-GGUF, unsloth/InternVL3-1B-Instruct, unsloth/InternVL3-1B-Instruct-GGUF, unsloth/InternVL3-2B-Instruct, unsloth/InternVL3-2B-Instruct-GGUF, unsloth/InternVL3-8B-Instruct-GGUF, unsloth/InternVL3-14B-Instruct, unsloth/InternVL3-14B-Instruct-GGUF, unsloth/InternVL3-38B-Instruct-GGUF, unsloth/InternVL3-78B-Instruct-GGUF, Mungert/InternVL3-1B-GGUF, Mungert/InternVL3-8B-GGUF, Wisdom-math/internvl_38b_spatial_reasoning, Wisdom-math/internvl_14b_spatial_reasoning, nottrz/InternVL3-14B-4bit-bnb-openai-chat, Tnt3o5/InternVL3-1B-base, OpenGVLab/InternVL3_5-241B-A28B-MPO, OpenGVLab/InternVL3_5-241B-A28B-Pretrained, OpenGVLab/InternVL3_5-241B-A28B-Instruct, OpenGVLab/InternVL3_5-38B-MPO, OpenGVLab/InternVL3_5-38B-Pretrained, OpenGVLab/InternVL3_5-30B-A3B-MPO, OpenGVLab/InternVL3_5-30B-A3B, OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview, OpenGVLab/InternVL3_5-30B-A3B-Pretrained, OpenGVLab/InternVL3_5-38B-Instruct, OpenGVLab/InternVL3_5-30B-A3B-Instruct, OpenGVLab/InternVL3_5-4B, OpenGVLab/InternVL3_5-8B-Pretrained, OpenGVLab/InternVL3_5-8B-MPO, OpenGVLab/InternVL3_5-8B-Instruct, OpenGVLab/InternVL3_5-2B-Pretrained, OpenGVLab/InternVL3_5-2B-Instruct, OpenGVLab/InternVL3_5-4B-Instruct, OpenGVLab/InternVL3_5-1B, OpenGVLab/InternVL3_5-2B, OpenGVLab/InternVL3_5-2B-MPO, OpenGVLab/InternVL3_5-4B-MPO, OpenGVLab/InternVL3_5-1B-MPO, OpenGVLab/InternVL3_5-4B-Pretrained, OpenGVLab/InternVL3_5-14B-Pretrained, OpenGVLab/InternVL3_5-14B-MPO, OpenGVLab/InternVL3_5-14B-Instruct, OpenGVLab/InternVL3_5-14B, OpenGVLab/InternVL3_5-1B-Instruct, OpenGVLab/InternVL3_5-1B-Pretrained, OpenGVLab/InternVL3_5-38B-HF, OpenGVLab/InternVL3_5-30B-A3B-HF, OpenGVLab/InternVL3_5-8B-HF, OpenGVLab/InternVL3_5-1B-HF, OpenGVLab/InternVL3_5-4B-HF, OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF, OpenGVLab/InternVL3_5-2B-HF, OpenGVLab/InternVL3_5-241B-A28B-HF, OpenGVLab/InternVL3_5-14B-HF, cpatonn/InternVL3_5-38B-AWQ-4bit, cpatonn/InternVL3_5-8B-AWQ-4bit, cpatonn/InternVL3_5-14B-AWQ-4bit, cpatonn/InternVL3_5-38B-AWQ-8bit, cpatonn/InternVL3_5-14B-AWQ-8bit, cpatonn/InternVL3_5-8B-AWQ-8bit, vlnkane/InternVL3_5-GPT-OSS-20B-A4B-Preview, mertunsal/InternVL3_5-30B-A3B, mertunsal/InternVL3_5-4B, mertunsal/InternVL3_5-2B, Mungert/InternVL3-78B-GGUF, OpenGVLab/InternVL3_5-2B-Flash, OpenGVLab/InternVL3_5-4B-Flash, OpenGVLab/InternVL3_5-1B-Flash, OpenGVLab/InternVL3_5-8B-Flash, OpenGVLab/InternVL3_5-14B-Flash, OpenGVLab/InternVL3_5-38B-Flash, OpenGVLab/InternVL3_5-30B-A3B-Flash, OpenGVLab/InternVL3_5-241B-A28B-Flash, KhanhXoe/InternVL3_custombook, iarchuk/InternVL, Userb1az/InternVL3-78B-GGUF, IVC-liuyuan/Intern_Vision_language, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s16000, lyx98/InternVL3_5_Flash-1B-HF, lyx98/InternVL3_5_Flash-2B-HF, lyx98/InternVL3_5_Flash-4B-HF, yiyangd/InternVL3_5-1B-HF-mix_base_oxe_1214_proximity_50pc_libero_text_5ac-s5000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_oxe_1214_proximity_50pc_libero_text_5ac-s5000, yiyangd/InternVL3_5-1B-HF-skip_libero_10_s16000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_10_s32000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_10_s48000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_10_s64000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s4000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s8000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s12000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s16000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_object_s4000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_object_s8000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_object_s12000_new, yiyangd/InternVL3_5-1B-HF-skip_libero_object_s16000_new, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s8000, yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s12000, yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s16000, yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s4000, yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s8000",
    "models_links": "https://huggingface.co/OpenGVLab/InternVL3-78B, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B, https://huggingface.co/OpenGVLab/InternVL3-8B, https://huggingface.co/OpenGVLab/InternVL3_5-8B, https://huggingface.co/OpenGVLab/InternVL3-14B, https://huggingface.co/OpenGVLab/InternVL3-1B, https://huggingface.co/OpenGVLab/InternVL3-1B-hf, https://huggingface.co/OpenGVLab/InternVL3_5-38B, https://huggingface.co/OpenGVLab/InternVL3-38B, https://huggingface.co/OpenGVLab/InternVL3-9B, https://huggingface.co/OpenGVLab/InternVL3-2B, https://huggingface.co/OpenGVLab/InternVL3-78B-Instruct, https://huggingface.co/OpenGVLab/InternVL3-1B-Instruct, https://huggingface.co/OpenGVLab/InternVL3-2B-Instruct, https://huggingface.co/OpenGVLab/InternVL3-38B-Instruct, https://huggingface.co/OpenGVLab/InternVL3-14B-Instruct, https://huggingface.co/OpenGVLab/InternVL3-8B-Instruct, https://huggingface.co/OpenGVLab/InternVL3-9B-Instruct, https://huggingface.co/OpenGVLab/InternVL3-1B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3-8B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3-2B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3-9B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3-14B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3-38B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3-78B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3-1B-AWQ, https://huggingface.co/OpenGVLab/InternVL3-2B-AWQ, https://huggingface.co/OpenGVLab/InternVL3-8B-AWQ, https://huggingface.co/OpenGVLab/InternVL3-14B-AWQ, https://huggingface.co/OpenGVLab/InternVL3-38B-AWQ, https://huggingface.co/OpenGVLab/InternVL3-9B-AWQ, https://huggingface.co/OpenGVLab/InternVL3-78B-AWQ, https://huggingface.co/OpenGVLab/InternVL3-2B-hf, https://huggingface.co/OpenGVLab/InternVL3-8B-hf, https://huggingface.co/OpenGVLab/InternVL3-14B-hf, https://huggingface.co/OpenGVLab/InternVL3-38B-hf, https://huggingface.co/OpenGVLab/InternVL3-78B-hf, https://huggingface.co/FriendliAI/InternVL3-9B-Instruct, https://huggingface.co/FriendliAI/InternVL3-1B-Instruct, https://huggingface.co/FriendliAI/InternVL3-2B-Instruct, https://huggingface.co/FriendliAI/InternVL3-8B-Instruct, https://huggingface.co/FriendliAI/InternVL3-14B-Instruct, https://huggingface.co/FriendliAI/InternVL3-38B-Instruct, https://huggingface.co/FriendliAI/InternVL3-78B-Instruct, https://huggingface.co/unsloth/InternVL3-1B, https://huggingface.co/unsloth/InternVL3-2B, https://huggingface.co/unsloth/InternVL3-1B-GGUF, https://huggingface.co/unsloth/InternVL3-2B-GGUF, https://huggingface.co/unsloth/InternVL3-8B, https://huggingface.co/unsloth/InternVL3-8B-GGUF, https://huggingface.co/unsloth/InternVL3-14B-GGUF, https://huggingface.co/unsloth/InternVL3-38B, https://huggingface.co/unsloth/InternVL3-38B-GGUF, https://huggingface.co/unsloth/InternVL3-78B-GGUF, https://huggingface.co/unsloth/InternVL3-1B-Instruct, https://huggingface.co/unsloth/InternVL3-1B-Instruct-GGUF, https://huggingface.co/unsloth/InternVL3-2B-Instruct, https://huggingface.co/unsloth/InternVL3-2B-Instruct-GGUF, https://huggingface.co/unsloth/InternVL3-8B-Instruct-GGUF, https://huggingface.co/unsloth/InternVL3-14B-Instruct, https://huggingface.co/unsloth/InternVL3-14B-Instruct-GGUF, https://huggingface.co/unsloth/InternVL3-38B-Instruct-GGUF, https://huggingface.co/unsloth/InternVL3-78B-Instruct-GGUF, https://huggingface.co/Mungert/InternVL3-1B-GGUF, https://huggingface.co/Mungert/InternVL3-8B-GGUF, https://huggingface.co/Wisdom-math/internvl_38b_spatial_reasoning, https://huggingface.co/Wisdom-math/internvl_14b_spatial_reasoning, https://huggingface.co/nottrz/InternVL3-14B-4bit-bnb-openai-chat, https://huggingface.co/Tnt3o5/InternVL3-1B-base, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-38B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-38B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B, https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-38B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-4B, https://huggingface.co/OpenGVLab/InternVL3_5-8B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-8B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-8B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-2B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-2B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-4B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-1B, https://huggingface.co/OpenGVLab/InternVL3_5-2B, https://huggingface.co/OpenGVLab/InternVL3_5-2B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-4B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-1B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-4B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-14B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-14B-MPO, https://huggingface.co/OpenGVLab/InternVL3_5-14B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-14B, https://huggingface.co/OpenGVLab/InternVL3_5-1B-Instruct, https://huggingface.co/OpenGVLab/InternVL3_5-1B-Pretrained, https://huggingface.co/OpenGVLab/InternVL3_5-38B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-8B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-1B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-4B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF, https://huggingface.co/OpenGVLab/InternVL3_5-2B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-HF, https://huggingface.co/OpenGVLab/InternVL3_5-14B-HF, https://huggingface.co/cpatonn/InternVL3_5-38B-AWQ-4bit, https://huggingface.co/cpatonn/InternVL3_5-8B-AWQ-4bit, https://huggingface.co/cpatonn/InternVL3_5-14B-AWQ-4bit, https://huggingface.co/cpatonn/InternVL3_5-38B-AWQ-8bit, https://huggingface.co/cpatonn/InternVL3_5-14B-AWQ-8bit, https://huggingface.co/cpatonn/InternVL3_5-8B-AWQ-8bit, https://huggingface.co/vlnkane/InternVL3_5-GPT-OSS-20B-A4B-Preview, https://huggingface.co/mertunsal/InternVL3_5-30B-A3B, https://huggingface.co/mertunsal/InternVL3_5-4B, https://huggingface.co/mertunsal/InternVL3_5-2B, https://huggingface.co/Mungert/InternVL3-78B-GGUF, https://huggingface.co/OpenGVLab/InternVL3_5-2B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-4B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-14B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-38B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Flash, https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Flash, https://huggingface.co/KhanhXoe/InternVL3_custombook, https://huggingface.co/iarchuk/InternVL, https://huggingface.co/Userb1az/InternVL3-78B-GGUF, https://huggingface.co/IVC-liuyuan/Intern_Vision_language, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s16000, https://huggingface.co/lyx98/InternVL3_5_Flash-1B-HF, https://huggingface.co/lyx98/InternVL3_5_Flash-2B-HF, https://huggingface.co/lyx98/InternVL3_5_Flash-4B-HF, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_oxe_1214_proximity_50pc_libero_text_5ac-s5000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_oxe_1214_proximity_50pc_libero_text_5ac-s5000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s16000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s32000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s48000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s64000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s4000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s8000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s12000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s16000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s4000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s8000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s12000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s16000_new, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s8000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s12000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s16000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s4000, https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s8000",
    "models_detailed": "[{\"name\": \"OpenGVLab/InternVL3-78B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-78B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3-8B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3-14B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-1B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-1B-hf\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-1B-hf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3-38B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-38B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-9B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-9B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"OpenGVLab/InternVL3-2B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-78B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-78B-Instruct\", \"task\": \"\", \"likes\": \"370\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-1B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-1B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-2B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-38B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-38B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-14B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-14B-Instruct\", \"task\": \"\", \"likes\": \"935\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-8B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-8B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-9B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-9B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"OpenGVLab/InternVL3-1B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-1B-Pretrained\", \"task\": \"\", \"likes\": \"101\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-8B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-8B-Pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-2B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-2B-Pretrained\", \"task\": \"\", \"likes\": \"119\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-9B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-9B-Pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 25\"}, {\"name\": \"OpenGVLab/InternVL3-14B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-14B-Pretrained\", \"task\": \"\", \"likes\": \"87\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-38B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-38B-Pretrained\", \"task\": \"\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-78B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-78B-Pretrained\", \"task\": \"\", \"likes\": \"68\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-1B-AWQ\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-1B-AWQ\", \"task\": \"\", \"likes\": \"162\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-2B-AWQ\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-2B-AWQ\", \"task\": \"\", \"likes\": \"144\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-8B-AWQ\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-8B-AWQ\", \"task\": \"\", \"likes\": \"811\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-14B-AWQ\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-14B-AWQ\", \"task\": \"\", \"likes\": \"390\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-38B-AWQ\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-38B-AWQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-9B-AWQ\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-9B-AWQ\", \"task\": \"\", \"likes\": \"66\", \"downloads\": \"\", \"updated\": \"Apr 17\"}, {\"name\": \"OpenGVLab/InternVL3-78B-AWQ\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-78B-AWQ\", \"task\": \"\", \"likes\": \"188\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenGVLab/InternVL3-2B-hf\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-2B-hf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"OpenGVLab/InternVL3-8B-hf\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-8B-hf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"OpenGVLab/InternVL3-14B-hf\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-14B-hf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"OpenGVLab/InternVL3-38B-hf\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-38B-hf\", \"task\": \"\", \"likes\": \"554\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"OpenGVLab/InternVL3-78B-hf\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3-78B-hf\", \"task\": \"\", \"likes\": \"619\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"FriendliAI/InternVL3-9B-Instruct\", \"link\": \"https://huggingface.co/FriendliAI/InternVL3-9B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"FriendliAI/InternVL3-1B-Instruct\", \"link\": \"https://huggingface.co/FriendliAI/InternVL3-1B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"FriendliAI/InternVL3-2B-Instruct\", \"link\": \"https://huggingface.co/FriendliAI/InternVL3-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"FriendliAI/InternVL3-8B-Instruct\", \"link\": \"https://huggingface.co/FriendliAI/InternVL3-8B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"FriendliAI/InternVL3-14B-Instruct\", \"link\": \"https://huggingface.co/FriendliAI/InternVL3-14B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"FriendliAI/InternVL3-38B-Instruct\", \"link\": \"https://huggingface.co/FriendliAI/InternVL3-38B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"FriendliAI/InternVL3-78B-Instruct\", \"link\": \"https://huggingface.co/FriendliAI/InternVL3-78B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"unsloth/InternVL3-1B\", \"link\": \"https://huggingface.co/unsloth/InternVL3-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-2B\", \"link\": \"https://huggingface.co/unsloth/InternVL3-2B\", \"task\": \"\", \"likes\": \"29\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-1B-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-1B-GGUF\", \"task\": \"\", \"likes\": \"332\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-2B-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-2B-GGUF\", \"task\": \"\", \"likes\": \"460\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-8B\", \"link\": \"https://huggingface.co/unsloth/InternVL3-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-8B-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-8B-GGUF\", \"task\": \"\", \"likes\": \"346\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-14B-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-14B-GGUF\", \"task\": \"\", \"likes\": \"293\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-38B\", \"link\": \"https://huggingface.co/unsloth/InternVL3-38B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-38B-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-38B-GGUF\", \"task\": \"\", \"likes\": \"388\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"unsloth/InternVL3-78B-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-78B-GGUF\", \"task\": \"\", \"likes\": \"511\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-1B-Instruct\", \"link\": \"https://huggingface.co/unsloth/InternVL3-1B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-1B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-1B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-2B-Instruct\", \"link\": \"https://huggingface.co/unsloth/InternVL3-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-2B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-2B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-8B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-8B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"397\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-14B-Instruct\", \"link\": \"https://huggingface.co/unsloth/InternVL3-14B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-14B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-14B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"311\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-38B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-38B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"195\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"unsloth/InternVL3-78B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/InternVL3-78B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"271\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"Mungert/InternVL3-1B-GGUF\", \"link\": \"https://huggingface.co/Mungert/InternVL3-1B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/InternVL3-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/InternVL3-8B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Wisdom-math/internvl_38b_spatial_reasoning\", \"link\": \"https://huggingface.co/Wisdom-math/internvl_38b_spatial_reasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 8\"}, {\"name\": \"Wisdom-math/internvl_14b_spatial_reasoning\", \"link\": \"https://huggingface.co/Wisdom-math/internvl_14b_spatial_reasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 8\"}, {\"name\": \"nottrz/InternVL3-14B-4bit-bnb-openai-chat\", \"link\": \"https://huggingface.co/nottrz/InternVL3-14B-4bit-bnb-openai-chat\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"Tnt3o5/InternVL3-1B-base\", \"link\": \"https://huggingface.co/Tnt3o5/InternVL3-1B-base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-MPO\", \"task\": \"\", \"likes\": \"66\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Pretrained\", \"task\": \"\", \"likes\": \"61\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-MPO\", \"task\": \"\", \"likes\": \"176\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-Pretrained\", \"task\": \"\", \"likes\": \"86\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-MPO\", \"task\": \"\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Pretrained\", \"task\": \"\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-Instruct\", \"task\": \"\", \"likes\": \"710\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Instruct\", \"task\": \"\", \"likes\": \"146\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-Pretrained\", \"task\": \"\", \"likes\": \"416\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-MPO\", \"task\": \"\", \"likes\": \"534\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-Pretrained\", \"task\": \"\", \"likes\": \"94\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-MPO\", \"task\": \"\", \"likes\": \"167\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-MPO\", \"task\": \"\", \"likes\": \"680\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-MPO\", \"task\": \"\", \"likes\": \"156\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-Pretrained\", \"task\": \"\", \"likes\": \"112\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-Pretrained\", \"task\": \"\", \"likes\": \"96\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-MPO\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-MPO\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-Instruct\", \"task\": \"\", \"likes\": \"714\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-Instruct\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-Instruct\", \"task\": \"\", \"likes\": \"796\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-Pretrained\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-Pretrained\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-HF\", \"task\": \"\", \"likes\": \"581\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-HF\", \"task\": \"\", \"likes\": \"123\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-HF\", \"task\": \"\", \"likes\": \"82\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-HF\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"cpatonn/InternVL3_5-38B-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/InternVL3_5-38B-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"cpatonn/InternVL3_5-8B-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/InternVL3_5-8B-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"cpatonn/InternVL3_5-14B-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/InternVL3_5-14B-AWQ-4bit\", \"task\": \"\", \"likes\": \"111\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"cpatonn/InternVL3_5-38B-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/InternVL3_5-38B-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"cpatonn/InternVL3_5-14B-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/InternVL3_5-14B-AWQ-8bit\", \"task\": \"\", \"likes\": \"118\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"cpatonn/InternVL3_5-8B-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/InternVL3_5-8B-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"vlnkane/InternVL3_5-GPT-OSS-20B-A4B-Preview\", \"link\": \"https://huggingface.co/vlnkane/InternVL3_5-GPT-OSS-20B-A4B-Preview\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"mertunsal/InternVL3_5-30B-A3B\", \"link\": \"https://huggingface.co/mertunsal/InternVL3_5-30B-A3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"mertunsal/InternVL3_5-4B\", \"link\": \"https://huggingface.co/mertunsal/InternVL3_5-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"mertunsal/InternVL3_5-2B\", \"link\": \"https://huggingface.co/mertunsal/InternVL3_5-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 20\"}, {\"name\": \"Mungert/InternVL3-78B-GGUF\", \"link\": \"https://huggingface.co/Mungert/InternVL3-78B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 27\"}, {\"name\": \"OpenGVLab/InternVL3_5-2B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-2B-Flash\", \"task\": \"\", \"likes\": \"419\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-4B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-4B-Flash\", \"task\": \"\", \"likes\": \"645\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-1B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash\", \"task\": \"\", \"likes\": \"809\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-8B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-14B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-14B-Flash\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-38B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-38B-Flash\", \"task\": \"\", \"likes\": \"151\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-30B-A3B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Flash\", \"task\": \"\", \"likes\": \"241\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"OpenGVLab/InternVL3_5-241B-A28B-Flash\", \"link\": \"https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Flash\", \"task\": \"\", \"likes\": \"135\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"KhanhXoe/InternVL3_custombook\", \"link\": \"https://huggingface.co/KhanhXoe/InternVL3_custombook\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 29\"}, {\"name\": \"iarchuk/InternVL\", \"link\": \"https://huggingface.co/iarchuk/InternVL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 13\"}, {\"name\": \"Userb1az/InternVL3-78B-GGUF\", \"link\": \"https://huggingface.co/Userb1az/InternVL3-78B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"IVC-liuyuan/Intern_Vision_language\", \"link\": \"https://huggingface.co/IVC-liuyuan/Intern_Vision_language\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"22 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_spatial_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"16 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_spatial_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_object_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_5ac_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"lyx98/InternVL3_5_Flash-1B-HF\", \"link\": \"https://huggingface.co/lyx98/InternVL3_5_Flash-1B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"8 days ago\"}, {\"name\": \"lyx98/InternVL3_5_Flash-2B-HF\", \"link\": \"https://huggingface.co/lyx98/InternVL3_5_Flash-2B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"8 days ago\"}, {\"name\": \"lyx98/InternVL3_5_Flash-4B-HF\", \"link\": \"https://huggingface.co/lyx98/InternVL3_5_Flash-4B-HF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"8 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_oxe_1214_proximity_50pc_libero_text_5ac-s5000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_oxe_1214_proximity_50pc_libero_text_5ac-s5000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_oxe_1214_proximity_50pc_libero_text_5ac-s5000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_oxe_1214_proximity_50pc_libero_text_5ac-s5000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_10_s16000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s16000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_10_s32000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s32000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_10_s48000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s48000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_10_s64000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_10_s64000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s4000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s4000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s8000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s8000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s12000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s12000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s16000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_goal_s16000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_object_s4000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s4000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_object_s8000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s8000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_object_s12000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s12000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-skip_libero_object_s16000_new\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-skip_libero_object_s16000_new\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_libero_text_oxe_1024_s5000-libero_object_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_object_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_5ac_s5000-libero_spatial_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_proximity_50pc_libero_text_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s12000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s12000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s16000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s16000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s4000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s4000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s8000\", \"link\": \"https://huggingface.co/yiyangd/InternVL3_5-1B-HF-mix_base_random_50pc_libero_text_s5000-libero_goal_s8000\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "OpenGVLab/MMPR-v1.2-prompts",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2-prompts",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/MMPR-v1.2-prompts\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2-prompts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.05579",
    "first_seen_date": "2025-04-11",
    "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.05579TAPNext: Tracking Any Point (TAP) as Next Token PredictionPublished on Apr 8\u00b7Submitted byNiels Roggeon Apr 11Upvote4Authors:Artem Zholus,Carl Doersch,Yi Yang,Skanda Koppula,Viorica Patraucean,Xu Owen He,Ignacio Rocco,Mehdi S. M. Sajjadi,Sarath Chandar,Ross GoroshinAbstractTAPNext addresses video tracking by framing it as sequential masked token decoding, achieving state-of-the-art performance while eliminating tracking-specific biases.AI-generated summaryTracking Any Point (TAP) in a video is a challenging computer vision problem\nwith many demonstrated applications in robotics, video editing, and 3D\nreconstruction. Existing methods for TAP rely heavily on complex\ntracking-specific inductive biases and heuristics, limiting their generality\nand potential for scaling. To address these challenges, we present TAPNext, a\nnew approach that casts TAP assequential masked token decoding. Our model is\ncausal, tracks in a purely online fashion, and removes tracking-specific\ninductive biases. This enables TAPNext to run with minimal latency, and removes\nthe temporal windowing required by many existing state of art trackers. Despite\nits simplicity, TAPNext achieves a new state-of-the-art tracking performance\namong both online and offline trackers. Finally, we present evidence that many\nwidely used tracking heuristics emerge naturally in TAPNext through end-to-end\ntraining.View arXiv pageView PDFProject pageGitHub1.76kAdd to collectionCommunitynielsrPaper submitterApr 11Code:https://github.com/google-deepmind/tapnetSee translationReplylibrarian-botApr 12This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APITracktention: Leveraging Point Tracking to Attend Videos Faster and Better(2025)Online Dense Point Tracking with Streaming Memory(2025)SPMTrack: Spatio-Temporal Parameter",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/google-deepmind/tapnet",
    "hf_paper_url": "https://huggingface.co/papers/2504.05579",
    "arxiv_url": "https://arxiv.org/abs/2504.05579",
    "num_models": 1,
    "models_list": "google/tapnet",
    "models_links": "https://huggingface.co/google/tapnet",
    "models_detailed": "[{\"name\": \"google/tapnet\", \"link\": \"https://huggingface.co/google/tapnet\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.04842",
    "first_seen_date": "2025-04-10",
    "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion Synthesis",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.04842FantasyTalking: Realistic Talking Portrait Generation via Coherent\n  Motion SynthesisPublished on Apr 7\u00b7Submitted bywangqiangon Apr 10Upvote35+27Authors:Mengchao Wang,Qiang Wang,Fan Jiang,Yaqi Fan,Yunpeng Zhang,Yonggang Qi,Kun Zhao,Mu XuAbstractA novel framework uses pretrained video diffusion transformer models to generate realistic, high-fidelity talking portraits with controlled motion dynamics, achieving better realism and identity preservation.AI-generated summaryCreating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrainedvideo diffusion transformermodel to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is adual-stage audio-visual alignmentstrategy. In the first stage,\nwe employ aclip-level trainingscheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using alip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with afacial-focused cross-attentionmodule that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion inten",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/Fantasy-AMAP/fantasy-talking",
    "hf_paper_url": "https://huggingface.co/papers/2504.04842",
    "arxiv_url": "https://arxiv.org/abs/2504.04842",
    "num_models": 2,
    "models_list": "acvlab/FantasyTalking, RedbeardNZ/FantasyTalking",
    "models_links": "https://huggingface.co/acvlab/FantasyTalking, https://huggingface.co/RedbeardNZ/FantasyTalking",
    "models_detailed": "[{\"name\": \"acvlab/FantasyTalking\", \"link\": \"https://huggingface.co/acvlab/FantasyTalking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 9\"}, {\"name\": \"RedbeardNZ/FantasyTalking\", \"link\": \"https://huggingface.co/RedbeardNZ/FantasyTalking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 7\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.06958",
    "first_seen_date": "2025-04-10",
    "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-Tuning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.06958VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement\n  Fine-TuningPublished on Apr 9\u00b7Submitted byAKon Apr 10Upvote13+5Authors:Xinhao Li,Ziang Yan,Desen Meng,Lu Dong,Xiangyu Zeng,Yinan He,Yali Wang,Yu Qiao,Yi Wang,Limin WangAbstractReinforcement Fine-Tuning with Group Relative Policy Optimization enhances video multimodal large language models, improving spatio-temporal perception without compromising chat abilities.AI-generated summaryRecent advancements in reinforcement learning have significantly advanced the\nreasoning capabilities of multimodal large language models (MLLMs). While\napproaches such asGroup Relative Policy Optimization(GRPO) and rule-based\nreward mechanisms demonstrate promise in text and image domains, their\napplication to video understanding remains limited. This paper presents a\nsystematic exploration ofReinforcement Fine-Tuning(RFT) with GRPO for video\nMLLMs, aiming to enhancespatio-temporal perceptionwhile maintaining general\ncapabilities. Our experiments reveal that RFT is highly data-efficient for\ntask-specific improvements. Through multi-task RFT on spatio-temporal\nperception objectives with limited samples, we develop VideoChat-R1, a powerful\nvideo MLLM that achieves state-of-the-art performance on spatio-temporal\nperception tasks without sacrificing chat ability, while exhibiting emerging\nspatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1\nboosts performance several-fold in tasks liketemporal grounding(+31.8) andobject tracking(+31.2). Additionally, it significantly improves on general QA\nbenchmarks such asVideoMME(+0.9),MVBench(+1.0), andPerception Test(+0.9).\nOur findings underscore the potential of RFT for specialized task enhancement\nof Video MLLMs. We hope our work offers valuable insights for future RL\nresearch in video MLLMs.View arXiv pageView PDFGitHub250autoAdd to collectionCommunityakhaliqPape",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/opengvlab/videochat-r1",
    "hf_paper_url": "https://huggingface.co/papers/2504.06958",
    "arxiv_url": "https://arxiv.org/abs/2504.06958",
    "num_models": 4,
    "models_list": "OpenGVLab/VideoChat-R1_5-7B, OpenGVLab/VideoChat-R1_7B, OpenGVLab/VideoChat-R1_7B_caption, OpenGVLab/VideoChat-R1-thinking_7B",
    "models_links": "https://huggingface.co/OpenGVLab/VideoChat-R1_5-7B, https://huggingface.co/OpenGVLab/VideoChat-R1_7B, https://huggingface.co/OpenGVLab/VideoChat-R1_7B_caption, https://huggingface.co/OpenGVLab/VideoChat-R1-thinking_7B",
    "models_detailed": "[{\"name\": \"OpenGVLab/VideoChat-R1_5-7B\", \"link\": \"https://huggingface.co/OpenGVLab/VideoChat-R1_5-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"OpenGVLab/VideoChat-R1_7B\", \"link\": \"https://huggingface.co/OpenGVLab/VideoChat-R1_7B\", \"task\": \"\", \"likes\": \"411\", \"downloads\": \"\", \"updated\": \"Apr 22\"}, {\"name\": \"OpenGVLab/VideoChat-R1_7B_caption\", \"link\": \"https://huggingface.co/OpenGVLab/VideoChat-R1_7B_caption\", \"task\": \"\", \"likes\": \"106\", \"downloads\": \"\", \"updated\": \"Apr 22\"}, {\"name\": \"OpenGVLab/VideoChat-R1-thinking_7B\", \"link\": \"https://huggingface.co/OpenGVLab/VideoChat-R1-thinking_7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 13\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.05535",
    "first_seen_date": "2025-04-09",
    "title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\n  Alignment with Human Values",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.05535COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for\n  Alignment with Human ValuesPublished on Apr 7\u00b7Submitted bySiweiWuon Apr 9Upvote44+36Authors:M-A-P Team,Siwei Wu,Jincheng Ren,Xinrun Du,Shuyue Guo,Xingwei Qu,Yiming Liang,Jie Liu,Yunwen Li,Tianyu Zheng,Boyu Feng,Huaqing Yuan,Zenith Wang,Jiaheng Liu,Wenhao Huang,Chenglin Cai,Haoran Que,Jian Yang,Yuelin Bai,Zekun Moore Wang,Zhouliang Yu,Qunshu Lin+10 authorsAbstractA new Chinese preference dataset and reward model are introduced to scale human preference alignment using LLMs, enhancing performance and cost-effectiveness.AI-generated summaryAligning large language models (LLMs) with human preferences has achieved\nremarkable success. However, existing Chinesepreference datasets are limited\nby small scale, narrow domain coverage, and lack of rigorous data validation.\nAdditionally, the reliance on human annotators for instruction and response\nlabeling significantly constrains the scalability of humanpreference datasets.\nTo address these challenges, we design an LLM-based Chinesepreference datasetannotation pipelinewith no human intervention. Specifically, we crawled and\ncarefully filtered 92k high-quality Chinese queries and employed 15 mainstreamLLMsto generate and score chosen-rejected response pairs. Based on it, we\nintroduceCOIG-P(Chinese Open Instruction Generalist - Preference), a\nhigh-quality, large-scale Chinesepreference dataset, comprises 1,009k Chinese\npreference pairs spanning 6 diversedomains: Chat, Code, Math, Logic, Novel,\nand Role. Building uponCOIG-P, to reduce the overhead of usingLLMsfor\nscoring, we trained a 8B-sized ChineseReward Model(CRM) and meticulously\nconstructed a ChineseReward Benchmark(CRBench). Evaluation results based onAlignBenchliu2024alignbenchbenchmarkingchinesealignment show that thatCOIG-Psignificantly outperforms other Chinesepreference datasets, and it\nbrings signific",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/multimodal-art-projection/COIG-P",
    "hf_paper_url": "https://huggingface.co/papers/2504.05535",
    "arxiv_url": "https://arxiv.org/abs/2504.05535",
    "num_models": 6,
    "models_list": "m-a-p/Qwen2.5-Instruct-7B-COIG-P, m-a-p/Qwen2-Instruct-7B-COIG-P, m-a-p/Infinity-Instruct-3M-0625-Llama3-8B-COIG-P, m-a-p/Infinity-Instruct-3M-0625-Mistral-7B-COIG-P, m-a-p/Infinity-Instruct-3M-0625-Qwen2-7B-COIG-P, m-a-p/CRM_llama3",
    "models_links": "https://huggingface.co/m-a-p/Qwen2.5-Instruct-7B-COIG-P, https://huggingface.co/m-a-p/Qwen2-Instruct-7B-COIG-P, https://huggingface.co/m-a-p/Infinity-Instruct-3M-0625-Llama3-8B-COIG-P, https://huggingface.co/m-a-p/Infinity-Instruct-3M-0625-Mistral-7B-COIG-P, https://huggingface.co/m-a-p/Infinity-Instruct-3M-0625-Qwen2-7B-COIG-P, https://huggingface.co/m-a-p/CRM_llama3",
    "models_detailed": "[{\"name\": \"m-a-p/Qwen2.5-Instruct-7B-COIG-P\", \"link\": \"https://huggingface.co/m-a-p/Qwen2.5-Instruct-7B-COIG-P\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"m-a-p/Qwen2-Instruct-7B-COIG-P\", \"link\": \"https://huggingface.co/m-a-p/Qwen2-Instruct-7B-COIG-P\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"m-a-p/Infinity-Instruct-3M-0625-Llama3-8B-COIG-P\", \"link\": \"https://huggingface.co/m-a-p/Infinity-Instruct-3M-0625-Llama3-8B-COIG-P\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"m-a-p/Infinity-Instruct-3M-0625-Mistral-7B-COIG-P\", \"link\": \"https://huggingface.co/m-a-p/Infinity-Instruct-3M-0625-Mistral-7B-COIG-P\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"m-a-p/Infinity-Instruct-3M-0625-Qwen2-7B-COIG-P\", \"link\": \"https://huggingface.co/m-a-p/Infinity-Instruct-3M-0625-Qwen2-7B-COIG-P\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"m-a-p/CRM_llama3\", \"link\": \"https://huggingface.co/m-a-p/CRM_llama3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}]",
    "num_datasets": 3,
    "datasets_list": "m-a-p/COIG-P, m-a-p/COIG-P-CRM, m-a-p/COIG-CRBench",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/COIG-P, https://huggingface.co/datasets/m-a-p/COIG-P-CRM, https://huggingface.co/datasets/m-a-p/COIG-CRBench",
    "datasets_detailed": "[{\"name\": \"m-a-p/COIG-P\", \"link\": \"https://huggingface.co/datasets/m-a-p/COIG-P\", \"task\": \"\", \"likes\": \"159\", \"downloads\": \"\", \"updated\": \"Apr 15\", \"size\": \"\"}, {\"name\": \"m-a-p/COIG-P-CRM\", \"link\": \"https://huggingface.co/datasets/m-a-p/COIG-P-CRM\", \"task\": \"\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Apr 9\", \"size\": \"\"}, {\"name\": \"m-a-p/COIG-CRBench\", \"link\": \"https://huggingface.co/datasets/m-a-p/COIG-CRBench\", \"task\": \"\", \"likes\": \"32\", \"downloads\": \"\", \"updated\": \"Apr 9\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.05299",
    "first_seen_date": "2025-04-08",
    "title": "SmolVLM: Redefining small and efficient multimodal models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.05299SmolVLM: Redefining small and efficient multimodal modelsPublished on Apr 7\u00b7Submitted byAndres Marafiotion Apr 8#1 Paper of the day\u00b7Hugging FaceUpvote202+194Authors:Andr\u00e9s Marafioti,Orr Zohar,Miquel Farr\u00e9,Merve Noyan,Elie Bakouch,Pedro Cuenca,Cyril Zakka,Loubna Ben Allal,Anton Lozhkov,Nouamane Tazi,Vaibhav Srivastav,Joshua Lochner,Hugo Larcher,Mathieu Morlon,Lewis Tunstall,Leandro von Werra,Thomas WolfAbstractSmolVLM, a series of compact multimodal models, achieves high performance with minimal GPU memory usage, making efficient deployment on mobile and edge devices possible.AI-generated summaryLarge Vision-Language Models(VLMs) deliver exceptional performance but\nrequire significant computational resources, limiting their deployment on\nmobile and edge devices. SmallerVLMstypically mirror design choices of larger\nmodels, such as extensiveimage tokenization, leading to inefficientGPU memoryusage and constrained practicality for on-device applications.\n  We introduceSmolVLM, a series of compactmultimodal modelsspecifically\nengineered for resource-efficientinference. We systematically explorearchitectural configurations,tokenization strategies, anddata curationoptimized for low computational overhead. Through this, we identify key design\nchoices that yield substantial performance gains on image and video tasks with\nminimal memory footprints.\n  Our smallest model,SmolVLM-256M, uses less than 1GBGPU memoryduringinferenceand outperforms the 300-times largerIdefics-80Bmodel, despite an\n18-month development gap. Our largest model, at 2.2B parameters, rivals\nstate-of-the-artVLMsconsuming twice theGPU memory.SmolVLMmodels extend\nbeyond static images, demonstrating robustvideo comprehensioncapabilities.\n  Our results emphasize that strategic architectural optimizations, aggressive\nyet efficient tokenization, and carefully curated training data significantly\nenhance multimodal perfo",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/huggingface/smollm",
    "hf_paper_url": "https://huggingface.co/papers/2504.05299",
    "arxiv_url": "https://arxiv.org/abs/2504.05299",
    "num_models": 28,
    "models_list": "HuggingFaceTB/SmolVLM-Instruct, HuggingFaceTB/SmolVLM-256M-Instruct, HuggingFaceTB/SmolVLM2-2.2B-Instruct, HuggingFaceTB/SmolVLM-500M-Instruct, HuggingFaceTB/SmolVLM2-500M-Video-Instruct, HuggingFaceTB/SmolVLM2-256M-Video-Instruct, HuggingFaceTB/SmolVLM2-2.2B-Base, yushan777/SmolVLM-Instruct, yushan777/SmolVLM-500M-Instruct, yushan777/SmolVLM-256M-Instruct, Mungert/SmolVLM-500M-Instruct-GGUF, Mungert/SmolVLM-Instruct-GGUF, Mungert/SmolVLM-256M-Instruct-GGUF, Prince-1/SmolVLM-Instruct-GGUF-RKllm, configint/SmolVLM2-500M-Video-Instruct-Action, configint/SmolVLM2-500M-Video-Instruct-ActionTokens, swarecito/smol-256, configint/SmolVLM2-256M-Video-Instruct-Action, configint/SmolVLM2-256M-Video-Instruct-ActionTokens, dev-bjoern/smolvlm-int4-ov, configint/SmolVLM2-256M-Video-Instruct-ActionTokens-Dim, witnesschain/SmolVLM2-2.2B-Instruct, anuragpradhan/SmolVLM2-500M-Video-Instruct-bnb-4bit, anuragpradhan/SmolVLM2-2.2B-Instruct-bnb-4bit, Amirhossein75/VLM-Image-Captioning, cedpsam/SmolVLM2-2.2B-Instruct-ao-autoquant, Prince-1/SmolVLM-Instruct-RKllm, genevera/SmolVLM-Instruct-heretic",
    "models_links": "https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct, https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct, https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Instruct, https://huggingface.co/HuggingFaceTB/SmolVLM-500M-Instruct, https://huggingface.co/HuggingFaceTB/SmolVLM2-500M-Video-Instruct, https://huggingface.co/HuggingFaceTB/SmolVLM2-256M-Video-Instruct, https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Base, https://huggingface.co/yushan777/SmolVLM-Instruct, https://huggingface.co/yushan777/SmolVLM-500M-Instruct, https://huggingface.co/yushan777/SmolVLM-256M-Instruct, https://huggingface.co/Mungert/SmolVLM-500M-Instruct-GGUF, https://huggingface.co/Mungert/SmolVLM-Instruct-GGUF, https://huggingface.co/Mungert/SmolVLM-256M-Instruct-GGUF, https://huggingface.co/Prince-1/SmolVLM-Instruct-GGUF-RKllm, https://huggingface.co/configint/SmolVLM2-500M-Video-Instruct-Action, https://huggingface.co/configint/SmolVLM2-500M-Video-Instruct-ActionTokens, https://huggingface.co/swarecito/smol-256, https://huggingface.co/configint/SmolVLM2-256M-Video-Instruct-Action, https://huggingface.co/configint/SmolVLM2-256M-Video-Instruct-ActionTokens, https://huggingface.co/dev-bjoern/smolvlm-int4-ov, https://huggingface.co/configint/SmolVLM2-256M-Video-Instruct-ActionTokens-Dim, https://huggingface.co/witnesschain/SmolVLM2-2.2B-Instruct, https://huggingface.co/anuragpradhan/SmolVLM2-500M-Video-Instruct-bnb-4bit, https://huggingface.co/anuragpradhan/SmolVLM2-2.2B-Instruct-bnb-4bit, https://huggingface.co/Amirhossein75/VLM-Image-Captioning, https://huggingface.co/cedpsam/SmolVLM2-2.2B-Instruct-ao-autoquant, https://huggingface.co/Prince-1/SmolVLM-Instruct-RKllm, https://huggingface.co/genevera/SmolVLM-Instruct-heretic",
    "models_detailed": "[{\"name\": \"HuggingFaceTB/SmolVLM-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"HuggingFaceTB/SmolVLM-256M-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"HuggingFaceTB/SmolVLM-500M-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolVLM-500M-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"HuggingFaceTB/SmolVLM2-500M-Video-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolVLM2-500M-Video-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolVLM2-256M-Video-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"HuggingFaceTB/SmolVLM2-2.2B-Base\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolVLM2-2.2B-Base\", \"task\": \"\", \"likes\": \"117\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"yushan777/SmolVLM-Instruct\", \"link\": \"https://huggingface.co/yushan777/SmolVLM-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"yushan777/SmolVLM-500M-Instruct\", \"link\": \"https://huggingface.co/yushan777/SmolVLM-500M-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"yushan777/SmolVLM-256M-Instruct\", \"link\": \"https://huggingface.co/yushan777/SmolVLM-256M-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"Mungert/SmolVLM-500M-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/SmolVLM-500M-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/SmolVLM-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/SmolVLM-Instruct-GGUF\", \"task\": \"\", \"likes\": \"301\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/SmolVLM-256M-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/SmolVLM-256M-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Prince-1/SmolVLM-Instruct-GGUF-RKllm\", \"link\": \"https://huggingface.co/Prince-1/SmolVLM-Instruct-GGUF-RKllm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"configint/SmolVLM2-500M-Video-Instruct-Action\", \"link\": \"https://huggingface.co/configint/SmolVLM2-500M-Video-Instruct-Action\", \"task\": \"\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"configint/SmolVLM2-500M-Video-Instruct-ActionTokens\", \"link\": \"https://huggingface.co/configint/SmolVLM2-500M-Video-Instruct-ActionTokens\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"swarecito/smol-256\", \"link\": \"https://huggingface.co/swarecito/smol-256\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"configint/SmolVLM2-256M-Video-Instruct-Action\", \"link\": \"https://huggingface.co/configint/SmolVLM2-256M-Video-Instruct-Action\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"configint/SmolVLM2-256M-Video-Instruct-ActionTokens\", \"link\": \"https://huggingface.co/configint/SmolVLM2-256M-Video-Instruct-ActionTokens\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"dev-bjoern/smolvlm-int4-ov\", \"link\": \"https://huggingface.co/dev-bjoern/smolvlm-int4-ov\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"configint/SmolVLM2-256M-Video-Instruct-ActionTokens-Dim\", \"link\": \"https://huggingface.co/configint/SmolVLM2-256M-Video-Instruct-ActionTokens-Dim\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"witnesschain/SmolVLM2-2.2B-Instruct\", \"link\": \"https://huggingface.co/witnesschain/SmolVLM2-2.2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"anuragpradhan/SmolVLM2-500M-Video-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/anuragpradhan/SmolVLM2-500M-Video-Instruct-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"anuragpradhan/SmolVLM2-2.2B-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/anuragpradhan/SmolVLM2-2.2B-Instruct-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 2\"}, {\"name\": \"Amirhossein75/VLM-Image-Captioning\", \"link\": \"https://huggingface.co/Amirhossein75/VLM-Image-Captioning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 1\"}, {\"name\": \"cedpsam/SmolVLM2-2.2B-Instruct-ao-autoquant\", \"link\": \"https://huggingface.co/cedpsam/SmolVLM2-2.2B-Instruct-ao-autoquant\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"Prince-1/SmolVLM-Instruct-RKllm\", \"link\": \"https://huggingface.co/Prince-1/SmolVLM-Instruct-RKllm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"genevera/SmolVLM-Instruct-heretic\", \"link\": \"https://huggingface.co/genevera/SmolVLM-Instruct-heretic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.02605",
    "first_seen_date": "2025-04-07",
    "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.02605Multi-SWE-bench: A Multilingual Benchmark for Issue ResolvingPublished on Apr 3\u00b7Submitted byDaoguang Zanon Apr 7#1 Paper of the day\u00b7ByteDance SeedUpvote48+40Authors:Daoguang Zan,Zhirong Huang,Wei Liu,Hanwu Chen,Linhao Zhang,Shulin Xin,Lu Chen,Qi Liu,Xiaojian Zhong,Aoyan Li,Siyao Liu,Yongsheng Xiao,Liangqiang Chen,Yuyu Zhang,Jing Su,Tianyu Liu,Rui Long,Kai Shen,Liang XiangAbstractA multilingual benchmark, Multi-SWE-bench, is introduced to evaluate Large Language Models across various programming languages and is complemented by a RL dataset, Multi-SWE-RL, to advance issue-resolving tasks.AI-generated summaryThe task of issue resolving is to modify acodebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce amultilingual issue-resolving benchmark,calledMulti-SWE-bench,coveringJava,TypeScript,JavaScript,Go,Rust,C, andC++. It includes a\ntotal of 1,632high-quality instances, which werecarefully annotated from\n2,456candidates by 68expert annotators, ensuring that the benchmarkcan\nprovide an accurate and reliable evaluation. Based onMulti-SWE-bench, we\nevaluate a series ofstate-of-the-art modelsusing three representative methods\n(Agentless,SWE-agent, andOpenHands) and present acomprehensive analysis with\nkey empirical insights. In addition, we launch aMulti-SWE-RLopen-sourcecommunity, aimed at building large-scalereinforcement learning (RL)training\ndatasets for issue-resolving tasks. As an initialcontribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entiredata production pipeline, along with detailed tutorials,\nencourag",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/multi-swe-bench/multi-swe-bench",
    "hf_paper_url": "https://huggingface.co/papers/2504.02605",
    "arxiv_url": "https://arxiv.org/abs/2504.02605",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 6,
    "datasets_list": "ByteDance-Seed/Multi-SWE-bench_trajs, ByteDance-Seed/Multi-SWE-RL, ByteDance-Seed/Multi-SWE-bench, ByteDance-Seed/Multi-SWE-bench_mini, ByteDance-Seed/Multi-SWE-bench-flash, mteb/MultiSWEbenchRR",
    "datasets_links": "https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench_trajs, https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-RL, https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench, https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench_mini, https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench-flash, https://huggingface.co/datasets/mteb/MultiSWEbenchRR",
    "datasets_detailed": "[{\"name\": \"ByteDance-Seed/Multi-SWE-bench_trajs\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench_trajs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}, {\"name\": \"ByteDance-Seed/Multi-SWE-RL\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-RL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\", \"size\": \"\"}, {\"name\": \"ByteDance-Seed/Multi-SWE-bench\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\", \"size\": \"\"}, {\"name\": \"ByteDance-Seed/Multi-SWE-bench_mini\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench_mini\", \"task\": \"\", \"likes\": \"102\", \"downloads\": \"\", \"updated\": \"Jun 29\", \"size\": \"\"}, {\"name\": \"ByteDance-Seed/Multi-SWE-bench-flash\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/Multi-SWE-bench-flash\", \"task\": \"\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}, {\"name\": \"mteb/MultiSWEbenchRR\", \"link\": \"https://huggingface.co/datasets/mteb/MultiSWEbenchRR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.03601",
    "first_seen_date": "2025-04-07",
    "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.03601APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human InterplayPublished on Apr 4\u00b7Submitted byAKon Apr 7Upvote17+9Authors:Akshara Prabhakar,Zuxin Liu,Weiran Yao,Jianguo Zhang,Ming Zhu,Shiyu Wang,Zhiwei Liu,Tulika Awalgaonkar,Haolin Chen,Thai Hoang,Juan Carlos Niebles,Shelby Heinecke,Huan Wang,Silvio Savarese,Caiming XiongAbstractThe APIGen-MT framework generates high-quality multi-turn agent data using detailed task blueprints and simulated human-agent interactions, leading to superior model performance and consistency.AI-generated summaryTraining effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduceAPIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, ouragentic pipelineproduces detailed task\nblueprints withground-truth actions, leveraging a committee ofLLM reviewersanditerative feedback loops. These blueprints are then transformed into\ncompleteinteraction trajectoriesthrough simulated human-agent interplay. We\ntrain a family of models -- thexLAM-2-fc-rseries with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on tau-bench andBFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly inmulti-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both thesynthetic datacollected\nand the trainedxLAM-2-fc-rmodels to advance research in AI agents. Models are\navailable onHuggingFaceat\nhttps://huggingface.co/collections/Salesfo",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.03601",
    "arxiv_url": "https://arxiv.org/abs/2504.03601",
    "num_models": 15,
    "models_list": "Salesforce/Llama-xLAM-2-8b-fc-r, Salesforce/Llama-xLAM-2-70b-fc-r, Salesforce/xLAM-2-32b-fc-r, Salesforce/xLAM-2-3b-fc-r, Salesforce/xLAM-2-1b-fc-r, Salesforce/xLAM-2-3b-fc-r-gguf, Salesforce/xLAM-2-1b-fc-r-gguf, Salesforce/Llama-xLAM-2-8b-fc-r-gguf, amd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid, kmhalvin/xLAM-2-1b-fc-r-SpinQuant-ET, licongwei/xLAM-2-3b-fc-r-SpinQuant-ET, Mungert/xLAM-2-3b-fc-r-GGUF, Mungert/Llama-xLAM-2-8b-fc-r-GGUF, Mungert/xLAM-2-32b-fc-r-GGUF, sunkencity/Llama-xLAM-2-8b-fc-r-blasphemer",
    "models_links": "https://huggingface.co/Salesforce/Llama-xLAM-2-8b-fc-r, https://huggingface.co/Salesforce/Llama-xLAM-2-70b-fc-r, https://huggingface.co/Salesforce/xLAM-2-32b-fc-r, https://huggingface.co/Salesforce/xLAM-2-3b-fc-r, https://huggingface.co/Salesforce/xLAM-2-1b-fc-r, https://huggingface.co/Salesforce/xLAM-2-3b-fc-r-gguf, https://huggingface.co/Salesforce/xLAM-2-1b-fc-r-gguf, https://huggingface.co/Salesforce/Llama-xLAM-2-8b-fc-r-gguf, https://huggingface.co/amd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid, https://huggingface.co/kmhalvin/xLAM-2-1b-fc-r-SpinQuant-ET, https://huggingface.co/licongwei/xLAM-2-3b-fc-r-SpinQuant-ET, https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF, https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF, https://huggingface.co/Mungert/xLAM-2-32b-fc-r-GGUF, https://huggingface.co/sunkencity/Llama-xLAM-2-8b-fc-r-blasphemer",
    "models_detailed": "[{\"name\": \"Salesforce/Llama-xLAM-2-8b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/Llama-xLAM-2-8b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/Llama-xLAM-2-70b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/Llama-xLAM-2-70b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"444\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-32b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-32b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-3b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-3b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-1b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-1b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-3b-fc-r-gguf\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-3b-fc-r-gguf\", \"task\": \"Text Generation\", \"likes\": \"584\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-1b-fc-r-gguf\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-1b-fc-r-gguf\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/Llama-xLAM-2-8b-fc-r-gguf\", \"link\": \"https://huggingface.co/Salesforce/Llama-xLAM-2-8b-fc-r-gguf\", \"task\": \"Text Generation\", \"likes\": \"744\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"amd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid\", \"link\": \"https://huggingface.co/amd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"kmhalvin/xLAM-2-1b-fc-r-SpinQuant-ET\", \"link\": \"https://huggingface.co/kmhalvin/xLAM-2-1b-fc-r-SpinQuant-ET\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 27\"}, {\"name\": \"licongwei/xLAM-2-3b-fc-r-SpinQuant-ET\", \"link\": \"https://huggingface.co/licongwei/xLAM-2-3b-fc-r-SpinQuant-ET\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"Mungert/xLAM-2-3b-fc-r-GGUF\", \"link\": \"https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF\", \"task\": \"Text Generation\", \"likes\": \"552\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/Llama-xLAM-2-8b-fc-r-GGUF\", \"link\": \"https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF\", \"task\": \"Text Generation\", \"likes\": \"318\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/xLAM-2-32b-fc-r-GGUF\", \"link\": \"https://huggingface.co/Mungert/xLAM-2-32b-fc-r-GGUF\", \"task\": \"Text Generation\", \"likes\": \"590\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"sunkencity/Llama-xLAM-2-8b-fc-r-blasphemer\", \"link\": \"https://huggingface.co/sunkencity/Llama-xLAM-2-8b-fc-r-blasphemer\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\"}]",
    "num_datasets": 1,
    "datasets_list": "Salesforce/APIGen-MT-5k",
    "datasets_links": "https://huggingface.co/datasets/Salesforce/APIGen-MT-5k",
    "datasets_detailed": "[{\"name\": \"Salesforce/APIGen-MT-5k\", \"link\": \"https://huggingface.co/datasets/Salesforce/APIGen-MT-5k\", \"task\": \"\", \"likes\": \"748\", \"downloads\": \"\", \"updated\": \"Oct 10\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.01943",
    "first_seen_date": "2025-04-04",
    "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.01943OpenCodeReasoning: Advancing Data Distillation for Competitive CodingPublished on Apr 2\u00b7Submitted bySomshubra Majumdaron Apr 4Upvote15+7Authors:Wasi Uddin Ahmad,Sean Narenthiran,Somshubra Majumdar,Aleksander Ficek,Siddhartha Jain,Jocelyn Huang,Vahid Noroozi,Boris GinsburgAbstractA superior SFT dataset improves coding capabilities in distilled reasoning models surpassing reinforcement learning alternatives.AI-generated summarySince the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superiorsupervised fine-tuning(SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% onLiveCodeBenchand 24.6% onCodeContests, surpassing alternatives trained withreinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact ofcode execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritizeinstruction diversityover solution\ncorrectness. Finally, we also analyze thetoken efficiencyand reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community.View arXiv pageView PDFAdd to collectionCommunitysmajumdar94Paper submitterApr 4Dataset -https://huggingface.co/datasets/nvidia/OpenCodeReasoningSee translationReplylibrarian-botApr 5This is an automated message from theLibrarian Bot. I ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2504.01943",
    "arxiv_url": "https://arxiv.org/abs/2504.01943",
    "num_models": 51,
    "models_list": "nvidia/OpenReasoning-Nemotron-32B, nvidia/OpenCodeReasoning-Nemotron-32B, nvidia/OpenReasoning-Nemotron-1.5B, nvidia/OpenReasoning-Nemotron-7B, nvidia/OpenCodeReasoning-Nemotron-7B, nvidia/OpenCodeReasoning-Nemotron-14B, nvidia/OpenCodeReasoning-Nemotron-32B-IOI, lmstudio-community/OpenCodeReasoning-Nemotron-7B-GGUF, lmstudio-community/OpenCodeReasoning-Nemotron-14B-GGUF, lmstudio-community/OpenCodeReasoning-Nemotron-32B-GGUF, lmstudio-community/OpenCodeReasoning-Nemotron-32B-IOI-GGUF, stelterlab/OpenCodeReasoning-Nemotron-14B-AWQ, stelterlab/OpenCodeReasoning-Nemotron-32B-AWQ, stelterlab/OpenCodeReasoning-Nemotron-32B-IOI-AWQ, lucyknada/nvidia_OpenCodeReasoning-Nemotron-14B-exl2, lucyknada/nvidia_OpenCodeReasoning-Nemotron-7B-exl2, Mungert/OpenCodeReasoning-Nemotron-32B-GGUF, Mungert/OpenCodeReasoning-Nemotron-7B-GGUF, Mungert/OpenCodeReasoning-Nemotron-14B-GGUF, Mungert/OpenCodeReasoning-Nemotron-32B-IOI-GGUF, nvidia/OpenCodeReasoning-Nemotron-1.1-14B, nvidia/OpenCodeReasoning-Nemotron-1.1-32B, nvidia/OpenCodeReasoning-Nemotron-1.1-7B, Epistates/OpenCodeReasoning-Nemotron-1.1-32B-AWQ, gabriellarson/OpenCodeReasoning-Nemotron-1.1-32B-GGUF, gabriellarson/OpenCodeReasoning-Nemotron-1.1-14B-GGUF, gabriellarson/OpenCodeReasoning-Nemotron-1.1-7B-GGUF, quantized4all/OpenCodeReasoning-Nemotron-1.1-14B-GGUF, quantized4all/OpenCodeReasoning-Nemotron-1.1-32B-GGUF, quantized4all/OpenCodeReasoning-Nemotron-1.1-7B-GGUF, nvidia/OpenReasoning-Nemotron-14B, gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF, gabriellarson/OpenReasoning-Nemotron-7B-GGUF, gabriellarson/OpenReasoning-Nemotron-14B-GGUF, gabriellarson/OpenReasoning-Nemotron-32B-GGUF, unsloth/OpenReasoning-Nemotron-32B, unsloth/OpenReasoning-Nemotron-32B-GGUF, Mungert/OpenReasoning-Nemotron-32B-GGUF, codys12/OpenReasoning-Nemotron-32B, Mungert/OpenReasoning-Nemotron-7B-GGUF, Mungert/OpenReasoning-Nemotron-1.5B-GGUF, jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8, Mungert/OpenReasoning-Nemotron-14B-GGUF, groxaxo/OpenCodeReasoning-Nemotron-1.1-32B-GPTQ-W8A16, Prince-1/OpenReasoning-Nemotron-7B, onnx-community/OpenReasoning-Nemotron-7B, onnx-community/OpenReasoning-Nemotron-1.5B, Prince-1/OpenReasoning-Nemotron-1.5B, pamanseau/OpenReasoning-Nemotron-32B, Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit, QuantFactory/OpenReasoning-Nemotron-7B-GGUF",
    "models_links": "https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B, https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B, https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B, https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B, https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-7B, https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-14B, https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B-IOI, https://huggingface.co/lmstudio-community/OpenCodeReasoning-Nemotron-7B-GGUF, https://huggingface.co/lmstudio-community/OpenCodeReasoning-Nemotron-14B-GGUF, https://huggingface.co/lmstudio-community/OpenCodeReasoning-Nemotron-32B-GGUF, https://huggingface.co/lmstudio-community/OpenCodeReasoning-Nemotron-32B-IOI-GGUF, https://huggingface.co/stelterlab/OpenCodeReasoning-Nemotron-14B-AWQ, https://huggingface.co/stelterlab/OpenCodeReasoning-Nemotron-32B-AWQ, https://huggingface.co/stelterlab/OpenCodeReasoning-Nemotron-32B-IOI-AWQ, https://huggingface.co/lucyknada/nvidia_OpenCodeReasoning-Nemotron-14B-exl2, https://huggingface.co/lucyknada/nvidia_OpenCodeReasoning-Nemotron-7B-exl2, https://huggingface.co/Mungert/OpenCodeReasoning-Nemotron-32B-GGUF, https://huggingface.co/Mungert/OpenCodeReasoning-Nemotron-7B-GGUF, https://huggingface.co/Mungert/OpenCodeReasoning-Nemotron-14B-GGUF, https://huggingface.co/Mungert/OpenCodeReasoning-Nemotron-32B-IOI-GGUF, https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B, https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B, https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B, https://huggingface.co/Epistates/OpenCodeReasoning-Nemotron-1.1-32B-AWQ, https://huggingface.co/gabriellarson/OpenCodeReasoning-Nemotron-1.1-32B-GGUF, https://huggingface.co/gabriellarson/OpenCodeReasoning-Nemotron-1.1-14B-GGUF, https://huggingface.co/gabriellarson/OpenCodeReasoning-Nemotron-1.1-7B-GGUF, https://huggingface.co/quantized4all/OpenCodeReasoning-Nemotron-1.1-14B-GGUF, https://huggingface.co/quantized4all/OpenCodeReasoning-Nemotron-1.1-32B-GGUF, https://huggingface.co/quantized4all/OpenCodeReasoning-Nemotron-1.1-7B-GGUF, https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-7B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-14B-GGUF, https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B, https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/Mungert/OpenReasoning-Nemotron-32B-GGUF, https://huggingface.co/codys12/OpenReasoning-Nemotron-32B, https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF, https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF, https://huggingface.co/jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8, https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF, https://huggingface.co/groxaxo/OpenCodeReasoning-Nemotron-1.1-32B-GPTQ-W8A16, https://huggingface.co/Prince-1/OpenReasoning-Nemotron-7B, https://huggingface.co/onnx-community/OpenReasoning-Nemotron-7B, https://huggingface.co/onnx-community/OpenReasoning-Nemotron-1.5B, https://huggingface.co/Prince-1/OpenReasoning-Nemotron-1.5B, https://huggingface.co/pamanseau/OpenReasoning-Nemotron-32B, https://huggingface.co/Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit, https://huggingface.co/QuantFactory/OpenReasoning-Nemotron-7B-GGUF",
    "models_detailed": "[{\"name\": \"nvidia/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenCodeReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"96\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"449\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"408\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"nvidia/OpenCodeReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"187\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/OpenCodeReasoning-Nemotron-14B\", \"link\": \"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/OpenCodeReasoning-Nemotron-32B-IOI\", \"link\": \"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-32B-IOI\", \"task\": \"Text Generation\", \"likes\": \"78\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"lmstudio-community/OpenCodeReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/OpenCodeReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"32\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"lmstudio-community/OpenCodeReasoning-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/OpenCodeReasoning-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"lmstudio-community/OpenCodeReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/OpenCodeReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"35\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"lmstudio-community/OpenCodeReasoning-Nemotron-32B-IOI-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/OpenCodeReasoning-Nemotron-32B-IOI-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"stelterlab/OpenCodeReasoning-Nemotron-14B-AWQ\", \"link\": \"https://huggingface.co/stelterlab/OpenCodeReasoning-Nemotron-14B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"stelterlab/OpenCodeReasoning-Nemotron-32B-AWQ\", \"link\": \"https://huggingface.co/stelterlab/OpenCodeReasoning-Nemotron-32B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"stelterlab/OpenCodeReasoning-Nemotron-32B-IOI-AWQ\", \"link\": \"https://huggingface.co/stelterlab/OpenCodeReasoning-Nemotron-32B-IOI-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"lucyknada/nvidia_OpenCodeReasoning-Nemotron-14B-exl2\", \"link\": \"https://huggingface.co/lucyknada/nvidia_OpenCodeReasoning-Nemotron-14B-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 9\"}, {\"name\": \"lucyknada/nvidia_OpenCodeReasoning-Nemotron-7B-exl2\", \"link\": \"https://huggingface.co/lucyknada/nvidia_OpenCodeReasoning-Nemotron-7B-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 9\"}, {\"name\": \"Mungert/OpenCodeReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenCodeReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"466\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/OpenCodeReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenCodeReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"275\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/OpenCodeReasoning-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenCodeReasoning-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"403\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/OpenCodeReasoning-Nemotron-32B-IOI-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenCodeReasoning-Nemotron-32B-IOI-GGUF\", \"task\": \"Text Generation\", \"likes\": \"390\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"nvidia/OpenCodeReasoning-Nemotron-1.1-14B\", \"link\": \"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B\", \"task\": \"Text Generation\", \"likes\": \"170\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"nvidia/OpenCodeReasoning-Nemotron-1.1-32B\", \"link\": \"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B\", \"task\": \"Text Generation\", \"likes\": \"97\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"nvidia/OpenCodeReasoning-Nemotron-1.1-7B\", \"link\": \"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B\", \"task\": \"Text Generation\", \"likes\": \"314\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"Epistates/OpenCodeReasoning-Nemotron-1.1-32B-AWQ\", \"link\": \"https://huggingface.co/Epistates/OpenCodeReasoning-Nemotron-1.1-32B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"44\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"gabriellarson/OpenCodeReasoning-Nemotron-1.1-32B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenCodeReasoning-Nemotron-1.1-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"gabriellarson/OpenCodeReasoning-Nemotron-1.1-14B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenCodeReasoning-Nemotron-1.1-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"176\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"gabriellarson/OpenCodeReasoning-Nemotron-1.1-7B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenCodeReasoning-Nemotron-1.1-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 8\"}, {\"name\": \"quantized4all/OpenCodeReasoning-Nemotron-1.1-14B-GGUF\", \"link\": \"https://huggingface.co/quantized4all/OpenCodeReasoning-Nemotron-1.1-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"quantized4all/OpenCodeReasoning-Nemotron-1.1-32B-GGUF\", \"link\": \"https://huggingface.co/quantized4all/OpenCodeReasoning-Nemotron-1.1-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"quantized4all/OpenCodeReasoning-Nemotron-1.1-7B-GGUF\", \"link\": \"https://huggingface.co/quantized4all/OpenCodeReasoning-Nemotron-1.1-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"154\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"nvidia/OpenReasoning-Nemotron-14B\", \"link\": \"https://huggingface.co/nvidia/OpenReasoning-Nemotron-14B\", \"task\": \"Text Generation\", \"likes\": \"303\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"182\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"gabriellarson/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"145\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"unsloth/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"unsloth/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/unsloth/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 21\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"610\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"codys12/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/codys12/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"273\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"187\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8\", \"link\": \"https://huggingface.co/jncraton/OpenReasoning-Nemotron-1.5B-ct2-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"Mungert/OpenReasoning-Nemotron-14B-GGUF\", \"link\": \"https://huggingface.co/Mungert/OpenReasoning-Nemotron-14B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"groxaxo/OpenCodeReasoning-Nemotron-1.1-32B-GPTQ-W8A16\", \"link\": \"https://huggingface.co/groxaxo/OpenCodeReasoning-Nemotron-1.1-32B-GPTQ-W8A16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"Prince-1/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/Prince-1/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"onnx-community/OpenReasoning-Nemotron-7B\", \"link\": \"https://huggingface.co/onnx-community/OpenReasoning-Nemotron-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"onnx-community/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/onnx-community/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"Prince-1/OpenReasoning-Nemotron-1.5B\", \"link\": \"https://huggingface.co/Prince-1/OpenReasoning-Nemotron-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"pamanseau/OpenReasoning-Nemotron-32B\", \"link\": \"https://huggingface.co/pamanseau/OpenReasoning-Nemotron-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit\", \"link\": \"https://huggingface.co/Frane92O/OpenReasoning-Nemotron-7B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 20\"}, {\"name\": \"QuantFactory/OpenReasoning-Nemotron-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/OpenReasoning-Nemotron-7B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"135\", \"downloads\": \"\", \"updated\": \"Sep 1\"}]",
    "num_datasets": 9,
    "datasets_list": "nvidia/OpenCodeReasoning, nvidia/Nemotron-Competitive-Programming-v1, sealad886/OpenCodeReasoning_messages, MaziyarPanahi/OpenCodeReasoning_ShareGPT, Compumacy/NMT-opencode, Dannalily/MontageLie, NewstaR/CoTton-64k-6725-Collective, NewstaR/CoTton-67k-6725-Collective, edgerunner-ai/nvidia__Nemotron-Competitive-Programming-v1",
    "datasets_links": "https://huggingface.co/datasets/nvidia/OpenCodeReasoning, https://huggingface.co/datasets/nvidia/Nemotron-Competitive-Programming-v1, https://huggingface.co/datasets/sealad886/OpenCodeReasoning_messages, https://huggingface.co/datasets/MaziyarPanahi/OpenCodeReasoning_ShareGPT, https://huggingface.co/datasets/Compumacy/NMT-opencode, https://huggingface.co/datasets/Dannalily/MontageLie, https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective, https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective, https://huggingface.co/datasets/edgerunner-ai/nvidia__Nemotron-Competitive-Programming-v1",
    "datasets_detailed": "[{\"name\": \"nvidia/OpenCodeReasoning\", \"link\": \"https://huggingface.co/datasets/nvidia/OpenCodeReasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"nvidia/Nemotron-Competitive-Programming-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Nemotron-Competitive-Programming-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\", \"size\": \"\"}, {\"name\": \"sealad886/OpenCodeReasoning_messages\", \"link\": \"https://huggingface.co/datasets/sealad886/OpenCodeReasoning_messages\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\", \"size\": \"\"}, {\"name\": \"MaziyarPanahi/OpenCodeReasoning_ShareGPT\", \"link\": \"https://huggingface.co/datasets/MaziyarPanahi/OpenCodeReasoning_ShareGPT\", \"task\": \"\", \"likes\": \"588\", \"downloads\": \"\", \"updated\": \"Apr 7\", \"size\": \"\"}, {\"name\": \"Compumacy/NMT-opencode\", \"link\": \"https://huggingface.co/datasets/Compumacy/NMT-opencode\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 11\", \"size\": \"\"}, {\"name\": \"Dannalily/MontageLie\", \"link\": \"https://huggingface.co/datasets/Dannalily/MontageLie\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-64k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Jun 8\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-67k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Jun 7\", \"size\": \"\"}, {\"name\": \"edgerunner-ai/nvidia__Nemotron-Competitive-Programming-v1\", \"link\": \"https://huggingface.co/datasets/edgerunner-ai/nvidia__Nemotron-Competitive-Programming-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"2 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2504.00824",
    "first_seen_date": "2025-04-03",
    "title": "ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.00824ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate CitationsPublished on Apr 1\u00b7Submitted byWenhu Chenon Apr 3Upvote43+35Authors:Yubo Wang,Xueguang Ma,Ping Nie,Huaye Zeng,Zhiheng Lyu,Yuxuan Zhang,Benjamin Schneider,Yi Lu,Xiang Yue,Wenhu ChenAbstractScholarCopilot is a unified framework that enhances large language models by dynamically retrieving and integrating scholarly citations, improving accuracy and effectiveness in academic text generation.AI-generated summaryAcademic writingrequires both coherent text generation and precise citation\nof relevant literature. Although recentRetrieval-Augmented Generation (RAG)systems have significantly improved factual accuracy in general-purpose text\ngeneration, their capacity to adequately support professionalacademic writingremains limited. In this work, we introduceScholarCopilot, a unified framework\ndesigned to enhance existinglarge language modelsfor generating professional\nacademic articles with accurate and contextually relevant citations.ScholarCopilotdynamically determines when to retrieve scholarly references by\ngenerating aretrieval token[RET], and then utilizes its representation to\nlook up relevant citations from a database. The retrieved references are fed\ninto the model to augment the generation process. We jointly optimize both the\ngeneration and citation tasks within a single framework to increase efficiency.\nTrained on 500K papers from arXiv, our model achieves a top-1 retrieval\naccuracy of 40.1% on our evaluation dataset, outperforming baselines such as\nE5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic\nwriting samples,ScholarCopilotscores 16.2/25 in generation quality (measured\nacross relevance, coherence, academic rigor, completeness, and innovation),\nsurpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct\n(15.8/25). Human studies also",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/TIGER-AI-Lab/ScholarCopilot",
    "hf_paper_url": "https://huggingface.co/papers/2504.00824",
    "arxiv_url": "https://arxiv.org/abs/2504.00824",
    "num_models": 1,
    "models_list": "TIGER-Lab/ScholarCopilot-v1",
    "models_links": "https://huggingface.co/TIGER-Lab/ScholarCopilot-v1",
    "models_detailed": "[{\"name\": \"TIGER-Lab/ScholarCopilot-v1\", \"link\": \"https://huggingface.co/TIGER-Lab/ScholarCopilot-v1\", \"task\": \"\", \"likes\": \"27\", \"downloads\": \"\", \"updated\": \"Apr 3\"}]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/ScholarCopilot-Data-v1",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/ScholarCopilot-Data-v1",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/ScholarCopilot-Data-v1\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/ScholarCopilot-Data-v1\", \"task\": \"\", \"likes\": \"135\", \"downloads\": \"\", \"updated\": \"Apr 3\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.20783",
    "first_seen_date": "2025-04-03",
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.20783Understanding R1-Zero-Like Training: A Critical PerspectivePublished on Mar 26\u00b7Submitted byZichenon Apr 3Upvote58+50Authors:Zichen Liu,Changyu Chen,Wenjun Li,Penghui Qi,Tianyu Pang,Chao Du,Wee Sun Lee,Min LinAbstractA study of R1-Zero training identifies pretraining effects on RL performance and proffers Dr. GRPO to enhance token efficiency, achieving superior accuracy on AIME 2024.AI-generated summaryDeepSeek-R1-Zero has shown thatreinforcement learning(RL) at scale can\ndirectly enhance the reasoning capabilities ofLLMswithout supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models andRL. We investigate a wide\nrange of base models, includingDeepSeek-V3-Base, to understand how pretraining\ncharacteristics influenceRLperformance. Our analysis reveals thatDeepSeek-V3-Basealready exhibit ''Aha moment'', whileQwen2.5base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias inGroup Relative Policy Optimization(GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduceDr. GRPO, an unbiased\noptimization method that improvestoken efficiencywhile maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy onAIME 2024with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.View arXiv pageView PDFGitHub1.18kAdd to collectionCommunitylkevinzcPaper authorPaper submitterApr 3\u2022edited Apr 3No description provided.ReplybeatccjiangApr 3It's so surprising to meet this interesting work again.See translationReplylibrarian-botApr 4This is an automated message from theLibrarian Bot. I found th",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/sail-sg/understand-r1-zero",
    "hf_paper_url": "https://huggingface.co/papers/2503.20783",
    "arxiv_url": "https://arxiv.org/abs/2503.20783",
    "num_models": 13,
    "models_list": "osmosis-ai/osmosis-mcp-4b, sail/Qwen2.5-Math-7B-Oat-Zero, notbadai/notbad_v1_0_mistral_24b, TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2, lkevinzc/Llama-3.2-3B-NuminaQA, sail/Llama-3.2-3B-Oat-Zero, sail/Qwen2.5-Math-1.5B-Oat-Zero, notbadai/notbad_v1_1_mistral_24b, Mungert/osmosis-mcp-4b-GGUF, Prince-1/Osmosis-Mcp-Rkllm, Prince-1/Osmosis-mcp-4b, onnx-community/Osmosis-mcp-4b, qiuxi337/IntrinSight-4B",
    "models_links": "https://huggingface.co/osmosis-ai/osmosis-mcp-4b, https://huggingface.co/sail/Qwen2.5-Math-7B-Oat-Zero, https://huggingface.co/notbadai/notbad_v1_0_mistral_24b, https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2, https://huggingface.co/lkevinzc/Llama-3.2-3B-NuminaQA, https://huggingface.co/sail/Llama-3.2-3B-Oat-Zero, https://huggingface.co/sail/Qwen2.5-Math-1.5B-Oat-Zero, https://huggingface.co/notbadai/notbad_v1_1_mistral_24b, https://huggingface.co/Mungert/osmosis-mcp-4b-GGUF, https://huggingface.co/Prince-1/Osmosis-Mcp-Rkllm, https://huggingface.co/Prince-1/Osmosis-mcp-4b, https://huggingface.co/onnx-community/Osmosis-mcp-4b, https://huggingface.co/qiuxi337/IntrinSight-4B",
    "models_detailed": "[{\"name\": \"osmosis-ai/osmosis-mcp-4b\", \"link\": \"https://huggingface.co/osmosis-ai/osmosis-mcp-4b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"sail/Qwen2.5-Math-7B-Oat-Zero\", \"link\": \"https://huggingface.co/sail/Qwen2.5-Math-7B-Oat-Zero\", \"task\": \"Text Generation\", \"likes\": \"742\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"notbadai/notbad_v1_0_mistral_24b\", \"link\": \"https://huggingface.co/notbadai/notbad_v1_0_mistral_24b\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Apr 7\"}, {\"name\": \"TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2\", \"link\": \"https://huggingface.co/TorpedoSoftware/Luau-Devstral-24B-Instruct-v0.2\", \"task\": \"Text Generation\", \"likes\": \"880\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"lkevinzc/Llama-3.2-3B-NuminaQA\", \"link\": \"https://huggingface.co/lkevinzc/Llama-3.2-3B-NuminaQA\", \"task\": \"Text Generation\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"sail/Llama-3.2-3B-Oat-Zero\", \"link\": \"https://huggingface.co/sail/Llama-3.2-3B-Oat-Zero\", \"task\": \"Text Generation\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"sail/Qwen2.5-Math-1.5B-Oat-Zero\", \"link\": \"https://huggingface.co/sail/Qwen2.5-Math-1.5B-Oat-Zero\", \"task\": \"Text Generation\", \"likes\": \"82\", \"downloads\": \"\", \"updated\": \"Jun 30\"}, {\"name\": \"notbadai/notbad_v1_1_mistral_24b\", \"link\": \"https://huggingface.co/notbadai/notbad_v1_1_mistral_24b\", \"task\": \"Text Generation\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"Apr 7\"}, {\"name\": \"Mungert/osmosis-mcp-4b-GGUF\", \"link\": \"https://huggingface.co/Mungert/osmosis-mcp-4b-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Prince-1/Osmosis-Mcp-Rkllm\", \"link\": \"https://huggingface.co/Prince-1/Osmosis-Mcp-Rkllm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"Prince-1/Osmosis-mcp-4b\", \"link\": \"https://huggingface.co/Prince-1/Osmosis-mcp-4b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"onnx-community/Osmosis-mcp-4b\", \"link\": \"https://huggingface.co/onnx-community/Osmosis-mcp-4b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 11\"}, {\"name\": \"qiuxi337/IntrinSight-4B\", \"link\": \"https://huggingface.co/qiuxi337/IntrinSight-4B\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Aug 18\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.18924",
    "first_seen_date": "2025-04-03",
    "title": "MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech Synthesis",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.18924MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for\n  Zero-Shot Speech SynthesisPublished on Feb 26\u00b7Submitted byNiels Roggeon Apr 3Upvote16+8Authors:Ziyue Jiang,Yi Ren,Ruiqi Li,Shengpeng Ji,Boyang Zhang,Zhenhui Ye,Chen Zhang,Bai Jionghao,Xiaoda Yang,Jialong Zuo,Yu Zhang,Rui Liu,Xiang Yin,Zhou ZhaoAbstractMegaTTS 3, a zero-shot text-to-speech system, uses a sparse alignment algorithm and latent diffusion transformer for high-quality speech generation with flexible accent control.AI-generated summaryWhile recent zero-shot text-to-speech (TTS) models have significantly\nimproved speech quality and expressiveness, mainstream systems still suffer\nfrom issues related to speech-text alignment modeling: 1) models without\nexplicit speech-text alignment modeling exhibit less robustness, especially for\nhard sentences in practical applications; 2) predefined alignment-based models\nsuffer from naturalness constraints of forced alignments. This paper introduces\nMegaTTS 3, a TTS system featuring an innovative sparse alignment\nalgorithm that guides thelatent diffusion transformer(DiT). Specifically, we\nprovide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of\nalignment without limiting the search space, thereby achieving high\nnaturalness. Moreover, we employ amulti-condition classifier-free guidancestrategy for accent intensity adjustment and adopt thepiecewise rectified flowtechnique to accelerate the generation process. Experiments demonstrate that\nMegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports\nhighly flexible control over accent intensity. Notably, our system can generate\nhigh-quality one-minute speech with only 8 sampling steps. Audio samples are\navailable at https://sditdemo.github.io/sditdemo/.View arXiv pageView PDFProject pageAdd to collectionCommunitynielsrPaper submitterApr 3Project page:https://sditdemo.github",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.18924",
    "arxiv_url": "https://arxiv.org/abs/2502.18924",
    "num_models": 4,
    "models_list": "ByteDance/MegaTTS3, drbaph/MegaTTS3-WaveVAE, RedbeardNZ/MegaTTS3, kp-forks/MegaTTS3",
    "models_links": "https://huggingface.co/ByteDance/MegaTTS3, https://huggingface.co/drbaph/MegaTTS3-WaveVAE, https://huggingface.co/RedbeardNZ/MegaTTS3, https://huggingface.co/kp-forks/MegaTTS3",
    "models_detailed": "[{\"name\": \"ByteDance/MegaTTS3\", \"link\": \"https://huggingface.co/ByteDance/MegaTTS3\", \"task\": \"Text-to-Speech\", \"likes\": \"238\", \"downloads\": \"\", \"updated\": \"Apr 4\"}, {\"name\": \"drbaph/MegaTTS3-WaveVAE\", \"link\": \"https://huggingface.co/drbaph/MegaTTS3-WaveVAE\", \"task\": \"Text-to-Speech\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"RedbeardNZ/MegaTTS3\", \"link\": \"https://huggingface.co/RedbeardNZ/MegaTTS3\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"kp-forks/MegaTTS3\", \"link\": \"https://huggingface.co/kp-forks/MegaTTS3\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 4\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2504.01017",
    "first_seen_date": "2025-04-02",
    "title": "Scaling Language-Free Visual Representation Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2504.01017Scaling Language-Free Visual Representation LearningPublished on Apr 1\u00b7Submitted byPeter Tongon Apr 2Upvote32+24Authors:David Fan,Shengbang Tong,Jiachen Zhu,Koustuv Sinha,Zhuang Liu,Xinlei Chen,Michael Rabbat,Nicolas Ballas,Yann LeCun,Amir Bar,Saining XieAbstractVisual self-supervised learning matches language-supervised visual pretraining performance on VQA and vision benchmarks when both are trained on the same dataset and scaled appropriately.AI-generated summaryVisual Self-Supervised Learning(SSL) currently underperforms Contrastive\nLanguage-Image Pretraining (CLIP) in multimodal settings such as Visual\nQuestion Answering (VQA). This multimodal gap is often attributed to the\nsemantics introduced by language supervision, even though visual SSL andCLIPmodels are often trained on different data. In this work, we ask the question:\n\"Do visual self-supervised approaches lag behindCLIPdue to the lack of\nlanguage supervision, or differences in the training data?\" We study this\nquestion by training both visual SSL andCLIPmodels on the sameMetaCLIPdata,\nand leveragingVQAas a diverse testbed forvision encoders. In this controlled\nsetup, visual SSL models scale better thanCLIPmodels in terms of data and\nmodel capacity, and visual SSL performance does not saturate even after scaling\nup to 7B parameters. Consequently, we observe visual SSL methods achieveCLIP-level performance on a wide range ofVQAand classic vision benchmarks.\nThese findings demonstrate that pure visual SSL can match language-supervised\nvisual pretraining at scale, opening new opportunities for vision-centric\nrepresentation learning.View arXiv pageView PDFProject pageGitHub245autoAdd to collectionCommunitytsbppPaper authorPaper submitterApr 2We explored \"scaling\" visual SSL models and uncover many interesting  insights.See translation2 replies\u00b7modricwangApr 2nice work!Expand 1\n\t\t\t\t\t\treplylibrarian-botApr 3This is",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2504,
    "github_repo": "https://github.com/dfan/webssl",
    "hf_paper_url": "https://huggingface.co/papers/2504.01017",
    "arxiv_url": "https://arxiv.org/abs/2504.01017",
    "num_models": 17,
    "models_list": "facebook/webssl-dino7b-full8b-518, facebook/webssl-dino300m-full2b-224, facebook/webssl-dino1b-full2b-224, facebook/webssl-dino7b-full8b-224, facebook/webssl-dino2b-full2b-224, facebook/webssl-dino3b-full2b-224, facebook/webssl-dino5b-full2b-224, facebook/webssl-dino7b-full8b-378, facebook/webssl-dino2b-light2b-224, facebook/webssl-dino2b-heavy2b-224, facebook/webssl-dino3b-light2b-224, facebook/webssl-dino3b-heavy2b-224, facebook/webssl-mae300m-full2b-224, facebook/webssl-mae700m-full2b-224, facebook/webssl-mae1b-full2b-224, facebook/webssl-mae2b-full2b-224, facebook/webssl-mae3b-full2b-224",
    "models_links": "https://huggingface.co/facebook/webssl-dino7b-full8b-518, https://huggingface.co/facebook/webssl-dino300m-full2b-224, https://huggingface.co/facebook/webssl-dino1b-full2b-224, https://huggingface.co/facebook/webssl-dino7b-full8b-224, https://huggingface.co/facebook/webssl-dino2b-full2b-224, https://huggingface.co/facebook/webssl-dino3b-full2b-224, https://huggingface.co/facebook/webssl-dino5b-full2b-224, https://huggingface.co/facebook/webssl-dino7b-full8b-378, https://huggingface.co/facebook/webssl-dino2b-light2b-224, https://huggingface.co/facebook/webssl-dino2b-heavy2b-224, https://huggingface.co/facebook/webssl-dino3b-light2b-224, https://huggingface.co/facebook/webssl-dino3b-heavy2b-224, https://huggingface.co/facebook/webssl-mae300m-full2b-224, https://huggingface.co/facebook/webssl-mae700m-full2b-224, https://huggingface.co/facebook/webssl-mae1b-full2b-224, https://huggingface.co/facebook/webssl-mae2b-full2b-224, https://huggingface.co/facebook/webssl-mae3b-full2b-224",
    "models_detailed": "[{\"name\": \"facebook/webssl-dino7b-full8b-518\", \"link\": \"https://huggingface.co/facebook/webssl-dino7b-full8b-518\", \"task\": \"\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino300m-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino300m-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino1b-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino1b-full2b-224\", \"task\": \"\", \"likes\": \"997\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino7b-full8b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino7b-full8b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino2b-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino2b-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino3b-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino3b-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino5b-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino5b-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino7b-full8b-378\", \"link\": \"https://huggingface.co/facebook/webssl-dino7b-full8b-378\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino2b-light2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino2b-light2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino2b-heavy2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino2b-heavy2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino3b-light2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino3b-light2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-dino3b-heavy2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-dino3b-heavy2b-224\", \"task\": \"\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-mae300m-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-mae300m-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-mae700m-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-mae700m-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-mae1b-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-mae1b-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-mae2b-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-mae2b-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"facebook/webssl-mae3b-full2b-224\", \"link\": \"https://huggingface.co/facebook/webssl-mae3b-full2b-224\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.19794",
    "first_seen_date": "2025-04-01",
    "title": "PAVE: Patching and Adapting Video Large Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.19794PAVE: Patching and Adapting Video Large Language ModelsPublished on Mar 25\u00b7Submitted byZhuoming Liuon Apr 1Upvote3Authors:Zhuoming Liu,Yiquan Li,Khoi Duc Nguyen,Yiwu Zhong,Yin LiAbstractPAVE is a flexible framework that adapts pre-trained Video LLMs with lightweight patches for diverse downstream tasks, enhancing performance while requiring minimal additional resources.AI-generated summaryPre-trained video large language models (Video LLMs) exhibit remarkable\nreasoning capabilities, yet adapting these models to new tasks involving\nadditional modalities or data types (e.g., audio or 3D information) remains\nchallenging. In this paper, we presentPAVE, a flexible framework for adapting\npre-trainedVideo LLMsto downstream tasks withside-channel signals, such as\naudio, 3D cues, or multi-view videos.PAVEintroduces lightweightadapters,\nreferred to as \"patches,\" which add a small number of parameters and operations\nto a base model without modifying its architecture or pre-trained weights. In\ndoing so,PAVEcan effectively adapt the pre-trained base model to support\ndiverse downstream tasks, includingaudio-visual question answering, 3D\nreasoning,multi-view video recognition, and high frame rate video\nunderstanding. Across these tasks,PAVEsignificantly enhances the performance\nof the base model, surpassing state-of-the-art task-specific models while\nincurring a minor cost of ~0.1% additional FLOPs and parameters. Further,PAVEsupportsmulti-task learningand generalizes well across differentVideo LLMs.\nOur code is available at https://github.com/dragonlzm/PAVE.View arXiv pageView PDFProject pageGitHub26Add to collectionCommunityzhuomingliuPaper authorPaper submitterApr 1\u2022edited Apr 2Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D informat",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/dragonlzm/PAVE",
    "hf_paper_url": "https://huggingface.co/papers/2503.19794",
    "arxiv_url": "https://arxiv.org/abs/2503.19794",
    "num_models": 2,
    "models_list": "nvidia/MambaVision-B-1K, zhuomingliu/PAVE",
    "models_links": "https://huggingface.co/nvidia/MambaVision-B-1K, https://huggingface.co/zhuomingliu/PAVE",
    "models_detailed": "[{\"name\": \"nvidia/MambaVision-B-1K\", \"link\": \"https://huggingface.co/nvidia/MambaVision-B-1K\", \"task\": \"Image Classification\", \"likes\": \"300\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"zhuomingliu/PAVE\", \"link\": \"https://huggingface.co/zhuomingliu/PAVE\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\"}]",
    "num_datasets": 2,
    "datasets_list": "zhuomingliu/PAVEDataset, zhuomingliu/PAVE_others",
    "datasets_links": "https://huggingface.co/datasets/zhuomingliu/PAVEDataset, https://huggingface.co/datasets/zhuomingliu/PAVE_others",
    "datasets_detailed": "[{\"name\": \"zhuomingliu/PAVEDataset\", \"link\": \"https://huggingface.co/datasets/zhuomingliu/PAVEDataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\", \"size\": \"\"}, {\"name\": \"zhuomingliu/PAVE_others\", \"link\": \"https://huggingface.co/datasets/zhuomingliu/PAVE_others\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.22673",
    "first_seen_date": "2025-04-01",
    "title": "ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.22673ActionStudio: A Lightweight Framework for Data and Training of Large\n  Action ModelsPublished on Mar 28\u00b7Submitted byJianguo Zhangon Apr 1Upvote12+4Authors:Jianguo Zhang,Thai Hoang,Ming Zhu,Zuxin Liu,Shiyu Wang,Tulika Awalgaonkar,Akshara Prabhakar,Haolin Chen,Weiran Yao,Zhiwei Liu,Juntao Tan,Juan Carlos Niebles,Shelby Heinecke,Huan Wang,Silvio Savarese,Caiming XiongAbstractActionStudio is a lightweight, extensible framework for training large action models that supports scalability, diverse training paradigms, and integrates preprocessing and verification tools, demonstrating strong performance on public and industry benchmarks.AI-generated summaryAction models are essential for enabling autonomous agents to perform complex\ntasks. However, training large action models remains challenging due to the\ndiversity of agent environments and the complexity of agentic data. Despite\ngrowing interest, existing infrastructure provides limited support for\nscalable, agent-specific fine-tuning. We presentActionStudio, a lightweight\nand extensible data and training framework designed for large action models.ActionStudiounifies heterogeneousagent trajectoriesthrough a standardized\nformat, supports diverse training paradigms includingLoRA,full fine-tuning,\nanddistributed setups, and integrates robustpreprocessingand verification\ntools. We validate its effectiveness across both public and realistic industry\nbenchmarks, demonstrating strong performance and practical scalability. We\nopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to\nfacilitate research in the community.View arXiv pageView PDFGitHub593Add to collectionCommunityjianguozhangPaper submitterApr 1\u2022edited Apr 1ActionStudio: A Lightweight Framework for Data and Training of Large Action ModelsThe framework is now open-source! Explore ourpaperandGithub Code and Agentic Data Trajectories under xLAMfor more inf",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/SalesforceAIResearch/xLAM",
    "hf_paper_url": "https://huggingface.co/papers/2503.22673",
    "arxiv_url": "https://arxiv.org/abs/2503.22673",
    "num_models": 20,
    "models_list": "Salesforce/xLAM-7b-fc-r, Salesforce/xLAM-1b-fc-r, Salesforce/Llama-xLAM-2-8b-fc-r, Salesforce/Llama-xLAM-2-70b-fc-r, Salesforce/xLAM-2-1b-fc-r, Salesforce/xLAM-8x7b-r, Salesforce/xLAM-8x22b-r, Salesforce/xLAM-7b-r, Salesforce/xLAM-2-32b-fc-r, Salesforce/xLAM-2-3b-fc-r, Salesforce/xLAM-2-3b-fc-r-gguf, Salesforce/xLAM-2-1b-fc-r-gguf, Salesforce/Llama-xLAM-2-8b-fc-r-gguf, amd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid, kmhalvin/xLAM-2-1b-fc-r-SpinQuant-ET, licongwei/xLAM-2-3b-fc-r-SpinQuant-ET, Mungert/xLAM-2-3b-fc-r-GGUF, Mungert/Llama-xLAM-2-8b-fc-r-GGUF, Mungert/xLAM-2-32b-fc-r-GGUF, sunkencity/Llama-xLAM-2-8b-fc-r-blasphemer",
    "models_links": "https://huggingface.co/Salesforce/xLAM-7b-fc-r, https://huggingface.co/Salesforce/xLAM-1b-fc-r, https://huggingface.co/Salesforce/Llama-xLAM-2-8b-fc-r, https://huggingface.co/Salesforce/Llama-xLAM-2-70b-fc-r, https://huggingface.co/Salesforce/xLAM-2-1b-fc-r, https://huggingface.co/Salesforce/xLAM-8x7b-r, https://huggingface.co/Salesforce/xLAM-8x22b-r, https://huggingface.co/Salesforce/xLAM-7b-r, https://huggingface.co/Salesforce/xLAM-2-32b-fc-r, https://huggingface.co/Salesforce/xLAM-2-3b-fc-r, https://huggingface.co/Salesforce/xLAM-2-3b-fc-r-gguf, https://huggingface.co/Salesforce/xLAM-2-1b-fc-r-gguf, https://huggingface.co/Salesforce/Llama-xLAM-2-8b-fc-r-gguf, https://huggingface.co/amd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid, https://huggingface.co/kmhalvin/xLAM-2-1b-fc-r-SpinQuant-ET, https://huggingface.co/licongwei/xLAM-2-3b-fc-r-SpinQuant-ET, https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF, https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF, https://huggingface.co/Mungert/xLAM-2-32b-fc-r-GGUF, https://huggingface.co/sunkencity/Llama-xLAM-2-8b-fc-r-blasphemer",
    "models_detailed": "[{\"name\": \"Salesforce/xLAM-7b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-7b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"77\", \"downloads\": \"\", \"updated\": \"Apr 11\"}, {\"name\": \"Salesforce/xLAM-1b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-1b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"374\", \"downloads\": \"\", \"updated\": \"Apr 11\"}, {\"name\": \"Salesforce/Llama-xLAM-2-8b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/Llama-xLAM-2-8b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/Llama-xLAM-2-70b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/Llama-xLAM-2-70b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"444\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-1b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-1b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-8x7b-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-8x7b-r\", \"task\": \"Text Generation\", \"likes\": \"47\", \"downloads\": \"\", \"updated\": \"Apr 11\"}, {\"name\": \"Salesforce/xLAM-8x22b-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-8x22b-r\", \"task\": \"Text Generation\", \"likes\": \"53\", \"downloads\": \"\", \"updated\": \"Apr 11\"}, {\"name\": \"Salesforce/xLAM-7b-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-7b-r\", \"task\": \"Text Generation\", \"likes\": \"790\", \"downloads\": \"\", \"updated\": \"Apr 11\"}, {\"name\": \"Salesforce/xLAM-2-32b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-32b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-3b-fc-r\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-3b-fc-r\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-3b-fc-r-gguf\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-3b-fc-r-gguf\", \"task\": \"Text Generation\", \"likes\": \"584\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/xLAM-2-1b-fc-r-gguf\", \"link\": \"https://huggingface.co/Salesforce/xLAM-2-1b-fc-r-gguf\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"Salesforce/Llama-xLAM-2-8b-fc-r-gguf\", \"link\": \"https://huggingface.co/Salesforce/Llama-xLAM-2-8b-fc-r-gguf\", \"task\": \"Text Generation\", \"likes\": \"744\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"amd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid\", \"link\": \"https://huggingface.co/amd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"kmhalvin/xLAM-2-1b-fc-r-SpinQuant-ET\", \"link\": \"https://huggingface.co/kmhalvin/xLAM-2-1b-fc-r-SpinQuant-ET\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 27\"}, {\"name\": \"licongwei/xLAM-2-3b-fc-r-SpinQuant-ET\", \"link\": \"https://huggingface.co/licongwei/xLAM-2-3b-fc-r-SpinQuant-ET\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"Mungert/xLAM-2-3b-fc-r-GGUF\", \"link\": \"https://huggingface.co/Mungert/xLAM-2-3b-fc-r-GGUF\", \"task\": \"Text Generation\", \"likes\": \"552\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/Llama-xLAM-2-8b-fc-r-GGUF\", \"link\": \"https://huggingface.co/Mungert/Llama-xLAM-2-8b-fc-r-GGUF\", \"task\": \"Text Generation\", \"likes\": \"318\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/xLAM-2-32b-fc-r-GGUF\", \"link\": \"https://huggingface.co/Mungert/xLAM-2-32b-fc-r-GGUF\", \"task\": \"Text Generation\", \"likes\": \"590\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"sunkencity/Llama-xLAM-2-8b-fc-r-blasphemer\", \"link\": \"https://huggingface.co/sunkencity/Llama-xLAM-2-8b-fc-r-blasphemer\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.23829",
    "first_seen_date": "2025-04-01",
    "title": "Expanding RL with Verifiable Rewards Across Diverse Domains",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.23829Expanding RL with Verifiable Rewards Across Diverse DomainsPublished on Mar 31\u00b7Submitted byAKon Apr 1Upvote23+15Authors:Yi Su,Dian Yu,Linfeng Song,Juntao Li,Haitao Mi,Zhaopeng Tu,Min Zhang,Dong YuAbstractReinforcement learning with verifiable rewards extended to diverse domains using model-based soft scoring outperforms existing LLMs in free-form answer settings with reliable reward signals.AI-generated summaryReinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement inbinary judgmentsacross different large\nlanguage models (LLMs) whenobjective reference answersexist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporatemodel-based soft scoringinto RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nByfine-tuninga base 7B model using variousRL algorithmsagainst our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such asQwen2.5-72B-InstructandDeepSeek-R1-Distill-Qwen-32Bby a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.View arXiv pageView PDFProject pageAdd to collectionCommunityakhaliqPaper submitterApr 1S",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2503.23829",
    "arxiv_url": "https://arxiv.org/abs/2503.23829",
    "num_models": 1,
    "models_list": "virtuoussy/Qwen2.5-7B-Instruct-RLVR",
    "models_links": "https://huggingface.co/virtuoussy/Qwen2.5-7B-Instruct-RLVR",
    "models_detailed": "[{\"name\": \"virtuoussy/Qwen2.5-7B-Instruct-RLVR\", \"link\": \"https://huggingface.co/virtuoussy/Qwen2.5-7B-Instruct-RLVR\", \"task\": \"\", \"likes\": \"58\", \"downloads\": \"\", \"updated\": \"May 4\"}]",
    "num_datasets": 8,
    "datasets_list": "allenai/Dolci-Think-RL-7B, allenai/Dolci-Think-RL-32B, allenai/Dolci-Instruct-RL, allenai/Dolci-Think-RL-7B-Completions-SFT, virtuoussy/Math-RLVR, virtuoussy/Multi-subject-RLVR, sarosavo/Master-RM, allenai/Dolci-Think-RL-7B-Completions-DPO",
    "datasets_links": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B, https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B, https://huggingface.co/datasets/allenai/Dolci-Instruct-RL, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT, https://huggingface.co/datasets/virtuoussy/Math-RLVR, https://huggingface.co/datasets/virtuoussy/Multi-subject-RLVR, https://huggingface.co/datasets/sarosavo/Master-RM, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO",
    "datasets_detailed": "[{\"name\": \"allenai/Dolci-Think-RL-7B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-32B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Instruct-RL\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Instruct-RL\", \"task\": \"\", \"likes\": \"774\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-SFT\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT\", \"task\": \"\", \"likes\": \"413\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"virtuoussy/Math-RLVR\", \"link\": \"https://huggingface.co/datasets/virtuoussy/Math-RLVR\", \"task\": \"\", \"likes\": \"101\", \"downloads\": \"\", \"updated\": \"Apr 16\", \"size\": \"\"}, {\"name\": \"virtuoussy/Multi-subject-RLVR\", \"link\": \"https://huggingface.co/datasets/virtuoussy/Multi-subject-RLVR\", \"task\": \"\", \"likes\": \"160\", \"downloads\": \"\", \"updated\": \"Apr 16\", \"size\": \"\"}, {\"name\": \"sarosavo/Master-RM\", \"link\": \"https://huggingface.co/datasets/sarosavo/Master-RM\", \"task\": \"\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"Jul 15\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-DPO\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO\", \"task\": \"\", \"likes\": \"332\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.24290",
    "first_seen_date": "2025-04-01",
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.24290Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement\n  Learning on the Base ModelPublished on Mar 31\u00b7Submitted byAKon Apr 1#3 Paper of the dayUpvote62+54Authors:Jingcheng Hu,Yinmin Zhang,Qi Han,Daxin Jiang,Xiangyu Zhang,Heung-Yeung ShumAbstractOpen-Reasoner-Zero achieves superior performance on reasoning benchmarks using a minimalist PPO approach, requiring fewer training steps than DeepSeek-R1-Zero.AI-generated summaryWe introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanillaPPOwithGAE(lambda=1, gamma=1) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length andbenchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model asDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance onAIME2024,MATH500, and theGPQA Diamondbenchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of thetraining steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.View arXiv pageView PDFProject pageGitHub2.08kAdd to collectionCommunityakhaliqPaper submitterApr 1See translation\ud83d\ude8044+ReplyhamzziApr 1Awesome\u2764\ufe0f22+Replylibrarian-botApr 3This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APILogic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning(2025)R1-Zero's\"Aha Moment\"in Visual Reasoning on a 2B Non-SFT Model(2025)DAPO: An Open-Source LLM Reinforcement Learning System at Scale(2025)Understandin",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
    "hf_paper_url": "https://huggingface.co/papers/2503.24290",
    "arxiv_url": "https://arxiv.org/abs/2503.24290",
    "num_models": 8,
    "models_list": "Open-Reasoner-Zero/Open-Reasoner-Zero-7B, Open-Reasoner-Zero/Open-Reasoner-Zero-32B, Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B, Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B, Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B, Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B, Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B, Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "models_links": "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B, https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B, https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B, https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B, https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B, https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B, https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B, https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "models_detailed": "[{\"name\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-7B\", \"link\": \"https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B\", \"task\": \"\", \"likes\": \"65\", \"downloads\": \"\", \"updated\": \"Apr 7\"}, {\"name\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-32B\", \"link\": \"https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Apr 7\"}, {\"name\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B\", \"link\": \"https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B\", \"task\": \"\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Apr 7\"}, {\"name\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B\", \"link\": \"https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B\", \"task\": \"\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Apr 6\"}, {\"name\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B\", \"link\": \"https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 7\"}, {\"name\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B\", \"link\": \"https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 6\"}, {\"name\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B\", \"link\": \"https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 7\"}, {\"name\": \"Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B\", \"link\": \"https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Apr 7\"}]",
    "num_datasets": 8,
    "datasets_list": "allenai/Dolci-Think-RL-7B, allenai/Dolci-Think-RL-32B, allenai/Dolci-Instruct-RL, allenai/Dolci-Think-RL-7B-Completions-SFT, Open-Reasoner-Zero/orz_math_57k_collection, Open-Reasoner-Zero/orz_math_13k_collection_hard, Open-Reasoner-Zero/orz_math_72k_collection_extended, allenai/Dolci-Think-RL-7B-Completions-DPO",
    "datasets_links": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B, https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B, https://huggingface.co/datasets/allenai/Dolci-Instruct-RL, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT, https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_57k_collection, https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_13k_collection_hard, https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_72k_collection_extended, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO",
    "datasets_detailed": "[{\"name\": \"allenai/Dolci-Think-RL-7B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-32B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Instruct-RL\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Instruct-RL\", \"task\": \"\", \"likes\": \"774\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-SFT\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT\", \"task\": \"\", \"likes\": \"413\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"Open-Reasoner-Zero/orz_math_57k_collection\", \"link\": \"https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_57k_collection\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"Apr 6\", \"size\": \"\"}, {\"name\": \"Open-Reasoner-Zero/orz_math_13k_collection_hard\", \"link\": \"https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_13k_collection_hard\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"Apr 6\", \"size\": \"\"}, {\"name\": \"Open-Reasoner-Zero/orz_math_72k_collection_extended\", \"link\": \"https://huggingface.co/datasets/Open-Reasoner-Zero/orz_math_72k_collection_extended\", \"task\": \"\", \"likes\": \"393\", \"downloads\": \"\", \"updated\": \"Apr 6\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-DPO\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO\", \"task\": \"\", \"likes\": \"332\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.21821",
    "first_seen_date": "2025-03-31",
    "title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.21821PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem SolvingPublished on Mar 26\u00b7Submitted byYilun Zhaoon Mar 31Upvote21+13Authors:Kaiyue Feng,Yilun Zhao,Yixin Liu,Tianyu Yang,Chen Zhao,John Sous,Arman CohanAbstractA comprehensive physics problem-solving benchmark reveals limitations in existing foundation models, guiding future improvements through error analysis and knowledge augmentation techniques.AI-generated summaryWe introduce PHYSICS, a comprehensive benchmark for university-level physics\nproblem solving. It contains 1297 expert-annotated problems covering six core\nareas: classical mechanics, quantum mechanics, thermodynamics and statistical\nmechanics, electromagnetism, atomic physics, and optics. Each problem requires\nadvanced physics knowledge and mathematical reasoning. We develop a robust\nautomated evaluation system for precise and reliable validation. Our evaluation\nof leading foundation models reveals substantial limitations. Even the most\nadvanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant\nchallenges in solving high-level scientific problems. Through comprehensive\nerror analysis, exploration of diverse prompting strategies, andRetrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify\nkey areas for improvement, laying the foundation for future advancements.View arXiv pageView PDFGitHub24Add to collectionCommunityyilunzhaoPaper authorPaper submitterMar 31Data and code are available athttps://github.com/yale-nlp/Physics!See translationReplylibrarian-botApr 1This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIPhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning(2025)UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/yale-nlp/Physics",
    "hf_paper_url": "https://huggingface.co/papers/2503.21821",
    "arxiv_url": "https://arxiv.org/abs/2503.21821",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "facebook/principia-bench",
    "datasets_links": "https://huggingface.co/datasets/facebook/principia-bench",
    "datasets_detailed": "[{\"name\": \"facebook/principia-bench\", \"link\": \"https://huggingface.co/datasets/facebook/principia-bench\", \"task\": \"\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.19786",
    "first_seen_date": "2025-03-27",
    "title": "Gemma 3 Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.19786Gemma 3 Technical ReportPublished on Mar 25\u00b7Submitted bytaesirion Mar 27#3 Paper of the dayUpvote54+46Authors:Gemma Team,Aishwarya Kamath,Johan Ferret,Shreya Pathak,Nino Vieillard,Ramona Merhej,Sarah Perrin,Tatiana Matejovicova,Alexandre Ram\u00e9,Morgane Rivi\u00e8re,Louis Rouillard,Thomas Mesnard,Geoffrey Cideron,Jean-bastien Grill,Sabela Ramos,Edouard Yvinec,Michelle Casbon,Etienne Pot,Ivo Penchev,Ga\u00ebl Liu,Francesco Visin,Kathleen Kenealy+194 authorsAbstractGemma 3 introduces vision capabilities, broader language coverage, and extended context length, featuring an optimized architecture and post-training enhancements to outperform previous versions.AI-generated summaryWe introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introducesvision understandingabilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce theKV-cache memorythat tends to explode\nwithlong context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span onlocal attentionshort. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained andinstruction finetunedversions. In particular, our\nnovelpost-training recipesignificantly improves the math, chat,instruction-followingandmultilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.View arXiv pageView PDFAdd to collectionCommunitytaesiriPaper submitterMar 27Gemma 3 Technical ReportSee translation1 reply\u00b7grantsingSep 22arXiv explained breakdown of this paper \ud83d\udc49https://arxivexplained.com/papers/gemma-3-technical-reportSee translationlibrarian-botMar 28This is an automat",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2503.19786",
    "arxiv_url": "https://arxiv.org/abs/2503.19786",
    "num_models": 88,
    "models_list": "google/gemma-3-270m, google/gemma-3-270m-it, google/vaultgemma-1b, unsloth/gemma-3-270m-it-GGUF, onnx-community/gemma-3-270m-it-ONNX, unsloth/gemma-3-270m-it, p-e-w/gemma-3-270m-it-heretic, Markr-AI/Gukbap-Gemma3-27B-VL, Markr-AI/Gukbap-Gemma3-12B-VL, DimensionSTP/gemma-3-12b-it-Ko-Reasoning, DimensionSTP/gemma-3-4b-it-Ko-Reasoning, Markr-AI/Gukbap-Gemma3-4B-VL, DimensionSTP/gemma-3-27b-it-Ko-Reasoning, PJMixers-Dev/Gemma-3-Earthen-Completion-v0.1-4B-QLoRA, PJMixers-Dev/Gemma-3-Earthen-Completion-v0.1-4B, PJMixers-Dev/Gemma-3-Earthen-v0.1-4B-QLoRA, PJMixers-Dev/Gemma-3-Earthen-v0.1-4B, PJMixers-Dev/Gemma-3-Earthen-v0.2-4B-QLoRA, PJMixers-Dev/Gemma-3-Earthen-v0.2-4B, Motif-Technologies/Motif-2.6B, stribomon/gemma3-siglip448, PJMixers-Dev/Gemma-3-Starshine-Earthen-v0.4-12B-QLoRA, OpenLLM-Korea/Motif-2.6B, google/gemma-3-270m-qat-q4_0-unquantized, google/gemma-3-270m-it-qat-q4_0-unquantized, unsloth/gemma-3-270m-it-unsloth-bnb-4bit, unsloth/gemma-3-270m-it-bnb-4bit, unsloth/gemma-3-270m-it-qat, unsloth/gemma-3-270m-it-qat-GGUF, unsloth/gemma-3-270m-it-qat-unsloth-bnb-4bit, unsloth/gemma-3-270m-it-qat-bnb-4bit, unsloth/gemma-3-270m, unsloth/gemma-3-270m-unsloth-bnb-4bit, unsloth/gemma-3-270m-bnb-4bit, unsloth/gemma-3-270m-it-torchao-FP8, litert-community/gemma-3-270m-it, lucyknada/google_gemma-3-270m-exl3, lucyknada/google_gemma-3-270m-it-exl3, Mungert/gemma-3-270m-it-GGUF, pixasocial/survival-uncensored-gemma-270m, yaya-sy/BaldGemma3-140m, Hasya018/gemma-3-270m-multihopping, d-s-b/Router, mahwizzzz/tinygemma-Urdu, naveennuwantha/NUXARA-AI-V1, Hirun9/gemma-3-270m, TamWaiban/gemma-3-270m-int4, TamWaiban/gemma-3-270m-autoquant, Sweelol-ai/gemma3-270m-dolly-teacher, Sweelol-ai/gemma3-270m-pruned-baseline-50pc, Sweelol-ai/kd-gemma3-pruned-dolly, Sweelol-ai/lora-gemma3-270m-dolly, Sweelol-ai/pt-gemma3-270m-dolly, Sweelol-ai/finetuned-pruned-gemma3-270m-dolly, sweelol/gemma3-270m-dolly-teacher, sweelol/gemma3-270m-pruned-baseline-50pc, sweelol/kd-gemma3-pruned-dolly, sweelol/lora-gemma3-270m-dolly, sweelol/pt-gemma3-270m-dolly, sweelol/finetuned-pruned-gemma3-270m-dolly, sweelol/gemma3-270m-pruned-base, QuantFactory/gemma-3-270m-it-GGUF, QuantFactory/gemma-3-270m-GGUF, Erland/gemma-3-270m-it, onnx-community/vaultgemma-1b-ONNX, cblbai/clm-1-small-beta, smartvest-llc/gemma-3-270m-it, Sci-fi-vy/gemma-3-270m-it-GGUF, sinhashubham/gemma-3-270m-it, Najin06/tacha_ft_gemma-3-270m-it, MadhavRupala/MK1, OpenKing/Gemma-270m-Non-Gated, OpenKing/Gemma-270m-it-non-gated, OpenKing/vualtgemma-1b-non-gated, DogManTC/def-not-gemma3-270m, viktoroo/gemma-3-270m-tools, simaai/gemma3-siglip448, chrisswanson/gemma-3-270m-it-qat-abliterated, coder3101/gemma-3-270m-it-heretic, HashNuke/google-gemma-3-270m-it, unsloth/gemma-3-270m-it-FP8-Dynamic, ZuzeTt/gemma-3-270m-it-heretic, ZuzeTt/gemma-3-270m-it-heretic-GGUF, jncraton/gemma-3-270m-it-ct2-int8, PJMixers-Dev/gemma-3-270m-it-fixed, PJMixers-Dev/gemma-3-270m-fixed, jncraton/gemma-3-270m-ct2-int8, Roman0/gemma-3-270m-it-heretic",
    "models_links": "https://huggingface.co/google/gemma-3-270m, https://huggingface.co/google/gemma-3-270m-it, https://huggingface.co/google/vaultgemma-1b, https://huggingface.co/unsloth/gemma-3-270m-it-GGUF, https://huggingface.co/onnx-community/gemma-3-270m-it-ONNX, https://huggingface.co/unsloth/gemma-3-270m-it, https://huggingface.co/p-e-w/gemma-3-270m-it-heretic, https://huggingface.co/Markr-AI/Gukbap-Gemma3-27B-VL, https://huggingface.co/Markr-AI/Gukbap-Gemma3-12B-VL, https://huggingface.co/DimensionSTP/gemma-3-12b-it-Ko-Reasoning, https://huggingface.co/DimensionSTP/gemma-3-4b-it-Ko-Reasoning, https://huggingface.co/Markr-AI/Gukbap-Gemma3-4B-VL, https://huggingface.co/DimensionSTP/gemma-3-27b-it-Ko-Reasoning, https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-Completion-v0.1-4B-QLoRA, https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-Completion-v0.1-4B, https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-v0.1-4B-QLoRA, https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-v0.1-4B, https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-v0.2-4B-QLoRA, https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-v0.2-4B, https://huggingface.co/Motif-Technologies/Motif-2.6B, https://huggingface.co/stribomon/gemma3-siglip448, https://huggingface.co/PJMixers-Dev/Gemma-3-Starshine-Earthen-v0.4-12B-QLoRA, https://huggingface.co/OpenLLM-Korea/Motif-2.6B, https://huggingface.co/google/gemma-3-270m-qat-q4_0-unquantized, https://huggingface.co/google/gemma-3-270m-it-qat-q4_0-unquantized, https://huggingface.co/unsloth/gemma-3-270m-it-unsloth-bnb-4bit, https://huggingface.co/unsloth/gemma-3-270m-it-bnb-4bit, https://huggingface.co/unsloth/gemma-3-270m-it-qat, https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF, https://huggingface.co/unsloth/gemma-3-270m-it-qat-unsloth-bnb-4bit, https://huggingface.co/unsloth/gemma-3-270m-it-qat-bnb-4bit, https://huggingface.co/unsloth/gemma-3-270m, https://huggingface.co/unsloth/gemma-3-270m-unsloth-bnb-4bit, https://huggingface.co/unsloth/gemma-3-270m-bnb-4bit, https://huggingface.co/unsloth/gemma-3-270m-it-torchao-FP8, https://huggingface.co/litert-community/gemma-3-270m-it, https://huggingface.co/lucyknada/google_gemma-3-270m-exl3, https://huggingface.co/lucyknada/google_gemma-3-270m-it-exl3, https://huggingface.co/Mungert/gemma-3-270m-it-GGUF, https://huggingface.co/pixasocial/survival-uncensored-gemma-270m, https://huggingface.co/yaya-sy/BaldGemma3-140m, https://huggingface.co/Hasya018/gemma-3-270m-multihopping, https://huggingface.co/d-s-b/Router, https://huggingface.co/mahwizzzz/tinygemma-Urdu, https://huggingface.co/naveennuwantha/NUXARA-AI-V1, https://huggingface.co/Hirun9/gemma-3-270m, https://huggingface.co/TamWaiban/gemma-3-270m-int4, https://huggingface.co/TamWaiban/gemma-3-270m-autoquant, https://huggingface.co/Sweelol-ai/gemma3-270m-dolly-teacher, https://huggingface.co/Sweelol-ai/gemma3-270m-pruned-baseline-50pc, https://huggingface.co/Sweelol-ai/kd-gemma3-pruned-dolly, https://huggingface.co/Sweelol-ai/lora-gemma3-270m-dolly, https://huggingface.co/Sweelol-ai/pt-gemma3-270m-dolly, https://huggingface.co/Sweelol-ai/finetuned-pruned-gemma3-270m-dolly, https://huggingface.co/sweelol/gemma3-270m-dolly-teacher, https://huggingface.co/sweelol/gemma3-270m-pruned-baseline-50pc, https://huggingface.co/sweelol/kd-gemma3-pruned-dolly, https://huggingface.co/sweelol/lora-gemma3-270m-dolly, https://huggingface.co/sweelol/pt-gemma3-270m-dolly, https://huggingface.co/sweelol/finetuned-pruned-gemma3-270m-dolly, https://huggingface.co/sweelol/gemma3-270m-pruned-base, https://huggingface.co/QuantFactory/gemma-3-270m-it-GGUF, https://huggingface.co/QuantFactory/gemma-3-270m-GGUF, https://huggingface.co/Erland/gemma-3-270m-it, https://huggingface.co/onnx-community/vaultgemma-1b-ONNX, https://huggingface.co/cblbai/clm-1-small-beta, https://huggingface.co/smartvest-llc/gemma-3-270m-it, https://huggingface.co/Sci-fi-vy/gemma-3-270m-it-GGUF, https://huggingface.co/sinhashubham/gemma-3-270m-it, https://huggingface.co/Najin06/tacha_ft_gemma-3-270m-it, https://huggingface.co/MadhavRupala/MK1, https://huggingface.co/OpenKing/Gemma-270m-Non-Gated, https://huggingface.co/OpenKing/Gemma-270m-it-non-gated, https://huggingface.co/OpenKing/vualtgemma-1b-non-gated, https://huggingface.co/DogManTC/def-not-gemma3-270m, https://huggingface.co/viktoroo/gemma-3-270m-tools, https://huggingface.co/simaai/gemma3-siglip448, https://huggingface.co/chrisswanson/gemma-3-270m-it-qat-abliterated, https://huggingface.co/coder3101/gemma-3-270m-it-heretic, https://huggingface.co/HashNuke/google-gemma-3-270m-it, https://huggingface.co/unsloth/gemma-3-270m-it-FP8-Dynamic, https://huggingface.co/ZuzeTt/gemma-3-270m-it-heretic, https://huggingface.co/ZuzeTt/gemma-3-270m-it-heretic-GGUF, https://huggingface.co/jncraton/gemma-3-270m-it-ct2-int8, https://huggingface.co/PJMixers-Dev/gemma-3-270m-it-fixed, https://huggingface.co/PJMixers-Dev/gemma-3-270m-fixed, https://huggingface.co/jncraton/gemma-3-270m-ct2-int8, https://huggingface.co/Roman0/gemma-3-270m-it-heretic",
    "models_detailed": "[{\"name\": \"google/gemma-3-270m\", \"link\": \"https://huggingface.co/google/gemma-3-270m\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"google/gemma-3-270m-it\", \"link\": \"https://huggingface.co/google/gemma-3-270m-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"google/vaultgemma-1b\", \"link\": \"https://huggingface.co/google/vaultgemma-1b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 12\"}, {\"name\": \"unsloth/gemma-3-270m-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 15\"}, {\"name\": \"onnx-community/gemma-3-270m-it-ONNX\", \"link\": \"https://huggingface.co/onnx-community/gemma-3-270m-it-ONNX\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"20 days ago\"}, {\"name\": \"unsloth/gemma-3-270m-it\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"p-e-w/gemma-3-270m-it-heretic\", \"link\": \"https://huggingface.co/p-e-w/gemma-3-270m-it-heretic\", \"task\": \"Text Generation\", \"likes\": \"724\", \"downloads\": \"\", \"updated\": \"Nov 16\"}, {\"name\": \"Markr-AI/Gukbap-Gemma3-27B-VL\", \"link\": \"https://huggingface.co/Markr-AI/Gukbap-Gemma3-27B-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"Markr-AI/Gukbap-Gemma3-12B-VL\", \"link\": \"https://huggingface.co/Markr-AI/Gukbap-Gemma3-12B-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"DimensionSTP/gemma-3-12b-it-Ko-Reasoning\", \"link\": \"https://huggingface.co/DimensionSTP/gemma-3-12b-it-Ko-Reasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"DimensionSTP/gemma-3-4b-it-Ko-Reasoning\", \"link\": \"https://huggingface.co/DimensionSTP/gemma-3-4b-it-Ko-Reasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"Markr-AI/Gukbap-Gemma3-4B-VL\", \"link\": \"https://huggingface.co/Markr-AI/Gukbap-Gemma3-4B-VL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"DimensionSTP/gemma-3-27b-it-Ko-Reasoning\", \"link\": \"https://huggingface.co/DimensionSTP/gemma-3-27b-it-Ko-Reasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"PJMixers-Dev/Gemma-3-Earthen-Completion-v0.1-4B-QLoRA\", \"link\": \"https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-Completion-v0.1-4B-QLoRA\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"PJMixers-Dev/Gemma-3-Earthen-Completion-v0.1-4B\", \"link\": \"https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-Completion-v0.1-4B\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"May 21\"}, {\"name\": \"PJMixers-Dev/Gemma-3-Earthen-v0.1-4B-QLoRA\", \"link\": \"https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-v0.1-4B-QLoRA\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"PJMixers-Dev/Gemma-3-Earthen-v0.1-4B\", \"link\": \"https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-v0.1-4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"PJMixers-Dev/Gemma-3-Earthen-v0.2-4B-QLoRA\", \"link\": \"https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-v0.2-4B-QLoRA\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"PJMixers-Dev/Gemma-3-Earthen-v0.2-4B\", \"link\": \"https://huggingface.co/PJMixers-Dev/Gemma-3-Earthen-v0.2-4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"Motif-Technologies/Motif-2.6B\", \"link\": \"https://huggingface.co/Motif-Technologies/Motif-2.6B\", \"task\": \"Text Generation\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"stribomon/gemma3-siglip448\", \"link\": \"https://huggingface.co/stribomon/gemma3-siglip448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\"}, {\"name\": \"PJMixers-Dev/Gemma-3-Starshine-Earthen-v0.4-12B-QLoRA\", \"link\": \"https://huggingface.co/PJMixers-Dev/Gemma-3-Starshine-Earthen-v0.4-12B-QLoRA\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 12\"}, {\"name\": \"OpenLLM-Korea/Motif-2.6B\", \"link\": \"https://huggingface.co/OpenLLM-Korea/Motif-2.6B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"google/gemma-3-270m-qat-q4_0-unquantized\", \"link\": \"https://huggingface.co/google/gemma-3-270m-qat-q4_0-unquantized\", \"task\": \"Text Generation\", \"likes\": \"273\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"google/gemma-3-270m-it-qat-q4_0-unquantized\", \"link\": \"https://huggingface.co/google/gemma-3-270m-it-qat-q4_0-unquantized\", \"task\": \"Text Generation\", \"likes\": \"349\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m-it-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m-it-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m-it-qat\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-qat\", \"task\": \"Text Generation\", \"likes\": \"410\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m-it-qat-GGUF\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-qat-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 15\"}, {\"name\": \"unsloth/gemma-3-270m-it-qat-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-qat-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"330\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m-it-qat-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-qat-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 17\"}, {\"name\": \"unsloth/gemma-3-270m-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m-it-torchao-FP8\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-torchao-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"litert-community/gemma-3-270m-it\", \"link\": \"https://huggingface.co/litert-community/gemma-3-270m-it\", \"task\": \"Text Generation\", \"likes\": \"734\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"lucyknada/google_gemma-3-270m-exl3\", \"link\": \"https://huggingface.co/lucyknada/google_gemma-3-270m-exl3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"lucyknada/google_gemma-3-270m-it-exl3\", \"link\": \"https://huggingface.co/lucyknada/google_gemma-3-270m-it-exl3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"Mungert/gemma-3-270m-it-GGUF\", \"link\": \"https://huggingface.co/Mungert/gemma-3-270m-it-GGUF\", \"task\": \"Text Generation\", \"likes\": \"310\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"pixasocial/survival-uncensored-gemma-270m\", \"link\": \"https://huggingface.co/pixasocial/survival-uncensored-gemma-270m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 16\"}, {\"name\": \"yaya-sy/BaldGemma3-140m\", \"link\": \"https://huggingface.co/yaya-sy/BaldGemma3-140m\", \"task\": \"Text Generation\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Aug 15\"}, {\"name\": \"Hasya018/gemma-3-270m-multihopping\", \"link\": \"https://huggingface.co/Hasya018/gemma-3-270m-multihopping\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Aug 16\"}, {\"name\": \"d-s-b/Router\", \"link\": \"https://huggingface.co/d-s-b/Router\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Aug 17\"}, {\"name\": \"mahwizzzz/tinygemma-Urdu\", \"link\": \"https://huggingface.co/mahwizzzz/tinygemma-Urdu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"naveennuwantha/NUXARA-AI-V1\", \"link\": \"https://huggingface.co/naveennuwantha/NUXARA-AI-V1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 21\"}, {\"name\": \"Hirun9/gemma-3-270m\", \"link\": \"https://huggingface.co/Hirun9/gemma-3-270m\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 21\"}, {\"name\": \"TamWaiban/gemma-3-270m-int4\", \"link\": \"https://huggingface.co/TamWaiban/gemma-3-270m-int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 23\"}, {\"name\": \"TamWaiban/gemma-3-270m-autoquant\", \"link\": \"https://huggingface.co/TamWaiban/gemma-3-270m-autoquant\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 23\"}, {\"name\": \"Sweelol-ai/gemma3-270m-dolly-teacher\", \"link\": \"https://huggingface.co/Sweelol-ai/gemma3-270m-dolly-teacher\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Sweelol-ai/gemma3-270m-pruned-baseline-50pc\", \"link\": \"https://huggingface.co/Sweelol-ai/gemma3-270m-pruned-baseline-50pc\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Sweelol-ai/kd-gemma3-pruned-dolly\", \"link\": \"https://huggingface.co/Sweelol-ai/kd-gemma3-pruned-dolly\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Sweelol-ai/lora-gemma3-270m-dolly\", \"link\": \"https://huggingface.co/Sweelol-ai/lora-gemma3-270m-dolly\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Sweelol-ai/pt-gemma3-270m-dolly\", \"link\": \"https://huggingface.co/Sweelol-ai/pt-gemma3-270m-dolly\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"Sweelol-ai/finetuned-pruned-gemma3-270m-dolly\", \"link\": \"https://huggingface.co/Sweelol-ai/finetuned-pruned-gemma3-270m-dolly\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"sweelol/gemma3-270m-dolly-teacher\", \"link\": \"https://huggingface.co/sweelol/gemma3-270m-dolly-teacher\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"sweelol/gemma3-270m-pruned-baseline-50pc\", \"link\": \"https://huggingface.co/sweelol/gemma3-270m-pruned-baseline-50pc\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"sweelol/kd-gemma3-pruned-dolly\", \"link\": \"https://huggingface.co/sweelol/kd-gemma3-pruned-dolly\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"sweelol/lora-gemma3-270m-dolly\", \"link\": \"https://huggingface.co/sweelol/lora-gemma3-270m-dolly\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"sweelol/pt-gemma3-270m-dolly\", \"link\": \"https://huggingface.co/sweelol/pt-gemma3-270m-dolly\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"sweelol/finetuned-pruned-gemma3-270m-dolly\", \"link\": \"https://huggingface.co/sweelol/finetuned-pruned-gemma3-270m-dolly\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"sweelol/gemma3-270m-pruned-base\", \"link\": \"https://huggingface.co/sweelol/gemma3-270m-pruned-base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"QuantFactory/gemma-3-270m-it-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/gemma-3-270m-it-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"QuantFactory/gemma-3-270m-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/gemma-3-270m-GGUF\", \"task\": \"Text Generation\", \"likes\": \"780\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"Erland/gemma-3-270m-it\", \"link\": \"https://huggingface.co/Erland/gemma-3-270m-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 10\"}, {\"name\": \"onnx-community/vaultgemma-1b-ONNX\", \"link\": \"https://huggingface.co/onnx-community/vaultgemma-1b-ONNX\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 12\"}, {\"name\": \"cblbai/clm-1-small-beta\", \"link\": \"https://huggingface.co/cblbai/clm-1-small-beta\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 13\"}, {\"name\": \"smartvest-llc/gemma-3-270m-it\", \"link\": \"https://huggingface.co/smartvest-llc/gemma-3-270m-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 15\"}, {\"name\": \"Sci-fi-vy/gemma-3-270m-it-GGUF\", \"link\": \"https://huggingface.co/Sci-fi-vy/gemma-3-270m-it-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"sinhashubham/gemma-3-270m-it\", \"link\": \"https://huggingface.co/sinhashubham/gemma-3-270m-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"Najin06/tacha_ft_gemma-3-270m-it\", \"link\": \"https://huggingface.co/Najin06/tacha_ft_gemma-3-270m-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 7\"}, {\"name\": \"MadhavRupala/MK1\", \"link\": \"https://huggingface.co/MadhavRupala/MK1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"OpenKing/Gemma-270m-Non-Gated\", \"link\": \"https://huggingface.co/OpenKing/Gemma-270m-Non-Gated\", \"task\": \"Text Generation\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"OpenKing/Gemma-270m-it-non-gated\", \"link\": \"https://huggingface.co/OpenKing/Gemma-270m-it-non-gated\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"OpenKing/vualtgemma-1b-non-gated\", \"link\": \"https://huggingface.co/OpenKing/vualtgemma-1b-non-gated\", \"task\": \"Text Generation\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"DogManTC/def-not-gemma3-270m\", \"link\": \"https://huggingface.co/DogManTC/def-not-gemma3-270m\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"viktoroo/gemma-3-270m-tools\", \"link\": \"https://huggingface.co/viktoroo/gemma-3-270m-tools\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\"}, {\"name\": \"simaai/gemma3-siglip448\", \"link\": \"https://huggingface.co/simaai/gemma3-siglip448\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\"}, {\"name\": \"chrisswanson/gemma-3-270m-it-qat-abliterated\", \"link\": \"https://huggingface.co/chrisswanson/gemma-3-270m-it-qat-abliterated\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Nov 20\"}, {\"name\": \"coder3101/gemma-3-270m-it-heretic\", \"link\": \"https://huggingface.co/coder3101/gemma-3-270m-it-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"HashNuke/google-gemma-3-270m-it\", \"link\": \"https://huggingface.co/HashNuke/google-gemma-3-270m-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"unsloth/gemma-3-270m-it-FP8-Dynamic\", \"link\": \"https://huggingface.co/unsloth/gemma-3-270m-it-FP8-Dynamic\", \"task\": \"Text Generation\", \"likes\": \"310\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"ZuzeTt/gemma-3-270m-it-heretic\", \"link\": \"https://huggingface.co/ZuzeTt/gemma-3-270m-it-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"ZuzeTt/gemma-3-270m-it-heretic-GGUF\", \"link\": \"https://huggingface.co/ZuzeTt/gemma-3-270m-it-heretic-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"jncraton/gemma-3-270m-it-ct2-int8\", \"link\": \"https://huggingface.co/jncraton/gemma-3-270m-it-ct2-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"PJMixers-Dev/gemma-3-270m-it-fixed\", \"link\": \"https://huggingface.co/PJMixers-Dev/gemma-3-270m-it-fixed\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\"}, {\"name\": \"PJMixers-Dev/gemma-3-270m-fixed\", \"link\": \"https://huggingface.co/PJMixers-Dev/gemma-3-270m-fixed\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\"}, {\"name\": \"jncraton/gemma-3-270m-ct2-int8\", \"link\": \"https://huggingface.co/jncraton/gemma-3-270m-ct2-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"2 days ago\"}, {\"name\": \"Roman0/gemma-3-270m-it-heretic\", \"link\": \"https://huggingface.co/Roman0/gemma-3-270m-it-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"10 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.20215",
    "first_seen_date": "2025-03-27",
    "title": "Qwen2.5-Omni Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.20215Qwen2.5-Omni Technical ReportPublished on Mar 26\u00b7Submitted byYuanjunLvon Mar 27#1 Paper of the dayUpvote168+160Authors:Jin Xu,Zhifang Guo,Jinzheng He,Hangrui Hu,Ting He,Shuai Bai,Keqin Chen,Jialin Wang,Yang Fan,Kai Dang,Bin Zhang,Xiong Wang,Yunfei Chu,Junyang LinAbstractQwen2.5-Omni is a multimodal model that processes text, images, audio, and video in a streaming fashion and generates text and speech using a dual-track architecture, achieving state-of-the-art performance on multimodal benchmarks.AI-generated summaryIn this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize ablock-wise processingapproach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we proposeThinker-Talker architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is adual-track autoregressive modelthat directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce asliding-window DiTthat restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art perf",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/QwenLM/Qwen2.5-Omni",
    "hf_paper_url": "https://huggingface.co/papers/2503.20215",
    "arxiv_url": "https://arxiv.org/abs/2503.20215",
    "num_models": 17,
    "models_list": "Qwen/Qwen2.5-Omni-7B, Qwen/Qwen2.5-Omni-3B, unsloth/Qwen2.5-Omni-7B-GGUF, unsloth/Qwen2.5-Omni-3B-GGUF, Abdoul-AI/Qwen2.5-Omni-7B-GGUF, imkebe/Qwen2.5-Omni-7B-rk3588-1.2.0, KE-Team/Ke-Omni-R, Qwen/Qwen2.5-Omni-7B-AWQ, Qwen/Qwen2.5-Omni-7B-GPTQ-Int4, KE-Team/Ke-Omni-R-3B, unsloth/Qwen2.5-Omni-7B, unsloth/Qwen2.5-Omni-3B, Sci-fi-vy/Qwen2.5-Omni-7B-GGUF, Mungert/Qwen2.5-Omni-3B-GGUF, Mungert/Qwen2.5-Omni-7B-GGUF, chaitnya26/Qwen2.5-Omni-3B-Fork, chaitnya26/Qwen2.5-Omni-7B-fork",
    "models_links": "https://huggingface.co/Qwen/Qwen2.5-Omni-7B, https://huggingface.co/Qwen/Qwen2.5-Omni-3B, https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF, https://huggingface.co/unsloth/Qwen2.5-Omni-3B-GGUF, https://huggingface.co/Abdoul-AI/Qwen2.5-Omni-7B-GGUF, https://huggingface.co/imkebe/Qwen2.5-Omni-7B-rk3588-1.2.0, https://huggingface.co/KE-Team/Ke-Omni-R, https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ, https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4, https://huggingface.co/KE-Team/Ke-Omni-R-3B, https://huggingface.co/unsloth/Qwen2.5-Omni-7B, https://huggingface.co/unsloth/Qwen2.5-Omni-3B, https://huggingface.co/Sci-fi-vy/Qwen2.5-Omni-7B-GGUF, https://huggingface.co/Mungert/Qwen2.5-Omni-3B-GGUF, https://huggingface.co/Mungert/Qwen2.5-Omni-7B-GGUF, https://huggingface.co/chaitnya26/Qwen2.5-Omni-3B-Fork, https://huggingface.co/chaitnya26/Qwen2.5-Omni-7B-fork",
    "models_detailed": "[{\"name\": \"Qwen/Qwen2.5-Omni-7B\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-Omni-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"Qwen/Qwen2.5-Omni-3B\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-Omni-3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"unsloth/Qwen2.5-Omni-7B-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-Omni-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 28\"}, {\"name\": \"unsloth/Qwen2.5-Omni-3B-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-Omni-3B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 28\"}, {\"name\": \"Abdoul-AI/Qwen2.5-Omni-7B-GGUF\", \"link\": \"https://huggingface.co/Abdoul-AI/Qwen2.5-Omni-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"imkebe/Qwen2.5-Omni-7B-rk3588-1.2.0\", \"link\": \"https://huggingface.co/imkebe/Qwen2.5-Omni-7B-rk3588-1.2.0\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 15\"}, {\"name\": \"KE-Team/Ke-Omni-R\", \"link\": \"https://huggingface.co/KE-Team/Ke-Omni-R\", \"task\": \"\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"Jun 12\"}, {\"name\": \"Qwen/Qwen2.5-Omni-7B-AWQ\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"Qwen/Qwen2.5-Omni-7B-GPTQ-Int4\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4\", \"task\": \"\", \"likes\": \"601\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"KE-Team/Ke-Omni-R-3B\", \"link\": \"https://huggingface.co/KE-Team/Ke-Omni-R-3B\", \"task\": \"\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Jun 16\"}, {\"name\": \"unsloth/Qwen2.5-Omni-7B\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-Omni-7B\", \"task\": \"\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"May 28\"}, {\"name\": \"unsloth/Qwen2.5-Omni-3B\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-Omni-3B\", \"task\": \"\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"May 28\"}, {\"name\": \"Sci-fi-vy/Qwen2.5-Omni-7B-GGUF\", \"link\": \"https://huggingface.co/Sci-fi-vy/Qwen2.5-Omni-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 31\"}, {\"name\": \"Mungert/Qwen2.5-Omni-3B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen2.5-Omni-3B-GGUF\", \"task\": \"\", \"likes\": \"615\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/Qwen2.5-Omni-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen2.5-Omni-7B-GGUF\", \"task\": \"\", \"likes\": \"466\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"chaitnya26/Qwen2.5-Omni-3B-Fork\", \"link\": \"https://huggingface.co/chaitnya26/Qwen2.5-Omni-3B-Fork\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"chaitnya26/Qwen2.5-Omni-7B-fork\", \"link\": \"https://huggingface.co/chaitnya26/Qwen2.5-Omni-7B-fork\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.20314",
    "first_seen_date": "2025-03-27",
    "title": "Wan: Open and Advanced Large-Scale Video Generative Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.20314Wan: Open and Advanced Large-Scale Video Generative ModelsPublished on Mar 26\u00b7Submitted byAKon Mar 27Upvote56+48Authors:WanTeam,Ang Wang,Baole Ai,Bin Wen,Chaojie Mao,Chen-Wei Xie,Di Chen,Feiwu Yu,Haiming Zhao,Jianxiao Yang,Jianyuan Zeng,Jiayu Wang,Jingfeng Zhang,Jingren Zhou,Jinkai Wang,Jixuan Chen,Kai Zhu,Kang Zhao,Keyu Yan,Lianghua Huang,Mengyang Feng,Ningyi Zhang+40 authorsAbstractWan, a comprehensive suite of video foundation models built on the diffusion transformer paradigm, advannces video generation by introducing a novel VAE, scalable pre-training strategies, and large-scale data curation, offering superior performance and versatility across various applications with both large and efficient models.AI-generated summaryThis report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries ofvideo generation. Built upon the\nmainstreamdiffusion transformerparadigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novelVAE,scalable pre-trainingstrategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws ofvideo generationwith respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, includingimage-to-vi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/Wan-Video/Wan2.1",
    "hf_paper_url": "https://huggingface.co/papers/2503.20314",
    "arxiv_url": "https://arxiv.org/abs/2503.20314",
    "num_models": 56,
    "models_list": "Wan-AI/Wan2.2-Animate-14B, Wan-AI/Wan2.2-I2V-A14B, Wan-AI/Wan2.1-VACE-14B, Wan-AI/Wan2.2-TI2V-5B, bullerwins/Wan2.2-I2V-A14B-GGUF, Wan-AI/Wan2.2-T2V-A14B, Wan-AI/Wan2.2-S2V-14B, Wan-AI/Wan2.2-TI2V-5B-Diffusers, Wan-AI/Wan2.2-T2V-A14B-Diffusers, Wan-AI/Wan2.1-FLF2V-14B-720P, Wan-AI/Wan2.1-FLF2V-14B-720P-diffusers, Wan-AI/Wan2.1-VACE-1.3B, bullerwins/Wan2.2-T2V-A14B-GGUF, rand0nmr/SFWan2.2-T2V-A14B-Diffusers, wan-community/Wan2.1-FLF2V-14B-720P, Wan-AI/Wan2.1-VACE-14B-diffusers, Wan-AI/Wan2.1-VACE-1.3B-diffusers, gaalos/Wan2.1-I2V-14B-720P-Diffusers-scaled, wavespeed/Wan2.1-VACE-14B-bf16, ai-forever/Wan2.1-T2V-14B-NABLA-0.7, ai-forever/Wan2.1-T2V-14B-NABLA-0.6-STA-11-3-3, ai-forever/Wan2.1-T2V-14B-NABLA-0.5-STA-11-5-5, Wan-AI/Wan2.2-I2V-A14B-Diffusers, inference-sh/Wan2.2-TI2V-5B-Diffusers, Runware/Wan2.2-TI2V-5B, lopho/Wan2.2-T2V-A14B-Diffusers_nf4_transformer, lopho/Wan2.2-T2V-A14B-Diffusers_nf4_transformer_2, lopho/Wan2.2-T2V-A14B-Diffusers_nf4_text_encoder, lopho/Wan2.2-T2V-A14B-Diffusers_bf16_vae, lopho/Wan2.2-T2V-A14B-Diffusers_bf16_transformer, lopho/Wan2.2-T2V-A14B-Diffusers_bf16_transformer_2, lopho/Wan2.2-T2V-A14B-Diffusers_bf16_text_encoder, lopho/Wan2.2-T2V-A14B-Diffusers_fp32_text_encoder, lopho/Wan2.2-T2V-A14B-Diffusers_fp32_vae, lopho/Wan2.2-T2V-A14B-Diffusers_fp32_transformer, lopho/Wan2.2-T2V-A14B-Diffusers_fp32_transformer_2, lopho/Wan2.2-T2V-A14B-Diffusers_tokenizer, lopho/Wan2.2-T2V-A14B-Diffusers_scheduler, lopho/Wan2.2-T2V-A14B-Diffusers_nf4, magespace/Wan2.2-I2V-A14B-Lightning-Diffusers, lopho/Wan2.2-I2V-A14B-Diffusers_nf4_transformer, lopho/Wan2.2-I2V-A14B-Diffusers_nf4_transformer_2, lopho/Wan2.2-I2V-A14B-Diffusers_nf4, chaitnya26/Wan2.2-TI2V-5B-fork, backups/Wan2.2-S2V-14B, weizhou03/SFWan2.2-T2V-A14B-Diffusers, jobs-git/Wan2.2-I2V-A14B, jobs-git/Wan2.2-I2V-A14B-Diffusers, jobs-git/Wan2.2-T2V-A14B, jobs-git/Wan2.2-T2V-A14B-Diffusers, aiplexdeveloper/Wan2.2-Animate-14B, rand0nmr/SFWan2.1-T2V-A1.3B-Diffusers, wangkanai/wan22-fp16-i2v-gguf, rand0nmr/SFWan2.2-T2V-14B-Diffusers, IRMChen/Wan2.2-Fun-A14B-Control-Diffusers, serizard1005/Wan-2.2-Animate-14B",
    "models_links": "https://huggingface.co/Wan-AI/Wan2.2-Animate-14B, https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B, https://huggingface.co/Wan-AI/Wan2.1-VACE-14B, https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B, https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF, https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B, https://huggingface.co/Wan-AI/Wan2.2-S2V-14B, https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers, https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers, https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P, https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P-diffusers, https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B, https://huggingface.co/bullerwins/Wan2.2-T2V-A14B-GGUF, https://huggingface.co/rand0nmr/SFWan2.2-T2V-A14B-Diffusers, https://huggingface.co/wan-community/Wan2.1-FLF2V-14B-720P, https://huggingface.co/Wan-AI/Wan2.1-VACE-14B-diffusers, https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B-diffusers, https://huggingface.co/gaalos/Wan2.1-I2V-14B-720P-Diffusers-scaled, https://huggingface.co/wavespeed/Wan2.1-VACE-14B-bf16, https://huggingface.co/ai-forever/Wan2.1-T2V-14B-NABLA-0.7, https://huggingface.co/ai-forever/Wan2.1-T2V-14B-NABLA-0.6-STA-11-3-3, https://huggingface.co/ai-forever/Wan2.1-T2V-14B-NABLA-0.5-STA-11-5-5, https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers, https://huggingface.co/inference-sh/Wan2.2-TI2V-5B-Diffusers, https://huggingface.co/Runware/Wan2.2-TI2V-5B, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_nf4_transformer, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_nf4_transformer_2, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_nf4_text_encoder, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_bf16_vae, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_bf16_transformer, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_bf16_transformer_2, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_bf16_text_encoder, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_fp32_text_encoder, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_fp32_vae, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_fp32_transformer, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_fp32_transformer_2, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_tokenizer, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_scheduler, https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_nf4, https://huggingface.co/magespace/Wan2.2-I2V-A14B-Lightning-Diffusers, https://huggingface.co/lopho/Wan2.2-I2V-A14B-Diffusers_nf4_transformer, https://huggingface.co/lopho/Wan2.2-I2V-A14B-Diffusers_nf4_transformer_2, https://huggingface.co/lopho/Wan2.2-I2V-A14B-Diffusers_nf4, https://huggingface.co/chaitnya26/Wan2.2-TI2V-5B-fork, https://huggingface.co/backups/Wan2.2-S2V-14B, https://huggingface.co/weizhou03/SFWan2.2-T2V-A14B-Diffusers, https://huggingface.co/jobs-git/Wan2.2-I2V-A14B, https://huggingface.co/jobs-git/Wan2.2-I2V-A14B-Diffusers, https://huggingface.co/jobs-git/Wan2.2-T2V-A14B, https://huggingface.co/jobs-git/Wan2.2-T2V-A14B-Diffusers, https://huggingface.co/aiplexdeveloper/Wan2.2-Animate-14B, https://huggingface.co/rand0nmr/SFWan2.1-T2V-A1.3B-Diffusers, https://huggingface.co/wangkanai/wan22-fp16-i2v-gguf, https://huggingface.co/rand0nmr/SFWan2.2-T2V-14B-Diffusers, https://huggingface.co/IRMChen/Wan2.2-Fun-A14B-Control-Diffusers, https://huggingface.co/serizard1005/Wan-2.2-Animate-14B",
    "models_detailed": "[{\"name\": \"Wan-AI/Wan2.2-Animate-14B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-Animate-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"Wan-AI/Wan2.2-I2V-A14B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"Wan-AI/Wan2.1-VACE-14B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-VACE-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"Wan-AI/Wan2.2-TI2V-5B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"bullerwins/Wan2.2-I2V-A14B-GGUF\", \"link\": \"https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"Wan-AI/Wan2.2-T2V-A14B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"Wan-AI/Wan2.2-S2V-14B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-S2V-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Wan-AI/Wan2.2-TI2V-5B-Diffusers\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"Wan-AI/Wan2.2-T2V-A14B-Diffusers\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"Wan-AI/Wan2.1-FLF2V-14B-720P\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 17\"}, {\"name\": \"Wan-AI/Wan2.1-FLF2V-14B-720P-diffusers\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P-diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 22\"}, {\"name\": \"Wan-AI/Wan2.1-VACE-1.3B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"bullerwins/Wan2.2-T2V-A14B-GGUF\", \"link\": \"https://huggingface.co/bullerwins/Wan2.2-T2V-A14B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"rand0nmr/SFWan2.2-T2V-A14B-Diffusers\", \"link\": \"https://huggingface.co/rand0nmr/SFWan2.2-T2V-A14B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"wan-community/Wan2.1-FLF2V-14B-720P\", \"link\": \"https://huggingface.co/wan-community/Wan2.1-FLF2V-14B-720P\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 17\"}, {\"name\": \"Wan-AI/Wan2.1-VACE-14B-diffusers\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-VACE-14B-diffusers\", \"task\": \"\", \"likes\": \"421\", \"downloads\": \"\", \"updated\": \"Jun 6\"}, {\"name\": \"Wan-AI/Wan2.1-VACE-1.3B-diffusers\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B-diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\"}, {\"name\": \"gaalos/Wan2.1-I2V-14B-720P-Diffusers-scaled\", \"link\": \"https://huggingface.co/gaalos/Wan2.1-I2V-14B-720P-Diffusers-scaled\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"wavespeed/Wan2.1-VACE-14B-bf16\", \"link\": \"https://huggingface.co/wavespeed/Wan2.1-VACE-14B-bf16\", \"task\": \"\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Jun 16\"}, {\"name\": \"ai-forever/Wan2.1-T2V-14B-NABLA-0.7\", \"link\": \"https://huggingface.co/ai-forever/Wan2.1-T2V-14B-NABLA-0.7\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"ai-forever/Wan2.1-T2V-14B-NABLA-0.6-STA-11-3-3\", \"link\": \"https://huggingface.co/ai-forever/Wan2.1-T2V-14B-NABLA-0.6-STA-11-3-3\", \"task\": \"\", \"likes\": \"88\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"ai-forever/Wan2.1-T2V-14B-NABLA-0.5-STA-11-5-5\", \"link\": \"https://huggingface.co/ai-forever/Wan2.1-T2V-14B-NABLA-0.5-STA-11-5-5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"Wan-AI/Wan2.2-I2V-A14B-Diffusers\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"inference-sh/Wan2.2-TI2V-5B-Diffusers\", \"link\": \"https://huggingface.co/inference-sh/Wan2.2-TI2V-5B-Diffusers\", \"task\": \"\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"Runware/Wan2.2-TI2V-5B\", \"link\": \"https://huggingface.co/Runware/Wan2.2-TI2V-5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 23\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_nf4_transformer\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_nf4_transformer\", \"task\": \"\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Aug 10\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_nf4_transformer_2\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_nf4_transformer_2\", \"task\": \"\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Aug 10\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_nf4_text_encoder\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_nf4_text_encoder\", \"task\": \"\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_bf16_vae\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_bf16_vae\", \"task\": \"\", \"likes\": \"164\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_bf16_transformer\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_bf16_transformer\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_bf16_transformer_2\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_bf16_transformer_2\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_bf16_text_encoder\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_bf16_text_encoder\", \"task\": \"\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_fp32_text_encoder\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_fp32_text_encoder\", \"task\": \"\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Aug 12\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_fp32_vae\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_fp32_vae\", \"task\": \"\", \"likes\": \"80\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_fp32_transformer\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_fp32_transformer\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_fp32_transformer_2\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_fp32_transformer_2\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_tokenizer\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_tokenizer\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_scheduler\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_scheduler\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 9\"}, {\"name\": \"lopho/Wan2.2-T2V-A14B-Diffusers_nf4\", \"link\": \"https://huggingface.co/lopho/Wan2.2-T2V-A14B-Diffusers_nf4\", \"task\": \"\", \"likes\": \"26\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\", \"link\": \"https://huggingface.co/magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 15\"}, {\"name\": \"lopho/Wan2.2-I2V-A14B-Diffusers_nf4_transformer\", \"link\": \"https://huggingface.co/lopho/Wan2.2-I2V-A14B-Diffusers_nf4_transformer\", \"task\": \"\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"lopho/Wan2.2-I2V-A14B-Diffusers_nf4_transformer_2\", \"link\": \"https://huggingface.co/lopho/Wan2.2-I2V-A14B-Diffusers_nf4_transformer_2\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"lopho/Wan2.2-I2V-A14B-Diffusers_nf4\", \"link\": \"https://huggingface.co/lopho/Wan2.2-I2V-A14B-Diffusers_nf4\", \"task\": \"\", \"likes\": \"26\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"chaitnya26/Wan2.2-TI2V-5B-fork\", \"link\": \"https://huggingface.co/chaitnya26/Wan2.2-TI2V-5B-fork\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"backups/Wan2.2-S2V-14B\", \"link\": \"https://huggingface.co/backups/Wan2.2-S2V-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"weizhou03/SFWan2.2-T2V-A14B-Diffusers\", \"link\": \"https://huggingface.co/weizhou03/SFWan2.2-T2V-A14B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"jobs-git/Wan2.2-I2V-A14B\", \"link\": \"https://huggingface.co/jobs-git/Wan2.2-I2V-A14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"jobs-git/Wan2.2-I2V-A14B-Diffusers\", \"link\": \"https://huggingface.co/jobs-git/Wan2.2-I2V-A14B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"jobs-git/Wan2.2-T2V-A14B\", \"link\": \"https://huggingface.co/jobs-git/Wan2.2-T2V-A14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"jobs-git/Wan2.2-T2V-A14B-Diffusers\", \"link\": \"https://huggingface.co/jobs-git/Wan2.2-T2V-A14B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"aiplexdeveloper/Wan2.2-Animate-14B\", \"link\": \"https://huggingface.co/aiplexdeveloper/Wan2.2-Animate-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 6\"}, {\"name\": \"rand0nmr/SFWan2.1-T2V-A1.3B-Diffusers\", \"link\": \"https://huggingface.co/rand0nmr/SFWan2.1-T2V-A1.3B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"wangkanai/wan22-fp16-i2v-gguf\", \"link\": \"https://huggingface.co/wangkanai/wan22-fp16-i2v-gguf\", \"task\": \"\", \"likes\": \"279\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"rand0nmr/SFWan2.2-T2V-14B-Diffusers\", \"link\": \"https://huggingface.co/rand0nmr/SFWan2.2-T2V-14B-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"IRMChen/Wan2.2-Fun-A14B-Control-Diffusers\", \"link\": \"https://huggingface.co/IRMChen/Wan2.2-Fun-A14B-Control-Diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"serizard1005/Wan-2.2-Animate-14B\", \"link\": \"https://huggingface.co/serizard1005/Wan-2.2-Animate-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 10\"}]",
    "num_datasets": 1,
    "datasets_list": "madebyollin/movirec",
    "datasets_links": "https://huggingface.co/datasets/madebyollin/movirec",
    "datasets_detailed": "[{\"name\": \"madebyollin/movirec\", \"link\": \"https://huggingface.co/datasets/madebyollin/movirec\", \"task\": \"\", \"likes\": \"901\", \"downloads\": \"\", \"updated\": \"5 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.19903",
    "first_seen_date": "2025-03-26",
    "title": "Scaling Vision Pre-Training to 4K Resolution",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.19903Scaling Vision Pre-Training to 4K ResolutionPublished on Mar 25\u00b7Submitted byBaifeng Shion Mar 26#2 Paper of the dayUpvote41+33Authors:Baifeng Shi,Boyi Li,Han Cai,Yao Lu,Sifei Liu,Marco Pavone,Jan Kautz,Song Han,Trevor Darrell,Pavlo Molchanov,Hongxu YinAbstractPS3 enables high-resolution vision pre-training with reduced computational cost, improving multi-modal LLM performance and efficiency.AI-generated summaryHigh-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introducePS3that scalesCLIP-style vision pre-trainingto 4K resolution\nwith a near-constant cost. Instead ofcontrastive learningon global image\nrepresentation,PS3is pre-trained by selectively processinglocal regionsand\ncontrasting them with localdetailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trainedPS3is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on theirsaliencyor\nrelevance to atext prompt. When applyingPS3tomulti-modal LLM(MLLM), the\nresulting model, namedVILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch asAnyResandS^2while using up to 4.3x fewer tokens.PS3also unlocks\nappealing scaling properties ofVILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts,VILA-HDoutperforms previousMLLMs such asNVILAandQwen2-VLacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose4KPro, a new benchmark\n",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/NVlabs/PS3",
    "hf_paper_url": "https://huggingface.co/papers/2503.19903",
    "arxiv_url": "https://arxiv.org/abs/2503.19903",
    "num_models": 14,
    "models_list": "nvidia/PS3-1.5K-SigLIP, nvidia/VILA-HD-8B-PS3-1.5K-SigLIP, nvidia/PS3-4K-SigLIP, nvidia/PS3-1.5K-SigLIP2, nvidia/VILA-HD-8B-PS3-4K-SigLIP, nvidia/PS3_Lang-1.5K-SigLIP2, nvidia/PS3-4K-SigLIP2, nvidia/PS3_Lang-4K-SigLIP2, nvidia/PS3-1.5K-C-RADIOv2, nvidia/PS3-4K-C-RADIOv2, nvidia/VILA-HD-8B-PS3-1.5K-SigLIP2, nvidia/VILA-HD-8B-PS3-4K-SigLIP2, nvidia/VILA-HD-8B-PS3-1.5K-C-RADIOv2, nvidia/VILA-HD-8B-PS3-4K-C-RADIOv2",
    "models_links": "https://huggingface.co/nvidia/PS3-1.5K-SigLIP, https://huggingface.co/nvidia/VILA-HD-8B-PS3-1.5K-SigLIP, https://huggingface.co/nvidia/PS3-4K-SigLIP, https://huggingface.co/nvidia/PS3-1.5K-SigLIP2, https://huggingface.co/nvidia/VILA-HD-8B-PS3-4K-SigLIP, https://huggingface.co/nvidia/PS3_Lang-1.5K-SigLIP2, https://huggingface.co/nvidia/PS3-4K-SigLIP2, https://huggingface.co/nvidia/PS3_Lang-4K-SigLIP2, https://huggingface.co/nvidia/PS3-1.5K-C-RADIOv2, https://huggingface.co/nvidia/PS3-4K-C-RADIOv2, https://huggingface.co/nvidia/VILA-HD-8B-PS3-1.5K-SigLIP2, https://huggingface.co/nvidia/VILA-HD-8B-PS3-4K-SigLIP2, https://huggingface.co/nvidia/VILA-HD-8B-PS3-1.5K-C-RADIOv2, https://huggingface.co/nvidia/VILA-HD-8B-PS3-4K-C-RADIOv2",
    "models_detailed": "[{\"name\": \"nvidia/PS3-1.5K-SigLIP\", \"link\": \"https://huggingface.co/nvidia/PS3-1.5K-SigLIP\", \"task\": \"\", \"likes\": \"64\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"nvidia/VILA-HD-8B-PS3-1.5K-SigLIP\", \"link\": \"https://huggingface.co/nvidia/VILA-HD-8B-PS3-1.5K-SigLIP\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/PS3-4K-SigLIP\", \"link\": \"https://huggingface.co/nvidia/PS3-4K-SigLIP\", \"task\": \"\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"nvidia/PS3-1.5K-SigLIP2\", \"link\": \"https://huggingface.co/nvidia/PS3-1.5K-SigLIP2\", \"task\": \"\", \"likes\": \"90\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/VILA-HD-8B-PS3-4K-SigLIP\", \"link\": \"https://huggingface.co/nvidia/VILA-HD-8B-PS3-4K-SigLIP\", \"task\": \"\", \"likes\": \"203\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/PS3_Lang-1.5K-SigLIP2\", \"link\": \"https://huggingface.co/nvidia/PS3_Lang-1.5K-SigLIP2\", \"task\": \"\", \"likes\": \"203\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/PS3-4K-SigLIP2\", \"link\": \"https://huggingface.co/nvidia/PS3-4K-SigLIP2\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/PS3_Lang-4K-SigLIP2\", \"link\": \"https://huggingface.co/nvidia/PS3_Lang-4K-SigLIP2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/PS3-1.5K-C-RADIOv2\", \"link\": \"https://huggingface.co/nvidia/PS3-1.5K-C-RADIOv2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/PS3-4K-C-RADIOv2\", \"link\": \"https://huggingface.co/nvidia/PS3-4K-C-RADIOv2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/VILA-HD-8B-PS3-1.5K-SigLIP2\", \"link\": \"https://huggingface.co/nvidia/VILA-HD-8B-PS3-1.5K-SigLIP2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/VILA-HD-8B-PS3-4K-SigLIP2\", \"link\": \"https://huggingface.co/nvidia/VILA-HD-8B-PS3-4K-SigLIP2\", \"task\": \"\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/VILA-HD-8B-PS3-1.5K-C-RADIOv2\", \"link\": \"https://huggingface.co/nvidia/VILA-HD-8B-PS3-1.5K-C-RADIOv2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"nvidia/VILA-HD-8B-PS3-4K-C-RADIOv2\", \"link\": \"https://huggingface.co/nvidia/VILA-HD-8B-PS3-4K-C-RADIOv2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.18908",
    "first_seen_date": "2025-03-25",
    "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.18908FFN Fusion: Rethinking Sequential Computation in Large Language ModelsPublished on Mar 24\u00b7Submitted byAKon Mar 25Upvote19+11Authors:Akhiad Bercovich,Mohammad Dabbah,Omri Puny,Ido Galil,Amnon Geifman,Yonatan Geifman,Izhak Golan,Ehud Karpas,Itay Levy,Zach Moshe,Najeeb Nabwani,Tomer Ronen,Itamar Schen,Elad Segal,Ido Shahaf,Oren Tropp,Ran Zilberstein,Ran El-YanivAbstractFFN Fusion reduces sequential computation in large language models by parallelizing FFN layers, achieving significant speedup and cost reduction while maintaining performance.AI-generated summaryWe introduceFFN Fusion, an architectural optimization technique that reduces\nsequential computation in large language models by identifying and exploiting\nnatural opportunities forparallelization. Our key insight is that sequences ofFeed-Forward Network(FFN) layers, particularly those remaining after the\nremoval of specific attention layers, can often be parallelized with minimal\naccuracy impact. We develop a principled methodology for identifying and fusing\nsuch sequences, transforming them into parallel operations that significantly\nreduceinference latencywhile preserving model behavior. Applying these\ntechniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base\n(Ultra-253B-Base), an efficient and soon-to-be publicly available model that\nachieves a 1.71X speedup ininference latencyand 35X lower per-token cost\nwhile maintaining strong performance across benchmarks. Through extensive\nexperiments on models from 49B to 253B parameters, we demonstrate that FFN\nFusion becomes increasingly effective at larger scales and can complement\nexisting optimization techniques likequantizationandpruning. Most\nintriguingly, we find that evenfull transformer blockscontaining both\nattention and FFN layers can sometimes be parallelized, suggesting new\ndirections for neural architecture design.View arXiv pageView PDFAd",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2503.18908",
    "arxiv_url": "https://arxiv.org/abs/2503.18908",
    "num_models": 12,
    "models_list": "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1, nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8, unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF, nvidia/Llama-3_1-Nemotron-Ultra-253B-CPT-v1, FriendliAI/Llama-3_1-Nemotron-Ultra-253B-v1, ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_1.35bpw_H6, Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.6bpw-h6-exl3, Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.25bpw-h6-exl3, Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.45bpw-h6-exl3, ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_3.0bpw_H6, unsloth/Llama-3_1-Nemotron-Ultra-253B-v1, DBMe/Llama-3_1-Nemotron-Ultra-253B-v1-exl3-2.7bpw",
    "models_links": "https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1, https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8, https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF, https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-CPT-v1, https://huggingface.co/FriendliAI/Llama-3_1-Nemotron-Ultra-253B-v1, https://huggingface.co/ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_1.35bpw_H6, https://huggingface.co/Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.6bpw-h6-exl3, https://huggingface.co/Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.25bpw-h6-exl3, https://huggingface.co/Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.45bpw-h6-exl3, https://huggingface.co/ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_3.0bpw_H6, https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1, https://huggingface.co/DBMe/Llama-3_1-Nemotron-Ultra-253B-v1-exl3-2.7bpw",
    "models_detailed": "[{\"name\": \"nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\", \"link\": \"https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8\", \"link\": \"https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1-FP8\", \"task\": \"Text Generation\", \"likes\": \"297\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF\", \"link\": \"https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"805\", \"downloads\": \"\", \"updated\": \"May 26\"}, {\"name\": \"nvidia/Llama-3_1-Nemotron-Ultra-253B-CPT-v1\", \"link\": \"https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-CPT-v1\", \"task\": \"Text Generation\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Jul 6\"}, {\"name\": \"FriendliAI/Llama-3_1-Nemotron-Ultra-253B-v1\", \"link\": \"https://huggingface.co/FriendliAI/Llama-3_1-Nemotron-Ultra-253B-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_1.35bpw_H6\", \"link\": \"https://huggingface.co/ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_1.35bpw_H6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 14\"}, {\"name\": \"Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.6bpw-h6-exl3\", \"link\": \"https://huggingface.co/Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.6bpw-h6-exl3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 27\"}, {\"name\": \"Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.25bpw-h6-exl3\", \"link\": \"https://huggingface.co/Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.25bpw-h6-exl3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 27\"}, {\"name\": \"Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.45bpw-h6-exl3\", \"link\": \"https://huggingface.co/Panchovix/Llama-3_1-Nemotron-Ultra-253B-v1-3.45bpw-h6-exl3\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Apr 28\"}, {\"name\": \"ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_3.0bpw_H6\", \"link\": \"https://huggingface.co/ArtusDev/nvidia_Llama-3_1-Nemotron-Ultra-253B-v1_EXL3_3.0bpw_H6\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 14\"}, {\"name\": \"unsloth/Llama-3_1-Nemotron-Ultra-253B-v1\", \"link\": \"https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 25\"}, {\"name\": \"DBMe/Llama-3_1-Nemotron-Ultra-253B-v1-exl3-2.7bpw\", \"link\": \"https://huggingface.co/DBMe/Llama-3_1-Nemotron-Ultra-253B-v1-exl3-2.7bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 6\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.15242",
    "first_seen_date": "2025-03-21",
    "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.15242BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?Published on Mar 19\u00b7Submitted byPierre Chambonon Mar 21Upvote10+2Authors:Pierre Chambon,Baptiste Roziere,Benoit Sagot,Gabriel SynnaeveAbstractBigO(Bench) evaluates generative language models' ability to understand and generate code with specific time and space complexities.AI-generated summaryWe introduce BigO(Bench), a novelcoding benchmarkdesigned to evaluate the\ncapabilities ofgenerative language modelsin understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer thealgorithmic complexityof any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from thecomplexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular,token-space reasoningmodels are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.View arXiv pageView PDFProject pageGitHub38Add to collectionCommunitypierrechambonPaper authorPaper submitterMar 21\u2022edited Mar 21Does your LLM truly comprehend the complexity of the code it generates? \ud83e\udd70Introducing our new non-saturated (for at least the coming week? \ud83d\ude09) benchmark:\u2728BigO(Bench)\u2728 - Can LLMs Generate Code wi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/facebookresearch/bigobench",
    "hf_paper_url": "https://huggingface.co/papers/2503.15242",
    "arxiv_url": "https://arxiv.org/abs/2503.15242",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "facebook/BigOBench",
    "datasets_links": "https://huggingface.co/datasets/facebook/BigOBench",
    "datasets_detailed": "[{\"name\": \"facebook/BigOBench\", \"link\": \"https://huggingface.co/datasets/facebook/BigOBench\", \"task\": \"\", \"likes\": \"479\", \"downloads\": \"\", \"updated\": \"Mar 20\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.15558",
    "first_seen_date": "2025-03-21",
    "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.15558Cosmos-Reason1: From Physical Common Sense To Embodied ReasoningPublished on Mar 18\u00b7Submitted byYin Cuion Mar 21Upvote50+42Authors:NVIDIA,Alisson Azzolini,Hannah Brandon,Prithvijit Chattopadhyay,Huayu Chen,Jinju Chu,Yin Cui,Jenna Diamond,Yifan Ding,Francesco Ferroni,Rama Govindaraju,Jinwei Gu,Siddharth Gururani,Imad El Hanafi,Zekun Hao,Jacob Huffman,Jingyi Jin,Brendan Johnson,Rizwan Khan,George Kurian,Elena Lantz,Nayeon Lee+23 authorsAbstractCosmos-Reason1 models, using hierarchical and two-dimensional ontologies for physical common sense and embodied reasoning, generate embodied decisions through multimodal large language models trained in vision and Physical AI stages.AI-generated summaryPhysical AIsystems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through longchain-of-thoughtreasoningprocesses. We begin by defining key capabilities forPhysical AIreasoning, with a focus onphysical common senseandembodied reasoning. To\nrepresentphysical common sense, we use ahierarchical ontologythat captures\nfundamental knowledge about space, time, and physics. Forembodied reasoning,\nwe rely on atwo-dimensional ontologythat generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages:vision pre-training, general supervised\nfine-tuning (SFT),Physical AI SFT, andPhysical AI reinforcement learning(RL)\nas the post-training. To evaluate our models, we build comprehensivebenchmarksforphysical common senseandembodied reasoningaccording to our ontologies.\nEvaluation results show thatPhysical AI SFTand reinforcement lea",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/nvidia-cosmos/cosmos-reason1",
    "hf_paper_url": "https://huggingface.co/papers/2503.15558",
    "arxiv_url": "https://arxiv.org/abs/2503.15558",
    "num_models": 7,
    "models_list": "nvidia/Cosmos-Reason1-7B, unsloth/Cosmos-Reason1-7B-GGUF, Mungert/Cosmos-Reason1-7B-GGUF, unsloth/Cosmos-Reason1-7B, unsloth/Cosmos-Reason1-7B-unsloth-bnb-4bit, unsloth/Cosmos-Reason1-7B-bnb-4bit, denizaybey/Cosmos-Reason1-7B",
    "models_links": "https://huggingface.co/nvidia/Cosmos-Reason1-7B, https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF, https://huggingface.co/Mungert/Cosmos-Reason1-7B-GGUF, https://huggingface.co/unsloth/Cosmos-Reason1-7B, https://huggingface.co/unsloth/Cosmos-Reason1-7B-unsloth-bnb-4bit, https://huggingface.co/unsloth/Cosmos-Reason1-7B-bnb-4bit, https://huggingface.co/denizaybey/Cosmos-Reason1-7B",
    "models_detailed": "[{\"name\": \"nvidia/Cosmos-Reason1-7B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Reason1-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"unsloth/Cosmos-Reason1-7B-GGUF\", \"link\": \"https://huggingface.co/unsloth/Cosmos-Reason1-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"Mungert/Cosmos-Reason1-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Cosmos-Reason1-7B-GGUF\", \"task\": \"\", \"likes\": \"154\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"unsloth/Cosmos-Reason1-7B\", \"link\": \"https://huggingface.co/unsloth/Cosmos-Reason1-7B\", \"task\": \"\", \"likes\": \"74\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"unsloth/Cosmos-Reason1-7B-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Cosmos-Reason1-7B-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"unsloth/Cosmos-Reason1-7B-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Cosmos-Reason1-7B-bnb-4bit\", \"task\": \"\", \"likes\": \"124\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"denizaybey/Cosmos-Reason1-7B\", \"link\": \"https://huggingface.co/denizaybey/Cosmos-Reason1-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 19\"}]",
    "num_datasets": 5,
    "datasets_list": "nvidia/Cosmos-Reason1-SFT-Dataset, nvidia/Cosmos-Reason1-Benchmark, nvidia/Cosmos-Reason1-RL-Dataset, StephenDHYang/Cosmos-Reason1-RL-Datasetx10, doni1122/Cosmos-Reason1-RL-Dataset",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Cosmos-Reason1-SFT-Dataset, https://huggingface.co/datasets/nvidia/Cosmos-Reason1-Benchmark, https://huggingface.co/datasets/nvidia/Cosmos-Reason1-RL-Dataset, https://huggingface.co/datasets/StephenDHYang/Cosmos-Reason1-RL-Datasetx10, https://huggingface.co/datasets/doni1122/Cosmos-Reason1-RL-Dataset",
    "datasets_detailed": "[{\"name\": \"nvidia/Cosmos-Reason1-SFT-Dataset\", \"link\": \"https://huggingface.co/datasets/nvidia/Cosmos-Reason1-SFT-Dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\", \"size\": \"\"}, {\"name\": \"nvidia/Cosmos-Reason1-Benchmark\", \"link\": \"https://huggingface.co/datasets/nvidia/Cosmos-Reason1-Benchmark\", \"task\": \"\", \"likes\": \"510\", \"downloads\": \"\", \"updated\": \"May 20\", \"size\": \"\"}, {\"name\": \"nvidia/Cosmos-Reason1-RL-Dataset\", \"link\": \"https://huggingface.co/datasets/nvidia/Cosmos-Reason1-RL-Dataset\", \"task\": \"\", \"likes\": \"892\", \"downloads\": \"\", \"updated\": \"May 20\", \"size\": \"\"}, {\"name\": \"StephenDHYang/Cosmos-Reason1-RL-Datasetx10\", \"link\": \"https://huggingface.co/datasets/StephenDHYang/Cosmos-Reason1-RL-Datasetx10\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\", \"size\": \"\"}, {\"name\": \"doni1122/Cosmos-Reason1-RL-Dataset\", \"link\": \"https://huggingface.co/datasets/doni1122/Cosmos-Reason1-RL-Dataset\", \"task\": \"\", \"likes\": \"892\", \"downloads\": \"\", \"updated\": \"11 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.16252",
    "first_seen_date": "2025-03-21",
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.16252Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement LearningPublished on Mar 20\u00b7Submitted byAKon Mar 21Upvote29+21Authors:Zhaowei Liu,Xin Guo,Fangqi Lou,Lingfeng Zeng,Jinyi Niu,Zixuan Wang,Jiajie Xu,Weige Cai,Ziwei Yang,Xueqian Zhao,Chao Li,Sheng Xu,Dezhi Chen,Yun Chen,Zuo Bai,Liwen ZhangAbstractFin-R1, a large language model tailored for finance, achieves state-of-the-art performance in financial reasoning tasks using supervised fine-tuning and reinforcement learning.AI-generated summaryReasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging afinancial reasoning datasetdistilled and processed based onDeepSeek-R1. Throughsupervised fine-tuning(SFT) andreinforcement learning(RL) training, it demonstrates performance\nclose toDeepSeek-R1with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves thestate-of-the-art(SOTA) in theFinQAandConvFinQAtasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.View arXiv pageView PDFGitHub718Add to collectionCommunityakhaliqPaper submitterMar 21See translationReplylibrarian-botMar 22This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIVision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models(2025)Audio-Reas",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/SUFE-AIFLM-Lab/Fin-R1",
    "hf_paper_url": "https://huggingface.co/papers/2503.16252",
    "arxiv_url": "https://arxiv.org/abs/2503.16252",
    "num_models": 3,
    "models_list": "SUFE-AIFLM-Lab/Fin-R1, Mungert/Fin-R1-GGUF, fenglui/Fin-R1-awq",
    "models_links": "https://huggingface.co/SUFE-AIFLM-Lab/Fin-R1, https://huggingface.co/Mungert/Fin-R1-GGUF, https://huggingface.co/fenglui/Fin-R1-awq",
    "models_detailed": "[{\"name\": \"SUFE-AIFLM-Lab/Fin-R1\", \"link\": \"https://huggingface.co/SUFE-AIFLM-Lab/Fin-R1\", \"task\": \"\", \"likes\": \"481\", \"downloads\": \"\", \"updated\": \"Mar 21\"}, {\"name\": \"Mungert/Fin-R1-GGUF\", \"link\": \"https://huggingface.co/Mungert/Fin-R1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"537\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"fenglui/Fin-R1-awq\", \"link\": \"https://huggingface.co/fenglui/Fin-R1-awq\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 16\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.16418",
    "first_seen_date": "2025-03-21",
    "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.16418InfiniteYou: Flexible Photo Recrafting While Preserving Your IdentityPublished on Mar 20\u00b7Submitted byAKon Mar 21Upvote36+28Authors:Liming Jiang,Qing Yan,Yumin Jia,Zichuan Liu,Hao Kang,Xin LuAbstractInfiniteYou leverages Diffusion Transformers to generate high-fidelity identity-preserved images, addressing issues of identity similarity, text-image alignment, and quality through a novel framework and training strategy.AI-generated summaryAchieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advancedDiffusion Transformers(DiTs)\nlike FLUX. We introduceInfiniteYou(InfU), one of the earliest robust\nframeworks leveragingDiTsfor this task.InfUaddresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central toInfUisInfuseNet, a component that injects identity features into the DiT base model\nviaresidual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, includingpretrainingandsupervised fine-tuning(SFT) withsynthetic single-person-multiple-sample(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate thatInfUachievesstate-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design ofInfUensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.View arXiv pageView PDFGitHub2.65kautoAdd to collectionCommunityakhaliqPaper submitterMar 21See translationReplyDrake79Mar 21See translation\ud83d\ude0e11+ReplyDrake79Mar 21Caffe drink luxuryReply6chanMar 21Project Page:https://bytedance.github.io/InfiniteYouCode:https://github.com/bytedance/InfiniteYou.See translationReplylibrarian-botMar 22This is an",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/bytedance/infiniteyou",
    "hf_paper_url": "https://huggingface.co/papers/2503.16418",
    "arxiv_url": "https://arxiv.org/abs/2503.16418",
    "num_models": 4,
    "models_list": "ByteDance/InfiniteYou, vuongminhkhoi4/ComfyUI_InfiniteYou, jobs-git/InfiniteYou, RedbeardNZ/ComfyUI_InfiniteYou",
    "models_links": "https://huggingface.co/ByteDance/InfiniteYou, https://huggingface.co/vuongminhkhoi4/ComfyUI_InfiniteYou, https://huggingface.co/jobs-git/InfiniteYou, https://huggingface.co/RedbeardNZ/ComfyUI_InfiniteYou",
    "models_detailed": "[{\"name\": \"ByteDance/InfiniteYou\", \"link\": \"https://huggingface.co/ByteDance/InfiniteYou\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"vuongminhkhoi4/ComfyUI_InfiniteYou\", \"link\": \"https://huggingface.co/vuongminhkhoi4/ComfyUI_InfiniteYou\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 29\"}, {\"name\": \"jobs-git/InfiniteYou\", \"link\": \"https://huggingface.co/jobs-git/InfiniteYou\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 22\"}, {\"name\": \"RedbeardNZ/ComfyUI_InfiniteYou\", \"link\": \"https://huggingface.co/RedbeardNZ/ComfyUI_InfiniteYou\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 28\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.15478",
    "first_seen_date": "2025-03-20",
    "title": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.15478SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  TasksPublished on Mar 19\u00b7Submitted byYifei Zhouon Mar 20Upvote13+5Authors:Yifei Zhou,Song Jiang,Yuandong Tian,Jason Weston,Sergey Levine,Sainbayar Sukhbaatar,Xian LiAbstractA novel RL algorithm, SWEET-RL, uses step-level rewards to enhance multi-turn interactions in LLMs, outperforming existing methods on a new benchmark for collaborative content creation tasks.AI-generated summaryLarge language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existingmulti-turn RLalgorithms for optimizingLLMagents fail to perform effectivecredit assignmentover multiple turns while\nleveraging the generalization capabilities ofLLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,ColBench, where anLLMagent interacts with a human collaborator over multiple\nturns to solve realistic tasks inbackend programmingandfrontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation fromTraining-time information), that uses a carefully\ndesigned optimization objective to train acritic modelwith access to\nadditionaltraining-time information. The critic providesstep-level rewardsfor improving thepolicy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success andwin ratesonColBenchcompared to other state-of-the-artmulti-turn RLalgorithms, enabling\nLlama-3.1-8B to match or exceed the performance ofGPT4-oin realistic\ncollaborative content creation.View arXiv pageView PDFGitHub255autoAdd to collectionCommunityyifeizhouPaper authorPaper submitterMar 20SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning TasksSee translationReplylibrarian-botMar 21This is an automated message from theLibrarian Bot. I found the following papers similar to thi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/facebookresearch/sweet_rl",
    "hf_paper_url": "https://huggingface.co/papers/2503.15478",
    "arxiv_url": "https://arxiv.org/abs/2503.15478",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "facebook/collaborative_agent_bench",
    "datasets_links": "https://huggingface.co/datasets/facebook/collaborative_agent_bench",
    "datasets_detailed": "[{\"name\": \"facebook/collaborative_agent_bench\", \"link\": \"https://huggingface.co/datasets/facebook/collaborative_agent_bench\", \"task\": \"\", \"likes\": \"121\", \"downloads\": \"\", \"updated\": \"Mar 20\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.14456",
    "first_seen_date": "2025-03-19",
    "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.14456RWKV-7 \"Goose\" with Expressive Dynamic State EvolutionPublished on Mar 18\u00b7Submitted byZhang Ruichongon Mar 19Upvote153+145Authors:Bo Peng,Ruichong Zhang,Daniel Goldstein,Eric Alcaide,Haowen Hou,Janna Lu,William Merrill,Guangyu Song,Kaifeng Tan,Saiteja Utpala,Nathan Wilce,Johan S. Wind,Tianyi Wu,Daniel Wuttke,Christian Zhou-ZhengAbstractRWKV-7 \"Goose\" achieves state-of-the-art performance in multilingual tasks with optimal memory and inference efficiency, exceeding Transformer capabilities in complexity.AI-generated summaryWe present RWKV-7 \"Goose\", a newsequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of thedelta rulewithvector-valued gatingandin-context learning rates, as well as arelaxed value replacement rule. We show\nthat RWKV-7 can performstate trackingand recognize all regular languages,\nwhile retainingparallelizabilityof training. This exceeds the capabilities ofTransformersunder standard complexity conjectures, which are limited to\nTC^0. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion tokenmultilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.View arXiv pageView PDFProject pageGitHub58Add to c",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/RWKV/RWKV-LM",
    "hf_paper_url": "https://huggingface.co/papers/2503.14456",
    "arxiv_url": "https://arxiv.org/abs/2503.14456",
    "num_models": 27,
    "models_list": "RWKV/RWKV7-Goose-World3-2.9B-HF, fla-hub/rwkv7-1.5B-world, fla-hub/rwkv7-0.1B-g1, Mungert/rwkv7-2.9B-g1-GGUF, fla-hub/rwkv7-168M-pile, fla-hub/rwkv7-191M-world, fla-hub/rwkv7-2.9B-world, fla-hub/rwkv7-0.4B-world, fla-hub/rwkv7-421M-pile, fla-hub/rwkv7-1.47B-pile, RWKV/RWKV7-Goose-World3-1.5B-HF, RWKV/RWKV7-Goose-World2.9-0.4B-HF, RWKV/RWKV7-Goose-World2.8-0.1B-HF, RWKV/RWKV7-Goose-Pile-168M-HF, RWKV/RWKV7-Goose-Pile-421M-HF, RWKV/RWKV7-Goose-Pile-1.47B-HF, fla-hub/rwkv7-0.4B-g1, fla-hub/rwkv7-1.5B-g1, fla-hub/rwkv7-2.9B-g1, fla-hub/rwkv7-7.2B-g0, Mungert/rwkv7-7.2B-g0-GGUF, Mungert/rwkv7-1.5B-g1-GGUF, Mungert/rwkv7-0.1B-g1-GGUF, fla-hub/rwkv7-7.2B-g0a, fla-hub/rwkv7-0.4B-g1a, fla-hub/rwkv7-0.1B-g1a, fla-hub/rwkv7-1.5B-g1a",
    "models_links": "https://huggingface.co/RWKV/RWKV7-Goose-World3-2.9B-HF, https://huggingface.co/fla-hub/rwkv7-1.5B-world, https://huggingface.co/fla-hub/rwkv7-0.1B-g1, https://huggingface.co/Mungert/rwkv7-2.9B-g1-GGUF, https://huggingface.co/fla-hub/rwkv7-168M-pile, https://huggingface.co/fla-hub/rwkv7-191M-world, https://huggingface.co/fla-hub/rwkv7-2.9B-world, https://huggingface.co/fla-hub/rwkv7-0.4B-world, https://huggingface.co/fla-hub/rwkv7-421M-pile, https://huggingface.co/fla-hub/rwkv7-1.47B-pile, https://huggingface.co/RWKV/RWKV7-Goose-World3-1.5B-HF, https://huggingface.co/RWKV/RWKV7-Goose-World2.9-0.4B-HF, https://huggingface.co/RWKV/RWKV7-Goose-World2.8-0.1B-HF, https://huggingface.co/RWKV/RWKV7-Goose-Pile-168M-HF, https://huggingface.co/RWKV/RWKV7-Goose-Pile-421M-HF, https://huggingface.co/RWKV/RWKV7-Goose-Pile-1.47B-HF, https://huggingface.co/fla-hub/rwkv7-0.4B-g1, https://huggingface.co/fla-hub/rwkv7-1.5B-g1, https://huggingface.co/fla-hub/rwkv7-2.9B-g1, https://huggingface.co/fla-hub/rwkv7-7.2B-g0, https://huggingface.co/Mungert/rwkv7-7.2B-g0-GGUF, https://huggingface.co/Mungert/rwkv7-1.5B-g1-GGUF, https://huggingface.co/Mungert/rwkv7-0.1B-g1-GGUF, https://huggingface.co/fla-hub/rwkv7-7.2B-g0a, https://huggingface.co/fla-hub/rwkv7-0.4B-g1a, https://huggingface.co/fla-hub/rwkv7-0.1B-g1a, https://huggingface.co/fla-hub/rwkv7-1.5B-g1a",
    "models_detailed": "[{\"name\": \"RWKV/RWKV7-Goose-World3-2.9B-HF\", \"link\": \"https://huggingface.co/RWKV/RWKV7-Goose-World3-2.9B-HF\", \"task\": \"Text Generation\", \"likes\": \"89\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"fla-hub/rwkv7-1.5B-world\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-1.5B-world\", \"task\": \"Text Generation\", \"likes\": \"195\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"fla-hub/rwkv7-0.1B-g1\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-0.1B-g1\", \"task\": \"Text Generation\", \"likes\": \"47\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"Mungert/rwkv7-2.9B-g1-GGUF\", \"link\": \"https://huggingface.co/Mungert/rwkv7-2.9B-g1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"783\", \"downloads\": \"\", \"updated\": \"Sep 27\"}, {\"name\": \"fla-hub/rwkv7-168M-pile\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-168M-pile\", \"task\": \"Text Generation\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"fla-hub/rwkv7-191M-world\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-191M-world\", \"task\": \"Text Generation\", \"likes\": \"58\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"fla-hub/rwkv7-2.9B-world\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-2.9B-world\", \"task\": \"Text Generation\", \"likes\": \"86\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"fla-hub/rwkv7-0.4B-world\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-0.4B-world\", \"task\": \"Text Generation\", \"likes\": \"44\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"fla-hub/rwkv7-421M-pile\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-421M-pile\", \"task\": \"Text Generation\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"fla-hub/rwkv7-1.47B-pile\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-1.47B-pile\", \"task\": \"Text Generation\", \"likes\": \"29\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"RWKV/RWKV7-Goose-World3-1.5B-HF\", \"link\": \"https://huggingface.co/RWKV/RWKV7-Goose-World3-1.5B-HF\", \"task\": \"Text Generation\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"RWKV/RWKV7-Goose-World2.9-0.4B-HF\", \"link\": \"https://huggingface.co/RWKV/RWKV7-Goose-World2.9-0.4B-HF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"RWKV/RWKV7-Goose-World2.8-0.1B-HF\", \"link\": \"https://huggingface.co/RWKV/RWKV7-Goose-World2.8-0.1B-HF\", \"task\": \"Text Generation\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"RWKV/RWKV7-Goose-Pile-168M-HF\", \"link\": \"https://huggingface.co/RWKV/RWKV7-Goose-Pile-168M-HF\", \"task\": \"Text Generation\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"RWKV/RWKV7-Goose-Pile-421M-HF\", \"link\": \"https://huggingface.co/RWKV/RWKV7-Goose-Pile-421M-HF\", \"task\": \"Text Generation\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"RWKV/RWKV7-Goose-Pile-1.47B-HF\", \"link\": \"https://huggingface.co/RWKV/RWKV7-Goose-Pile-1.47B-HF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"fla-hub/rwkv7-0.4B-g1\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-0.4B-g1\", \"task\": \"Text Generation\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"fla-hub/rwkv7-1.5B-g1\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-1.5B-g1\", \"task\": \"Text Generation\", \"likes\": \"38\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"fla-hub/rwkv7-2.9B-g1\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-2.9B-g1\", \"task\": \"Text Generation\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"fla-hub/rwkv7-7.2B-g0\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-7.2B-g0\", \"task\": \"Text Generation\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"Mungert/rwkv7-7.2B-g0-GGUF\", \"link\": \"https://huggingface.co/Mungert/rwkv7-7.2B-g0-GGUF\", \"task\": \"Text Generation\", \"likes\": \"245\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/rwkv7-1.5B-g1-GGUF\", \"link\": \"https://huggingface.co/Mungert/rwkv7-1.5B-g1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/rwkv7-0.1B-g1-GGUF\", \"link\": \"https://huggingface.co/Mungert/rwkv7-0.1B-g1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"311\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"fla-hub/rwkv7-7.2B-g0a\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-7.2B-g0a\", \"task\": \"Text Generation\", \"likes\": \"35\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"fla-hub/rwkv7-0.4B-g1a\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-0.4B-g1a\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Sep 15\"}, {\"name\": \"fla-hub/rwkv7-0.1B-g1a\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-0.1B-g1a\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Sep 15\"}, {\"name\": \"fla-hub/rwkv7-1.5B-g1a\", \"link\": \"https://huggingface.co/fla-hub/rwkv7-1.5B-g1a\", \"task\": \"Text Generation\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Sep 25\"}]",
    "num_datasets": 2,
    "datasets_list": "Goose-World/RWKV-World-v3, RWKV/RWKV-World-Listing",
    "datasets_links": "https://huggingface.co/datasets/Goose-World/RWKV-World-v3, https://huggingface.co/datasets/RWKV/RWKV-World-Listing",
    "datasets_detailed": "[{\"name\": \"Goose-World/RWKV-World-v3\", \"link\": \"https://huggingface.co/datasets/Goose-World/RWKV-World-v3\", \"task\": \"\", \"likes\": \"354\", \"downloads\": \"\", \"updated\": \"Apr 28\", \"size\": \"\"}, {\"name\": \"RWKV/RWKV-World-Listing\", \"link\": \"https://huggingface.co/datasets/RWKV/RWKV-World-Listing\", \"task\": \"\", \"likes\": \"169\", \"downloads\": \"\", \"updated\": \"Mar 19\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.14476",
    "first_seen_date": "2025-03-19",
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.14476DAPO: An Open-Source LLM Reinforcement Learning System at ScalePublished on Mar 18\u00b7Submitted byAKon Mar 19Upvote144+136Authors:Qiying Yu,Zheng Zhang,Ruofei Zhu,Yufeng Yuan,Xiaochen Zuo,Yu Yue,Tiantian Fan,Gaohong Liu,Lingjun Liu,Xin Liu,Haibin Lin,Zhiqi Lin,Bole Ma,Guangming Sheng,Yuxuan Tong,Chi Zhang,Mofan Zhang,Wang Zhang,Hang Zhu,Jinhua Zhu,Jiaze Chen,Jiangjie Chen+13 authorsAbstractThe DAPO algorithm, which includes decoupled clip and dynamic sampling policy optimization, enables open-source, high-performance reinforcement learning training for large-scale language models, enhancing reproducibility in the field.AI-generated summaryInference scaling empowersLLMswith unprecedented reasoning ability, withreinforcement learningas the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoningLLMsare concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\ntheDecoupled Clipand Dynamic sAmpling\nPolicy Optimization (DAPO) algorithm, and\nfullyopen-sourcea state-of-the-artlarge-scale RLsystem that achieves 50\npoints onAIME 2024usingQwen2.5-32Bbase model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, weopen-sourceour\ntraining code, which is built on theverl framework, along with a carefully\ncurated and processed dataset. These components of ouropen-sourcesystem\nenhance reproducibility and support future research in large-scale LLM RL.View arXiv pageView PDFProject pageGitHub1.68kautoAdd to collectionCommunityakhaliqPaper submitterMar 19See translationReplylibrarian-botMar 20This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Sema",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/BytedTsinghua-SIA/DAPO",
    "hf_paper_url": "https://huggingface.co/papers/2503.14476",
    "arxiv_url": "https://arxiv.org/abs/2503.14476",
    "num_models": 12,
    "models_list": "BytedTsinghua-SIA/DAPO-Qwen-32B, kangdawei/MMR-DAPO-8B, kangdawei/DAPO-No-DS, kangdawei/MMR-DAPO, kangdawei/DAPO-No-DS-7B, kangdawei/DAPO-No-DS-8B, kangdawei/MMR-DAPO-7B, kangdawei/DAPO, kangdawei/DAPO-8B, kangdawei/DAPO-7B, kangdawei/MMR-Sigmoid-DAPO, kangdawei/MMR-Sigmoid-DAPO-8B",
    "models_links": "https://huggingface.co/BytedTsinghua-SIA/DAPO-Qwen-32B, https://huggingface.co/kangdawei/MMR-DAPO-8B, https://huggingface.co/kangdawei/DAPO-No-DS, https://huggingface.co/kangdawei/MMR-DAPO, https://huggingface.co/kangdawei/DAPO-No-DS-7B, https://huggingface.co/kangdawei/DAPO-No-DS-8B, https://huggingface.co/kangdawei/MMR-DAPO-7B, https://huggingface.co/kangdawei/DAPO, https://huggingface.co/kangdawei/DAPO-8B, https://huggingface.co/kangdawei/DAPO-7B, https://huggingface.co/kangdawei/MMR-Sigmoid-DAPO, https://huggingface.co/kangdawei/MMR-Sigmoid-DAPO-8B",
    "models_detailed": "[{\"name\": \"BytedTsinghua-SIA/DAPO-Qwen-32B\", \"link\": \"https://huggingface.co/BytedTsinghua-SIA/DAPO-Qwen-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"kangdawei/MMR-DAPO-8B\", \"link\": \"https://huggingface.co/kangdawei/MMR-DAPO-8B\", \"task\": \"Text Generation\", \"likes\": \"347\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"kangdawei/DAPO-No-DS\", \"link\": \"https://huggingface.co/kangdawei/DAPO-No-DS\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"kangdawei/MMR-DAPO\", \"link\": \"https://huggingface.co/kangdawei/MMR-DAPO\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"kangdawei/DAPO-No-DS-7B\", \"link\": \"https://huggingface.co/kangdawei/DAPO-No-DS-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"kangdawei/DAPO-No-DS-8B\", \"link\": \"https://huggingface.co/kangdawei/DAPO-No-DS-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"kangdawei/MMR-DAPO-7B\", \"link\": \"https://huggingface.co/kangdawei/MMR-DAPO-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"kangdawei/DAPO\", \"link\": \"https://huggingface.co/kangdawei/DAPO\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"kangdawei/DAPO-8B\", \"link\": \"https://huggingface.co/kangdawei/DAPO-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"kangdawei/DAPO-7B\", \"link\": \"https://huggingface.co/kangdawei/DAPO-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"kangdawei/MMR-Sigmoid-DAPO\", \"link\": \"https://huggingface.co/kangdawei/MMR-Sigmoid-DAPO\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 3\"}, {\"name\": \"kangdawei/MMR-Sigmoid-DAPO-8B\", \"link\": \"https://huggingface.co/kangdawei/MMR-Sigmoid-DAPO-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"2 days ago\"}]",
    "num_datasets": 14,
    "datasets_list": "open-r1/DAPO-Math-17k-Processed, allenai/Dolci-Think-RL-7B, allenai/Dolci-Think-RL-32B, allenai/Dolci-Instruct-RL, ftajwar/srt_test_dataset, ftajwar/deduplicated_dapo_dataset, ftajwar/dapo_easy_one_third_sorted_by_frequency_of_majority_answer, ftajwar/dapo_easy_one_third_sorted_by_pass_rate, sungyub/dapo-math-17k-verl, sungyub/math-verl-unified, amishor/reinforce-learning, Rexopia/DAPO_Math_17K_Selected, allenai/Dolci-Think-RL-7B-Completions-DPO, allenai/Dolci-Think-RL-7B-Completions-SFT",
    "datasets_links": "https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B, https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B, https://huggingface.co/datasets/allenai/Dolci-Instruct-RL, https://huggingface.co/datasets/ftajwar/srt_test_dataset, https://huggingface.co/datasets/ftajwar/deduplicated_dapo_dataset, https://huggingface.co/datasets/ftajwar/dapo_easy_one_third_sorted_by_frequency_of_majority_answer, https://huggingface.co/datasets/ftajwar/dapo_easy_one_third_sorted_by_pass_rate, https://huggingface.co/datasets/sungyub/dapo-math-17k-verl, https://huggingface.co/datasets/sungyub/math-verl-unified, https://huggingface.co/datasets/amishor/reinforce-learning, https://huggingface.co/datasets/Rexopia/DAPO_Math_17K_Selected, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT",
    "datasets_detailed": "[{\"name\": \"open-r1/DAPO-Math-17k-Processed\", \"link\": \"https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 10\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-32B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Instruct-RL\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Instruct-RL\", \"task\": \"\", \"likes\": \"774\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}, {\"name\": \"ftajwar/srt_test_dataset\", \"link\": \"https://huggingface.co/datasets/ftajwar/srt_test_dataset\", \"task\": \"\", \"likes\": \"273\", \"downloads\": \"\", \"updated\": \"May 28\", \"size\": \"\"}, {\"name\": \"ftajwar/deduplicated_dapo_dataset\", \"link\": \"https://huggingface.co/datasets/ftajwar/deduplicated_dapo_dataset\", \"task\": \"\", \"likes\": \"92\", \"downloads\": \"\", \"updated\": \"May 28\", \"size\": \"\"}, {\"name\": \"ftajwar/dapo_easy_one_third_sorted_by_frequency_of_majority_answer\", \"link\": \"https://huggingface.co/datasets/ftajwar/dapo_easy_one_third_sorted_by_frequency_of_majority_answer\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 28\", \"size\": \"\"}, {\"name\": \"ftajwar/dapo_easy_one_third_sorted_by_pass_rate\", \"link\": \"https://huggingface.co/datasets/ftajwar/dapo_easy_one_third_sorted_by_pass_rate\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 28\", \"size\": \"\"}, {\"name\": \"sungyub/dapo-math-17k-verl\", \"link\": \"https://huggingface.co/datasets/sungyub/dapo-math-17k-verl\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 9\", \"size\": \"\"}, {\"name\": \"sungyub/math-verl-unified\", \"link\": \"https://huggingface.co/datasets/sungyub/math-verl-unified\", \"task\": \"\", \"likes\": \"50\", \"downloads\": \"\", \"updated\": \"Nov 9\", \"size\": \"\"}, {\"name\": \"amishor/reinforce-learning\", \"link\": \"https://huggingface.co/datasets/amishor/reinforce-learning\", \"task\": \"\", \"likes\": \"951\", \"downloads\": \"\", \"updated\": \"Oct 19\", \"size\": \"\"}, {\"name\": \"Rexopia/DAPO_Math_17K_Selected\", \"link\": \"https://huggingface.co/datasets/Rexopia/DAPO_Math_17K_Selected\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"30 days ago\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-DPO\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO\", \"task\": \"\", \"likes\": \"332\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-SFT\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT\", \"task\": \"\", \"likes\": \"413\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.14492",
    "first_seen_date": "2025-03-19",
    "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.14492Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  ControlPublished on Mar 18\u00b7Submitted byAKon Mar 19Upvote20+12Authors:NVIDIA,Hassan Abu Alhaija,Jose Alvarez,Maciej Bala,Tiffany Cai,Tianshi Cao,Liz Cha,Joshua Chen,Mike Chen,Francesco Ferroni,Sanja Fidler,Dieter Fox,Yunhao Ge,Jinwei Gu,Ali Hassani,Michael Isaev,Pooya Jannaty,Shiyi Lan,Tobias Lasser,Huan Ling,Ming-Yu Liu,Xian Liu+17 authorsAbstractCosmos-Transfer is a conditional world generation model that uses adaptable spatial controls for segmentation, depth, and edge to generate real-time world simulations for Physical AI applications.AI-generated summaryWe introduce Cosmos-Transfer, aconditional world generation modelthat can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such assegmentation,depth, andedge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, includingSim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications forPhysical AI,\nincludingrobotics Sim2Realandautonomous vehicle data enrichment. We further\ndemonstrate aninference scaling strategyto achieve real-time world generation\nwith anNVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.View arXiv pageView PDFGitHub750autoAdd to collectionCommunityakhaliqPaper submitterMar 19See translationReplylibrarian-botMar 20This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APILearning Real-World Action-Vi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/nvidia-cosmos/cosmos-transfer1",
    "hf_paper_url": "https://huggingface.co/papers/2503.14492",
    "arxiv_url": "https://arxiv.org/abs/2503.14492",
    "num_models": 4,
    "models_list": "nvidia/Cosmos-Transfer1-7B, nvidia/Cosmos-Transfer1-7B-Sample-AV, nvidia/Cosmos-Transfer1-7B-4KUpscaler, nvidia/Cosmos-Transfer1-7B-Sample-AV-Single2MultiView",
    "models_links": "https://huggingface.co/nvidia/Cosmos-Transfer1-7B, https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV, https://huggingface.co/nvidia/Cosmos-Transfer1-7B-4KUpscaler, https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV-Single2MultiView",
    "models_detailed": "[{\"name\": \"nvidia/Cosmos-Transfer1-7B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Transfer1-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"nvidia/Cosmos-Transfer1-7B-Sample-AV\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"nvidia/Cosmos-Transfer1-7B-4KUpscaler\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Transfer1-7B-4KUpscaler\", \"task\": \"\", \"likes\": \"89\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"nvidia/Cosmos-Transfer1-7B-Sample-AV-Single2MultiView\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV-Single2MultiView\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}]",
    "num_datasets": 2,
    "datasets_list": "nvidia/PhysicalAI-Robotics-Manipulation-Augmented, nvidia/Cosmos-Transfer1-7B-Sample-AV-Data-Example",
    "datasets_links": "https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-Manipulation-Augmented, https://huggingface.co/datasets/nvidia/Cosmos-Transfer1-7B-Sample-AV-Data-Example",
    "datasets_detailed": "[{\"name\": \"nvidia/PhysicalAI-Robotics-Manipulation-Augmented\", \"link\": \"https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-Manipulation-Augmented\", \"task\": \"\", \"likes\": \"428\", \"downloads\": \"\", \"updated\": \"Jun 5\", \"size\": \"\"}, {\"name\": \"nvidia/Cosmos-Transfer1-7B-Sample-AV-Data-Example\", \"link\": \"https://huggingface.co/datasets/nvidia/Cosmos-Transfer1-7B-Sample-AV-Data-Example\", \"task\": \"\", \"likes\": \"130\", \"downloads\": \"\", \"updated\": \"Mar 19\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.08153",
    "first_seen_date": "2025-03-18",
    "title": "WISA: World Simulator Assistant for Physics-Aware Text-to-Video\n  Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.08153WISA: World Simulator Assistant for Physics-Aware Text-to-Video\n  GenerationPublished on Mar 11\u00b7Submitted byJunon Mar 18Upvote3Authors:Jing Wang,Ao Ma,Ke Cao,Jun Zheng,Zhanjie Zhang,Jiasong Feng,Shanyuan Liu,Yuhang Ma,Bo Cheng,Dawei Leng,Yuhui Yin,Xiaodan LiangAbstractWISA enhances text-to-video models' physical accuracy by decomposing and embedding physical principles using MoPA and a Physical Classifier, validated on a new dataset of 32,000 videos representing 17 physical laws.AI-generated summaryRecent rapid advancements intext-to-video(T2V) generation, such as SoRA and\nKling, have shown great potential for building world simulators. However,\ncurrentT2Vmodels struggle to grasp abstract physical principles and generate\nvideos that adhere tophysical laws. This challenge arises primarily from a\nlack of clear guidance on physical information due to a significant gap between\nabstract physical principles and generation models. To this end, we introduce\ntheWorld Simulator Assistant(WISA), an effective framework for decomposing\nand incorporating physical principles intoT2Vmodels. Specifically,WISAdecomposes physical principles into textual physical descriptions, qualitative\nphysical categories, and quantitative physical properties. To effectively embed\nthese physical attributes into the generation process,WISAincorporates\nseveral key designs, includingMixture-of-Physical-Experts Attention(MoPA) and\naPhysical Classifier, enhancing the model's physics awareness. Furthermore,\nmost existing datasets feature videos where physical phenomena are either\nweakly represented or entangled with multiple co-occurring processes, limiting\ntheir suitability as dedicated resources for learning explicit physical\nprinciples. We propose a novel video dataset,WISA-32K, collected based on\nqualitative physical categories. It consists of 32,000 videos, representing 17physical lawsacross three domains",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/360CVGroup/WISA",
    "hf_paper_url": "https://huggingface.co/papers/2503.08153",
    "arxiv_url": "https://arxiv.org/abs/2503.08153",
    "num_models": 1,
    "models_list": "qihoo360/WISA",
    "models_links": "https://huggingface.co/qihoo360/WISA",
    "models_detailed": "[{\"name\": \"qihoo360/WISA\", \"link\": \"https://huggingface.co/qihoo360/WISA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 16\"}]",
    "num_datasets": 1,
    "datasets_list": "qihoo360/WISA-80K",
    "datasets_links": "https://huggingface.co/datasets/qihoo360/WISA-80K",
    "datasets_detailed": "[{\"name\": \"qihoo360/WISA-80K\", \"link\": \"https://huggingface.co/datasets/qihoo360/WISA-80K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.13435",
    "first_seen_date": "2025-03-18",
    "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.13435WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and ScenesPublished on Mar 17\u00b7Submitted byLing Yangon Mar 18Upvote18+10Authors:Ling Yang,Kaixin Zhu,Juanxi Tian,Bohan Zeng,Mingbao Lin,Hongjuan Pei,Wentao Zhang,Shuicheng YanAbstractA novel 4D reconstruction benchmark, WideRange4D, and method, Progress4D, are introduced to address challenges in reconstructing scenes with wide-range object spatial movements.AI-generated summaryWith the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing4D reconstructionmethods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current4D reconstructionbenchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involvewide-range spatial movements,\nhighlighting the limitations of existing4D reconstructiondatasets.\nAdditionally, existing4D reconstructionmethods rely ondeformation fieldsto\nestimate the dynamics of 3D objects, butdeformation fieldsstruggle withwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction withwide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel4D reconstructionbenchmark,WideRange4D. This benchmark\nincludes rich4D scene datawithlarge spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new4D reconstructionmethod,Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments onWideRange4D, showing that ourProgress4Doutperforms\nexisting state-of-the-art4D rec",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/Gen-Verse/WideRange4D",
    "hf_paper_url": "https://huggingface.co/papers/2503.13435",
    "arxiv_url": "https://arxiv.org/abs/2503.13435",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "Gen-Verse/WideRange4D",
    "datasets_links": "https://huggingface.co/datasets/Gen-Verse/WideRange4D",
    "datasets_detailed": "[{\"name\": \"Gen-Verse/WideRange4D\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/WideRange4D\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.11576",
    "first_seen_date": "2025-03-17",
    "title": "SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.11576SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversionPublished on Mar 14\u00b7Submitted byAndres Marafiotion Mar 17#2 Paper of the dayUpvote120+112Authors:Ahmed Nassar,Andres Marafioti,Matteo Omenetti,Maksym Lysak,Nikolaos Livathinos,Christoph Auer,Lucas Morin,Rafael Teixeira de Lima,Yusik Kim,A. Said Gurbuz,Michele Dolfi,Miquel Farr\u00e9,Peter W. J. StaarAbstractSmolDocling is a compact vision-language model that performs end-to-end document conversion with robust performance across various document types using 256M parameters and a new markup format.AI-generated summaryWe introduce SmolDocling, an ultra-compactvision-language modeltargetingend-to-end document conversion. Our model comprehensively processes entire\npages by generatingDocTags, a newuniversal markup formatthat captures allpage elementsin their full context with location. Unlike existing approaches\nthat rely onlarge foundational models, orensemble solutionsthat rely on\nhandcrafted pipelines of multiplespecialized models, SmolDocling offers an\nend-to-end conversion for accurately capturing content, structure and spatial\nlocation of document elements in a 256M parametersvision-language model.\nSmolDocling exhibits robust performance in correctly reproducing document\nfeatures such ascode listings,tables,equations,charts,lists, and more\nacross a diverse range of document types including business documents, academic\npapers, technical reports, patents, and forms -- significantly extending beyond\nthe commonly observed focus on scientific papers. Additionally, we contribute\nnovelpublicly sourced datasetsforcharts,tables,equations, and code\nrecognition. Experimental results demonstrate that SmolDocling competes with\notherVision Language Modelsthat are up to 27 times larger in size, while\nreducingcomputational requirementssubstantially. The model is currently\navailable, datasets w",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/docling-project/docling",
    "hf_paper_url": "https://huggingface.co/papers/2503.11576",
    "arxiv_url": "https://arxiv.org/abs/2503.11576",
    "num_models": 13,
    "models_list": "docling-project/SmolDocling-256M-preview, ibm-granite/granite-docling-258M, prithivMLmods/granite-docling-258M-f32-GGUF, docling-project/CodeFormulaV2, zboyles/SmolDocling-256M-preview-bf16, Compumacy/sm_doc, kp-forks/SmolDocling-256M-preview, Mungert/SmolDocling-256M-preview-GGUF, Userb1az/granite-docling-258M-GGUF, Mungert/granite-docling-258M-GGUF, pbebbo/granite-docling-258m-fixed, philipp-zettl/ibm-granite__granite-docling-258M, ScottMacdonellDC/granite-docling-258M",
    "models_links": "https://huggingface.co/docling-project/SmolDocling-256M-preview, https://huggingface.co/ibm-granite/granite-docling-258M, https://huggingface.co/prithivMLmods/granite-docling-258M-f32-GGUF, https://huggingface.co/docling-project/CodeFormulaV2, https://huggingface.co/zboyles/SmolDocling-256M-preview-bf16, https://huggingface.co/Compumacy/sm_doc, https://huggingface.co/kp-forks/SmolDocling-256M-preview, https://huggingface.co/Mungert/SmolDocling-256M-preview-GGUF, https://huggingface.co/Userb1az/granite-docling-258M-GGUF, https://huggingface.co/Mungert/granite-docling-258M-GGUF, https://huggingface.co/pbebbo/granite-docling-258m-fixed, https://huggingface.co/philipp-zettl/ibm-granite__granite-docling-258M, https://huggingface.co/ScottMacdonellDC/granite-docling-258M",
    "models_detailed": "[{\"name\": \"docling-project/SmolDocling-256M-preview\", \"link\": \"https://huggingface.co/docling-project/SmolDocling-256M-preview\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"ibm-granite/granite-docling-258M\", \"link\": \"https://huggingface.co/ibm-granite/granite-docling-258M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 23\"}, {\"name\": \"prithivMLmods/granite-docling-258M-f32-GGUF\", \"link\": \"https://huggingface.co/prithivMLmods/granite-docling-258M-f32-GGUF\", \"task\": \"\", \"likes\": \"277\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"docling-project/CodeFormulaV2\", \"link\": \"https://huggingface.co/docling-project/CodeFormulaV2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"zboyles/SmolDocling-256M-preview-bf16\", \"link\": \"https://huggingface.co/zboyles/SmolDocling-256M-preview-bf16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"Compumacy/sm_doc\", \"link\": \"https://huggingface.co/Compumacy/sm_doc\", \"task\": \"\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"kp-forks/SmolDocling-256M-preview\", \"link\": \"https://huggingface.co/kp-forks/SmolDocling-256M-preview\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Mungert/SmolDocling-256M-preview-GGUF\", \"link\": \"https://huggingface.co/Mungert/SmolDocling-256M-preview-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Userb1az/granite-docling-258M-GGUF\", \"link\": \"https://huggingface.co/Userb1az/granite-docling-258M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\"}, {\"name\": \"Mungert/granite-docling-258M-GGUF\", \"link\": \"https://huggingface.co/Mungert/granite-docling-258M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"pbebbo/granite-docling-258m-fixed\", \"link\": \"https://huggingface.co/pbebbo/granite-docling-258m-fixed\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"philipp-zettl/ibm-granite__granite-docling-258M\", \"link\": \"https://huggingface.co/philipp-zettl/ibm-granite__granite-docling-258M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 10\"}, {\"name\": \"ScottMacdonellDC/granite-docling-258M\", \"link\": \"https://huggingface.co/ScottMacdonellDC/granite-docling-258M\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}]",
    "num_datasets": 6,
    "datasets_list": "docling-project/SynthCodeNet, HuggingFaceM4/DoclingMatix, docling-project/SynthChartNet, docling-project/SynthFormulaNet, pranavvmurthy26/DoclingMatix_5K, pranavvmurthy26/DoclingMatix_500",
    "datasets_links": "https://huggingface.co/datasets/docling-project/SynthCodeNet, https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix, https://huggingface.co/datasets/docling-project/SynthChartNet, https://huggingface.co/datasets/docling-project/SynthFormulaNet, https://huggingface.co/datasets/pranavvmurthy26/DoclingMatix_5K, https://huggingface.co/datasets/pranavvmurthy26/DoclingMatix_500",
    "datasets_detailed": "[{\"name\": \"docling-project/SynthCodeNet\", \"link\": \"https://huggingface.co/datasets/docling-project/SynthCodeNet\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\", \"size\": \"\"}, {\"name\": \"HuggingFaceM4/DoclingMatix\", \"link\": \"https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"docling-project/SynthChartNet\", \"link\": \"https://huggingface.co/datasets/docling-project/SynthChartNet\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\", \"size\": \"\"}, {\"name\": \"docling-project/SynthFormulaNet\", \"link\": \"https://huggingface.co/datasets/docling-project/SynthFormulaNet\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"pranavvmurthy26/DoclingMatix_5K\", \"link\": \"https://huggingface.co/datasets/pranavvmurthy26/DoclingMatix_5K\", \"task\": \"\", \"likes\": \"54\", \"downloads\": \"\", \"updated\": \"Oct 22\", \"size\": \"\"}, {\"name\": \"pranavvmurthy26/DoclingMatix_500\", \"link\": \"https://huggingface.co/datasets/pranavvmurthy26/DoclingMatix_500\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Oct 30\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.11579",
    "first_seen_date": "2025-03-17",
    "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.11579Vamba: Understanding Hour-Long Videos with Hybrid Mamba-TransformersPublished on Mar 14\u00b7Submitted byNiels Roggeon Mar 17Upvote22+14Authors:Weiming Ren,Wentao Ma,Huan Yang,Cong Wei,Ge Zhang,Wenhu ChenAbstractVAMBA, a hybrid Mamba-Transformer model, reduces computational costs and GPU memory usage while improving accuracy on long video inputs through linear complexity encoding.AI-generated summaryState-of-the-arttransformer-based large multimodal models(LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existingtoken compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employsMamba-2 blocksto\nencode video tokens with linear complexity. Without any token reduction,VAMBAcan encode more than 1024 frames (640times360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input,VAMBAachieves at least 50% reduction inGPU memory usageduring training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate thatVAMBAimproves accuracy by 4.3% on the challenginghour-long video understandingbenchmarkLVBenchover prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and shortvideo understanding tasks.View arXiv pageView PDFGitHub97Add to collectionCommunitynielsrPaper submitterMar 17Code:https://github.com/TIGER-AI-Lab/VambaSee translationReplylibrarian-botMar 18This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Sema",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/TIGER-AI-Lab/Vamba",
    "hf_paper_url": "https://huggingface.co/papers/2503.11579",
    "arxiv_url": "https://arxiv.org/abs/2503.11579",
    "num_models": 1,
    "models_list": "TIGER-Lab/Vamba-Qwen2-VL-7B",
    "models_links": "https://huggingface.co/TIGER-Lab/Vamba-Qwen2-VL-7B",
    "models_detailed": "[{\"name\": \"TIGER-Lab/Vamba-Qwen2-VL-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/Vamba-Qwen2-VL-7B\", \"task\": \"\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Mar 18\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.11651",
    "first_seen_date": "2025-03-17",
    "title": "VGGT: Visual Geometry Grounded Transformer",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.11651VGGT: Visual Geometry Grounded TransformerPublished on Mar 14\u00b7Submitted byJianyuan Wangon Mar 17\u00b7AI at MetaUpvote34+26Authors:Jianyuan Wang,Minghao Chen,Nikita Karaev,Andrea Vedaldi,Christian Rupprecht,David NovotnyAbstractVGGT, a feed-forward neural network, efficiently infers multiple 3D attributes from single or multiple views, outperforming alternatives and enhancing downstream tasks without post-processing.AI-generated summaryWe present VGGT, afeed-forward neural networkthat directly infers all key\n3D attributes of a scene, includingcamera parameters,point maps,depth maps,\nand3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation,multi-view depth estimation, dense point\ncloud reconstruction, and3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such asnon-rigid point trackingandfeed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt.View arXiv pageView PDFProject pageGitHub12kAdd to collectionCommunityJianyuanWangPaper authorPaper submitterMar 17A feed-forward neural network that directly infers all key 3D attributes of a scene, including extrinsic and intrinsic camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views, within seconds.See translationReplylibrarian-botMar 18This is an automated message from theLibrarian Bot. I found the following papers similar t",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/facebookresearch/vggt",
    "hf_paper_url": "https://huggingface.co/papers/2503.11651",
    "arxiv_url": "https://arxiv.org/abs/2503.11651",
    "num_models": 3,
    "models_list": "facebook/VGGT-1B, facebook/VGGT-1B-Commercial, nvidia/nvpanoptix-3d",
    "models_links": "https://huggingface.co/facebook/VGGT-1B, https://huggingface.co/facebook/VGGT-1B-Commercial, https://huggingface.co/nvidia/nvpanoptix-3d",
    "models_detailed": "[{\"name\": \"facebook/VGGT-1B\", \"link\": \"https://huggingface.co/facebook/VGGT-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 17\"}, {\"name\": \"facebook/VGGT-1B-Commercial\", \"link\": \"https://huggingface.co/facebook/VGGT-1B-Commercial\", \"task\": \"\", \"likes\": \"74\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"nvidia/nvpanoptix-3d\", \"link\": \"https://huggingface.co/nvidia/nvpanoptix-3d\", \"task\": \"\", \"likes\": \"48\", \"downloads\": \"\", \"updated\": \"7 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.10291",
    "first_seen_date": "2025-03-14",
    "title": "VisualPRM: An Effective Process Reward Model for Multimodal Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.10291VisualPRM: An Effective Process Reward Model for Multimodal ReasoningPublished on Mar 13\u00b7Submitted byWeiyun Wangon Mar 14Upvote36+28Authors:Weiyun Wang,Zhangwei Gao,Lianjie Chen,Zhe Chen,Jinguo Zhu,Xiangyu Zhao,Yangzhou Liu,Yue Cao,Shenglong Ye,Xizhou Zhu,Lewei Lu,Haodong Duan,Yu Qiao,Jifeng Dai,Wenhai WangAbstractVisualPRM enhances multimodal reasoning capabilities in MLLMs using Best-of-N strategies and a process supervision dataset, surpassing other models in benchmarks.AI-generated summaryWe introduce VisualPRM, an advancedmultimodal Process Reward Model(PRM)\nwith 8B parameters, which improves the reasoning abilities of existingMultimodal Large Language Models(MLLMs) across different model scales and\nfamilies withBest-of-N(BoN) evaluation strategies. Specifically, our model\nimproves the reasoning performance of three types of MLLMs and four different\nmodel scales. Even when applied to the highly capableInternVL2.5-78B, it\nachieves a 5.9-point improvement across sevenmultimodal reasoning benchmarks.\nExperimental results show that our model exhibits superior performance compared\ntoOutcome Reward ModelsandSelf-Consistencyduring BoN evaluation. To\nfacilitate the training of multimodal PRMs, we construct a multimodal process\nsupervision datasetVisualPRM400Kusing an automated data pipeline. For the\nevaluation of multimodal PRMs, we proposeVisualProcessBench, a benchmark with\nhuman-annotatedstep-wise correctness labels, to measure the abilities of PRMs\nto detect erroneous steps inmultimodal reasoning tasks. We hope that our work\ncan inspire more future research and contribute to the development of MLLMs.\nOur model, data, and benchmark are released in\nhttps://internvl.github.io/blog/2025-03-13-VisualPRM/.View arXiv pageView PDFProject pageGitHub3.57kautoAdd to collectionCommunityWeiyun1025Paper authorPaper submitterMar 14The main contributions of this paper are as follows:Vi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/open-compass/VLMEvalKit",
    "hf_paper_url": "https://huggingface.co/papers/2503.10291",
    "arxiv_url": "https://arxiv.org/abs/2503.10291",
    "num_models": 2,
    "models_list": "OpenGVLab/VisualPRM-8B, OpenGVLab/VisualPRM-8B-v1_1",
    "models_links": "https://huggingface.co/OpenGVLab/VisualPRM-8B, https://huggingface.co/OpenGVLab/VisualPRM-8B-v1_1",
    "models_detailed": "[{\"name\": \"OpenGVLab/VisualPRM-8B\", \"link\": \"https://huggingface.co/OpenGVLab/VisualPRM-8B\", \"task\": \"\", \"likes\": \"539\", \"downloads\": \"\", \"updated\": \"May 6\"}, {\"name\": \"OpenGVLab/VisualPRM-8B-v1_1\", \"link\": \"https://huggingface.co/OpenGVLab/VisualPRM-8B-v1_1\", \"task\": \"\", \"likes\": \"90\", \"downloads\": \"\", \"updated\": \"May 29\"}]",
    "num_datasets": 4,
    "datasets_list": "OpenGVLab/VisualPRM400K-v1.1, OpenGVLab/VisualProcessBench, OpenGVLab/VisualPRM400K, OpenGVLab/VisualPRM400K-v1.1-Raw",
    "datasets_links": "https://huggingface.co/datasets/OpenGVLab/VisualPRM400K-v1.1, https://huggingface.co/datasets/OpenGVLab/VisualProcessBench, https://huggingface.co/datasets/OpenGVLab/VisualPRM400K, https://huggingface.co/datasets/OpenGVLab/VisualPRM400K-v1.1-Raw",
    "datasets_detailed": "[{\"name\": \"OpenGVLab/VisualPRM400K-v1.1\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/VisualPRM400K-v1.1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\", \"size\": \"\"}, {\"name\": \"OpenGVLab/VisualProcessBench\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/VisualProcessBench\", \"task\": \"\", \"likes\": \"120\", \"downloads\": \"\", \"updated\": \"Mar 18\", \"size\": \"\"}, {\"name\": \"OpenGVLab/VisualPRM400K\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/VisualPRM400K\", \"task\": \"\", \"likes\": \"117\", \"downloads\": \"\", \"updated\": \"Apr 15\", \"size\": \"\"}, {\"name\": \"OpenGVLab/VisualPRM400K-v1.1-Raw\", \"link\": \"https://huggingface.co/datasets/OpenGVLab/VisualPRM400K-v1.1-Raw\", \"task\": \"\", \"likes\": \"67\", \"downloads\": \"\", \"updated\": \"Apr 15\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.10460",
    "first_seen_date": "2025-03-14",
    "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.10460Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  BeyondPublished on Mar 13\u00b7Submitted byAKon Mar 14Upvote29+21Authors:Liang Wen,Yunke Cai,Fenrui Xiao,Xin He,Qi An,Zhenyu Duan,Yimin Du,Junchen Liu,Lifu Tang,Xiaowei Lv,Haosheng Zou,Yongchao Deng,Shousheng Jia,Xiangzheng ZhangAbstractThe Light-R1 series develops strong long-chain-of-thought models through curriculum training, semi-on-policy DPO, and reinforcement learning, achieving state-of-the-art performance in math and general domains.AI-generated summaryThis paper presents our work on the Light-R1 series, with models, data, and\ncode all released.\n  We first focus on traininglong COT modelsfrom scratch, specifically\nstarting from models initially lacking long COT capabilities. Using acurriculum trainingrecipe consisting of two-stage SFT andsemi-on-policy DPO,\nwe train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in\nsuperior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite\nbeing trained exclusively on math data, Light-R1-32B shows strong\ngeneralization across other domains. In the subsequent phase of this work, we\nhighlight the significant benefit of the 3k dataset constructed for the second\nSFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled\nmodels using this dataset, we obtain new SOTA models in 7B and 14B, while the\n32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applyingreinforcement learning,\nspecificallyGRPO, on long-COT models to further improve reasoning performance.\nWe successfully train our final Light-R1-14B-DS with RL, achieving SOTA\nperformance among 14B parameter models in math. With AIME24 & 25 scores of 74.0\nand 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and\nDeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected\nbehavior, show",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/qihoo360/light-r1",
    "hf_paper_url": "https://huggingface.co/papers/2503.10460",
    "arxiv_url": "https://arxiv.org/abs/2503.10460",
    "num_models": 12,
    "models_list": "Intelligent-Internet/II-Medical-8B, qihoo360/Light-R1-32B, qihoo360/Light-R1-14B-DS, qihoo360/Light-R1-7B-DS, qihoo360/Light-R1-32B-DS, limcheekin/Light-R1-14B-DS-rk3588-1.1.4, RichardErkhov/qihoo360_-_Light-R1-7B-DS-8bits, EventHorizon-AI/Light-R1-14B-DS-abliterated, Apel-sin/light-R1-14B-DS-exl2, II-Vietnam/II-Medical-8B-SFT, QuantFactory/II-Medical-8B-GGUF, wenliang1990/Light-IF",
    "models_links": "https://huggingface.co/Intelligent-Internet/II-Medical-8B, https://huggingface.co/qihoo360/Light-R1-32B, https://huggingface.co/qihoo360/Light-R1-14B-DS, https://huggingface.co/qihoo360/Light-R1-7B-DS, https://huggingface.co/qihoo360/Light-R1-32B-DS, https://huggingface.co/limcheekin/Light-R1-14B-DS-rk3588-1.1.4, https://huggingface.co/RichardErkhov/qihoo360_-_Light-R1-7B-DS-8bits, https://huggingface.co/EventHorizon-AI/Light-R1-14B-DS-abliterated, https://huggingface.co/Apel-sin/light-R1-14B-DS-exl2, https://huggingface.co/II-Vietnam/II-Medical-8B-SFT, https://huggingface.co/QuantFactory/II-Medical-8B-GGUF, https://huggingface.co/wenliang1990/Light-IF",
    "models_detailed": "[{\"name\": \"Intelligent-Internet/II-Medical-8B\", \"link\": \"https://huggingface.co/Intelligent-Internet/II-Medical-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 12\"}, {\"name\": \"qihoo360/Light-R1-32B\", \"link\": \"https://huggingface.co/qihoo360/Light-R1-32B\", \"task\": \"Text Generation\", \"likes\": \"44\", \"downloads\": \"\", \"updated\": \"Mar 17\"}, {\"name\": \"qihoo360/Light-R1-14B-DS\", \"link\": \"https://huggingface.co/qihoo360/Light-R1-14B-DS\", \"task\": \"Text Generation\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"Mar 17\"}, {\"name\": \"qihoo360/Light-R1-7B-DS\", \"link\": \"https://huggingface.co/qihoo360/Light-R1-7B-DS\", \"task\": \"Text Generation\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"Mar 17\"}, {\"name\": \"qihoo360/Light-R1-32B-DS\", \"link\": \"https://huggingface.co/qihoo360/Light-R1-32B-DS\", \"task\": \"Text Generation\", \"likes\": \"26\", \"downloads\": \"\", \"updated\": \"Mar 17\"}, {\"name\": \"limcheekin/Light-R1-14B-DS-rk3588-1.1.4\", \"link\": \"https://huggingface.co/limcheekin/Light-R1-14B-DS-rk3588-1.1.4\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"RichardErkhov/qihoo360_-_Light-R1-7B-DS-8bits\", \"link\": \"https://huggingface.co/RichardErkhov/qihoo360_-_Light-R1-7B-DS-8bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\"}, {\"name\": \"EventHorizon-AI/Light-R1-14B-DS-abliterated\", \"link\": \"https://huggingface.co/EventHorizon-AI/Light-R1-14B-DS-abliterated\", \"task\": \"Text Generation\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Mar 31\"}, {\"name\": \"Apel-sin/light-R1-14B-DS-exl2\", \"link\": \"https://huggingface.co/Apel-sin/light-R1-14B-DS-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 16\"}, {\"name\": \"II-Vietnam/II-Medical-8B-SFT\", \"link\": \"https://huggingface.co/II-Vietnam/II-Medical-8B-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"QuantFactory/II-Medical-8B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/II-Medical-8B-GGUF\", \"task\": \"\", \"likes\": \"250\", \"downloads\": \"\", \"updated\": \"Jun 15\"}, {\"name\": \"wenliang1990/Light-IF\", \"link\": \"https://huggingface.co/wenliang1990/Light-IF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}]",
    "num_datasets": 5,
    "datasets_list": "qihoo360/Light-R1-SFTData, qihoo360/Light-R1-DPOData, NewstaR/CoTton-64k-6725-Collective, NewstaR/CoTton-67k-6725-Collective, RJTPP/Light-R1-SFTData-Reformatted",
    "datasets_links": "https://huggingface.co/datasets/qihoo360/Light-R1-SFTData, https://huggingface.co/datasets/qihoo360/Light-R1-DPOData, https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective, https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective, https://huggingface.co/datasets/RJTPP/Light-R1-SFTData-Reformatted",
    "datasets_detailed": "[{\"name\": \"qihoo360/Light-R1-SFTData\", \"link\": \"https://huggingface.co/datasets/qihoo360/Light-R1-SFTData\", \"task\": \"\", \"likes\": \"264\", \"downloads\": \"\", \"updated\": \"Mar 17\", \"size\": \"\"}, {\"name\": \"qihoo360/Light-R1-DPOData\", \"link\": \"https://huggingface.co/datasets/qihoo360/Light-R1-DPOData\", \"task\": \"\", \"likes\": \"228\", \"downloads\": \"\", \"updated\": \"Mar 17\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-64k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Jun 8\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-67k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Jun 7\", \"size\": \"\"}, {\"name\": \"RJTPP/Light-R1-SFTData-Reformatted\", \"link\": \"https://huggingface.co/datasets/RJTPP/Light-R1-SFTData-Reformatted\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.10582",
    "first_seen_date": "2025-03-14",
    "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.10582VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  SearchPublished on Mar 13\u00b7Submitted byWenhu Chenon Mar 14Upvote24+16Authors:Yiming Jia,Jiachen Li,Xiang Yue,Bo Li,Ping Nie,Kai Zou,Wenhu ChenAbstractThe VisualWebInstruct approach enhances vision-language models' reasoning abilities through a large, diverse, and high-quality multimodal dataset created from search engine data.AI-generated summaryVision-Language Modelshave made significant progress on many\nperception-focused tasks, however, their progress on reasoning-focused tasks\nseem to be limited due to the lack of high-quality and diverse training data.\nIn this work, we aim to address the scarcity issue of reasoning-focusedmultimodal datasets. We propose VisualWebInstruct - a novel approach that\nleverages search engine to create a diverse, and high-quality dataset spanning\nmultiple disciplines like math, physics, finance, chemistry, etc. Starting with\nmeticulously selected 30,000 seed images, we employ Google Image search to\nidentify websites containing similar images. We collect and process the HTMLs\nfrom over 700K unique URL sources. Through a pipeline of content extraction,\nfiltering and synthesis, we build a dataset of approximately 900Kquestion-answer pairs, with 40% beingvisual QApairs and the rest astext QApairs. Models fine-tuned on VisualWebInstruct demonstrate significant\nperformance gains: (1) training fromLlava-OV-midshows 10-20% absolute point\ngains across benchmarks, (2) training fromMAmmoTH-VLshows 5% absoluate gain.\nOur best modelMAmmoTH-VL2shows state-of-the-art performance within the 10B\nparameter class onMMMU-Pro-std(40.7%),MathVerse(42.6%), andDynaMath(55.7%). These remarkable results highlight the effectiveness of our dataset in\nenhancing VLMs' reasoning capabilities for complex multimodal tasks.View arXiv pageView PDFProject pageGitHub35Add to collectionCommunitywenhuPaper au",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/TIGER-AI-Lab/VisualWebInstruct",
    "hf_paper_url": "https://huggingface.co/papers/2503.10582",
    "arxiv_url": "https://arxiv.org/abs/2503.10582",
    "num_models": 1,
    "models_list": "TIGER-Lab/MAmmoTH-VL2",
    "models_links": "https://huggingface.co/TIGER-Lab/MAmmoTH-VL2",
    "models_detailed": "[{\"name\": \"TIGER-Lab/MAmmoTH-VL2\", \"link\": \"https://huggingface.co/TIGER-Lab/MAmmoTH-VL2\", \"task\": \"\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"May 7\"}]",
    "num_datasets": 4,
    "datasets_list": "TIGER-Lab/VisualWebInstruct, TIGER-Lab/VisualWebInstruct_Verified, TIGER-Lab/VisualWebInstruct-Recall, TIGER-Lab/VisualWebInstruct-Seed",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct, https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct_Verified, https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall, https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Seed",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/VisualWebInstruct\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 12\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/VisualWebInstruct_Verified\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct_Verified\", \"task\": \"\", \"likes\": \"668\", \"downloads\": \"\", \"updated\": \"Oct 24\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/VisualWebInstruct-Recall\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Recall\", \"task\": \"\", \"likes\": \"613\", \"downloads\": \"\", \"updated\": \"Mar 16\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/VisualWebInstruct-Seed\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct-Seed\", \"task\": \"\", \"likes\": \"275\", \"downloads\": \"\", \"updated\": \"Mar 16\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.07572",
    "first_seen_date": "2025-03-12",
    "title": "Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.07572Optimizing Test-Time Compute via Meta Reinforcement Fine-TuningPublished on Mar 10\u00b7Submitted byYuxiao Quon Mar 12Upvote47+39Authors:Yuxiao Qu,Matthew Y. R. Yang,Amrith Setlur,Lewis Tunstall,Edward Emanuel Beeching,Ruslan Salakhutdinov,Aviral KumarAbstractThe paper formalizes test-time compute optimization as a meta-reinforcement learning problem, introducing Meta Reinforcement Fine-Tuning (MRT) to enhance performance and token efficiency in large language models.AI-generated summaryTraining models to effectively usetest-time computeis crucial for improving\nthe reasoning performance of LLMs. Current methods mostly do so via fine-tuning\non search traces or runningRLwith 0/1 outcome reward, but do these approaches\nefficiently utilizetest-time compute? Would these approaches continue to scale\nas the budget improves? In this paper, we try to answer these questions. We\nformalize the problem of optimizingtest-time computeas a meta-reinforcement\nlearning (RL) problem, which provides a principled perspective on spendingtest-time compute. This perspective enables us to view the long output stream\nfrom the LLM as consisting of several episodes run at test time and leads us to\nuse a notion ofcumulative regretover output tokens as a way to measure the\nefficacy oftest-time compute. Akin to howRLalgorithms can best tradeoff\nexploration and exploitation over training, minimizingcumulative regretwould\nalso provide the best balance between exploration and exploitation in the token\nstream. While we show that state-of-the-art models do not minimize regret, one\ncan do so by maximizing adense reward bonusin conjunction with the outcome\n0/1 rewardRL. This bonus is the ''progress'' made by each subsequent block in\nthe output stream, quantified by the change in the likelihood of eventual\nsuccess. Using these insights, we developMeta Reinforcement Fine-Tuning, orMRT, a new class of fine-tuning me",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/CMU-AIRe/MRT",
    "hf_paper_url": "https://huggingface.co/papers/2503.07572",
    "arxiv_url": "https://arxiv.org/abs/2503.07572",
    "num_models": 3,
    "models_list": "Zyphra/ZR1-1.5B, Mungert/ZR1-1.5B-GGUF, cgus/ZR1-1.5B-exl2",
    "models_links": "https://huggingface.co/Zyphra/ZR1-1.5B, https://huggingface.co/Mungert/ZR1-1.5B-GGUF, https://huggingface.co/cgus/ZR1-1.5B-exl2",
    "models_detailed": "[{\"name\": \"Zyphra/ZR1-1.5B\", \"link\": \"https://huggingface.co/Zyphra/ZR1-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"Mungert/ZR1-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/ZR1-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"403\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"cgus/ZR1-1.5B-exl2\", \"link\": \"https://huggingface.co/cgus/ZR1-1.5B-exl2\", \"task\": \"Text Generation\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"Apr 12\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.08638",
    "first_seen_date": "2025-03-12",
    "title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.08638YuE: Scaling Open Foundation Models for Long-Form Music GenerationPublished on Mar 11\u00b7Submitted byRuibin Yuanon Mar 12#3 Paper of the dayUpvote71+63Authors:Ruibin Yuan,Hanfeng Lin,Shuyue Guo,Ge Zhang,Jiahao Pan,Yongyi Zang,Haohe Liu,Yiming Liang,Wenye Ma,Xingjian Du,Xinrun Du,Zhen Ye,Tianyu Zheng,Yinghao Ma,Minghao Liu,Zeyue Tian,Ziya Zhou,Liumeng Xue,Xingwei Qu,Yizhi Li,Shangda Wu,Tianhao Shen+35 authorsAbstractYuE, a family of open foundation models based on LLaMA2, can generate long-form music with aligned lyrics, coherent structure, and appropriate accompaniment using innovative techniques in next-token prediction, conditioning, and pre-training.AI-generated summaryWe tackle the task of long-formmusic generation--particularly the\nchallenging lyrics-to-song problem--by introducing YuE, a family of\nopen foundation models based on the LLaMA2 architecture. Specifically, YuE\nscales to trillions of tokens and generates up to five minutes of music while\nmaintaining lyrical alignment, coherent musical structure, and engaging vocal\nmelodies with appropriate accompaniment. It achieves this through (1)track-decoupled next-token predictionto overcome dense mixture signals, (2)structural progressive conditioningfor long-context lyrical alignment, and (3)\namultitask,multiphase pre-trainingrecipe to converge and generalize. In\naddition, we redesign thein-context learningtechnique formusic generation,\nenabling versatilestyle transfer(e.g., converting Japanese city pop into an\nEnglish rap while preserving the original accompaniment) and bidirectional\ngeneration. Through extensive evaluation, we demonstrate that YuE matches or\neven surpasses some of the proprietary systems in musicality and vocal agility.\nIn addition, fine-tuning YuE enables additional controls and enhanced support\nfor tail languages. Furthermore, beyond generation, we show that YuE's learned\nrepresentations can perfo",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/multimodal-art-projection/YuE",
    "hf_paper_url": "https://huggingface.co/papers/2503.08638",
    "arxiv_url": "https://arxiv.org/abs/2503.08638",
    "num_models": 10,
    "models_list": "m-a-p/YuE-s1-7B-anneal-en-cot, m-a-p/YuE-s2-1B-general, m-a-p/YuE-s1-7B-anneal-en-icl, m-a-p/YuE-s1-7B-anneal-zh-cot, m-a-p/YuE-upsampler, m-a-p/YuE-s1-7B-anneal-jp-kr-cot, m-a-p/YuE-s1-7B-anneal-jp-kr-icl, m-a-p/YuE-s1-7B-anneal-zh-icl, m-a-p/YuE-s1-0.5B, zenlm/zen-musician",
    "models_links": "https://huggingface.co/m-a-p/YuE-s1-7B-anneal-en-cot, https://huggingface.co/m-a-p/YuE-s2-1B-general, https://huggingface.co/m-a-p/YuE-s1-7B-anneal-en-icl, https://huggingface.co/m-a-p/YuE-s1-7B-anneal-zh-cot, https://huggingface.co/m-a-p/YuE-upsampler, https://huggingface.co/m-a-p/YuE-s1-7B-anneal-jp-kr-cot, https://huggingface.co/m-a-p/YuE-s1-7B-anneal-jp-kr-icl, https://huggingface.co/m-a-p/YuE-s1-7B-anneal-zh-icl, https://huggingface.co/m-a-p/YuE-s1-0.5B, https://huggingface.co/zenlm/zen-musician",
    "models_detailed": "[{\"name\": \"m-a-p/YuE-s1-7B-anneal-en-cot\", \"link\": \"https://huggingface.co/m-a-p/YuE-s1-7B-anneal-en-cot\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"m-a-p/YuE-s2-1B-general\", \"link\": \"https://huggingface.co/m-a-p/YuE-s2-1B-general\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"m-a-p/YuE-s1-7B-anneal-en-icl\", \"link\": \"https://huggingface.co/m-a-p/YuE-s1-7B-anneal-en-icl\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"m-a-p/YuE-s1-7B-anneal-zh-cot\", \"link\": \"https://huggingface.co/m-a-p/YuE-s1-7B-anneal-zh-cot\", \"task\": \"Text Generation\", \"likes\": \"359\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"m-a-p/YuE-upsampler\", \"link\": \"https://huggingface.co/m-a-p/YuE-upsampler\", \"task\": \"Text Generation\", \"likes\": \"112\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"m-a-p/YuE-s1-7B-anneal-jp-kr-cot\", \"link\": \"https://huggingface.co/m-a-p/YuE-s1-7B-anneal-jp-kr-cot\", \"task\": \"Text Generation\", \"likes\": \"488\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"m-a-p/YuE-s1-7B-anneal-jp-kr-icl\", \"link\": \"https://huggingface.co/m-a-p/YuE-s1-7B-anneal-jp-kr-icl\", \"task\": \"Text Generation\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"m-a-p/YuE-s1-7B-anneal-zh-icl\", \"link\": \"https://huggingface.co/m-a-p/YuE-s1-7B-anneal-zh-icl\", \"task\": \"Text Generation\", \"likes\": \"259\", \"downloads\": \"\", \"updated\": \"Mar 12\"}, {\"name\": \"m-a-p/YuE-s1-0.5B\", \"link\": \"https://huggingface.co/m-a-p/YuE-s1-0.5B\", \"task\": \"Text Generation\", \"likes\": \"73\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"zenlm/zen-musician\", \"link\": \"https://huggingface.co/zenlm/zen-musician\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.09089",
    "first_seen_date": "2025-03-12",
    "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.09089LocAgent: Graph-Guided LLM Agents for Code LocalizationPublished on Mar 12\u00b7Submitted byXiangru Tangon Mar 12Upvote13+5Authors:Zhaoling Chen,Xiangru Tang,Gangda Deng,Fang Wu,Jialong Wu,Zhiwei Jiang,Viktor Prasanna,Arman Cohan,Xingyao WangAbstractCode localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.View arXiv pageView PDFGitHub560autoAdd to collectionCommunityRTT1Paper authorPaper submitterMar 13Very interesting paper on code localizationSee translationReplylibrarian-botMar 14This is an automated message from theLibrarian Bot. ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/gersteinlab/locagent",
    "hf_paper_url": "https://huggingface.co/papers/2503.09089",
    "arxiv_url": "https://arxiv.org/abs/2503.09089",
    "num_models": 2,
    "models_list": "czlll/Qwen2.5-Coder-32B-CL, czlll/Qwen2.5-Coder-7B-CL",
    "models_links": "https://huggingface.co/czlll/Qwen2.5-Coder-32B-CL, https://huggingface.co/czlll/Qwen2.5-Coder-7B-CL",
    "models_detailed": "[{\"name\": \"czlll/Qwen2.5-Coder-32B-CL\", \"link\": \"https://huggingface.co/czlll/Qwen2.5-Coder-32B-CL\", \"task\": \"Text Generation\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"czlll/Qwen2.5-Coder-7B-CL\", \"link\": \"https://huggingface.co/czlll/Qwen2.5-Coder-7B-CL\", \"task\": \"Text Generation\", \"likes\": \"53\", \"downloads\": \"\", \"updated\": \"Mar 16\"}]",
    "num_datasets": 4,
    "datasets_list": "czlll/Loc-Bench_V1, mteb/LocBenchRR, czlll/Loc-Bench_V0.2, czlll/Loc-Bench_V0.1",
    "datasets_links": "https://huggingface.co/datasets/czlll/Loc-Bench_V1, https://huggingface.co/datasets/mteb/LocBenchRR, https://huggingface.co/datasets/czlll/Loc-Bench_V0.2, https://huggingface.co/datasets/czlll/Loc-Bench_V0.1",
    "datasets_detailed": "[{\"name\": \"czlll/Loc-Bench_V1\", \"link\": \"https://huggingface.co/datasets/czlll/Loc-Bench_V1\", \"task\": \"\", \"likes\": \"560\", \"downloads\": \"\", \"updated\": \"Apr 27\", \"size\": \"\"}, {\"name\": \"mteb/LocBenchRR\", \"link\": \"https://huggingface.co/datasets/mteb/LocBenchRR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 3\", \"size\": \"\"}, {\"name\": \"czlll/Loc-Bench_V0.2\", \"link\": \"https://huggingface.co/datasets/czlll/Loc-Bench_V0.2\", \"task\": \"\", \"likes\": \"660\", \"downloads\": \"\", \"updated\": \"Apr 27\", \"size\": \"\"}, {\"name\": \"czlll/Loc-Bench_V0.1\", \"link\": \"https://huggingface.co/datasets/czlll/Loc-Bench_V0.1\", \"task\": \"\", \"likes\": \"660\", \"downloads\": \"\", \"updated\": \"Apr 27\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.07598",
    "first_seen_date": "2025-03-11",
    "title": "VACE: All-in-One Video Creation and Editing",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.07598VACE: All-in-One Video Creation and EditingPublished on Mar 10\u00b7Submitted byYSHon Mar 11Upvote56+48Authors:Zeyinzi Jiang,Zhen Han,Chaojie Mao,Jingfeng Zhang,Yulin Pan,Yu LiuAbstractVACE, an all-in-one framework for video creation and editing, integrates multiple tasks within a unified model using a Video Condition Unit and Context Adapter for flexible and consistent video synthesis.AI-generated summaryDiffusion Transformerhas demonstrated powerful capability and scalability in\ngenerating high-quality images and videos. Further pursuing the unification of\ngeneration and editing tasks has yielded significant progress in the domain of\nimage content creation. However, due to the intrinsic demands for consistency\nacross both temporal and spatial dynamics, achieving a unified approach forvideo synthesisremains challenging. We introduce VACE, which enables users to\nperform Video tasks within an All-in-one framework for Creation and Editing.\nThese tasks includereference-to-video generation,video-to-video editing, andmasked video-to-video editing. Specifically, we effectively integrate the\nrequirements of various tasks by organizing video task inputs, such as editing,\nreference, and masking, into a unified interface referred to as the Video\nCondition Unit (VCU). Furthermore, by utilizing aContext Adapterstructure, we\ninject different task concepts into the model using formalized representations\nof temporal and spatial dimensions, allowing it to handle arbitrary video\nsynthesis tasks flexibly. Extensive experiments demonstrate that the unified\nmodel of VACE achieves performance on par with task-specific models across\nvarious subtasks. Simultaneously, it enables diverse applications through\nversatile task combinations. Project page:\nhttps://ali-vilab.github.io/VACE-Page/.View arXiv pageView PDFProject pageGitHub3.51kAdd to collectionCommunityBestWishYshPaper submitterMar 11Page:http",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/ali-vilab/VACE",
    "hf_paper_url": "https://huggingface.co/papers/2503.07598",
    "arxiv_url": "https://arxiv.org/abs/2503.07598",
    "num_models": 8,
    "models_list": "Wan-AI/Wan2.1-VACE-14B, ali-vilab/VACE-Wan2.1-1.3B-Preview, Wan-AI/Wan2.1-VACE-1.3B, Wan-AI/Wan2.1-VACE-14B-diffusers, ali-vilab/VACE-LTX-Video-0.9, ali-vilab/VACE-Annotators, Wan-AI/Wan2.1-VACE-1.3B-diffusers, wavespeed/Wan2.1-VACE-14B-bf16",
    "models_links": "https://huggingface.co/Wan-AI/Wan2.1-VACE-14B, https://huggingface.co/ali-vilab/VACE-Wan2.1-1.3B-Preview, https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B, https://huggingface.co/Wan-AI/Wan2.1-VACE-14B-diffusers, https://huggingface.co/ali-vilab/VACE-LTX-Video-0.9, https://huggingface.co/ali-vilab/VACE-Annotators, https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B-diffusers, https://huggingface.co/wavespeed/Wan2.1-VACE-14B-bf16",
    "models_detailed": "[{\"name\": \"Wan-AI/Wan2.1-VACE-14B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-VACE-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"ali-vilab/VACE-Wan2.1-1.3B-Preview\", \"link\": \"https://huggingface.co/ali-vilab/VACE-Wan2.1-1.3B-Preview\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"Wan-AI/Wan2.1-VACE-1.3B\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"Wan-AI/Wan2.1-VACE-14B-diffusers\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-VACE-14B-diffusers\", \"task\": \"\", \"likes\": \"421\", \"downloads\": \"\", \"updated\": \"Jun 6\"}, {\"name\": \"ali-vilab/VACE-LTX-Video-0.9\", \"link\": \"https://huggingface.co/ali-vilab/VACE-LTX-Video-0.9\", \"task\": \"\", \"likes\": \"45\", \"downloads\": \"\", \"updated\": \"Apr 2\"}, {\"name\": \"ali-vilab/VACE-Annotators\", \"link\": \"https://huggingface.co/ali-vilab/VACE-Annotators\", \"task\": \"\", \"likes\": \"135\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"Wan-AI/Wan2.1-VACE-1.3B-diffusers\", \"link\": \"https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B-diffusers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 6\"}, {\"name\": \"wavespeed/Wan2.1-VACE-14B-bf16\", \"link\": \"https://huggingface.co/wavespeed/Wan2.1-VACE-14B-bf16\", \"task\": \"\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Jun 16\"}]",
    "num_datasets": 1,
    "datasets_list": "ali-vilab/VACE-Benchmark",
    "datasets_links": "https://huggingface.co/datasets/ali-vilab/VACE-Benchmark",
    "datasets_detailed": "[{\"name\": \"ali-vilab/VACE-Benchmark\", \"link\": \"https://huggingface.co/datasets/ali-vilab/VACE-Benchmark\", \"task\": \"\", \"likes\": \"696\", \"downloads\": \"\", \"updated\": \"Oct 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.04872",
    "first_seen_date": "2025-03-10",
    "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.04872TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge DistillationPublished on Mar 6\u00b7Submitted byAKon Mar 10Upvote15+7Authors:Lin Sun,Guangxiang Zhao,Xiaoqi Jian,Yuhan Wu,Weihong Lin,Yongfu Zhu,Change Jia,Linglin Zhang,Jinzhu Wu,Junfeng Ran,Sai-er Hu,Zihan Jiang,Junting Zhou,Wenrui Liu,Bin Cui,Tong Yang,Xiangzheng ZhangAbstractThe Branch-Merge distillation approach selectively transfers knowledge from a large teacher model to specialized student models and then merges them for improved generalization and reduced computational cost in LLMs.AI-generated summaryThe challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation andtransfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce theBranch-Merge distillationapproach, which enhancesmodel compressionthrough\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\nselectively distilled into specialized student models viadomain-specific supervised fine-tuning(SFT); And (2) the Merge Phase, where\nthese student models are merged to enablecross-domain knowledge transferand\nimprove generalization. We validate our distillation approach usingDeepSeek-R1as the teacher andDeepSeek-R1-Distill-Qwen-32Bas the student. The resulting\nmerged model,TinyR1-32B-Preview, outperforms its counterpartDeepSeek-R1-Distill-Qwen-32Bacross multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance toDeepSeek-R1on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.View arXiv pageView PDFAdd to collectionCommunityakhaliqPaper submitterMar 10See translationReplylibrarian",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2503.04872",
    "arxiv_url": "https://arxiv.org/abs/2503.04872",
    "num_models": 4,
    "models_list": "qihoo360/TinyR1-32B-Preview, PKU-DS-LAB/FairyR1-32B, PKU-DS-LAB/FairyR1-14B-Preview, Mungert/FairyR1-32B-GGUF",
    "models_links": "https://huggingface.co/qihoo360/TinyR1-32B-Preview, https://huggingface.co/PKU-DS-LAB/FairyR1-32B, https://huggingface.co/PKU-DS-LAB/FairyR1-14B-Preview, https://huggingface.co/Mungert/FairyR1-32B-GGUF",
    "models_detailed": "[{\"name\": \"qihoo360/TinyR1-32B-Preview\", \"link\": \"https://huggingface.co/qihoo360/TinyR1-32B-Preview\", \"task\": \"Text Generation\", \"likes\": \"131\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"PKU-DS-LAB/FairyR1-32B\", \"link\": \"https://huggingface.co/PKU-DS-LAB/FairyR1-32B\", \"task\": \"Text Generation\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"PKU-DS-LAB/FairyR1-14B-Preview\", \"link\": \"https://huggingface.co/PKU-DS-LAB/FairyR1-14B-Preview\", \"task\": \"Text Generation\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"May 27\"}, {\"name\": \"Mungert/FairyR1-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/FairyR1-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"333\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.05236",
    "first_seen_date": "2025-03-09",
    "title": "Unified Reward Model for Multimodal Understanding and Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.05236Unified Reward Model for Multimodal Understanding and GenerationPublished on Mar 7\u00b7Submitted bySII-Yibin Wangon Mar 10Upvote122+114Authors:Yibin Wang,Yuhang Zang,Hao Li,Cheng Jin,Jiaqi WangAbstractA unified reward model for multimodal understanding and generation improves the assessment and preference alignment in image and video tasks through joint learning and direct preference optimization.AI-generated summaryRecent advances in human preference alignment have significantly enhanced\nmultimodal generation and understanding. A key approach is training reward\nmodels to guidepreference optimization. However, existing models are often\ntask-specific, limiting their adaptability across diverse visual applications.\nWe also argue that jointly learning to assess multiple tasks may foster a\nsynergistic effect, where improved image understanding enhances image\ngeneration assessment, and refined image evaluation benefits video assessment\nthrough better frame analysis. To this end, this paper proposesUnifiedReward,\nthe first unified reward model for multimodal understanding and generation\nassessment, enabling bothpairwise rankingandpointwise scoring, which can be\nemployed forvision model preference alignment. Specifically, (1) we first\ndevelopUnifiedRewardon our constructed large-scale human preference dataset,\nincluding both image and video generation/understanding tasks. (2) Then, it is\nutilized to automatically construct high-quality preference pair data based on\nthe vision models, fine-gradually filtering their outputs through pair ranking\nand point sifting. (3) Finally, these data are used for their preference\nalignment throughDirect Preference Optimization (DPO). Experimental results\ndemonstrate that joint learning to assess diverse visual tasks can lead to\nsubstantial mutual benefits and we apply our pipeline to both image and video\nunderstanding/generation tasks, significant",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/CodeGoat24/UnifiedReward",
    "hf_paper_url": "https://huggingface.co/papers/2503.05236",
    "arxiv_url": "https://arxiv.org/abs/2503.05236",
    "num_models": 25,
    "models_list": "CodeGoat24/UnifiedReward-7b-v1.5, CodeGoat24/UnifiedReward-7b, CodeGoat24/UnifiedReward-qwen-7b, CodeGoat24/UnifiedReward-Edit-qwen-7b, CodeGoat24/llava-onevision-qwen2-7b-ov-unifiedreward-dpo, CodeGoat24/LLaVA-Video-7B-Qwen2-UnifiedReward-DPO, CodeGoat24/T2V-Turbo, CodeGoat24/sdxl-turbo-unified-reward-dpo, CodeGoat24/UnifiedReward-0.5b, CodeGoat24/UnifiedReward-qwen-3b, CodeGoat24/UnifiedReward-qwen-32b, CodeGoat24/UnifiedReward-2.0-qwen-7b, CodeGoat24/UnifiedReward-2.0-qwen-3b, CodeGoat24/UnifiedReward-2.0-qwen-32b, CodeGoat24/UnifiedReward-Edit-qwen-3b, CodeGoat24/UnifiedReward-Edit-qwen-32b, CodeGoat24/UnifiedReward-Edit-qwen-72b, CodeGoat24/UnifiedReward-2.0-qwen3vl-8b, CodeGoat24/UnifiedReward-2.0-qwen3vl-2b, CodeGoat24/UnifiedReward-2.0-qwen3vl-4b, CodeGoat24/UnifiedReward-Edit-qwen3vl-2b, CodeGoat24/UnifiedReward-Edit-qwen3vl-8b, CodeGoat24/UnifiedReward-Edit-qwen3vl-4b, CodeGoat24/UnifiedReward-2.0-qwen3vl-32b, CodeGoat24/UnifiedReward-Edit-qwen3vl-32b",
    "models_links": "https://huggingface.co/CodeGoat24/UnifiedReward-7b-v1.5, https://huggingface.co/CodeGoat24/UnifiedReward-7b, https://huggingface.co/CodeGoat24/UnifiedReward-qwen-7b, https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen-7b, https://huggingface.co/CodeGoat24/llava-onevision-qwen2-7b-ov-unifiedreward-dpo, https://huggingface.co/CodeGoat24/LLaVA-Video-7B-Qwen2-UnifiedReward-DPO, https://huggingface.co/CodeGoat24/T2V-Turbo, https://huggingface.co/CodeGoat24/sdxl-turbo-unified-reward-dpo, https://huggingface.co/CodeGoat24/UnifiedReward-0.5b, https://huggingface.co/CodeGoat24/UnifiedReward-qwen-3b, https://huggingface.co/CodeGoat24/UnifiedReward-qwen-32b, https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen-7b, https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen-3b, https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen-32b, https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen-3b, https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen-32b, https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen-72b, https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen3vl-8b, https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen3vl-2b, https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen3vl-4b, https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen3vl-2b, https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen3vl-8b, https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen3vl-4b, https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen3vl-32b, https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen3vl-32b",
    "models_detailed": "[{\"name\": \"CodeGoat24/UnifiedReward-7b-v1.5\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-7b-v1.5\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"CodeGoat24/UnifiedReward-7b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-7b\", \"task\": \"\", \"likes\": \"68\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"CodeGoat24/UnifiedReward-qwen-7b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-qwen-7b\", \"task\": \"\", \"likes\": \"297\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/UnifiedReward-Edit-qwen-7b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen-7b\", \"task\": \"\", \"likes\": \"163\", \"downloads\": \"\", \"updated\": \"Oct 22\"}, {\"name\": \"CodeGoat24/llava-onevision-qwen2-7b-ov-unifiedreward-dpo\", \"link\": \"https://huggingface.co/CodeGoat24/llava-onevision-qwen2-7b-ov-unifiedreward-dpo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/LLaVA-Video-7B-Qwen2-UnifiedReward-DPO\", \"link\": \"https://huggingface.co/CodeGoat24/LLaVA-Video-7B-Qwen2-UnifiedReward-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/T2V-Turbo\", \"link\": \"https://huggingface.co/CodeGoat24/T2V-Turbo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/sdxl-turbo-unified-reward-dpo\", \"link\": \"https://huggingface.co/CodeGoat24/sdxl-turbo-unified-reward-dpo\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/UnifiedReward-0.5b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-0.5b\", \"task\": \"\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/UnifiedReward-qwen-3b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-qwen-3b\", \"task\": \"\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/UnifiedReward-qwen-32b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-qwen-32b\", \"task\": \"\", \"likes\": \"34\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"CodeGoat24/UnifiedReward-2.0-qwen-7b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen-7b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"CodeGoat24/UnifiedReward-2.0-qwen-3b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen-3b\", \"task\": \"\", \"likes\": \"116\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"CodeGoat24/UnifiedReward-2.0-qwen-32b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen-32b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"CodeGoat24/UnifiedReward-Edit-qwen-3b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen-3b\", \"task\": \"\", \"likes\": \"921\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"CodeGoat24/UnifiedReward-Edit-qwen-32b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen-32b\", \"task\": \"\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"Oct 26\"}, {\"name\": \"CodeGoat24/UnifiedReward-Edit-qwen-72b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen-72b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"CodeGoat24/UnifiedReward-2.0-qwen3vl-8b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen3vl-8b\", \"task\": \"\", \"likes\": \"195\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"CodeGoat24/UnifiedReward-2.0-qwen3vl-2b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen3vl-2b\", \"task\": \"\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Nov 8\"}, {\"name\": \"CodeGoat24/UnifiedReward-2.0-qwen3vl-4b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen3vl-4b\", \"task\": \"\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"CodeGoat24/UnifiedReward-Edit-qwen3vl-2b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen3vl-2b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"CodeGoat24/UnifiedReward-Edit-qwen3vl-8b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen3vl-8b\", \"task\": \"\", \"likes\": \"60\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"CodeGoat24/UnifiedReward-Edit-qwen3vl-4b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen3vl-4b\", \"task\": \"\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"CodeGoat24/UnifiedReward-2.0-qwen3vl-32b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-2.0-qwen3vl-32b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"CodeGoat24/UnifiedReward-Edit-qwen3vl-32b\", \"link\": \"https://huggingface.co/CodeGoat24/UnifiedReward-Edit-qwen3vl-32b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\"}]",
    "num_datasets": 10,
    "datasets_list": "CodeGoat24/UnifiedReward-2.0-T2X-score-data, CodeGoat24/LLaVA-Critic-113k, CodeGoat24/VideoDPO, CodeGoat24/EvalMuse, CodeGoat24/HPD, CodeGoat24/LiFT-HRA, CodeGoat24/OIP, CodeGoat24/ShareGPTVideo-DPO, CodeGoat24/VideoFeedback, CodeGoat24/ImageGen-CoT-Reward-5K",
    "datasets_links": "https://huggingface.co/datasets/CodeGoat24/UnifiedReward-2.0-T2X-score-data, https://huggingface.co/datasets/CodeGoat24/LLaVA-Critic-113k, https://huggingface.co/datasets/CodeGoat24/VideoDPO, https://huggingface.co/datasets/CodeGoat24/EvalMuse, https://huggingface.co/datasets/CodeGoat24/HPD, https://huggingface.co/datasets/CodeGoat24/LiFT-HRA, https://huggingface.co/datasets/CodeGoat24/OIP, https://huggingface.co/datasets/CodeGoat24/ShareGPTVideo-DPO, https://huggingface.co/datasets/CodeGoat24/VideoFeedback, https://huggingface.co/datasets/CodeGoat24/ImageGen-CoT-Reward-5K",
    "datasets_detailed": "[{\"name\": \"CodeGoat24/UnifiedReward-2.0-T2X-score-data\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/UnifiedReward-2.0-T2X-score-data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 26\", \"size\": \"\"}, {\"name\": \"CodeGoat24/LLaVA-Critic-113k\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/LLaVA-Critic-113k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/VideoDPO\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/VideoDPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/EvalMuse\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/EvalMuse\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/HPD\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/HPD\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/LiFT-HRA\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/LiFT-HRA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\", \"size\": \"\"}, {\"name\": \"CodeGoat24/OIP\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/OIP\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/ShareGPTVideo-DPO\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/ShareGPTVideo-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\", \"size\": \"\"}, {\"name\": \"CodeGoat24/VideoFeedback\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/VideoFeedback\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\", \"size\": \"\"}, {\"name\": \"CodeGoat24/ImageGen-CoT-Reward-5K\", \"link\": \"https://huggingface.co/datasets/CodeGoat24/ImageGen-CoT-Reward-5K\", \"task\": \"\", \"likes\": \"126\", \"downloads\": \"\", \"updated\": \"Aug 29\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.03983",
    "first_seen_date": "2025-03-07",
    "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding\n  and Expert Reasoning Abilities",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.03983Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding\n  and Expert Reasoning AbilitiesPublished on Mar 6\u00b7Submitted byGhoshon Mar 7Upvote26+18Authors:Sreyan Ghosh,Zhifeng Kong,Sonal Kumar,S Sakshi,Jaehyeon Kim,Wei Ping,Rafael Valle,Dinesh Manocha,Bryan CatanzaroAbstractAF2, an advanced Audio-Language Model, demonstrates state-of-the-art performance on over 20 benchmarks and extends audio understanding to long segments using the LongAudio dataset and LongAudioBench.AI-generated summaryUnderstanding and reasoning over non-speech sounds and music are crucial for\nboth humans and AI agents to interact effectively with their environments. In\nthis paper, we introduceAudio Flamingo 2(AF2), anAudio-Language Model(ALM)\nwith advanced audio understanding and reasoning capabilities. AF2 leverages (i)\na customCLAP model, (ii) syntheticAudio QA datafor fine-grained audio\nreasoning, and (iii) amulti-stage curriculum learningstrategy. AF2 achieves\nstate-of-the-art performance with only a 3B parameter small language model,\nsurpassing large open-source and proprietary models across over 20 benchmarks.\nNext, for the first time, we extend audio understanding to long audio segments\n(30 secs to 5 mins) and proposeLongAudio, a large and novel dataset for\ntraining ALMs onlong audio captioningand question-answering tasks.\nFine-tuning AF2 onLongAudioleads to exceptional performance on our proposedLongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio\nunderstanding capabilities. We conduct extensive ablation studies to confirm\nthe efficacy of our approach. Project Website:\nhttps://research.nvidia.com/labs/adlr/AF2/.View arXiv pageView PDFProject pageGitHub920Add to collectionCommunitySreyan88Paper submitterMar 7Demo:https://huggingface.co/spaces/nvidia/audio-flamingo-2Code and checkpoints:https://github.com/NVIDIA/audio-flamingoSee translationReplylibrarian",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/NVIDIA/audio-flamingo",
    "hf_paper_url": "https://huggingface.co/papers/2503.03983",
    "arxiv_url": "https://arxiv.org/abs/2503.03983",
    "num_models": 4,
    "models_list": "nvidia/audio-flamingo-2, nvidia/audio-flamingo-2-0.5B, nvidia/audio-flamingo-2-SoundCoT, nvidia/audio-flamingo-2-1.5B",
    "models_links": "https://huggingface.co/nvidia/audio-flamingo-2, https://huggingface.co/nvidia/audio-flamingo-2-0.5B, https://huggingface.co/nvidia/audio-flamingo-2-SoundCoT, https://huggingface.co/nvidia/audio-flamingo-2-1.5B",
    "models_detailed": "[{\"name\": \"nvidia/audio-flamingo-2\", \"link\": \"https://huggingface.co/nvidia/audio-flamingo-2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 18\"}, {\"name\": \"nvidia/audio-flamingo-2-0.5B\", \"link\": \"https://huggingface.co/nvidia/audio-flamingo-2-0.5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}, {\"name\": \"nvidia/audio-flamingo-2-SoundCoT\", \"link\": \"https://huggingface.co/nvidia/audio-flamingo-2-SoundCoT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"nvidia/audio-flamingo-2-1.5B\", \"link\": \"https://huggingface.co/nvidia/audio-flamingo-2-1.5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 25\"}]",
    "num_datasets": 5,
    "datasets_list": "nvidia/AudioSkills, nvidia/AF-Think, nvidia/LongAudio, nvidia/AF-Chat, sonalkum/AudioSkills-Llama3",
    "datasets_links": "https://huggingface.co/datasets/nvidia/AudioSkills, https://huggingface.co/datasets/nvidia/AF-Think, https://huggingface.co/datasets/nvidia/LongAudio, https://huggingface.co/datasets/nvidia/AF-Chat, https://huggingface.co/datasets/sonalkum/AudioSkills-Llama3",
    "datasets_detailed": "[{\"name\": \"nvidia/AudioSkills\", \"link\": \"https://huggingface.co/datasets/nvidia/AudioSkills\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 12\", \"size\": \"\"}, {\"name\": \"nvidia/AF-Think\", \"link\": \"https://huggingface.co/datasets/nvidia/AF-Think\", \"task\": \"\", \"likes\": \"529\", \"downloads\": \"\", \"updated\": \"Aug 7\", \"size\": \"\"}, {\"name\": \"nvidia/LongAudio\", \"link\": \"https://huggingface.co/datasets/nvidia/LongAudio\", \"task\": \"\", \"likes\": \"364\", \"downloads\": \"\", \"updated\": \"Aug 8\", \"size\": \"\"}, {\"name\": \"nvidia/AF-Chat\", \"link\": \"https://huggingface.co/datasets/nvidia/AF-Chat\", \"task\": \"\", \"likes\": \"157\", \"downloads\": \"\", \"updated\": \"Jul 21\", \"size\": \"\"}, {\"name\": \"sonalkum/AudioSkills-Llama3\", \"link\": \"https://huggingface.co/datasets/sonalkum/AudioSkills-Llama3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 18\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.04378",
    "first_seen_date": "2025-03-07",
    "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for\n  Open-Ended General-Domain Tasks",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.04378Dedicated Feedback and Edit Models Empower Inference-Time Scaling for\n  Open-Ended General-Domain TasksPublished on Mar 6\u00b7Submitted byAKon Mar 7Upvote7Authors:Zhilin Wang,Jiaqi Zeng,Olivier Delalleau,Daniel Egert,Ellie Evans,Hoo-Chang Shin,Felipe Soares,Yi Dong,Oleksii KuchaievAbstractFeedback and Edit Models trained for inference-time scaling improve performance on open-ended tasks, surpassing other models on a competitive benchmark.AI-generated summaryInference-Time Scalinghas been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models forinference-time scalingrequire tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect data\nfor and train dedicatedFeedback and Edit Modelsthat are capable of performinginference-time scalingfor open-ended general-domain tasks. In our setup, one\nmodel generates an initial response, which are given feedback by a second\nmodel, that are then used by a third model to edit the response. We show that\nperformance onArena Hard, a benchmark strongly predictive ofChatbot Arena Elocan be boosted by scaling the number of initial response drafts, effective\nfeedback and edited responses. When scaled optimally, our setup based on 70B\nmodels from theLlama 3family can reach SoTA performance onArena Hardat 92.7\nas of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3.View arXiv pageView PDFAdd to collectionCommunityakhaliqPaper submitterMar 7See translationReplyFivethousandMar 7Hi, thanks for the great work! Will the datasets mentioned in the paper be made public?See transla",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2503.04378",
    "arxiv_url": "https://arxiv.org/abs/2503.04378",
    "num_models": 8,
    "models_list": "nvidia/Qwen3-Nemotron-32B-RLBFF, nvidia/Qwen3-Nemotron-32B-GenRM-Principle, nvidia/Llama-3.3-Nemotron-70B-Feedback, nvidia/Llama-3.3-Nemotron-70B-Reward-Principle, nvidia/Llama-3.3-Nemotron-70B-Edit, cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit, cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit, ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx",
    "models_links": "https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF, https://huggingface.co/nvidia/Qwen3-Nemotron-32B-GenRM-Principle, https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Feedback, https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward-Principle, https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Edit, https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit, https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit, https://huggingface.co/ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx",
    "models_detailed": "[{\"name\": \"nvidia/Qwen3-Nemotron-32B-RLBFF\", \"link\": \"https://huggingface.co/nvidia/Qwen3-Nemotron-32B-RLBFF\", \"task\": \"Text Generation\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"nvidia/Qwen3-Nemotron-32B-GenRM-Principle\", \"link\": \"https://huggingface.co/nvidia/Qwen3-Nemotron-32B-GenRM-Principle\", \"task\": \"Text Generation\", \"likes\": \"667\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"nvidia/Llama-3.3-Nemotron-70B-Feedback\", \"link\": \"https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Feedback\", \"task\": \"Text Generation\", \"likes\": \"77\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"nvidia/Llama-3.3-Nemotron-70B-Reward-Principle\", \"link\": \"https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward-Principle\", \"task\": \"Text Generation\", \"likes\": \"126\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"nvidia/Llama-3.3-Nemotron-70B-Edit\", \"link\": \"https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Edit\", \"task\": \"Text Generation\", \"likes\": \"63\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit\", \"link\": \"https://huggingface.co/cyankiwi/Qwen3-Nemotron-32B-RLBFF-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx\", \"link\": \"https://huggingface.co/ExaltedSlayer/Qwen3-Nemotron-32B-RLBFF-mxfp4-mlx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}]",
    "num_datasets": 2,
    "datasets_list": "nvidia/HelpSteer3, Jennny/h3_pairs",
    "datasets_links": "https://huggingface.co/datasets/nvidia/HelpSteer3, https://huggingface.co/datasets/Jennny/h3_pairs",
    "datasets_detailed": "[{\"name\": \"nvidia/HelpSteer3\", \"link\": \"https://huggingface.co/datasets/nvidia/HelpSteer3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 16\", \"size\": \"\"}, {\"name\": \"Jennny/h3_pairs\", \"link\": \"https://huggingface.co/datasets/Jennny/h3_pairs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 13\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.00329",
    "first_seen_date": "2025-03-06",
    "title": "ABC: Achieving Better Control of Multimodal Embeddings using VLMs",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.00329ABC: Achieving Better Control of Multimodal Embeddings using VLMsPublished on Mar 1\u00b7Submitted byWenhu Chenon Mar 6Upvote20+12Authors:Benjamin Schneider,Florian Kerschbaum,Wenhu ChenAbstractABC is a multimodal embedding model using a vision-language backbone that achieves top performance on MSCOCO, classification, and VQA tasks, offering high-quality representations with flexible natural language control.AI-generated summaryVisual embedding modelsexcel atzero-shot taskslike visual retrieval andclassification. However, these models cannot be used for tasks that contain\nambiguity or require user instruction. These tasks necessitate a multimodal\nembedding model, which outputs embeddings that combine visual and natural\nlanguage input. ExistingCLIP-based approachesembed images and text\nindependently, and fuse the result. We find that this results in weak\ninteractions between modalities, and poor user control over the representation.\nWe introduce ABC, an open-sourcemultimodal embedding modelthat uses a\nvision-language model backbone to deeply integrateimage featureswith natural\nlanguage instructions. ABC achieves bestfor-size performance on MSCOCO\nimage-to-text retrieval and is the top performing model onclassificationandVQA tasksin theMassive Multimodal Embedding Benchmark. With a strongly\nunified vision-language representation, ABC can use natural language to solve\nsubtle and potentially ambiguousvisual retrieval problems. To evaluate this\ncapability, we designCtrlBench, a benchmark that requires interleaving textual\ninstructions with image content for correct retrieval. ABC advances the state\nof multimodal embeddings by offering high-quality representations and flexible\nnatural language control. Our model and datasets are available at our project\npage.View arXiv pageView PDFProject pageGitHub18Add to collectionCommunitywenhuPaper authorPaper submitterMar 6We propose a novel ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/TIGER-AI-Lab/ABC",
    "hf_paper_url": "https://huggingface.co/papers/2503.00329",
    "arxiv_url": "https://arxiv.org/abs/2503.00329",
    "num_models": 2,
    "models_list": "TIGER-Lab/ABC-Qwen2VL-Pretrain, TIGER-Lab/ABC-Qwen2VL-Instruct",
    "models_links": "https://huggingface.co/TIGER-Lab/ABC-Qwen2VL-Pretrain, https://huggingface.co/TIGER-Lab/ABC-Qwen2VL-Instruct",
    "models_detailed": "[{\"name\": \"TIGER-Lab/ABC-Qwen2VL-Pretrain\", \"link\": \"https://huggingface.co/TIGER-Lab/ABC-Qwen2VL-Pretrain\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Mar 11\"}, {\"name\": \"TIGER-Lab/ABC-Qwen2VL-Instruct\", \"link\": \"https://huggingface.co/TIGER-Lab/ABC-Qwen2VL-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 11\"}]",
    "num_datasets": 2,
    "datasets_list": "TIGER-Lab/ABC-Pretraining-Data, TIGER-Lab/ABC-VG-Instruct",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data, https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/ABC-Pretraining-Data\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/ABC-Pretraining-Data\", \"task\": \"\", \"likes\": \"471\", \"downloads\": \"\", \"updated\": \"Aug 29\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/ABC-VG-Instruct\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/ABC-VG-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 29\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.03751",
    "first_seen_date": "2025-03-06",
    "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.03751GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  ControlPublished on Mar 5\u00b7Submitted byAKon Mar 6Upvote24+16Authors:Xuanchi Ren,Tianchang Shen,Jiahui Huang,Huan Ling,Yifan Lu,Merlin Nimier-David,Thomas M\u00fcller,Alexander Keller,Sanja Fidler,Jun GaoAbstractGEN3C, a generative video model, uses a 3D cache to achieve precise camera control and temporal consistency in video generation, outperforming previous methods in sparse-view novel view synthesis.AI-generated summaryWe present GEN3C, agenerative video modelwith preciseCamera Controlandtemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence.Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a3D cache:point cloudsobtained by predicting thepixel-wise depthof seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the2D renderingsof the3D cachewith\nthe newcamera trajectoryprovided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such asdriving scenesandmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/View arXiv pageView PDFGitHub1.21kautoAdd to coll",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/nv-tlabs/GEN3C",
    "hf_paper_url": "https://huggingface.co/papers/2503.03751",
    "arxiv_url": "https://arxiv.org/abs/2503.03751",
    "num_models": 1,
    "models_list": "nvidia/GEN3C-Cosmos-7B",
    "models_links": "https://huggingface.co/nvidia/GEN3C-Cosmos-7B",
    "models_detailed": "[{\"name\": \"nvidia/GEN3C-Cosmos-7B\", \"link\": \"https://huggingface.co/nvidia/GEN3C-Cosmos-7B\", \"task\": \"\", \"likes\": \"237\", \"downloads\": \"\", \"updated\": \"Jun 18\"}]",
    "num_datasets": 1,
    "datasets_list": "nvidia/GEN3C-Testing-Example",
    "datasets_links": "https://huggingface.co/datasets/nvidia/GEN3C-Testing-Example",
    "datasets_detailed": "[{\"name\": \"nvidia/GEN3C-Testing-Example\", \"link\": \"https://huggingface.co/datasets/nvidia/GEN3C-Testing-Example\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Aug 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2503.01103",
    "first_seen_date": "2025-03-04",
    "title": "Direct Discriminative Optimization: Your Likelihood-Based Visual\n  Generative Model is Secretly a GAN Discriminator",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.01103Direct Discriminative Optimization: Your Likelihood-Based Visual\n  Generative Model is Secretly a GAN DiscriminatorPublished on Mar 3\u00b7Submitted byKaiwen Zhengon Mar 4Upvote5Authors:Kaiwen Zheng,Yongxin Chen,Huayu Chen,Guande He,Ming-Yu Liu,Jun Zhu,Qinsheng ZhangAbstractA unified framework called Direct Discriminative Optimization (DDO) enhances generative models by combining likelihood-based and GAN objectives, improving model fidelity and efficiency without joint training.AI-generated summaryWhilelikelihood-based generative models, particularly diffusion andautoregressive models, have achieved remarkable fidelity in visual generation,\nthemaximum likelihood estimation (MLE)objective inherently suffers from a\nmode-covering tendency that limits the generation quality under limited model\ncapacity. In this work, we proposeDirect Discriminative Optimization (DDO)as\na unified framework that bridges likelihood-based generative training and the\nGAN objective to bypass this fundamental constraint. Our key insight is to\nparameterize adiscriminatorimplicitly using thelikelihood ratiobetween a\nlearnable target model and a fixed reference model, drawing parallels with the\nphilosophy ofDirect Preference Optimization (DPO). Unlike GANs, this\nparameterization eliminates the need for joint training ofgeneratoranddiscriminatornetworks, allowing for direct, efficient, and effective\nfinetuning of a well-trained model to its full potential beyond the limits of\nMLE. DDO can be performed iteratively in a self-play manner for progressive\nmodel refinement, with each round requiring less than 1% of pretraining epochs.\nOur experiments demonstrate the effectiveness of DDO by significantly advancing\nthe previousSOTA diffusion model EDM, reducingFID scoresfrom 1.79/1.58 to\nnew records of 1.30/0.97 on CIFAR-10/ImageNet-64datasets, and by consistently\nimproving bothguidance-freeandCFG-enhancedFIDs of v",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2503.01103",
    "arxiv_url": "https://arxiv.org/abs/2503.01103",
    "num_models": 1,
    "models_list": "nvidia/DirectDiscriminativeOptimization",
    "models_links": "https://huggingface.co/nvidia/DirectDiscriminativeOptimization",
    "models_detailed": "[{\"name\": \"nvidia/DirectDiscriminativeOptimization\", \"link\": \"https://huggingface.co/nvidia/DirectDiscriminativeOptimization\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.01743",
    "first_seen_date": "2025-03-04",
    "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language\n  Models via Mixture-of-LoRAs",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.01743Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language\n  Models via Mixture-of-LoRAsPublished on Mar 3\u00b7Submitted byYoung Jin Kimon Mar 4#1 Paper of the dayUpvote89+81Authors:Abdelrahman Abouelenin,Atabak Ashfaq,Adam Atkinson,Hany Awadalla,Nguyen Bach,Jianmin Bao,Alon Benhaim,Martin Cai,Vishrav Chaudhary,Congcong Chen,Dong Chen,Dongdong Chen,Junkun Chen,Weizhu Chen,Yen-Chun Chen,Yi-ling Chen,Qi Dai,Xiyang Dai,Ruchao Fan,Mei Gao,Min Gao,Amit Garg+51 authorsAbstractPhi-4-Mini and Phi-4-Multimodal are compact yet highly capable language and multimodal models, featuring advanced attention mechanisms and LoRA adapters for efficient and powerful reasoning across various tasks.AI-generated summaryWe introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable\nlanguage and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language\nmodel trained on high-quality web and synthetic data, significantly\noutperforming recent open-source models of similar size and matching the\nperformance of models twice its size on math and coding tasks requiring complex\nreasoning. This achievement is driven by a carefully curated synthetic data\nrecipe emphasizing high-quality math and coding datasets. Compared to its\npredecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of\n200K tokens to better support multilingual applications, as well as group query\nattention for more efficient long-sequence generation. Phi-4-Multimodal is a\nmultimodal model that integrates text, vision, and speech/audio input\nmodalities into a single model. Its novel modality extension approach leveragesLoRA adaptersandmodality-specific routersto allow multiple inference modes\ncombining various modalities without interference. For example, it now ranks\nfirst in theOpenASRleaderboard to date, although the LoRA component of the\nspeech/audio modality has just 460 million parameters. P",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2503.01743",
    "arxiv_url": "https://arxiv.org/abs/2503.01743",
    "num_models": 19,
    "models_list": "microsoft/Phi-4-multimodal-instruct, microsoft/Phi-4-mini-instruct, bubblspace/Bubbl-P4-multimodal-instruct, Mungert/Phi-4-mini-instruct-GGUF, NexaAI/phi4-mini-npu-turbo, imkebe/Phi-4-mini-instruct-rk3588-1.2.0, IronWolfAI/GoldenCrow, iqbalamo93/Phi-4-mini-instruct-GPTQ-4bit, iqbalamo93/Phi-4-mini-instruct-GPTQ-8bit, Bifrost-AI/Phi-4-bifrost-sol-3.8B, Bifrost-AI/Phi-4-bifrost-sol-3.8B-gguf, Lexius/Phi-4-multimodal-instruct, lakshmi97/Phi-4-multimodal-instruct, Frane92O/Phi-4-mini-instruct-bnb-4bit, ErazerControl/Phi-4-mini-instruct, gradientdegen/phi-4-mini-instruct-base, kumapo/Phi-4-multimodal-instruct, FriendliAI/Phi-4-multimodal-instruct, JacobLinCool/phi-4-audio",
    "models_links": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct, https://huggingface.co/microsoft/Phi-4-mini-instruct, https://huggingface.co/bubblspace/Bubbl-P4-multimodal-instruct, https://huggingface.co/Mungert/Phi-4-mini-instruct-GGUF, https://huggingface.co/NexaAI/phi4-mini-npu-turbo, https://huggingface.co/imkebe/Phi-4-mini-instruct-rk3588-1.2.0, https://huggingface.co/IronWolfAI/GoldenCrow, https://huggingface.co/iqbalamo93/Phi-4-mini-instruct-GPTQ-4bit, https://huggingface.co/iqbalamo93/Phi-4-mini-instruct-GPTQ-8bit, https://huggingface.co/Bifrost-AI/Phi-4-bifrost-sol-3.8B, https://huggingface.co/Bifrost-AI/Phi-4-bifrost-sol-3.8B-gguf, https://huggingface.co/Lexius/Phi-4-multimodal-instruct, https://huggingface.co/lakshmi97/Phi-4-multimodal-instruct, https://huggingface.co/Frane92O/Phi-4-mini-instruct-bnb-4bit, https://huggingface.co/ErazerControl/Phi-4-mini-instruct, https://huggingface.co/gradientdegen/phi-4-mini-instruct-base, https://huggingface.co/kumapo/Phi-4-multimodal-instruct, https://huggingface.co/FriendliAI/Phi-4-multimodal-instruct, https://huggingface.co/JacobLinCool/phi-4-audio",
    "models_detailed": "[{\"name\": \"microsoft/Phi-4-multimodal-instruct\", \"link\": \"https://huggingface.co/microsoft/Phi-4-multimodal-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"microsoft/Phi-4-mini-instruct\", \"link\": \"https://huggingface.co/microsoft/Phi-4-mini-instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"bubblspace/Bubbl-P4-multimodal-instruct\", \"link\": \"https://huggingface.co/bubblspace/Bubbl-P4-multimodal-instruct\", \"task\": \"\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"Apr 15\"}, {\"name\": \"Mungert/Phi-4-mini-instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/Phi-4-mini-instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"718\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"NexaAI/phi4-mini-npu-turbo\", \"link\": \"https://huggingface.co/NexaAI/phi4-mini-npu-turbo\", \"task\": \"Text Generation\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"imkebe/Phi-4-mini-instruct-rk3588-1.2.0\", \"link\": \"https://huggingface.co/imkebe/Phi-4-mini-instruct-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Apr 17\"}, {\"name\": \"IronWolfAI/GoldenCrow\", \"link\": \"https://huggingface.co/IronWolfAI/GoldenCrow\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 18\"}, {\"name\": \"iqbalamo93/Phi-4-mini-instruct-GPTQ-4bit\", \"link\": \"https://huggingface.co/iqbalamo93/Phi-4-mini-instruct-GPTQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"iqbalamo93/Phi-4-mini-instruct-GPTQ-8bit\", \"link\": \"https://huggingface.co/iqbalamo93/Phi-4-mini-instruct-GPTQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"40\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"Bifrost-AI/Phi-4-bifrost-sol-3.8B\", \"link\": \"https://huggingface.co/Bifrost-AI/Phi-4-bifrost-sol-3.8B\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Jun 1\"}, {\"name\": \"Bifrost-AI/Phi-4-bifrost-sol-3.8B-gguf\", \"link\": \"https://huggingface.co/Bifrost-AI/Phi-4-bifrost-sol-3.8B-gguf\", \"task\": \"Text Generation\", \"likes\": \"105\", \"downloads\": \"\", \"updated\": \"Jun 1\"}, {\"name\": \"Lexius/Phi-4-multimodal-instruct\", \"link\": \"https://huggingface.co/Lexius/Phi-4-multimodal-instruct\", \"task\": \"\", \"likes\": \"622\", \"downloads\": \"\", \"updated\": \"Jun 2\"}, {\"name\": \"lakshmi97/Phi-4-multimodal-instruct\", \"link\": \"https://huggingface.co/lakshmi97/Phi-4-multimodal-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"Frane92O/Phi-4-mini-instruct-bnb-4bit\", \"link\": \"https://huggingface.co/Frane92O/Phi-4-mini-instruct-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 21\"}, {\"name\": \"ErazerControl/Phi-4-mini-instruct\", \"link\": \"https://huggingface.co/ErazerControl/Phi-4-mini-instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"gradientdegen/phi-4-mini-instruct-base\", \"link\": \"https://huggingface.co/gradientdegen/phi-4-mini-instruct-base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 6\"}, {\"name\": \"kumapo/Phi-4-multimodal-instruct\", \"link\": \"https://huggingface.co/kumapo/Phi-4-multimodal-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"FriendliAI/Phi-4-multimodal-instruct\", \"link\": \"https://huggingface.co/FriendliAI/Phi-4-multimodal-instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"JacobLinCool/phi-4-audio\", \"link\": \"https://huggingface.co/JacobLinCool/phi-4-audio\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.01774",
    "first_seen_date": "2025-03-04",
    "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.01774Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion ModelsPublished on Mar 3\u00b7Submitted byJay Wuon Mar 4#3 Paper of the dayUpvote44+36Authors:Jay Zhangjie Wu,Yuxuan Zhang,Haithem Turki,Xuanchi Ren,Jun Gao,Mike Zheng Shou,Sanja Fidler,Zan Gojcic,Huan LingAbstractDifix3D+, a single-step diffusion model pipeline, enhances 3D reconstruction and novel-view synthesis by improving photorealism and reducing artifacts from extreme viewpoints.AI-generated summaryNeural Radiance Fieldsand3D Gaussian Splattinghave revolutionized 3D\nreconstruction andnovel-view synthesistask. However, achieving photorealistic\nrendering from extreme novel viewpoints remains challenging, as artifacts\npersist across representations. In this work, we introduce Difix3D+, a novel\npipeline designed to enhance3D reconstructionandnovel-view synthesisthrough\nsingle-stepdiffusion models. At the core of our approach is Difix, a\nsingle-step image diffusion model trained to enhance and remove artifacts in\nrendered novel views caused byunderconstrained regionsof the 3D\nrepresentation. Difix serves two critical roles in our pipeline. First, it is\nused during the reconstruction phase to clean uppseudo-training viewsthat are\nrendered from the reconstruction and then distilled back into 3D. This greatly\nenhancesunderconstrained regionsand improves the overall 3D representation\nquality. More importantly, Difix also acts as aneural enhancerduring\ninference, effectively removing residual artifacts arising from imperfect 3D\nsupervision and the limited capacity of current reconstruction models. Difix3D+\nis a general solution, a single model compatible with both NeRF and 3DGS\nrepresentations, and it achieves an average 2times improvement inFID scoreover baselines while maintaining3D consistency.View arXiv pageView PDFProject pageAdd to collectionCommunityjaywPaper authorPaper submitterMar 4\u2022edited Mar 4Difix3D+:",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2503.01774",
    "arxiv_url": "https://arxiv.org/abs/2503.01774",
    "num_models": 4,
    "models_list": "nvidia/difix, nvidia/Fixer, nvidia/difix_ref, EulerHuaji/difix_ref",
    "models_links": "https://huggingface.co/nvidia/difix, https://huggingface.co/nvidia/Fixer, https://huggingface.co/nvidia/difix_ref, https://huggingface.co/EulerHuaji/difix_ref",
    "models_detailed": "[{\"name\": \"nvidia/difix\", \"link\": \"https://huggingface.co/nvidia/difix\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"nvidia/Fixer\", \"link\": \"https://huggingface.co/nvidia/Fixer\", \"task\": \"\", \"likes\": \"121\", \"downloads\": \"\", \"updated\": \"18 days ago\"}, {\"name\": \"nvidia/difix_ref\", \"link\": \"https://huggingface.co/nvidia/difix_ref\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 11\"}, {\"name\": \"EulerHuaji/difix_ref\", \"link\": \"https://huggingface.co/EulerHuaji/difix_ref\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2503.01807",
    "first_seen_date": "2025-03-04",
    "title": "Large-Scale Data Selection for Instruction Tuning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2503.01807Large-Scale Data Selection for Instruction TuningPublished on Mar 3\u00b7Submitted byHamish Ivisonon Mar 4Upvote14+6Authors:Hamish Ivison,Muru Zhang,Faeze Brahman,Pang Wei Koh,Pradeep DasigiAbstractA study on the scalability of data selection methods in instruction-tuning large language models reveals that a simpler, compute-efficient variant of representation-based data selection outperforms more complex methods.AI-generated summarySelecting high-quality training data from a larger pool is a crucial step\nwheninstruction-tuninglanguage models, as carefully curated datasets often\nproduce models that outperform those trained on much larger, noisier datasets.Automated data selectionapproaches forinstruction-tuningare typically tested\nby selecting small datasets (roughly 10k samples) from small pools (100-200k\nsamples). However, popular deployed instruction-tuned models often train on\nhundreds of thousands to millions of samples, subsampled from even larger data\npools. We present a systematic study of how well data selection methods scale\nto these settings, selecting up to 2.5M samples from pools of up to 5.8M\nsamples and evaluating across 7 diverse tasks. We show that many recently\nproposed methods fall short of random selection in this setting (while using\nmore compute), and even decline in performance when given access to larger\npools of data to select over. However, we find that a variant ofrepresentation-based data selection(RDS+), which usesweighted mean poolingofpretrained LM hidden states, consistently outperforms more complex methods\nacross all settings tested -- all whilst being more compute-efficient. Our\nfindings highlight that the scaling properties of proposed automated selection\nmethods should be more closely examined. We release our code, data, and models\nat https://github.com/hamishivi/automated-instruction-selection.View arXiv pageView PDFProject pageGitHub51aut",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2503,
    "github_repo": "https://github.com/hamishivi/automated-instruction-selection",
    "hf_paper_url": "https://huggingface.co/papers/2503.01807",
    "arxiv_url": "https://arxiv.org/abs/2503.01807",
    "num_models": 5,
    "models_list": "hamishivi/llama-3.1-tulu-3-multitask-rrmax-939k-sft, hamishivi/tulu-2-multitask-rrmax-326k-sft, hamishivi/llama-3.1-tulu-3-arena-hard-939k-sft, hamishivi/tulu-2-arena-hard-326k-sft, hamishivi/tulu-2-wildchat-326k-sft",
    "models_links": "https://huggingface.co/hamishivi/llama-3.1-tulu-3-multitask-rrmax-939k-sft, https://huggingface.co/hamishivi/tulu-2-multitask-rrmax-326k-sft, https://huggingface.co/hamishivi/llama-3.1-tulu-3-arena-hard-939k-sft, https://huggingface.co/hamishivi/tulu-2-arena-hard-326k-sft, https://huggingface.co/hamishivi/tulu-2-wildchat-326k-sft",
    "models_detailed": "[{\"name\": \"hamishivi/llama-3.1-tulu-3-multitask-rrmax-939k-sft\", \"link\": \"https://huggingface.co/hamishivi/llama-3.1-tulu-3-multitask-rrmax-939k-sft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\"}, {\"name\": \"hamishivi/tulu-2-multitask-rrmax-326k-sft\", \"link\": \"https://huggingface.co/hamishivi/tulu-2-multitask-rrmax-326k-sft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\"}, {\"name\": \"hamishivi/llama-3.1-tulu-3-arena-hard-939k-sft\", \"link\": \"https://huggingface.co/hamishivi/llama-3.1-tulu-3-arena-hard-939k-sft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\"}, {\"name\": \"hamishivi/tulu-2-arena-hard-326k-sft\", \"link\": \"https://huggingface.co/hamishivi/tulu-2-arena-hard-326k-sft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\"}, {\"name\": \"hamishivi/tulu-2-wildchat-326k-sft\", \"link\": \"https://huggingface.co/hamishivi/tulu-2-wildchat-326k-sft\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\"}]",
    "num_datasets": 16,
    "datasets_list": "hamishivi/tulu-3-unfiltered, hamishivi/tulu-2-unfiltered, hamishivi/rds-sels-tydiqa-shots-top326k, hamishivi/rds-sels-tulu-3-arena-hard-939k, hamishivi/rds-sels-tulu-3-multitask-rrmax-939k, hamishivi/lsds_data, hamishivi/rds-sels-alpacafarm-top326k, hamishivi/rds-sels-arena-hard-top326k, hamishivi/rds-sels-codex-top326k, hamishivi/rds-sels-bbh-shots-top326k, hamishivi/rds-sels-gsm8k-shots-top326k, hamishivi/rds-sels-mmlu-shots-top326k, hamishivi/rds-sels-squad-top326k, hamishivi/rds-sels-wildchat-top326k, hamishivi/rds-sels-multitask-rrmax-top326k, hamishivi/200k-tulu-2-unbalanced",
    "datasets_links": "https://huggingface.co/datasets/hamishivi/tulu-3-unfiltered, https://huggingface.co/datasets/hamishivi/tulu-2-unfiltered, https://huggingface.co/datasets/hamishivi/rds-sels-tydiqa-shots-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-tulu-3-arena-hard-939k, https://huggingface.co/datasets/hamishivi/rds-sels-tulu-3-multitask-rrmax-939k, https://huggingface.co/datasets/hamishivi/lsds_data, https://huggingface.co/datasets/hamishivi/rds-sels-alpacafarm-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-arena-hard-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-codex-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-bbh-shots-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-gsm8k-shots-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-mmlu-shots-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-squad-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-wildchat-top326k, https://huggingface.co/datasets/hamishivi/rds-sels-multitask-rrmax-top326k, https://huggingface.co/datasets/hamishivi/200k-tulu-2-unbalanced",
    "datasets_detailed": "[{\"name\": \"hamishivi/tulu-3-unfiltered\", \"link\": \"https://huggingface.co/datasets/hamishivi/tulu-3-unfiltered\", \"task\": \"\", \"likes\": \"165\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/tulu-2-unfiltered\", \"link\": \"https://huggingface.co/datasets/hamishivi/tulu-2-unfiltered\", \"task\": \"\", \"likes\": \"87\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-tydiqa-shots-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-tydiqa-shots-top326k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-tulu-3-arena-hard-939k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-tulu-3-arena-hard-939k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-tulu-3-multitask-rrmax-939k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-tulu-3-multitask-rrmax-939k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/lsds_data\", \"link\": \"https://huggingface.co/datasets/hamishivi/lsds_data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-alpacafarm-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-alpacafarm-top326k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-arena-hard-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-arena-hard-top326k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-codex-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-codex-top326k\", \"task\": \"\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-bbh-shots-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-bbh-shots-top326k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-gsm8k-shots-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-gsm8k-shots-top326k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-mmlu-shots-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-mmlu-shots-top326k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-squad-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-squad-top326k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-wildchat-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-wildchat-top326k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/rds-sels-multitask-rrmax-top326k\", \"link\": \"https://huggingface.co/datasets/hamishivi/rds-sels-multitask-rrmax-top326k\", \"task\": \"\", \"likes\": \"43\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"hamishivi/200k-tulu-2-unbalanced\", \"link\": \"https://huggingface.co/datasets/hamishivi/200k-tulu-2-unbalanced\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.18772",
    "first_seen_date": "2025-02-27",
    "title": "Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.18772Plutus: Benchmarking Large Language Models in Low-Resource Greek FinancePublished on Feb 26\u00b7Submitted byJimin Huangon Feb 27\u00b7The Fin AIUpvote33+25Authors:Xueqing Peng,Triantafillos Papadopoulos,Efstathia Soufleri,Polydoros Giannouris,Ruoyu Xiang,Yan Wang,Lingfei Qian,Jimin Huang,Qianqian Xie,Sophia AnaniadouAbstractThe first Greek Financial Evaluation Benchmark (Plutus-ben) and Greek Financial LLM (Plutus-8B) address the challenges of financial NLP in Greek due to linguistic complexity and lack of domain-specific data.AI-generated summaryDespite Greece's pivotal role in the global economy,large language models(LLMs) remain underexplored for Greek financial context due to the linguistic\ncomplexity of Greek and the scarcity of domain-specific datasets. Previous\nefforts inmultilingual financial natural language processing(NLP) have\nexposed considerable performance disparities, yet no dedicated Greek financial\nbenchmarks or Greek-specific financial LLMs have been developed until now. To\nbridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation\nBenchmark, and Plutus-8B, the pioneeringGreek Financial LLM, fine-tuned with\nGreek domain-specific data. Plutus-ben addresses five core financial NLP tasks\nin Greek: numeric and textualnamed entity recognition,question answering,abstractive summarization, andtopic classification, thereby facilitating\nsystematic and reproducible LLM assessments. To underpin these tasks, we\npresent three novel, high-qualityGreek financial datasets, thoroughly\nannotated by expert native Greek speakers, augmented by two existing resources.\nOur comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek\nfinancial NLP remains challenging due to linguistic complexity, domain-specific\nterminology, and financial reasoning gaps. These findings underscore the\nlimitations ofcross-lingual transfer, the necessity forfinancial expertisei",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.18772",
    "arxiv_url": "https://arxiv.org/abs/2502.18772",
    "num_models": 1,
    "models_list": "TheFinAI/plutus-8B-instruct",
    "models_links": "https://huggingface.co/TheFinAI/plutus-8B-instruct",
    "models_detailed": "[{\"name\": \"TheFinAI/plutus-8B-instruct\", \"link\": \"https://huggingface.co/TheFinAI/plutus-8B-instruct\", \"task\": \"Text Generation\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"Feb 27\"}]",
    "num_datasets": 5,
    "datasets_list": "TheFinAI/plutus-multifin, TheFinAI/plutus-finner-text, TheFinAI/plutus-QA, TheFinAI/plutus-finner-numeric, TheFinAI/plutus-fns2023",
    "datasets_links": "https://huggingface.co/datasets/TheFinAI/plutus-multifin, https://huggingface.co/datasets/TheFinAI/plutus-finner-text, https://huggingface.co/datasets/TheFinAI/plutus-QA, https://huggingface.co/datasets/TheFinAI/plutus-finner-numeric, https://huggingface.co/datasets/TheFinAI/plutus-fns2023",
    "datasets_detailed": "[{\"name\": \"TheFinAI/plutus-multifin\", \"link\": \"https://huggingface.co/datasets/TheFinAI/plutus-multifin\", \"task\": \"\", \"likes\": \"268\", \"downloads\": \"\", \"updated\": \"Feb 27\", \"size\": \"\"}, {\"name\": \"TheFinAI/plutus-finner-text\", \"link\": \"https://huggingface.co/datasets/TheFinAI/plutus-finner-text\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Feb 27\", \"size\": \"\"}, {\"name\": \"TheFinAI/plutus-QA\", \"link\": \"https://huggingface.co/datasets/TheFinAI/plutus-QA\", \"task\": \"\", \"likes\": \"540\", \"downloads\": \"\", \"updated\": \"Feb 27\", \"size\": \"\"}, {\"name\": \"TheFinAI/plutus-finner-numeric\", \"link\": \"https://huggingface.co/datasets/TheFinAI/plutus-finner-numeric\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Feb 27\", \"size\": \"\"}, {\"name\": \"TheFinAI/plutus-fns2023\", \"link\": \"https://huggingface.co/datasets/TheFinAI/plutus-fns2023\", \"task\": \"\", \"likes\": \"262\", \"downloads\": \"\", \"updated\": \"Feb 27\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.18934",
    "first_seen_date": "2025-02-27",
    "title": "Kanana: Compute-efficient Bilingual Language Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.18934Kanana: Compute-efficient Bilingual Language ModelsPublished on Feb 26\u00b7Submitted byMinho Ryuon Feb 27#2 Paper of the dayUpvote65+57Authors:Kanana LLM Team,Yunju Bak,Hojin Lee,Minho Ryu,Jiyeon Ham,Seungjae Jung,Daniel Wontae Nam,Taegyeong Eo,Donghun Lee,Doohae Jung,Boseop Kim,Nayeon Kim,Jaesun Park,Hyunho Kim,Hyunwoong Ko,Changmin Lee,Kyoung-Woon On,Seulye Baeg,Junrae Cho,Sunghee Jung,Jieun Kang,EungGyun Kim+7 authorsAbstractKanana, a series of bilingual language models, achieves superior performance in Korean and competitive performance in English with lower computational costs through efficient pre-training and post-training techniques.AI-generated summaryWe introduce Kanana, a series of bilingual language models that demonstrate\nexceeding performance in Korean and competitive performance in English. The\ncomputational cost of Kanana is significantly lower than that of\nstate-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, includinghigh quality data filtering,staged pre-training, depth\nup-scaling, andpruninganddistillation. Furthermore, the report outlines the\nmethodologies utilized during the post-training of the Kanana models,\nencompassingsupervised fine-tuningandpreference optimization, aimed at\nenhancing their capability for seamless interaction with users. Lastly, the\nreport elaborates on plausible approaches used for language model adaptation to\nspecific scenarios, such asembedding,retrieval augmented generation, andfunction calling. The Kanana model series spans from 2.1B to 32.5B parameters\nwith 2.1B models (base, instruct,embedding) publicly released to promote\nresearch on Korean language models.View arXiv pageView PDFProject pageGitHub276autoAdd to collectionCommunitybzantiumPaper authorPaper submitterFeb 27\u2022edited Feb 27Kakao just released Kanana LLM technical",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/kakao/kanana",
    "hf_paper_url": "https://huggingface.co/papers/2502.18934",
    "arxiv_url": "https://arxiv.org/abs/2502.18934",
    "num_models": 26,
    "models_list": "kakaocorp/kanana-nano-2.1b-instruct, kakaocorp/kanana-1.5-8b-instruct-2505, kakaocorp/kanana-nano-2.1b-base, kakaocorp/kanana-1.5-2.1b-instruct-2505, kakaocorp/kanana-nano-2.1b-embedding, datalama/kanana-nano-2.1b-embedding, taeshahn/my_new_model3, DimensionSTP/kanana-nano-2.1b-instruct-Ko-Reasoning, kakaocorp/kanana-1.5-2.1b-base, kakaocorp/kanana-1.5-8b-base, BCCard/kanana-1.5-8b-instruct-2505-FP8-Dynamic, lucyknada/kakaocorp_kanana-1.5-8b-instruct-2505-exl3, Mungert/kanana-1.5-8b-instruct-2505-GGUF, kakaocorp/kanana-1.5-15.7b-a3b-base, Mungert/kanana-1.5-2.1b-instruct-2505-GGUF, kakaocorp/kanana-1.5-15.7b-a3b-instruct, OpenLLM-Korea/kanana-nano-2.1b-base, OpenLLM-Korea/kanana-nano-2.1b-instruct, OpenLLM-Korea/kanana-1.5-8b-instruct-2505, OpenLLM-Korea/kanana-1.5-2.1b-instruct-2505, OpenLLM-Korea/kanana-1.5-2.1b-base, OpenLLM-Korea/kanana-1.5-8b-base, OpenLLM-Korea/kanana-nano-2.1b-embedding, OpenLLM-Korea/kanana-1.5-15.7b-a3b-base, OpenLLM-Korea/kanana-1.5-15.7b-a3b-instruct, squeezebits/kanana-1.5-2.1b-instruct-2505-mlx",
    "models_links": "https://huggingface.co/kakaocorp/kanana-nano-2.1b-instruct, https://huggingface.co/kakaocorp/kanana-1.5-8b-instruct-2505, https://huggingface.co/kakaocorp/kanana-nano-2.1b-base, https://huggingface.co/kakaocorp/kanana-1.5-2.1b-instruct-2505, https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding, https://huggingface.co/datalama/kanana-nano-2.1b-embedding, https://huggingface.co/taeshahn/my_new_model3, https://huggingface.co/DimensionSTP/kanana-nano-2.1b-instruct-Ko-Reasoning, https://huggingface.co/kakaocorp/kanana-1.5-2.1b-base, https://huggingface.co/kakaocorp/kanana-1.5-8b-base, https://huggingface.co/BCCard/kanana-1.5-8b-instruct-2505-FP8-Dynamic, https://huggingface.co/lucyknada/kakaocorp_kanana-1.5-8b-instruct-2505-exl3, https://huggingface.co/Mungert/kanana-1.5-8b-instruct-2505-GGUF, https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-base, https://huggingface.co/Mungert/kanana-1.5-2.1b-instruct-2505-GGUF, https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct, https://huggingface.co/OpenLLM-Korea/kanana-nano-2.1b-base, https://huggingface.co/OpenLLM-Korea/kanana-nano-2.1b-instruct, https://huggingface.co/OpenLLM-Korea/kanana-1.5-8b-instruct-2505, https://huggingface.co/OpenLLM-Korea/kanana-1.5-2.1b-instruct-2505, https://huggingface.co/OpenLLM-Korea/kanana-1.5-2.1b-base, https://huggingface.co/OpenLLM-Korea/kanana-1.5-8b-base, https://huggingface.co/OpenLLM-Korea/kanana-nano-2.1b-embedding, https://huggingface.co/OpenLLM-Korea/kanana-1.5-15.7b-a3b-base, https://huggingface.co/OpenLLM-Korea/kanana-1.5-15.7b-a3b-instruct, https://huggingface.co/squeezebits/kanana-1.5-2.1b-instruct-2505-mlx",
    "models_detailed": "[{\"name\": \"kakaocorp/kanana-nano-2.1b-instruct\", \"link\": \"https://huggingface.co/kakaocorp/kanana-nano-2.1b-instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"kakaocorp/kanana-1.5-8b-instruct-2505\", \"link\": \"https://huggingface.co/kakaocorp/kanana-1.5-8b-instruct-2505\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"kakaocorp/kanana-nano-2.1b-base\", \"link\": \"https://huggingface.co/kakaocorp/kanana-nano-2.1b-base\", \"task\": \"Text Generation\", \"likes\": \"697\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"kakaocorp/kanana-1.5-2.1b-instruct-2505\", \"link\": \"https://huggingface.co/kakaocorp/kanana-1.5-2.1b-instruct-2505\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"kakaocorp/kanana-nano-2.1b-embedding\", \"link\": \"https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding\", \"task\": \"Text Generation\", \"likes\": \"352\", \"downloads\": \"\", \"updated\": \"Jun 13\"}, {\"name\": \"datalama/kanana-nano-2.1b-embedding\", \"link\": \"https://huggingface.co/datalama/kanana-nano-2.1b-embedding\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 16\"}, {\"name\": \"taeshahn/my_new_model3\", \"link\": \"https://huggingface.co/taeshahn/my_new_model3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 15\"}, {\"name\": \"DimensionSTP/kanana-nano-2.1b-instruct-Ko-Reasoning\", \"link\": \"https://huggingface.co/DimensionSTP/kanana-nano-2.1b-instruct-Ko-Reasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"kakaocorp/kanana-1.5-2.1b-base\", \"link\": \"https://huggingface.co/kakaocorp/kanana-1.5-2.1b-base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"kakaocorp/kanana-1.5-8b-base\", \"link\": \"https://huggingface.co/kakaocorp/kanana-1.5-8b-base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\"}, {\"name\": \"BCCard/kanana-1.5-8b-instruct-2505-FP8-Dynamic\", \"link\": \"https://huggingface.co/BCCard/kanana-1.5-8b-instruct-2505-FP8-Dynamic\", \"task\": \"Text Generation\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"May 23\"}, {\"name\": \"lucyknada/kakaocorp_kanana-1.5-8b-instruct-2505-exl3\", \"link\": \"https://huggingface.co/lucyknada/kakaocorp_kanana-1.5-8b-instruct-2505-exl3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 23\"}, {\"name\": \"Mungert/kanana-1.5-8b-instruct-2505-GGUF\", \"link\": \"https://huggingface.co/Mungert/kanana-1.5-8b-instruct-2505-GGUF\", \"task\": \"Text Generation\", \"likes\": \"481\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"kakaocorp/kanana-1.5-15.7b-a3b-base\", \"link\": \"https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-base\", \"task\": \"Text Generation\", \"likes\": \"35\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"Mungert/kanana-1.5-2.1b-instruct-2505-GGUF\", \"link\": \"https://huggingface.co/Mungert/kanana-1.5-2.1b-instruct-2505-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"kakaocorp/kanana-1.5-15.7b-a3b-instruct\", \"link\": \"https://huggingface.co/kakaocorp/kanana-1.5-15.7b-a3b-instruct\", \"task\": \"Text Generation\", \"likes\": \"700\", \"downloads\": \"\", \"updated\": \"Sep 11\"}, {\"name\": \"OpenLLM-Korea/kanana-nano-2.1b-base\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-nano-2.1b-base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/kanana-nano-2.1b-instruct\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-nano-2.1b-instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/kanana-1.5-8b-instruct-2505\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-1.5-8b-instruct-2505\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/kanana-1.5-2.1b-instruct-2505\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-1.5-2.1b-instruct-2505\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/kanana-1.5-2.1b-base\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-1.5-2.1b-base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/kanana-1.5-8b-base\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-1.5-8b-base\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/kanana-nano-2.1b-embedding\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-nano-2.1b-embedding\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/kanana-1.5-15.7b-a3b-base\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-1.5-15.7b-a3b-base\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"OpenLLM-Korea/kanana-1.5-15.7b-a3b-instruct\", \"link\": \"https://huggingface.co/OpenLLM-Korea/kanana-1.5-15.7b-a3b-instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"squeezebits/kanana-1.5-2.1b-instruct-2505-mlx\", \"link\": \"https://huggingface.co/squeezebits/kanana-1.5-2.1b-instruct-2505-mlx\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 13\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.19400",
    "first_seen_date": "2025-02-27",
    "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  Understanding",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.19400TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem\n  UnderstandingPublished on Feb 26\u00b7Submitted byAKon Feb 27Upvote48+40Authors:Max Ku,Thomas Chong,Jonathan Leung,Krish Shah,Alvin Yu,Wenhu ChenAbstractTheoremExplainAgent uses agentic planning and Manim animations to generate long-form visual explanations for theorems, revealing deeper reasoning flaws not apparent in text-based explanations.AI-generated summaryUnderstanding domain-specific theorems often requires more than just\ntext-based reasoning; effective communication through structured visual\nexplanations is crucial for deeper comprehension. Whilelarge language models(LLMs) demonstrate strong performance in text-based theorem reasoning, their\nability to generate coherent and pedagogically meaningful visual explanations\nremains an open challenge. In this work, we introduce TheoremExplainAgent, anagentic approachfor generating long-formtheorem explanation videos(over 5\nminutes) usingManim animations. To systematically evaluate multimodal theorem\nexplanations, we proposeTheoremExplainBench, a benchmark covering 240 theorems\nacross multiple STEM disciplines, along with 5automated evaluation metrics.\nOur results reveal thatagentic planningis essential for generating detailed\nlong-form videos, and theo3-mini agentachieves a success rate of 93.8% and an\noverall score of 0.77. However, our quantitative and qualitative studies show\nthat most of the videos produced exhibit minor issues with visual element\nlayout. Furthermore, multimodal explanations expose deeper reasoning flaws that\ntext-based explanations fail to reveal, highlighting the importance of\nmultimodal explanations.View arXiv pageView PDFProject pageGitHub1.44kAdd to collectionCommunityakhaliqPaper submitterFeb 27https://tiger-ai-lab.github.io/TheoremExplainAgent/See translationReplylibrarian-botFeb 28This is an automated message from theLibraria",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/TIGER-AI-Lab/TheoremExplainAgent",
    "hf_paper_url": "https://huggingface.co/papers/2502.19400",
    "arxiv_url": "https://arxiv.org/abs/2502.19400",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/TheoremExplainBench",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/TheoremExplainBench",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/TheoremExplainBench\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/TheoremExplainBench\", \"task\": \"\", \"likes\": \"240\", \"downloads\": \"\", \"updated\": \"Mar 31\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.15122",
    "first_seen_date": "2025-02-25",
    "title": "MONSTER: Monash Scalable Time Series Evaluation Repository",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.15122MONSTER: Monash Scalable Time Series Evaluation RepositoryPublished on Feb 21\u00b7Submitted byAnguson Feb 25Upvote4Authors:Angus Dempster,Navid Mohammadi Foumani,Chang Wei Tan,Lynn Miller,Amish Mishra,Mahsa Salehi,Charlotte Pelletier,Daniel F. Schmidt,Geoffrey I. WebbAbstractA new repository, MONSTER, aimed at diversifying time series classification by providing larger datasets to challenge traditional models and improve scalability.AI-generated summaryWe introduceMONSTER-the MONash Scalable Time Series Evaluation Repository-a\ncollection of large datasets fortime series classification. The field of time\nseries classification has benefitted from common benchmarks set by theUCRandUEAtime series classificationrepositories. However, the datasets in these\nbenchmarks are small, with median sizes of 217 and 255 examples, respectively.\nIn consequence they favour a narrow subspace of models that are optimised to\nachieve low classification error on a wide variety of smaller datasets, that\nis, models that minimise variance, and give little weight to computational\nissues such asscalability. Our hope is to diversify the field by introducing\nbenchmarks using larger datasets. We believe that there is enormous potential\nfor new progress in the field by engaging with the theoretical and practical\nchallenges of learning effectively from larger quantities of data.View arXiv pageView PDFGitHub19autoAdd to collectionCommunityangus924Paper authorPaper submitterFeb 25Initial release of MONSTER, a new benchmark collection of large datasets (10K\u201350M) for time series classification, with baseline results for key models.GitHub:https://github.com/Navidfoumani/monsterSee translationReplylibrarian-botFeb 26This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APISundial: A Family of Highly Capab",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/Navidfoumani/monster",
    "hf_paper_url": "https://huggingface.co/papers/2502.15122",
    "arxiv_url": "https://arxiv.org/abs/2502.15122",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 29,
    "datasets_list": "monster-monash/Pedestrian, monster-monash/FruitFlies, monster-monash/Traffic, monster-monash/Tiselac, monster-monash/WhaleSounds, monster-monash/FordChallenge, monster-monash/AudioMNIST, monster-monash/AudioMNIST-DS, monster-monash/InsectSound, monster-monash/MosquitoSound, monster-monash/CornellWhaleChallenge, monster-monash/LakeIce, monster-monash/S2Agri-10pc-17, monster-monash/S2Agri-10pc-34, monster-monash/S2Agri-17, monster-monash/S2Agri-34, monster-monash/TimeSen2Crop, monster-monash/CrowdSourced, monster-monash/DREAMERA, monster-monash/DREAMERV, monster-monash/STEW, monster-monash/Opportunity, monster-monash/PAMAP2, monster-monash/Skoda, monster-monash/UCIActivity, monster-monash/USCActivity, monster-monash/WISDM, monster-monash/WISDM2, monster-monash/LenDB",
    "datasets_links": "https://huggingface.co/datasets/monster-monash/Pedestrian, https://huggingface.co/datasets/monster-monash/FruitFlies, https://huggingface.co/datasets/monster-monash/Traffic, https://huggingface.co/datasets/monster-monash/Tiselac, https://huggingface.co/datasets/monster-monash/WhaleSounds, https://huggingface.co/datasets/monster-monash/FordChallenge, https://huggingface.co/datasets/monster-monash/AudioMNIST, https://huggingface.co/datasets/monster-monash/AudioMNIST-DS, https://huggingface.co/datasets/monster-monash/InsectSound, https://huggingface.co/datasets/monster-monash/MosquitoSound, https://huggingface.co/datasets/monster-monash/CornellWhaleChallenge, https://huggingface.co/datasets/monster-monash/LakeIce, https://huggingface.co/datasets/monster-monash/S2Agri-10pc-17, https://huggingface.co/datasets/monster-monash/S2Agri-10pc-34, https://huggingface.co/datasets/monster-monash/S2Agri-17, https://huggingface.co/datasets/monster-monash/S2Agri-34, https://huggingface.co/datasets/monster-monash/TimeSen2Crop, https://huggingface.co/datasets/monster-monash/CrowdSourced, https://huggingface.co/datasets/monster-monash/DREAMERA, https://huggingface.co/datasets/monster-monash/DREAMERV, https://huggingface.co/datasets/monster-monash/STEW, https://huggingface.co/datasets/monster-monash/Opportunity, https://huggingface.co/datasets/monster-monash/PAMAP2, https://huggingface.co/datasets/monster-monash/Skoda, https://huggingface.co/datasets/monster-monash/UCIActivity, https://huggingface.co/datasets/monster-monash/USCActivity, https://huggingface.co/datasets/monster-monash/WISDM, https://huggingface.co/datasets/monster-monash/WISDM2, https://huggingface.co/datasets/monster-monash/LenDB",
    "datasets_detailed": "[{\"name\": \"monster-monash/Pedestrian\", \"link\": \"https://huggingface.co/datasets/monster-monash/Pedestrian\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/FruitFlies\", \"link\": \"https://huggingface.co/datasets/monster-monash/FruitFlies\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/Traffic\", \"link\": \"https://huggingface.co/datasets/monster-monash/Traffic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/Tiselac\", \"link\": \"https://huggingface.co/datasets/monster-monash/Tiselac\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/WhaleSounds\", \"link\": \"https://huggingface.co/datasets/monster-monash/WhaleSounds\", \"task\": \"\", \"likes\": \"458\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/FordChallenge\", \"link\": \"https://huggingface.co/datasets/monster-monash/FordChallenge\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/AudioMNIST\", \"link\": \"https://huggingface.co/datasets/monster-monash/AudioMNIST\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/AudioMNIST-DS\", \"link\": \"https://huggingface.co/datasets/monster-monash/AudioMNIST-DS\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/InsectSound\", \"link\": \"https://huggingface.co/datasets/monster-monash/InsectSound\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/MosquitoSound\", \"link\": \"https://huggingface.co/datasets/monster-monash/MosquitoSound\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/CornellWhaleChallenge\", \"link\": \"https://huggingface.co/datasets/monster-monash/CornellWhaleChallenge\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/LakeIce\", \"link\": \"https://huggingface.co/datasets/monster-monash/LakeIce\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/S2Agri-10pc-17\", \"link\": \"https://huggingface.co/datasets/monster-monash/S2Agri-10pc-17\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/S2Agri-10pc-34\", \"link\": \"https://huggingface.co/datasets/monster-monash/S2Agri-10pc-34\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/S2Agri-17\", \"link\": \"https://huggingface.co/datasets/monster-monash/S2Agri-17\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/S2Agri-34\", \"link\": \"https://huggingface.co/datasets/monster-monash/S2Agri-34\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/TimeSen2Crop\", \"link\": \"https://huggingface.co/datasets/monster-monash/TimeSen2Crop\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 15\", \"size\": \"\"}, {\"name\": \"monster-monash/CrowdSourced\", \"link\": \"https://huggingface.co/datasets/monster-monash/CrowdSourced\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/DREAMERA\", \"link\": \"https://huggingface.co/datasets/monster-monash/DREAMERA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/DREAMERV\", \"link\": \"https://huggingface.co/datasets/monster-monash/DREAMERV\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/STEW\", \"link\": \"https://huggingface.co/datasets/monster-monash/STEW\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/Opportunity\", \"link\": \"https://huggingface.co/datasets/monster-monash/Opportunity\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/PAMAP2\", \"link\": \"https://huggingface.co/datasets/monster-monash/PAMAP2\", \"task\": \"\", \"likes\": \"331\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/Skoda\", \"link\": \"https://huggingface.co/datasets/monster-monash/Skoda\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/UCIActivity\", \"link\": \"https://huggingface.co/datasets/monster-monash/UCIActivity\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/USCActivity\", \"link\": \"https://huggingface.co/datasets/monster-monash/USCActivity\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/WISDM\", \"link\": \"https://huggingface.co/datasets/monster-monash/WISDM\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/WISDM2\", \"link\": \"https://huggingface.co/datasets/monster-monash/WISDM2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}, {\"name\": \"monster-monash/LenDB\", \"link\": \"https://huggingface.co/datasets/monster-monash/LenDB\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.16614",
    "first_seen_date": "2025-02-25",
    "title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language\n  Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.16614CodeCriticBench: A Holistic Code Critique Benchmark for Large Language\n  ModelsPublished on Feb 23\u00b7Submitted byJiaheng Liuon Feb 25Upvote27+19Authors:Alexander Zhang,Marcus Dong,Jiaheng Liu,Wei Zhang,Yejie Wang,Jian Yang,Ge Zhang,Tianyu Liu,Zhongyuan Peng,Yingshui Tan,Yuanxing Zhang,Zhexu Wang,Weixun Wang,Yancheng He,Ken Deng,Wangchunshu Zhou,Wenhao Huang,Zhaoxiang ZhangAbstractA new benchmark, CodeCriticBench, is introduced to evaluate code critique capacity of LLMs by addressing limitations in existing benchmarks, such as narrow focus and insufficient task diversity.AI-generated summaryThecritique capacityofLarge Language Models (LLMs)is essential forreasoning abilities, which can provide necessary suggestions (e.g., detailed\nanalysis and constructive feedback). Therefore, how to evaluate the critique\ncapacity of LLMs has drawn great attention and severalcritique benchmarkshave\nbeen proposed. However, existingcritique benchmarksusually have the following\nlimitations: (1). Focusing on diverse reasoning tasks in general domains and\ninsufficient evaluation oncode tasks(e.g., only coveringcode generationtask), where the difficulty of queries is relatively easy (e.g., the code\nqueries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive\nevaluation from different dimensions. To address these limitations, we\nintroduce a holistic code critique benchmark for LLMs called CodeCriticBench.\nSpecifically, our CodeCriticBench includes two mainstreamcode tasks(i.e.,code generationandcode QA) with different difficulties. Besides, theevaluation protocolsincludebasic critique evaluationand advanced critique\nevaluation for different characteristics, where fine-grained evaluation\nchecklists are well-designed for advanced settings. Finally, we conduct\nextensive experimental results of existing LLMs, which show the effectiveness\nof CodeCriticBench.View arXiv pageView PDFGi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/multimodal-art-projection/CodeCriticBench",
    "hf_paper_url": "https://huggingface.co/papers/2502.16614",
    "arxiv_url": "https://arxiv.org/abs/2502.16614",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "m-a-p/CodeCriticBench",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/CodeCriticBench",
    "datasets_detailed": "[{\"name\": \"m-a-p/CodeCriticBench\", \"link\": \"https://huggingface.co/datasets/m-a-p/CodeCriticBench\", \"task\": \"\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"Nov 2\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.13995",
    "first_seen_date": "2025-02-24",
    "title": "FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.13995FantasyID: Face Knowledge Enhanced ID-Preserving Video GenerationPublished on Feb 19\u00b7Submitted byYSHon Feb 24Upvote9+1Authors:Yunpeng Zhang,Qiang Wang,Fan Jiang,Yaqi Fan,Mu Xu,Yonggang QiAbstractA novel tuning-free IPT2V framework using diffusion transformers with 3D facial geometry prior and layer-aware adaptive mechanism achieves superior identity preservation and facial dynamics.AI-generated summaryTuning-free approaches adapting large-scale pre-trained video diffusion\nmodels foridentity-preserving text-to-video generation(IPT2V) have gained\npopularity recently due to their efficacy and scalability. However, significant\nchallenges remain to achieve satisfied facial dynamics while keeping the\nidentity unchanged. In this work, we present a novel tuning-freeIPT2Vframework by enhancing face knowledge of the pre-trained video model built ondiffusion transformers(DiT), dubbed FantasyID. Essentially, 3D facial geometry\nprior is incorporated to ensure plausible facial structures during video\nsynthesis. To prevent the model from learning copy-paste shortcuts that simply\nreplicate reference face across frames, amulti-view face augmentationstrategy\nis devised to capture diverse 2D facial appearance features, hence increasing\nthe dynamics over the facial expressions and head poses. Additionally, after\nblending the 2D and 3D features as guidance, instead of naively employingcross-attentionto inject guidance cues intoDiTlayers, a learnablelayer-aware adaptive mechanismis employed to selectively inject the fused\nfeatures into each individualDiTlayers, facilitating balanced modeling of\nidentity preservation and motion dynamics. Experimental results validate our\nmodel's superiority over the current tuning-freeIPT2Vmethods.View arXiv pageView PDFProject pageGitHub77Add to collectionCommunityBestWishYshPaper submitterFeb 24Page:https://fantasy-amap.github.io/fantasy-id/See translationRe",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/Fantasy-AMAP/fantasy-id",
    "hf_paper_url": "https://huggingface.co/papers/2502.13995",
    "arxiv_url": "https://arxiv.org/abs/2502.13995",
    "num_models": 1,
    "models_list": "acvlab/FantasyID",
    "models_links": "https://huggingface.co/acvlab/FantasyID",
    "models_detailed": "[{\"name\": \"acvlab/FantasyID\", \"link\": \"https://huggingface.co/acvlab/FantasyID\", \"task\": \"\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Apr 25\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.14949",
    "first_seen_date": "2025-02-24",
    "title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and\n  Document Understanding",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.14949KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and\n  Document UnderstandingPublished on Feb 20\u00b7Submitted byAhmed Heaklon Feb 24Upvote9+1Authors:Ahmed Heakl,Abdullah Sohail,Mukul Ranjan,Rania Hossam,Ghazi Ahmed,Mohamed El-Geish,Omar Maher,Zhiqiang Shen,Fahad Khan,Salman KhanAbstractKITAB-Bench, a comprehensive Arabic OCR benchmark, highlights the strengths and limitations of modern vision-language models in recognizing Arabic text, especially in complex scenarios like PDF-to-Markdown conversion.AI-generated summaryWith the growing adoption ofRetrieval-Augmented Generation(RAG) in document\nprocessing, robusttext recognitionhas become increasingly critical for\nknowledge extraction. WhileOCR(Optical Character Recognition) for English and\nother languages benefits from large datasets and well-established benchmarks,Arabic OCRfaces unique challenges due to itscursive script, right-to-left\ntext flow, and complex typographic and calligraphic features. We presentKITAB-Bench, a comprehensiveArabic OCRbenchmark that fills the gaps in\ncurrent evaluation systems. Our benchmark comprises 8,809 samples across 9\nmajor domains and 36 sub-domains, encompassing diverse document types including\nhandwritten text, structured tables, and specialized coverage of 21 chart types\nfor business intelligence. Our findings show that modernvision-language models(such asGPT-4,Gemini, andQwen) outperform traditionalOCRapproaches (likeEasyOCR,PaddleOCR, andSurya) by an average of 60% inCharacter Error Rate(CER). Furthermore, we highlight significant limitations of currentArabic OCRmodels, particularly inPDF-to-Markdownconversion, where the best modelGemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in\naccurately recognizing Arabic text, including issues withcomplex fonts,numeral recognitionerrors,word elongation, andtable structure detection.\nThis work establi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/mbzuai-oryx/KITAB-Bench",
    "hf_paper_url": "https://huggingface.co/papers/2502.14949",
    "arxiv_url": "https://arxiv.org/abs/2502.14949",
    "num_models": 1,
    "models_list": "FatimahEmadEldin/Qari-OCR-Fine-Tuned-Kitab-Benchmark",
    "models_links": "https://huggingface.co/FatimahEmadEldin/Qari-OCR-Fine-Tuned-Kitab-Benchmark",
    "models_detailed": "[{\"name\": \"FatimahEmadEldin/Qari-OCR-Fine-Tuned-Kitab-Benchmark\", \"link\": \"https://huggingface.co/FatimahEmadEldin/Qari-OCR-Fine-Tuned-Kitab-Benchmark\", \"task\": \"Text Generation\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Sep 20\"}]",
    "num_datasets": 23,
    "datasets_list": "ahmedheakl/arocrbench_tables, ahmedheakl/arocrbench_khatt, ahmedheakl/arocrbench_khattparagraph, ahmedheakl/arocrbench_arabicocr, ahmedheakl/arocrbench_synthesizear, ahmedheakl/arocrbench_patsocr, ahmedheakl/arocrbench_historyar, ahmedheakl/arocrbench_historicalbooks, ahmedheakl/arocrbench_adab, ahmedheakl/arocrbench_muharaf, ahmedheakl/arocrbench_onlinekhatt, ahmedheakl/arocrbench_isippt, ahmedheakl/arocrbench_hindawi, ahmedheakl/arocrbench_evarest, ahmedheakl/arocrbench_charts, ahmedheakl/arocrbench_bcelayout, ahmedheakl/arocrbench_diagrams, ahmedheakl/arocrbench_doclaynet, ahmedheakl/arocrbench_ourslines, ahmedheakl/arocrbench_diagramsvqa, ahmedheakl/arocrbench_chartsvqa, ahmedheakl/arocrbench_mtvqa, ahmedheakl/arocrbench_patdvqa",
    "datasets_links": "https://huggingface.co/datasets/ahmedheakl/arocrbench_tables, https://huggingface.co/datasets/ahmedheakl/arocrbench_khatt, https://huggingface.co/datasets/ahmedheakl/arocrbench_khattparagraph, https://huggingface.co/datasets/ahmedheakl/arocrbench_arabicocr, https://huggingface.co/datasets/ahmedheakl/arocrbench_synthesizear, https://huggingface.co/datasets/ahmedheakl/arocrbench_patsocr, https://huggingface.co/datasets/ahmedheakl/arocrbench_historyar, https://huggingface.co/datasets/ahmedheakl/arocrbench_historicalbooks, https://huggingface.co/datasets/ahmedheakl/arocrbench_adab, https://huggingface.co/datasets/ahmedheakl/arocrbench_muharaf, https://huggingface.co/datasets/ahmedheakl/arocrbench_onlinekhatt, https://huggingface.co/datasets/ahmedheakl/arocrbench_isippt, https://huggingface.co/datasets/ahmedheakl/arocrbench_hindawi, https://huggingface.co/datasets/ahmedheakl/arocrbench_evarest, https://huggingface.co/datasets/ahmedheakl/arocrbench_charts, https://huggingface.co/datasets/ahmedheakl/arocrbench_bcelayout, https://huggingface.co/datasets/ahmedheakl/arocrbench_diagrams, https://huggingface.co/datasets/ahmedheakl/arocrbench_doclaynet, https://huggingface.co/datasets/ahmedheakl/arocrbench_ourslines, https://huggingface.co/datasets/ahmedheakl/arocrbench_diagramsvqa, https://huggingface.co/datasets/ahmedheakl/arocrbench_chartsvqa, https://huggingface.co/datasets/ahmedheakl/arocrbench_mtvqa, https://huggingface.co/datasets/ahmedheakl/arocrbench_patdvqa",
    "datasets_detailed": "[{\"name\": \"ahmedheakl/arocrbench_tables\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_tables\", \"task\": \"\", \"likes\": \"456\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_khatt\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_khatt\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Apr 1\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_khattparagraph\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_khattparagraph\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_arabicocr\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_arabicocr\", \"task\": \"\", \"likes\": \"50\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_synthesizear\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_synthesizear\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_patsocr\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_patsocr\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_historyar\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_historyar\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_historicalbooks\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_historicalbooks\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_adab\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_adab\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_muharaf\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_muharaf\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_onlinekhatt\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_onlinekhatt\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_isippt\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_isippt\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_hindawi\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_hindawi\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_evarest\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_evarest\", \"task\": \"\", \"likes\": \"800\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_charts\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_charts\", \"task\": \"\", \"likes\": \"576\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_bcelayout\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_bcelayout\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_diagrams\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_diagrams\", \"task\": \"\", \"likes\": \"226\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_doclaynet\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_doclaynet\", \"task\": \"\", \"likes\": \"400\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_ourslines\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_ourslines\", \"task\": \"\", \"likes\": \"378\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_diagramsvqa\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_diagramsvqa\", \"task\": \"\", \"likes\": \"102\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_chartsvqa\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_chartsvqa\", \"task\": \"\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_mtvqa\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_mtvqa\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"ahmedheakl/arocrbench_patdvqa\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/arocrbench_patdvqa\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.14377",
    "first_seen_date": "2025-02-21",
    "title": "RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.14377RelaCtrl: Relevance-Guided Efficient Control for Diffusion TransformersPublished on Feb 20\u00b7Submitted byAKon Feb 21Upvote12+4Authors:Ke Cao,Jing Wang,Ao Ma,Jiasong Feng,Zhanjie Zhang,Xuanhua He,Shanyuan Liu,Bo Cheng,Dawei Leng,Yuhui Yin,Jie ZhangAbstractThe Relevance-Guided Efficient Controllable Generation framework optimizes control signal integration in the Diffusion Transformer, enhancing performance while reducing parameters and computation.AI-generated summaryTheDiffusion Transformerplays a pivotal role in advancingtext-to-imageandtext-to-videogeneration, owing primarily to its inherent scalability. However,\nexisting controlleddiffusion transformermethods incur significant parameter\nandcomputational overheads and suffer from inefficient resource allocation due\nto their failure to account for the varying relevance ofcontrol informationacross different transformer layers. To address this, we propose the\nRelevance-Guided Efficient Controllable Generation framework, RelaCtrl,\nenabling efficient and resource-optimized integration of control signals into\ntheDiffusion Transformer. First, we evaluate the relevance of each layer in\ntheDiffusion Transformerto thecontrol informationby assessing the\n\"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on\nboth the quality of generation and the control effectiveness during inference.\nBased on the strength of the relevance, we then tailor the positioning,\nparameter scale, and modeling capacity of the control layers to reduce\nunnecessary parameters and redundant computations. Additionally, to further\nimprove efficiency, we replace theself-attentionandFFNin the commonly used\ncopy block with the carefully designedTwo-Dimensional Shuffle Mixer(TDSM),\nenabling efficient implementation of both thetoken mixerandchannel mixer.\nBoth qualitative and quantitative experimental results demonstrate that our\napproach achi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.14377",
    "arxiv_url": "https://arxiv.org/abs/2502.14377",
    "num_models": 1,
    "models_list": "qihoo360/RelaCtrl",
    "models_links": "https://huggingface.co/qihoo360/RelaCtrl",
    "models_detailed": "[{\"name\": \"qihoo360/RelaCtrl\", \"link\": \"https://huggingface.co/qihoo360/RelaCtrl\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 16\"}]",
    "num_datasets": 1,
    "datasets_list": "qihoo360/WISA-80K",
    "datasets_links": "https://huggingface.co/datasets/qihoo360/WISA-80K",
    "datasets_detailed": "[{\"name\": \"qihoo360/WISA-80K\", \"link\": \"https://huggingface.co/datasets/qihoo360/WISA-80K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 1\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.14739",
    "first_seen_date": "2025-02-21",
    "title": "SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.14739SuperGPQA: Scaling LLM Evaluation across 285 Graduate DisciplinesPublished on Feb 20\u00b7Submitted byAKon Feb 21#3 Paper of the dayUpvote106+98Authors:M-A-P Team,Xinrun Du,Yifan Yao,Kaijing Ma,Bingli Wang,Tianyu Zheng,Kang Zhu,Minghao Liu,Yiming Liang,Xiaolong Jin,Zhenlin Wei,Chujie Zheng,Kaixing Deng,Shuyue Guo,Shian Jia,Sichao Jiang,Yiyan Liao,Rui Li,Qinrui Li,Sirun Li,Yizhi Li,Yunwen Li+73 authorsAbstractSuperGPQA, a benchmark evaluating LLMs across 285 disciplines, reveals performance gaps and offers insights into the collaborative filtering process with human experts.AI-generated summaryLarge language models(LLMs) have demonstrated remarkable proficiency in\nmainstream academic disciplines such as mathematics, physics, and computer\nscience. However, human knowledge encompasses over 200 specialized disciplines,\nfar exceeding the scope of existing benchmarks. The capabilities ofLLMsin\nmany of these specialized fields-particularly in light industry, agriculture,\nand service-oriented disciplines-remain inadequately evaluated. To address this\ngap, we presentSuperGPQA, a comprehensive benchmark that evaluates\ngraduate-level knowledge and reasoning capabilities across 285 disciplines. Our\nbenchmark employs a novelHuman-LLM collaborative filteringmechanism to\neliminate trivial or ambiguous questions through iterative refinement based on\nboth LLM responses and expert feedback. Our experimental results reveal\nsignificant room for improvement in the performance of current state-of-the-artLLMsacross diverse knowledge domains (e.g., the reasoning-focused modelDeepSeek-R1achieved the highest accuracy of 61.82% onSuperGPQA), highlighting\nthe considerable gap between current model capabilities and artificial general\nintelligence. Additionally, we present comprehensive insights from our\nmanagement of a large-scale annotation process, involving over 80 expert\nannotators and an interactive",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.14739",
    "arxiv_url": "https://arxiv.org/abs/2502.14739",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 3,
    "datasets_list": "m-a-p/SuperGPQA, facebook/principia-bench, Medusan/SuperGPQA",
    "datasets_links": "https://huggingface.co/datasets/m-a-p/SuperGPQA, https://huggingface.co/datasets/facebook/principia-bench, https://huggingface.co/datasets/Medusan/SuperGPQA",
    "datasets_detailed": "[{\"name\": \"m-a-p/SuperGPQA\", \"link\": \"https://huggingface.co/datasets/m-a-p/SuperGPQA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\", \"size\": \"\"}, {\"name\": \"facebook/principia-bench\", \"link\": \"https://huggingface.co/datasets/facebook/principia-bench\", \"task\": \"\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"3 days ago\", \"size\": \"\"}, {\"name\": \"Medusan/SuperGPQA\", \"link\": \"https://huggingface.co/datasets/Medusan/SuperGPQA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 14\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.14786",
    "first_seen_date": "2025-02-21",
    "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense Features",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.14786SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic\n  Understanding, Localization, and Dense FeaturesPublished on Feb 20\u00b7Submitted byAKon Feb 21#2 Paper of the dayUpvote156+148Authors:Michael Tschannen,Alexey Gritsenko,Xiao Wang,Muhammad Ferjad Naeem,Ibrahim Alabdulmohsin,Nikhil Parthasarathy,Talfan Evans,Lucas Beyer,Ye Xia,Basil Mustafa,Olivier H\u00e9naff,Jeremiah Harmsen,Andreas Steiner,Xiaohua ZhaiAbstractSigLIP 2, a multilingual vision-language encoder, improves upon SigLIP with unified training techniques, enhancing performance in zero-shot classification, image-text retrieval, localization, and dense prediction across various model sizes and data diversity.AI-generated summaryWe introduce SigLIP 2, a family of new multilingualvision-language encodersthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text trainingobjective with several prior,\nindependently developed techniques into a unified recipe -- this includescaptioning-based pretraining,self-supervised losses(self-distillation, masked\nprediction) andonline data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincludingzero-shot classification,image-text retrieval, and transfer\nperformance when extractingvisual representations for Vision-Language Models\n(VLMs). Furthermore, the new trainingrecipe leads to significant improvements\nonlocalizationanddense predictiontasks. We also train variants which\nsupportmultiple resolutionsand preserve the input'snative aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasingtechniques, leadingto much bettermultilingual understandingand improvedfairness. To allow users to trade off inference cost with performance, we\nreleasemodel checkpointsat four sizes:ViT-B(86M), L (303M),So400m(400M),\nandg(1B).View a",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/google-research/big_vision",
    "hf_paper_url": "https://huggingface.co/papers/2502.14786",
    "arxiv_url": "https://arxiv.org/abs/2502.14786",
    "num_models": 137,
    "models_list": "google/siglip2-base-patch16-224, google/siglip2-so400m-patch14-384, google/siglip2-so400m-patch16-naflex, google/siglip2-so400m-patch16-512, google/siglip2-large-patch16-256, RedRocket/JTP-3, google/siglip2-base-patch32-256, google/siglip2-base-patch16-256, google/siglip2-base-patch16-384, google/siglip2-base-patch16-512, google/siglip2-large-patch16-384, google/siglip2-large-patch16-512, google/siglip2-giant-opt-patch16-256, google/siglip2-giant-opt-patch16-384, google/siglip2-so400m-patch14-224, google/siglip2-so400m-patch16-256, google/siglip2-so400m-patch16-384, google/siglip2-base-patch16-naflex, google/siglip2-base-patch16-224-jax, google/siglip2-base-patch16-256-jax, google/siglip2-base-patch16-384-jax, google/siglip2-base-patch16-512-jax, google/siglip2-base-patch16-naflex-jax, google/siglip2-base-patch32-256-jax, google/siglip2-giant-opt-patch16-256-jax, google/siglip2-giant-opt-patch16-384-jax, google/siglip2-large-patch16-256-jax, google/siglip2-large-patch16-384-jax, google/siglip2-large-patch16-512-jax, google/siglip2-so400m-patch14-224-jax, google/siglip2-so400m-patch14-384-jax, google/siglip2-so400m-patch16-256-jax, google/siglip2-so400m-patch16-384-jax, google/siglip2-so400m-patch16-512-jax, google/siglip2-so400m-patch16-naflex-jax, timm/ViT-B-16-SigLIP2-256, timm/ViT-B-32-SigLIP2-256, timm/ViT-B-16-SigLIP2, timm/ViT-B-16-SigLIP2-384, timm/ViT-B-16-SigLIP2-512, timm/ViT-L-16-SigLIP2-256, timm/ViT-L-16-SigLIP2-384, timm/ViT-L-16-SigLIP2-512, timm/ViT-SO400M-14-SigLIP2, timm/ViT-SO400M-14-SigLIP2-378, timm/ViT-SO400M-16-SigLIP2-256, timm/ViT-SO400M-16-SigLIP2-384, timm/ViT-SO400M-16-SigLIP2-512, timm/ViT-gopt-16-SigLIP2-256, timm/ViT-gopt-16-SigLIP2-384, timm/vit_so400m_patch16_siglip_gap_512.v2_webli, timm/vit_base_patch16_siglip_224.v2_webli, timm/vit_base_patch16_siglip_256.v2_webli, timm/vit_base_patch16_siglip_384.v2_webli, timm/vit_base_patch16_siglip_512.v2_webli, timm/vit_base_patch16_siglip_gap_224.v2_webli, timm/vit_base_patch16_siglip_gap_256.v2_webli, timm/vit_base_patch16_siglip_gap_384.v2_webli, timm/vit_base_patch16_siglip_gap_512.v2_webli, timm/vit_base_patch32_siglip_256.v2_webli, timm/vit_base_patch32_siglip_gap_256.v2_webli, timm/vit_giantopt_patch16_siglip_256.v2_webli, timm/vit_giantopt_patch16_siglip_384.v2_webli, timm/vit_giantopt_patch16_siglip_gap_256.v2_webli, timm/vit_giantopt_patch16_siglip_gap_384.v2_webli, timm/vit_large_patch16_siglip_256.v2_webli, timm/vit_large_patch16_siglip_384.v2_webli, timm/vit_large_patch16_siglip_512.v2_webli, timm/vit_large_patch16_siglip_gap_256.v2_webli, timm/vit_large_patch16_siglip_gap_384.v2_webli, timm/vit_large_patch16_siglip_gap_512.v2_webli, timm/vit_so400m_patch14_siglip_224.v2_webli, timm/vit_so400m_patch14_siglip_378.v2_webli, timm/vit_so400m_patch14_siglip_gap_224.v2_webli, timm/vit_so400m_patch14_siglip_gap_378.v2_webli, timm/vit_so400m_patch16_siglip_256.v2_webli, timm/vit_so400m_patch16_siglip_384.v2_webli, timm/vit_so400m_patch16_siglip_512.v2_webli, timm/vit_so400m_patch16_siglip_gap_256.v2_webli, timm/vit_so400m_patch16_siglip_gap_384.v2_webli, rahim-xelpmoc/siglip2-base-patch16-384, prithivMLmods/Fashion-Mnist-SigLIP2, prithivMLmods/Painting-126-DomainNet, prithivMLmods/Sketch-126-DomainNet, prithivMLmods/Multisource-121-DomainNet, prithivMLmods/Clipart-126-DomainNet, fancyfeast/so400m-long, prithivMLmods/RESISC45-SigLIP2, prithivMLmods/3D-Printed-Or-Not-SigLIP2, prithivMLmods/Watermark-Detection-SigLIP2, prithivMLmods/PACS-DG-SigLIP2, prithivMLmods/Formula-Text-Detection, prithivMLmods/siglip2-x256-explicit-content, prithivMLmods/siglip2-x256p32-explicit-content, prithivMLmods/Realistic-Gender-Classification, prithivMLmods/IMAGENETTE, prithivMLmods/DOZE-GUARD-RLDD, prithivMLmods/AIorNot-SigLIP2, prithivMLmods/OpenSDI-Flux.1-SigLIP2, prithivMLmods/OpenSDI-SD1.5-SigLIP2, prithivMLmods/OpenSDI-SD2.1-SigLIP2, prithivMLmods/OpenSDI-SD3-SigLIP2, prithivMLmods/OpenSDI-SDXL-SigLIP2, prithivMLmods/open-deepfake-detection, prithivMLmods/siglip2-mini-explicit-content, prithivMLmods/open-age-detection, prithivMLmods/Flood-Image-Detection, prithivMLmods/Forest-Fire-Detection, prithivMLmods/tooth-agenesis-siglip2, prithivMLmods/shoe-type-detection, prithivMLmods/facial-age-detection, KarteeMonkeys/appy-mod-beta1, KarteeMonkeys/appy-monkey-local-96.07, timm/naflexvit_base_patch16_gap.e300_s576_in1k, timm/naflexvit_base_patch16_par_gap.e300_s576_in1k, timm/naflexvit_base_patch16_parfac_gap.e300_s576_in1k, liuliuliuliuliuliuliuliu/siglip2-so400m-patch14-384, prithivMLmods/WebClick-AgentBrowse-SigLIP2, Rishabh5150/Watermark-Detection-SigLIP2, sharmajai901/Watermark-detection, birder-project/vit_so400m_p14_ap_siglip-v2-webli, timm/naflexvit_so400m_patch16_siglip.v2_webli, timm/naflexvit_base_patch16_siglip.v2_webli, prithivMLmods/Face-Confidence-SigLIP2, OPPOer/AndesVL-4B-Thinking, OPPOer/AndesVL-4B-Instruct, OPPOer/AndesVL-2B-Instruct, OPPOer/AndesVL-2B-Thinking, OPPOer/AndesVL-1B-Instruct, OPPOer/AndesVL-1B-Thinking, OPPOer/AndesVL-0_6B-Instruct, OPPOer/AndesVL-0_6B-Thinking, prithivMLmods/Image-Guard-2.0-Post0.1, ganeshhgupta/tooth-agenesis-model, replyquickflorida/tooth-agenesis-model, classtag/siq-vl_siglip2-so400m-patch14-224_qwen2.5-1.5b-instruct_stage1, classtag/siq-vl_siglip2-so400m-patch16-512_qwen2.5-1.5b-instruct_stage1",
    "models_links": "https://huggingface.co/google/siglip2-base-patch16-224, https://huggingface.co/google/siglip2-so400m-patch14-384, https://huggingface.co/google/siglip2-so400m-patch16-naflex, https://huggingface.co/google/siglip2-so400m-patch16-512, https://huggingface.co/google/siglip2-large-patch16-256, https://huggingface.co/RedRocket/JTP-3, https://huggingface.co/google/siglip2-base-patch32-256, https://huggingface.co/google/siglip2-base-patch16-256, https://huggingface.co/google/siglip2-base-patch16-384, https://huggingface.co/google/siglip2-base-patch16-512, https://huggingface.co/google/siglip2-large-patch16-384, https://huggingface.co/google/siglip2-large-patch16-512, https://huggingface.co/google/siglip2-giant-opt-patch16-256, https://huggingface.co/google/siglip2-giant-opt-patch16-384, https://huggingface.co/google/siglip2-so400m-patch14-224, https://huggingface.co/google/siglip2-so400m-patch16-256, https://huggingface.co/google/siglip2-so400m-patch16-384, https://huggingface.co/google/siglip2-base-patch16-naflex, https://huggingface.co/google/siglip2-base-patch16-224-jax, https://huggingface.co/google/siglip2-base-patch16-256-jax, https://huggingface.co/google/siglip2-base-patch16-384-jax, https://huggingface.co/google/siglip2-base-patch16-512-jax, https://huggingface.co/google/siglip2-base-patch16-naflex-jax, https://huggingface.co/google/siglip2-base-patch32-256-jax, https://huggingface.co/google/siglip2-giant-opt-patch16-256-jax, https://huggingface.co/google/siglip2-giant-opt-patch16-384-jax, https://huggingface.co/google/siglip2-large-patch16-256-jax, https://huggingface.co/google/siglip2-large-patch16-384-jax, https://huggingface.co/google/siglip2-large-patch16-512-jax, https://huggingface.co/google/siglip2-so400m-patch14-224-jax, https://huggingface.co/google/siglip2-so400m-patch14-384-jax, https://huggingface.co/google/siglip2-so400m-patch16-256-jax, https://huggingface.co/google/siglip2-so400m-patch16-384-jax, https://huggingface.co/google/siglip2-so400m-patch16-512-jax, https://huggingface.co/google/siglip2-so400m-patch16-naflex-jax, https://huggingface.co/timm/ViT-B-16-SigLIP2-256, https://huggingface.co/timm/ViT-B-32-SigLIP2-256, https://huggingface.co/timm/ViT-B-16-SigLIP2, https://huggingface.co/timm/ViT-B-16-SigLIP2-384, https://huggingface.co/timm/ViT-B-16-SigLIP2-512, https://huggingface.co/timm/ViT-L-16-SigLIP2-256, https://huggingface.co/timm/ViT-L-16-SigLIP2-384, https://huggingface.co/timm/ViT-L-16-SigLIP2-512, https://huggingface.co/timm/ViT-SO400M-14-SigLIP2, https://huggingface.co/timm/ViT-SO400M-14-SigLIP2-378, https://huggingface.co/timm/ViT-SO400M-16-SigLIP2-256, https://huggingface.co/timm/ViT-SO400M-16-SigLIP2-384, https://huggingface.co/timm/ViT-SO400M-16-SigLIP2-512, https://huggingface.co/timm/ViT-gopt-16-SigLIP2-256, https://huggingface.co/timm/ViT-gopt-16-SigLIP2-384, https://huggingface.co/timm/vit_so400m_patch16_siglip_gap_512.v2_webli, https://huggingface.co/timm/vit_base_patch16_siglip_224.v2_webli, https://huggingface.co/timm/vit_base_patch16_siglip_256.v2_webli, https://huggingface.co/timm/vit_base_patch16_siglip_384.v2_webli, https://huggingface.co/timm/vit_base_patch16_siglip_512.v2_webli, https://huggingface.co/timm/vit_base_patch16_siglip_gap_224.v2_webli, https://huggingface.co/timm/vit_base_patch16_siglip_gap_256.v2_webli, https://huggingface.co/timm/vit_base_patch16_siglip_gap_384.v2_webli, https://huggingface.co/timm/vit_base_patch16_siglip_gap_512.v2_webli, https://huggingface.co/timm/vit_base_patch32_siglip_256.v2_webli, https://huggingface.co/timm/vit_base_patch32_siglip_gap_256.v2_webli, https://huggingface.co/timm/vit_giantopt_patch16_siglip_256.v2_webli, https://huggingface.co/timm/vit_giantopt_patch16_siglip_384.v2_webli, https://huggingface.co/timm/vit_giantopt_patch16_siglip_gap_256.v2_webli, https://huggingface.co/timm/vit_giantopt_patch16_siglip_gap_384.v2_webli, https://huggingface.co/timm/vit_large_patch16_siglip_256.v2_webli, https://huggingface.co/timm/vit_large_patch16_siglip_384.v2_webli, https://huggingface.co/timm/vit_large_patch16_siglip_512.v2_webli, https://huggingface.co/timm/vit_large_patch16_siglip_gap_256.v2_webli, https://huggingface.co/timm/vit_large_patch16_siglip_gap_384.v2_webli, https://huggingface.co/timm/vit_large_patch16_siglip_gap_512.v2_webli, https://huggingface.co/timm/vit_so400m_patch14_siglip_224.v2_webli, https://huggingface.co/timm/vit_so400m_patch14_siglip_378.v2_webli, https://huggingface.co/timm/vit_so400m_patch14_siglip_gap_224.v2_webli, https://huggingface.co/timm/vit_so400m_patch14_siglip_gap_378.v2_webli, https://huggingface.co/timm/vit_so400m_patch16_siglip_256.v2_webli, https://huggingface.co/timm/vit_so400m_patch16_siglip_384.v2_webli, https://huggingface.co/timm/vit_so400m_patch16_siglip_512.v2_webli, https://huggingface.co/timm/vit_so400m_patch16_siglip_gap_256.v2_webli, https://huggingface.co/timm/vit_so400m_patch16_siglip_gap_384.v2_webli, https://huggingface.co/rahim-xelpmoc/siglip2-base-patch16-384, https://huggingface.co/prithivMLmods/Fashion-Mnist-SigLIP2, https://huggingface.co/prithivMLmods/Painting-126-DomainNet, https://huggingface.co/prithivMLmods/Sketch-126-DomainNet, https://huggingface.co/prithivMLmods/Multisource-121-DomainNet, https://huggingface.co/prithivMLmods/Clipart-126-DomainNet, https://huggingface.co/fancyfeast/so400m-long, https://huggingface.co/prithivMLmods/RESISC45-SigLIP2, https://huggingface.co/prithivMLmods/3D-Printed-Or-Not-SigLIP2, https://huggingface.co/prithivMLmods/Watermark-Detection-SigLIP2, https://huggingface.co/prithivMLmods/PACS-DG-SigLIP2, https://huggingface.co/prithivMLmods/Formula-Text-Detection, https://huggingface.co/prithivMLmods/siglip2-x256-explicit-content, https://huggingface.co/prithivMLmods/siglip2-x256p32-explicit-content, https://huggingface.co/prithivMLmods/Realistic-Gender-Classification, https://huggingface.co/prithivMLmods/IMAGENETTE, https://huggingface.co/prithivMLmods/DOZE-GUARD-RLDD, https://huggingface.co/prithivMLmods/AIorNot-SigLIP2, https://huggingface.co/prithivMLmods/OpenSDI-Flux.1-SigLIP2, https://huggingface.co/prithivMLmods/OpenSDI-SD1.5-SigLIP2, https://huggingface.co/prithivMLmods/OpenSDI-SD2.1-SigLIP2, https://huggingface.co/prithivMLmods/OpenSDI-SD3-SigLIP2, https://huggingface.co/prithivMLmods/OpenSDI-SDXL-SigLIP2, https://huggingface.co/prithivMLmods/open-deepfake-detection, https://huggingface.co/prithivMLmods/siglip2-mini-explicit-content, https://huggingface.co/prithivMLmods/open-age-detection, https://huggingface.co/prithivMLmods/Flood-Image-Detection, https://huggingface.co/prithivMLmods/Forest-Fire-Detection, https://huggingface.co/prithivMLmods/tooth-agenesis-siglip2, https://huggingface.co/prithivMLmods/shoe-type-detection, https://huggingface.co/prithivMLmods/facial-age-detection, https://huggingface.co/KarteeMonkeys/appy-mod-beta1, https://huggingface.co/KarteeMonkeys/appy-monkey-local-96.07, https://huggingface.co/timm/naflexvit_base_patch16_gap.e300_s576_in1k, https://huggingface.co/timm/naflexvit_base_patch16_par_gap.e300_s576_in1k, https://huggingface.co/timm/naflexvit_base_patch16_parfac_gap.e300_s576_in1k, https://huggingface.co/liuliuliuliuliuliuliuliu/siglip2-so400m-patch14-384, https://huggingface.co/prithivMLmods/WebClick-AgentBrowse-SigLIP2, https://huggingface.co/Rishabh5150/Watermark-Detection-SigLIP2, https://huggingface.co/sharmajai901/Watermark-detection, https://huggingface.co/birder-project/vit_so400m_p14_ap_siglip-v2-webli, https://huggingface.co/timm/naflexvit_so400m_patch16_siglip.v2_webli, https://huggingface.co/timm/naflexvit_base_patch16_siglip.v2_webli, https://huggingface.co/prithivMLmods/Face-Confidence-SigLIP2, https://huggingface.co/OPPOer/AndesVL-4B-Thinking, https://huggingface.co/OPPOer/AndesVL-4B-Instruct, https://huggingface.co/OPPOer/AndesVL-2B-Instruct, https://huggingface.co/OPPOer/AndesVL-2B-Thinking, https://huggingface.co/OPPOer/AndesVL-1B-Instruct, https://huggingface.co/OPPOer/AndesVL-1B-Thinking, https://huggingface.co/OPPOer/AndesVL-0_6B-Instruct, https://huggingface.co/OPPOer/AndesVL-0_6B-Thinking, https://huggingface.co/prithivMLmods/Image-Guard-2.0-Post0.1, https://huggingface.co/ganeshhgupta/tooth-agenesis-model, https://huggingface.co/replyquickflorida/tooth-agenesis-model, https://huggingface.co/classtag/siq-vl_siglip2-so400m-patch14-224_qwen2.5-1.5b-instruct_stage1, https://huggingface.co/classtag/siq-vl_siglip2-so400m-patch16-512_qwen2.5-1.5b-instruct_stage1",
    "models_detailed": "[{\"name\": \"google/siglip2-base-patch16-224\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-224\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch14-384\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch14-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch16-naflex\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch16-naflex\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch16-512\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch16-512\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-large-patch16-256\", \"link\": \"https://huggingface.co/google/siglip2-large-patch16-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"RedRocket/JTP-3\", \"link\": \"https://huggingface.co/RedRocket/JTP-3\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"google/siglip2-base-patch32-256\", \"link\": \"https://huggingface.co/google/siglip2-base-patch32-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-256\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-384\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-512\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-512\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-large-patch16-384\", \"link\": \"https://huggingface.co/google/siglip2-large-patch16-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-large-patch16-512\", \"link\": \"https://huggingface.co/google/siglip2-large-patch16-512\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-giant-opt-patch16-256\", \"link\": \"https://huggingface.co/google/siglip2-giant-opt-patch16-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-giant-opt-patch16-384\", \"link\": \"https://huggingface.co/google/siglip2-giant-opt-patch16-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch14-224\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch14-224\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch16-256\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch16-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch16-384\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch16-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-naflex\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-naflex\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-224-jax\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-224-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-256-jax\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-256-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-384-jax\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-384-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-512-jax\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-512-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch16-naflex-jax\", \"link\": \"https://huggingface.co/google/siglip2-base-patch16-naflex-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-base-patch32-256-jax\", \"link\": \"https://huggingface.co/google/siglip2-base-patch32-256-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-giant-opt-patch16-256-jax\", \"link\": \"https://huggingface.co/google/siglip2-giant-opt-patch16-256-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-giant-opt-patch16-384-jax\", \"link\": \"https://huggingface.co/google/siglip2-giant-opt-patch16-384-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-large-patch16-256-jax\", \"link\": \"https://huggingface.co/google/siglip2-large-patch16-256-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-large-patch16-384-jax\", \"link\": \"https://huggingface.co/google/siglip2-large-patch16-384-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-large-patch16-512-jax\", \"link\": \"https://huggingface.co/google/siglip2-large-patch16-512-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch14-224-jax\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch14-224-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch14-384-jax\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch14-384-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch16-256-jax\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch16-256-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch16-384-jax\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch16-384-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch16-512-jax\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch16-512-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"google/siglip2-so400m-patch16-naflex-jax\", \"link\": \"https://huggingface.co/google/siglip2-so400m-patch16-naflex-jax\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-B-16-SigLIP2-256\", \"link\": \"https://huggingface.co/timm/ViT-B-16-SigLIP2-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-B-32-SigLIP2-256\", \"link\": \"https://huggingface.co/timm/ViT-B-32-SigLIP2-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-B-16-SigLIP2\", \"link\": \"https://huggingface.co/timm/ViT-B-16-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-B-16-SigLIP2-384\", \"link\": \"https://huggingface.co/timm/ViT-B-16-SigLIP2-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-B-16-SigLIP2-512\", \"link\": \"https://huggingface.co/timm/ViT-B-16-SigLIP2-512\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-L-16-SigLIP2-256\", \"link\": \"https://huggingface.co/timm/ViT-L-16-SigLIP2-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-L-16-SigLIP2-384\", \"link\": \"https://huggingface.co/timm/ViT-L-16-SigLIP2-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-L-16-SigLIP2-512\", \"link\": \"https://huggingface.co/timm/ViT-L-16-SigLIP2-512\", \"task\": \"Image Classification\", \"likes\": \"600\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-SO400M-14-SigLIP2\", \"link\": \"https://huggingface.co/timm/ViT-SO400M-14-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-SO400M-14-SigLIP2-378\", \"link\": \"https://huggingface.co/timm/ViT-SO400M-14-SigLIP2-378\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-SO400M-16-SigLIP2-256\", \"link\": \"https://huggingface.co/timm/ViT-SO400M-16-SigLIP2-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-SO400M-16-SigLIP2-384\", \"link\": \"https://huggingface.co/timm/ViT-SO400M-16-SigLIP2-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-SO400M-16-SigLIP2-512\", \"link\": \"https://huggingface.co/timm/ViT-SO400M-16-SigLIP2-512\", \"task\": \"Image Classification\", \"likes\": \"699\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-gopt-16-SigLIP2-256\", \"link\": \"https://huggingface.co/timm/ViT-gopt-16-SigLIP2-256\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/ViT-gopt-16-SigLIP2-384\", \"link\": \"https://huggingface.co/timm/ViT-gopt-16-SigLIP2-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch16_siglip_gap_512.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch16_siglip_gap_512.v2_webli\", \"task\": \"\", \"likes\": \"114\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch16_siglip_224.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_siglip_224.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch16_siglip_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_siglip_256.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch16_siglip_384.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_siglip_384.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch16_siglip_512.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_siglip_512.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch16_siglip_gap_224.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_siglip_gap_224.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch16_siglip_gap_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_siglip_gap_256.v2_webli\", \"task\": \"\", \"likes\": \"80\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch16_siglip_gap_384.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_siglip_gap_384.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch16_siglip_gap_512.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch16_siglip_gap_512.v2_webli\", \"task\": \"\", \"likes\": \"80\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch32_siglip_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch32_siglip_256.v2_webli\", \"task\": \"\", \"likes\": \"51\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_base_patch32_siglip_gap_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_base_patch32_siglip_gap_256.v2_webli\", \"task\": \"\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_giantopt_patch16_siglip_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_giantopt_patch16_siglip_256.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_giantopt_patch16_siglip_384.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_giantopt_patch16_siglip_384.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_giantopt_patch16_siglip_gap_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_giantopt_patch16_siglip_gap_256.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_giantopt_patch16_siglip_gap_384.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_giantopt_patch16_siglip_gap_384.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_large_patch16_siglip_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_siglip_256.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_large_patch16_siglip_384.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_siglip_384.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_large_patch16_siglip_512.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_siglip_512.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_large_patch16_siglip_gap_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_siglip_gap_256.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_large_patch16_siglip_gap_384.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_siglip_gap_384.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_large_patch16_siglip_gap_512.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_large_patch16_siglip_gap_512.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch14_siglip_224.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch14_siglip_224.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch14_siglip_378.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch14_siglip_378.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch14_siglip_gap_224.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch14_siglip_gap_224.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch14_siglip_gap_378.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch14_siglip_gap_378.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch16_siglip_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch16_siglip_256.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch16_siglip_384.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch16_siglip_384.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch16_siglip_512.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch16_siglip_512.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch16_siglip_gap_256.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch16_siglip_gap_256.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"timm/vit_so400m_patch16_siglip_gap_384.v2_webli\", \"link\": \"https://huggingface.co/timm/vit_so400m_patch16_siglip_gap_384.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"rahim-xelpmoc/siglip2-base-patch16-384\", \"link\": \"https://huggingface.co/rahim-xelpmoc/siglip2-base-patch16-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 25\"}, {\"name\": \"prithivMLmods/Fashion-Mnist-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/Fashion-Mnist-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"27\", \"downloads\": \"\", \"updated\": \"Mar 24\"}, {\"name\": \"prithivMLmods/Painting-126-DomainNet\", \"link\": \"https://huggingface.co/prithivMLmods/Painting-126-DomainNet\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 24\"}, {\"name\": \"prithivMLmods/Sketch-126-DomainNet\", \"link\": \"https://huggingface.co/prithivMLmods/Sketch-126-DomainNet\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 24\"}, {\"name\": \"prithivMLmods/Multisource-121-DomainNet\", \"link\": \"https://huggingface.co/prithivMLmods/Multisource-121-DomainNet\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 24\"}, {\"name\": \"prithivMLmods/Clipart-126-DomainNet\", \"link\": \"https://huggingface.co/prithivMLmods/Clipart-126-DomainNet\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 24\"}, {\"name\": \"fancyfeast/so400m-long\", \"link\": \"https://huggingface.co/fancyfeast/so400m-long\", \"task\": \"Image Classification\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Jul 25\"}, {\"name\": \"prithivMLmods/RESISC45-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/RESISC45-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"prithivMLmods/3D-Printed-Or-Not-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/3D-Printed-Or-Not-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"prithivMLmods/Watermark-Detection-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/Watermark-Detection-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"prithivMLmods/PACS-DG-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/PACS-DG-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"prithivMLmods/Formula-Text-Detection\", \"link\": \"https://huggingface.co/prithivMLmods/Formula-Text-Detection\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"prithivMLmods/siglip2-x256-explicit-content\", \"link\": \"https://huggingface.co/prithivMLmods/siglip2-x256-explicit-content\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"prithivMLmods/siglip2-x256p32-explicit-content\", \"link\": \"https://huggingface.co/prithivMLmods/siglip2-x256p32-explicit-content\", \"task\": \"Image Classification\", \"likes\": \"32\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"prithivMLmods/Realistic-Gender-Classification\", \"link\": \"https://huggingface.co/prithivMLmods/Realistic-Gender-Classification\", \"task\": \"Image Classification\", \"likes\": \"600\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"prithivMLmods/IMAGENETTE\", \"link\": \"https://huggingface.co/prithivMLmods/IMAGENETTE\", \"task\": \"Image Classification\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"May 14\"}, {\"name\": \"prithivMLmods/DOZE-GUARD-RLDD\", \"link\": \"https://huggingface.co/prithivMLmods/DOZE-GUARD-RLDD\", \"task\": \"Image Classification\", \"likes\": \"143\", \"downloads\": \"\", \"updated\": \"May 14\"}, {\"name\": \"prithivMLmods/AIorNot-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/AIorNot-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"May 14\"}, {\"name\": \"prithivMLmods/OpenSDI-Flux.1-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/OpenSDI-Flux.1-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"prithivMLmods/OpenSDI-SD1.5-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/OpenSDI-SD1.5-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"prithivMLmods/OpenSDI-SD2.1-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/OpenSDI-SD2.1-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"prithivMLmods/OpenSDI-SD3-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/OpenSDI-SD3-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"prithivMLmods/OpenSDI-SDXL-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/OpenSDI-SDXL-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"prithivMLmods/open-deepfake-detection\", \"link\": \"https://huggingface.co/prithivMLmods/open-deepfake-detection\", \"task\": \"Image Classification\", \"likes\": \"702\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"prithivMLmods/siglip2-mini-explicit-content\", \"link\": \"https://huggingface.co/prithivMLmods/siglip2-mini-explicit-content\", \"task\": \"Image Classification\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"May 19\"}, {\"name\": \"prithivMLmods/open-age-detection\", \"link\": \"https://huggingface.co/prithivMLmods/open-age-detection\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"prithivMLmods/Flood-Image-Detection\", \"link\": \"https://huggingface.co/prithivMLmods/Flood-Image-Detection\", \"task\": \"Image Classification\", \"likes\": \"37\", \"downloads\": \"\", \"updated\": \"May 26\"}, {\"name\": \"prithivMLmods/Forest-Fire-Detection\", \"link\": \"https://huggingface.co/prithivMLmods/Forest-Fire-Detection\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 26\"}, {\"name\": \"prithivMLmods/tooth-agenesis-siglip2\", \"link\": \"https://huggingface.co/prithivMLmods/tooth-agenesis-siglip2\", \"task\": \"Image Classification\", \"likes\": \"121\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"prithivMLmods/shoe-type-detection\", \"link\": \"https://huggingface.co/prithivMLmods/shoe-type-detection\", \"task\": \"Image Classification\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"prithivMLmods/facial-age-detection\", \"link\": \"https://huggingface.co/prithivMLmods/facial-age-detection\", \"task\": \"Image Classification\", \"likes\": \"430\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"KarteeMonkeys/appy-mod-beta1\", \"link\": \"https://huggingface.co/KarteeMonkeys/appy-mod-beta1\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"KarteeMonkeys/appy-monkey-local-96.07\", \"link\": \"https://huggingface.co/KarteeMonkeys/appy-monkey-local-96.07\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"timm/naflexvit_base_patch16_gap.e300_s576_in1k\", \"link\": \"https://huggingface.co/timm/naflexvit_base_patch16_gap.e300_s576_in1k\", \"task\": \"Image Classification\", \"likes\": \"293\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"timm/naflexvit_base_patch16_par_gap.e300_s576_in1k\", \"link\": \"https://huggingface.co/timm/naflexvit_base_patch16_par_gap.e300_s576_in1k\", \"task\": \"Image Classification\", \"likes\": \"287\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"timm/naflexvit_base_patch16_parfac_gap.e300_s576_in1k\", \"link\": \"https://huggingface.co/timm/naflexvit_base_patch16_parfac_gap.e300_s576_in1k\", \"task\": \"Image Classification\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"liuliuliuliuliuliuliuliu/siglip2-so400m-patch14-384\", \"link\": \"https://huggingface.co/liuliuliuliuliuliuliuliu/siglip2-so400m-patch14-384\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 21\"}, {\"name\": \"prithivMLmods/WebClick-AgentBrowse-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/WebClick-AgentBrowse-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 22\"}, {\"name\": \"Rishabh5150/Watermark-Detection-SigLIP2\", \"link\": \"https://huggingface.co/Rishabh5150/Watermark-Detection-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"sharmajai901/Watermark-detection\", \"link\": \"https://huggingface.co/sharmajai901/Watermark-detection\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\"}, {\"name\": \"birder-project/vit_so400m_p14_ap_siglip-v2-webli\", \"link\": \"https://huggingface.co/birder-project/vit_so400m_p14_ap_siglip-v2-webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 5\"}, {\"name\": \"timm/naflexvit_so400m_patch16_siglip.v2_webli\", \"link\": \"https://huggingface.co/timm/naflexvit_so400m_patch16_siglip.v2_webli\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"timm/naflexvit_base_patch16_siglip.v2_webli\", \"link\": \"https://huggingface.co/timm/naflexvit_base_patch16_siglip.v2_webli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 4\"}, {\"name\": \"prithivMLmods/Face-Confidence-SigLIP2\", \"link\": \"https://huggingface.co/prithivMLmods/Face-Confidence-SigLIP2\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"OPPOer/AndesVL-4B-Thinking\", \"link\": \"https://huggingface.co/OPPOer/AndesVL-4B-Thinking\", \"task\": \"\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"OPPOer/AndesVL-4B-Instruct\", \"link\": \"https://huggingface.co/OPPOer/AndesVL-4B-Instruct\", \"task\": \"\", \"likes\": \"58\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"OPPOer/AndesVL-2B-Instruct\", \"link\": \"https://huggingface.co/OPPOer/AndesVL-2B-Instruct\", \"task\": \"\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"OPPOer/AndesVL-2B-Thinking\", \"link\": \"https://huggingface.co/OPPOer/AndesVL-2B-Thinking\", \"task\": \"\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"OPPOer/AndesVL-1B-Instruct\", \"link\": \"https://huggingface.co/OPPOer/AndesVL-1B-Instruct\", \"task\": \"\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"OPPOer/AndesVL-1B-Thinking\", \"link\": \"https://huggingface.co/OPPOer/AndesVL-1B-Thinking\", \"task\": \"\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"OPPOer/AndesVL-0_6B-Instruct\", \"link\": \"https://huggingface.co/OPPOer/AndesVL-0_6B-Instruct\", \"task\": \"\", \"likes\": \"113\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"OPPOer/AndesVL-0_6B-Thinking\", \"link\": \"https://huggingface.co/OPPOer/AndesVL-0_6B-Thinking\", \"task\": \"\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"prithivMLmods/Image-Guard-2.0-Post0.1\", \"link\": \"https://huggingface.co/prithivMLmods/Image-Guard-2.0-Post0.1\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"ganeshhgupta/tooth-agenesis-model\", \"link\": \"https://huggingface.co/ganeshhgupta/tooth-agenesis-model\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"replyquickflorida/tooth-agenesis-model\", \"link\": \"https://huggingface.co/replyquickflorida/tooth-agenesis-model\", \"task\": \"Image Classification\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"classtag/siq-vl_siglip2-so400m-patch14-224_qwen2.5-1.5b-instruct_stage1\", \"link\": \"https://huggingface.co/classtag/siq-vl_siglip2-so400m-patch14-224_qwen2.5-1.5b-instruct_stage1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"23 days ago\"}, {\"name\": \"classtag/siq-vl_siglip2-so400m-patch16-512_qwen2.5-1.5b-instruct_stage1\", \"link\": \"https://huggingface.co/classtag/siq-vl_siglip2-so400m-patch16-512_qwen2.5-1.5b-instruct_stage1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}]",
    "num_datasets": 1,
    "datasets_list": "Intelligent-Internet/pd12m",
    "datasets_links": "https://huggingface.co/datasets/Intelligent-Internet/pd12m",
    "datasets_detailed": "[{\"name\": \"Intelligent-Internet/pd12m\", \"link\": \"https://huggingface.co/datasets/Intelligent-Internet/pd12m\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.14846",
    "first_seen_date": "2025-02-21",
    "title": "Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.14846Scaling Text-Rich Image Understanding via Code-Guided Synthetic\n  Multimodal Data GenerationPublished on Feb 20\u00b7Submitted byAKon Feb 21Upvote14+6Authors:Yue Yang,Ajay Patel,Matt Deitke,Tanmay Gupta,Luca Weihs,Andrew Head,Mark Yatskar,Chris Callison-Burch,Ranjay Krishna,Aniruddha Kembhavi,Christopher ClarkAbstractCoSyn uses text-only LLMs to generate synthetic text-rich multimodal data, achieving state-of-the-art performance on VLM benchmarks and enabling synthetic pointing data for grounding information within images.AI-generated summaryReasoning about images with rich text, such as charts and documents, is a\ncritical application ofvision-language models(VLMs). However, VLMs often\nstruggle in these domains due to the scarcity of diverse text-rich\nvision-language data. To address this challenge, we present CoSyn, a framework\nthat leverages the coding capabilities of text-only large language models\n(LLMs) to automatically createsynthetic text-rich multimodal data. Given input\ntext describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts\nan LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic\nimages. With the underlying code as textual representations of the synthetic\nimages, CoSyn can generate high-qualityinstruction-tuning data, again relying\non a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K\nimages and 2.7M rows of vision-languageinstruction-tuning data. Comprehensive\nexperiments on seven benchmarks demonstrate that models trained on our\nsynthetic data achieve state-of-the-art performance among competitive\nopen-source models, including Llama 3.2, and surpass proprietary models such as\nGPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing\ndata, enabling VLMs to ground information within input images, showcasing its\npotential for developingmultimodal agentscapable of acting in real-world\ne",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/allenai/pixmo-docs",
    "hf_paper_url": "https://huggingface.co/papers/2502.14846",
    "arxiv_url": "https://arxiv.org/abs/2502.14846",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 6,
    "datasets_list": "allenai/CoSyn-400K, allenai/CoSyn-point, DocTron-Hub/VinciCoder-1.6M-SFT, DocTron-Hub/VinciCoder-42k-RL, yyupenn/NutritionQA, yyupenn/DocPointQA",
    "datasets_links": "https://huggingface.co/datasets/allenai/CoSyn-400K, https://huggingface.co/datasets/allenai/CoSyn-point, https://huggingface.co/datasets/DocTron-Hub/VinciCoder-1.6M-SFT, https://huggingface.co/datasets/DocTron-Hub/VinciCoder-42k-RL, https://huggingface.co/datasets/yyupenn/NutritionQA, https://huggingface.co/datasets/yyupenn/DocPointQA",
    "datasets_detailed": "[{\"name\": \"allenai/CoSyn-400K\", \"link\": \"https://huggingface.co/datasets/allenai/CoSyn-400K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 28\", \"size\": \"\"}, {\"name\": \"allenai/CoSyn-point\", \"link\": \"https://huggingface.co/datasets/allenai/CoSyn-point\", \"task\": \"\", \"likes\": \"483\", \"downloads\": \"\", \"updated\": \"Feb 28\", \"size\": \"\"}, {\"name\": \"DocTron-Hub/VinciCoder-1.6M-SFT\", \"link\": \"https://huggingface.co/datasets/DocTron-Hub/VinciCoder-1.6M-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\", \"size\": \"\"}, {\"name\": \"DocTron-Hub/VinciCoder-42k-RL\", \"link\": \"https://huggingface.co/datasets/DocTron-Hub/VinciCoder-42k-RL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\", \"size\": \"\"}, {\"name\": \"yyupenn/NutritionQA\", \"link\": \"https://huggingface.co/datasets/yyupenn/NutritionQA\", \"task\": \"\", \"likes\": \"50\", \"downloads\": \"\", \"updated\": \"Jul 22\", \"size\": \"\"}, {\"name\": \"yyupenn/DocPointQA\", \"link\": \"https://huggingface.co/datasets/yyupenn/DocPointQA\", \"task\": \"\", \"likes\": \"300\", \"downloads\": \"\", \"updated\": \"Mar 1\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.13595",
    "first_seen_date": "2025-02-20",
    "title": "MMTEB: Massive Multilingual Text Embedding Benchmark",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.13595MMTEB: Massive Multilingual Text Embedding BenchmarkPublished on Feb 19\u00b7Submitted byNiklas Muennighoffon Feb 20\u00b7Massive Text Embedding BenchmarkUpvote43+35Authors:Kenneth Enevoldsen,Isaac Chung,Imene Kerboua,M\u00e1rton Kardos,Ashwin Mathur,David Stap,Jay Gala,Wissam Siblini,Dominik Krzemi\u0144ski,Genta Indra Winata,Saba Sturua,Saiteja Utpala,Mathieu Ciancone,Marion Schaeffer,Gabriel Sequeira,Diganta Misra,Shreeya Dhakal,Jonathan Rystr\u00f8m,Roman Solomatin,\u00d6mer \u00c7a\u011fatan,Akash Kundu,Martin Bernstorff+64 authorsAbstractThe MMTEB benchmark expands MTEB with over 500 tasks across 250+ languages to comprehensively evaluate text embeddings, finding that smaller multilingual models can outperform large LLMs in many cases.AI-generated summaryText embeddingsare typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce theMassive Multilingual Text Embedding Benchmark (MMTEB)- a large-scale,\ncommunity-driven expansion ofMTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEBincludes a diverse set of\nchallenging, novel tasks such asinstruction following, long-document\nretrieval, andcode retrieval, representing the largestmultilingualcollection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highlymultilingualbenchmarks, which we use to evaluate a\nrepresentative set of models. We find that whilelarge language models (LLMs)with billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel ismultilingual-e5-large-instructwith only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a noveldownsampling methodbased oninter-task correlation, ensuring a",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/embeddings-benchmark/mteb",
    "hf_paper_url": "https://huggingface.co/papers/2502.13595",
    "arxiv_url": "https://arxiv.org/abs/2502.13595",
    "num_models": 2,
    "models_list": "nvidia/llama-embed-nemotron-8b, MarcGrumpyOlejak/sts-mrl-en-de-base-v1",
    "models_links": "https://huggingface.co/nvidia/llama-embed-nemotron-8b, https://huggingface.co/MarcGrumpyOlejak/sts-mrl-en-de-base-v1",
    "models_detailed": "[{\"name\": \"nvidia/llama-embed-nemotron-8b\", \"link\": \"https://huggingface.co/nvidia/llama-embed-nemotron-8b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 15\"}, {\"name\": \"MarcGrumpyOlejak/sts-mrl-en-de-base-v1\", \"link\": \"https://huggingface.co/MarcGrumpyOlejak/sts-mrl-en-de-base-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}]",
    "num_datasets": 294,
    "datasets_list": "mteb/sts22-crosslingual-sts, mteb/sts12-sts, mteb/sickr-sts, mteb/sts13-sts, mteb/stsbenchmark-sts, mteb/arxiv-clustering-p2p, mteb/amazon_counterfactual, mteb/TurkicClassification, mteb/reddit-clustering, mteb/stackexchange-clustering, mteb/twentynewsgroups-clustering, mteb/twitterurlcorpus-pairclassification, mteb/sprintduplicatequestions-pairclassification, mteb/twittersemeval2015-pairclassification, mteb/biosses-sts, mteb/sts14-sts, mteb/sts15-sts, mteb/sts16-sts, mteb/arxiv-clustering-s2s, mteb/biorxiv-clustering-s2s, mteb/biorxiv-clustering-p2p, mteb/medrxiv-clustering-s2s, mteb/medrxiv-clustering-p2p, mteb/stackexchange-clustering-p2p, mteb/reddit-clustering-p2p, mteb/amazon_massive_scenario, mteb/amazon_massive_intent, mteb/banking77, mteb/sts17-crosslingual-sts, mteb/mtop_intent, mteb/mtop_domain, mteb/tatoeba-bitext-mining, mteb/bucc-bitext-mining, mteb/emotion, mteb/amazon_polarity, mteb/imdb, mteb/toxic_conversations_50k, mteb/tweet_sentiment_extraction, mteb/summeval, mteb/germanquad-retrieval, mteb/scifact, mteb/cqadupstack-android, mteb/cqadupstack-english, mteb/cqadupstack-gaming, mteb/cqadupstack-gis, mteb/cqadupstack-mathematica, mteb/cqadupstack-physics, mteb/cqadupstack-programmers, mteb/cqadupstack-stats, mteb/cqadupstack-tex, mteb/cqadupstack-unix, mteb/cqadupstack-webmasters, mteb/cqadupstack-wordpress, mteb/msmarco, mteb/arguana, mteb/dbpedia, mteb/nq, mteb/hotpotqa, mteb/trec-covid, mteb/fever, mteb/fiqa, mteb/climate-fever, mteb/quora, mteb/msmarco-v2, mteb/nfcorpus, mteb/legal_summarization, mteb/legalbench_consumer_contracts_qa, mteb/legalbench_corporate_lobbying, mteb/AILA_statutes, mteb/LeCaRDv2, mteb/LegalQuAD, mteb/GerDaLIRSmall, mteb/norec_classification, mteb/swerec_classification, mteb/norquad_retrieval, mteb/medical_qa, mteb/neuclir-2022, mteb/neuclir-2023, mteb/multi-hatecheck, mteb/eurlex-multilingual, mteb/multilingual-sentiment-classification, mteb/multilingual-scala-classification, mteb/biblenlp-corpus-mmteb, mteb/tweet_sentiment_multilingual, mteb/IndicSentiment, mteb/stsb_multi_mt, mteb/xnli, mteb/sib200, mteb/masakhanews, mteb/flores, mteb/NTREX, mteb/IN22-Conv, mteb/IN22-Gen, mteb/big-patent, mteb/wit, mteb/miracl-hard-negatives, mteb/Quora_PL_test_top_250_only_w_correct-v2, mteb/QuoraRetrieval_test_top_250_only_w_correct-v2, mteb/DBPedia_PL_test_top_250_only_w_correct-v2, mteb/MSMARCO_PL_test_top_250_only_w_correct-v2, mteb/DBPedia_test_top_250_only_w_correct-v2, mteb/ClimateFEVER_test_top_250_only_w_correct-v2, mteb/NQ_test_top_250_only_w_correct-v2, mteb/NQ_PL_test_top_250_only_w_correct-v2, mteb/HotpotQA_PL_test_top_250_only_w_correct-v2, mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2, mteb/FEVER_test_top_250_only_w_correct-v2, mteb/HotpotQA_test_top_250_only_w_correct-v2, mteb/TopiOCQA_validation_top_250_only_w_correct-v2, mteb/neuclir-2023-hard-negatives, mteb/MSMARCO_test_top_250_only_w_correct-v2, mteb/neuclir-2022-hard-negatives, mteb/jaqket, mteb/mrtidy, mteb/webis-touche2020-v3, mteb/mlsum, mteb/told-br, mteb/InstructIR-mteb, mteb/WikipediaRetrievalMultilingual, mteb/T2Retrieval, mteb/MMarcoRetrieval, mteb/DuRetrieval, mteb/CovidRetrieval, mteb/CmedqaRetrieval, mteb/EcomRetrieval, mteb/MedicalRetrieval, mteb/VideoRetrieval, mteb/AskUbuntuDupQuestions, mteb/IWSLT2017BitextMining, mteb/AmazonReviewsClassification, mteb/IndicReviewsClusteringP2P, mteb/XNLIV2, mteb/IndicCrosslingualSTS, mteb/HotelReviewSentimentClassification, mteb/TweetEmotionClassification, mteb/TenKGnadClassification, mteb/IndicQARetrieval, mteb/ArxivClassification, mteb/PatentClassification, mteb/FilipinoHateSpeechClassification, mteb/MyanmarNews, mteb/DutchBookReviewSentimentClassification, mteb/SwedishSentimentClassification, mteb/WisesightSentimentClassification, mteb/UrduRomanSentimentClassification, mteb/JSTS, mteb/climate-fever-v2, mteb/CQADupstack-Wordpress-PL, mteb/MSMARCO-PL, mteb/TRECCOVID-PL, mteb/NFCorpus-PL, mteb/NQ-PL, mteb/HotpotQA-PL, mteb/FiQA-PL, mteb/ArguAna-PL, mteb/Quora-PL, mteb/CQADupstack-Android-PL, mteb/DBPedia-PL, mteb/SciFact-PL, mteb/CQADupstack-English-PL, mteb/CQADupstack-Gaming-PL, mteb/CQADupstack-Gis-PL, mteb/CQADupstack-Mathematica-PL, mteb/CQADupstack-Physics-PL, mteb/CQADupstack-Programmers-PL, mteb/CQADupstack-Stats-PL, mteb/CQADupstack-Tex-PL, mteb/CQADupstack-Unix-PL, mteb/CQADupstack-Webmasters-PL, mteb/Touche2020-PL, mteb/LoTTE, mteb/MindSmallReranking, mteb/BIRCO-DorisMae-Test, mteb/BIRCO-Arguana-Test, mteb/BIRCO-ClinicalTrial-Test, mteb/BIRCO-WTB-Test, mteb/BIRCO-Relic-Test, mteb/NamaaMrTydiReranking, mteb/StackOverflowDupQuestions, mteb/WebLINXCandidatesReranking, mteb/AlloprofReranking, mteb/SyntecReranking, mteb/VoyageMMarcoReranking, mteb/ESCIReranking, mteb/WikipediaRerankingMultilingual, mteb/RuBQReranking, mteb/T2Reranking, mteb/MMarcoReranking, mteb/CMedQAv1-reranking, mteb/CMedQAv2-reranking, mteb/MIRACLReranking, mteb/BuiltBenchRetrieval, mteb/BuiltBenchReranking, GreenNode/stsbenchmark-sts-vn, GreenNode/biosses-sts-vn, GreenNode/sickr-sts-vn, GreenNode/arguana-vn, GreenNode/climate-fever-vn, GreenNode/cqadupstack-android-vn, GreenNode/cqadupstack-gaming-vn, GreenNode/cqadupstack-gis-vn, GreenNode/cqadupstack-mathematica-vn, GreenNode/cqadupstack-stats-vn, GreenNode/cqadupstack-tex-vn, GreenNode/cqadupstack-unix-vn, GreenNode/cqadupstack-webmasters-vn, GreenNode/cqadupstack-wordpress-vn, GreenNode/dbpedia-vn, GreenNode/fever-vn, GreenNode/scifact-vn, GreenNode/webis-touche2020-vn, GreenNode/trec-covid-vn, GreenNode/fiqa-vn, GreenNode/hotpotqa-vn, GreenNode/msmarco-vn, GreenNode/nfcorpus-vn, GreenNode/nq-vn, GreenNode/quora-vn, GreenNode/emotion-vn, GreenNode/banking77-vn, GreenNode/toxic-conversations-50k-vn, GreenNode/imdb-vn, GreenNode/tweet-sentiment-extraction-vn, GreenNode/amazon-counterfactual-vn, GreenNode/mtop-domain-vn, GreenNode/mtop-intent-vn, GreenNode/amazon-reviews-multi-vn, GreenNode/amazon-massive-intent-vn, GreenNode/amazon-massive-scenario-vn, GreenNode/amazon-polarity-vn, GreenNode/sprintduplicatequestions-pairclassification-vn, GreenNode/twittersemeval2015-pairclassification-vn, GreenNode/twitterurlcorpus-pairclassification-vn, GreenNode/cqadupstack-physics-vn, GreenNode/askubuntudupquestions-reranking-vn, GreenNode/stackoverflowdupquestions-reranking-vn, GreenNode/cqadupstack-programmers-vn, GreenNode/twentynewsgroups-clustering-vn, GreenNode/reddit-clustering-p2p-vn, GreenNode/stackexchange-clustering-p2p-vn, GreenNode/stackexchange-clustering-vn, GreenNode/reddit-clustering-vn, mteb/SentiRuEval2016, mteb/RuToxicOKMLCUPClassification, mteb/InappropriatenessClassificationv2, mteb/RuNLUIntentClassification, mteb/talemaader_pc, mteb/english-danish-parallel-corpus, mteb/VieQuADRetrieval, mteb/AFQMC, mteb/ATEC, mteb/AfriSentiClassification, mteb/AllegroReviews, mteb/AlloProfClusteringP2P, mteb/AlloProfClusteringS2S, mteb/AlloProfClusteringS2S.v2, mteb/AlloprofRetrieval, mteb/AngryTweetsClassification, mteb/ArguAna-Fa, mteb/ArguAna-NL, mteb/ArmenianParaphrasePC, mteb/BQ, mteb/BSARDRetrieval, mteb/BengaliSentimentAnalysis, mteb/BeytooteClustering, mteb/BlurbsClusteringP2P, mteb/BlurbsClusteringS2S, mteb/BornholmBitextMining, mteb/BrightLongRetrieval, mteb/BuiltBenchClusteringP2P, mteb/BuiltBenchClusteringS2S, mteb/BulgarianStoreReviewSentimentClassfication, mteb/CBD, mteb/CDSC-E, mteb/CDSC-R, mteb/CEDRClassification, mteb/CLSClusteringP2P, mteb/CLSClusteringP2P.v2, mteb/CLSClusteringS2S, mteb/CQADupstackGamingRetrieval-Fa, mteb/CQADupstackGisRetrieval-Fa, mteb/CQADupstackMathematicaRetrieval-Fa, mteb/CQADupstackPhysicsRetrieval-Fa, mteb/CQADupstackProgrammersRetrieval-Fa, mteb/CQADupstackStatsRetrieval-Fa, mteb/CQADupstackTexRetrieval-Fa, mteb/CQADupstackUnixRetrieval-Fa, mteb/CQADupstackWordpressRetrieval-Fa, mteb/CSFDSKMovieReviewSentimentClassification, mteb/CUREv1, mteb/CataloniaTweetClassification, mteb/ChemHotpotQARetrieval, mteb/ChemNQRetrieval, mteb/Cmnli",
    "datasets_links": "https://huggingface.co/datasets/mteb/sts22-crosslingual-sts, https://huggingface.co/datasets/mteb/sts12-sts, https://huggingface.co/datasets/mteb/sickr-sts, https://huggingface.co/datasets/mteb/sts13-sts, https://huggingface.co/datasets/mteb/stsbenchmark-sts, https://huggingface.co/datasets/mteb/arxiv-clustering-p2p, https://huggingface.co/datasets/mteb/amazon_counterfactual, https://huggingface.co/datasets/mteb/TurkicClassification, https://huggingface.co/datasets/mteb/reddit-clustering, https://huggingface.co/datasets/mteb/stackexchange-clustering, https://huggingface.co/datasets/mteb/twentynewsgroups-clustering, https://huggingface.co/datasets/mteb/twitterurlcorpus-pairclassification, https://huggingface.co/datasets/mteb/sprintduplicatequestions-pairclassification, https://huggingface.co/datasets/mteb/twittersemeval2015-pairclassification, https://huggingface.co/datasets/mteb/biosses-sts, https://huggingface.co/datasets/mteb/sts14-sts, https://huggingface.co/datasets/mteb/sts15-sts, https://huggingface.co/datasets/mteb/sts16-sts, https://huggingface.co/datasets/mteb/arxiv-clustering-s2s, https://huggingface.co/datasets/mteb/biorxiv-clustering-s2s, https://huggingface.co/datasets/mteb/biorxiv-clustering-p2p, https://huggingface.co/datasets/mteb/medrxiv-clustering-s2s, https://huggingface.co/datasets/mteb/medrxiv-clustering-p2p, https://huggingface.co/datasets/mteb/stackexchange-clustering-p2p, https://huggingface.co/datasets/mteb/reddit-clustering-p2p, https://huggingface.co/datasets/mteb/amazon_massive_scenario, https://huggingface.co/datasets/mteb/amazon_massive_intent, https://huggingface.co/datasets/mteb/banking77, https://huggingface.co/datasets/mteb/sts17-crosslingual-sts, https://huggingface.co/datasets/mteb/mtop_intent, https://huggingface.co/datasets/mteb/mtop_domain, https://huggingface.co/datasets/mteb/tatoeba-bitext-mining, https://huggingface.co/datasets/mteb/bucc-bitext-mining, https://huggingface.co/datasets/mteb/emotion, https://huggingface.co/datasets/mteb/amazon_polarity, https://huggingface.co/datasets/mteb/imdb, https://huggingface.co/datasets/mteb/toxic_conversations_50k, https://huggingface.co/datasets/mteb/tweet_sentiment_extraction, https://huggingface.co/datasets/mteb/summeval, https://huggingface.co/datasets/mteb/germanquad-retrieval, https://huggingface.co/datasets/mteb/scifact, https://huggingface.co/datasets/mteb/cqadupstack-android, https://huggingface.co/datasets/mteb/cqadupstack-english, https://huggingface.co/datasets/mteb/cqadupstack-gaming, https://huggingface.co/datasets/mteb/cqadupstack-gis, https://huggingface.co/datasets/mteb/cqadupstack-mathematica, https://huggingface.co/datasets/mteb/cqadupstack-physics, https://huggingface.co/datasets/mteb/cqadupstack-programmers, https://huggingface.co/datasets/mteb/cqadupstack-stats, https://huggingface.co/datasets/mteb/cqadupstack-tex, https://huggingface.co/datasets/mteb/cqadupstack-unix, https://huggingface.co/datasets/mteb/cqadupstack-webmasters, https://huggingface.co/datasets/mteb/cqadupstack-wordpress, https://huggingface.co/datasets/mteb/msmarco, https://huggingface.co/datasets/mteb/arguana, https://huggingface.co/datasets/mteb/dbpedia, https://huggingface.co/datasets/mteb/nq, https://huggingface.co/datasets/mteb/hotpotqa, https://huggingface.co/datasets/mteb/trec-covid, https://huggingface.co/datasets/mteb/fever, https://huggingface.co/datasets/mteb/fiqa, https://huggingface.co/datasets/mteb/climate-fever, https://huggingface.co/datasets/mteb/quora, https://huggingface.co/datasets/mteb/msmarco-v2, https://huggingface.co/datasets/mteb/nfcorpus, https://huggingface.co/datasets/mteb/legal_summarization, https://huggingface.co/datasets/mteb/legalbench_consumer_contracts_qa, https://huggingface.co/datasets/mteb/legalbench_corporate_lobbying, https://huggingface.co/datasets/mteb/AILA_statutes, https://huggingface.co/datasets/mteb/LeCaRDv2, https://huggingface.co/datasets/mteb/LegalQuAD, https://huggingface.co/datasets/mteb/GerDaLIRSmall, https://huggingface.co/datasets/mteb/norec_classification, https://huggingface.co/datasets/mteb/swerec_classification, https://huggingface.co/datasets/mteb/norquad_retrieval, https://huggingface.co/datasets/mteb/medical_qa, https://huggingface.co/datasets/mteb/neuclir-2022, https://huggingface.co/datasets/mteb/neuclir-2023, https://huggingface.co/datasets/mteb/multi-hatecheck, https://huggingface.co/datasets/mteb/eurlex-multilingual, https://huggingface.co/datasets/mteb/multilingual-sentiment-classification, https://huggingface.co/datasets/mteb/multilingual-scala-classification, https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb, https://huggingface.co/datasets/mteb/tweet_sentiment_multilingual, https://huggingface.co/datasets/mteb/IndicSentiment, https://huggingface.co/datasets/mteb/stsb_multi_mt, https://huggingface.co/datasets/mteb/xnli, https://huggingface.co/datasets/mteb/sib200, https://huggingface.co/datasets/mteb/masakhanews, https://huggingface.co/datasets/mteb/flores, https://huggingface.co/datasets/mteb/NTREX, https://huggingface.co/datasets/mteb/IN22-Conv, https://huggingface.co/datasets/mteb/IN22-Gen, https://huggingface.co/datasets/mteb/big-patent, https://huggingface.co/datasets/mteb/wit, https://huggingface.co/datasets/mteb/miracl-hard-negatives, https://huggingface.co/datasets/mteb/Quora_PL_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/QuoraRetrieval_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/DBPedia_PL_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/MSMARCO_PL_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/DBPedia_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/ClimateFEVER_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/NQ_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/NQ_PL_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/HotpotQA_PL_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/FEVER_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/HotpotQA_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/TopiOCQA_validation_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/neuclir-2023-hard-negatives, https://huggingface.co/datasets/mteb/MSMARCO_test_top_250_only_w_correct-v2, https://huggingface.co/datasets/mteb/neuclir-2022-hard-negatives, https://huggingface.co/datasets/mteb/jaqket, https://huggingface.co/datasets/mteb/mrtidy, https://huggingface.co/datasets/mteb/webis-touche2020-v3, https://huggingface.co/datasets/mteb/mlsum, https://huggingface.co/datasets/mteb/told-br, https://huggingface.co/datasets/mteb/InstructIR-mteb, https://huggingface.co/datasets/mteb/WikipediaRetrievalMultilingual, https://huggingface.co/datasets/mteb/T2Retrieval, https://huggingface.co/datasets/mteb/MMarcoRetrieval, https://huggingface.co/datasets/mteb/DuRetrieval, https://huggingface.co/datasets/mteb/CovidRetrieval, https://huggingface.co/datasets/mteb/CmedqaRetrieval, https://huggingface.co/datasets/mteb/EcomRetrieval, https://huggingface.co/datasets/mteb/MedicalRetrieval, https://huggingface.co/datasets/mteb/VideoRetrieval, https://huggingface.co/datasets/mteb/AskUbuntuDupQuestions, https://huggingface.co/datasets/mteb/IWSLT2017BitextMining, https://huggingface.co/datasets/mteb/AmazonReviewsClassification, https://huggingface.co/datasets/mteb/IndicReviewsClusteringP2P, https://huggingface.co/datasets/mteb/XNLIV2, https://huggingface.co/datasets/mteb/IndicCrosslingualSTS, https://huggingface.co/datasets/mteb/HotelReviewSentimentClassification, https://huggingface.co/datasets/mteb/TweetEmotionClassification, https://huggingface.co/datasets/mteb/TenKGnadClassification, https://huggingface.co/datasets/mteb/IndicQARetrieval, https://huggingface.co/datasets/mteb/ArxivClassification, https://huggingface.co/datasets/mteb/PatentClassification, https://huggingface.co/datasets/mteb/FilipinoHateSpeechClassification, https://huggingface.co/datasets/mteb/MyanmarNews, https://huggingface.co/datasets/mteb/DutchBookReviewSentimentClassification, https://huggingface.co/datasets/mteb/SwedishSentimentClassification, https://huggingface.co/datasets/mteb/WisesightSentimentClassification, https://huggingface.co/datasets/mteb/UrduRomanSentimentClassification, https://huggingface.co/datasets/mteb/JSTS, https://huggingface.co/datasets/mteb/climate-fever-v2, https://huggingface.co/datasets/mteb/CQADupstack-Wordpress-PL, https://huggingface.co/datasets/mteb/MSMARCO-PL, https://huggingface.co/datasets/mteb/TRECCOVID-PL, https://huggingface.co/datasets/mteb/NFCorpus-PL, https://huggingface.co/datasets/mteb/NQ-PL, https://huggingface.co/datasets/mteb/HotpotQA-PL, https://huggingface.co/datasets/mteb/FiQA-PL, https://huggingface.co/datasets/mteb/ArguAna-PL, https://huggingface.co/datasets/mteb/Quora-PL, https://huggingface.co/datasets/mteb/CQADupstack-Android-PL, https://huggingface.co/datasets/mteb/DBPedia-PL, https://huggingface.co/datasets/mteb/SciFact-PL, https://huggingface.co/datasets/mteb/CQADupstack-English-PL, https://huggingface.co/datasets/mteb/CQADupstack-Gaming-PL, https://huggingface.co/datasets/mteb/CQADupstack-Gis-PL, https://huggingface.co/datasets/mteb/CQADupstack-Mathematica-PL, https://huggingface.co/datasets/mteb/CQADupstack-Physics-PL, https://huggingface.co/datasets/mteb/CQADupstack-Programmers-PL, https://huggingface.co/datasets/mteb/CQADupstack-Stats-PL, https://huggingface.co/datasets/mteb/CQADupstack-Tex-PL, https://huggingface.co/datasets/mteb/CQADupstack-Unix-PL, https://huggingface.co/datasets/mteb/CQADupstack-Webmasters-PL, https://huggingface.co/datasets/mteb/Touche2020-PL, https://huggingface.co/datasets/mteb/LoTTE, https://huggingface.co/datasets/mteb/MindSmallReranking, https://huggingface.co/datasets/mteb/BIRCO-DorisMae-Test, https://huggingface.co/datasets/mteb/BIRCO-Arguana-Test, https://huggingface.co/datasets/mteb/BIRCO-ClinicalTrial-Test, https://huggingface.co/datasets/mteb/BIRCO-WTB-Test, https://huggingface.co/datasets/mteb/BIRCO-Relic-Test, https://huggingface.co/datasets/mteb/NamaaMrTydiReranking, https://huggingface.co/datasets/mteb/StackOverflowDupQuestions, https://huggingface.co/datasets/mteb/WebLINXCandidatesReranking, https://huggingface.co/datasets/mteb/AlloprofReranking, https://huggingface.co/datasets/mteb/SyntecReranking, https://huggingface.co/datasets/mteb/VoyageMMarcoReranking, https://huggingface.co/datasets/mteb/ESCIReranking, https://huggingface.co/datasets/mteb/WikipediaRerankingMultilingual, https://huggingface.co/datasets/mteb/RuBQReranking, https://huggingface.co/datasets/mteb/T2Reranking, https://huggingface.co/datasets/mteb/MMarcoReranking, https://huggingface.co/datasets/mteb/CMedQAv1-reranking, https://huggingface.co/datasets/mteb/CMedQAv2-reranking, https://huggingface.co/datasets/mteb/MIRACLReranking, https://huggingface.co/datasets/mteb/BuiltBenchRetrieval, https://huggingface.co/datasets/mteb/BuiltBenchReranking, https://huggingface.co/datasets/GreenNode/stsbenchmark-sts-vn, https://huggingface.co/datasets/GreenNode/biosses-sts-vn, https://huggingface.co/datasets/GreenNode/sickr-sts-vn, https://huggingface.co/datasets/GreenNode/arguana-vn, https://huggingface.co/datasets/GreenNode/climate-fever-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-android-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-gaming-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-gis-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-mathematica-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-stats-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-tex-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-unix-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-webmasters-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-wordpress-vn, https://huggingface.co/datasets/GreenNode/dbpedia-vn, https://huggingface.co/datasets/GreenNode/fever-vn, https://huggingface.co/datasets/GreenNode/scifact-vn, https://huggingface.co/datasets/GreenNode/webis-touche2020-vn, https://huggingface.co/datasets/GreenNode/trec-covid-vn, https://huggingface.co/datasets/GreenNode/fiqa-vn, https://huggingface.co/datasets/GreenNode/hotpotqa-vn, https://huggingface.co/datasets/GreenNode/msmarco-vn, https://huggingface.co/datasets/GreenNode/nfcorpus-vn, https://huggingface.co/datasets/GreenNode/nq-vn, https://huggingface.co/datasets/GreenNode/quora-vn, https://huggingface.co/datasets/GreenNode/emotion-vn, https://huggingface.co/datasets/GreenNode/banking77-vn, https://huggingface.co/datasets/GreenNode/toxic-conversations-50k-vn, https://huggingface.co/datasets/GreenNode/imdb-vn, https://huggingface.co/datasets/GreenNode/tweet-sentiment-extraction-vn, https://huggingface.co/datasets/GreenNode/amazon-counterfactual-vn, https://huggingface.co/datasets/GreenNode/mtop-domain-vn, https://huggingface.co/datasets/GreenNode/mtop-intent-vn, https://huggingface.co/datasets/GreenNode/amazon-reviews-multi-vn, https://huggingface.co/datasets/GreenNode/amazon-massive-intent-vn, https://huggingface.co/datasets/GreenNode/amazon-massive-scenario-vn, https://huggingface.co/datasets/GreenNode/amazon-polarity-vn, https://huggingface.co/datasets/GreenNode/sprintduplicatequestions-pairclassification-vn, https://huggingface.co/datasets/GreenNode/twittersemeval2015-pairclassification-vn, https://huggingface.co/datasets/GreenNode/twitterurlcorpus-pairclassification-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-physics-vn, https://huggingface.co/datasets/GreenNode/askubuntudupquestions-reranking-vn, https://huggingface.co/datasets/GreenNode/stackoverflowdupquestions-reranking-vn, https://huggingface.co/datasets/GreenNode/cqadupstack-programmers-vn, https://huggingface.co/datasets/GreenNode/twentynewsgroups-clustering-vn, https://huggingface.co/datasets/GreenNode/reddit-clustering-p2p-vn, https://huggingface.co/datasets/GreenNode/stackexchange-clustering-p2p-vn, https://huggingface.co/datasets/GreenNode/stackexchange-clustering-vn, https://huggingface.co/datasets/GreenNode/reddit-clustering-vn, https://huggingface.co/datasets/mteb/SentiRuEval2016, https://huggingface.co/datasets/mteb/RuToxicOKMLCUPClassification, https://huggingface.co/datasets/mteb/InappropriatenessClassificationv2, https://huggingface.co/datasets/mteb/RuNLUIntentClassification, https://huggingface.co/datasets/mteb/talemaader_pc, https://huggingface.co/datasets/mteb/english-danish-parallel-corpus, https://huggingface.co/datasets/mteb/VieQuADRetrieval, https://huggingface.co/datasets/mteb/AFQMC, https://huggingface.co/datasets/mteb/ATEC, https://huggingface.co/datasets/mteb/AfriSentiClassification, https://huggingface.co/datasets/mteb/AllegroReviews, https://huggingface.co/datasets/mteb/AlloProfClusteringP2P, https://huggingface.co/datasets/mteb/AlloProfClusteringS2S, https://huggingface.co/datasets/mteb/AlloProfClusteringS2S.v2, https://huggingface.co/datasets/mteb/AlloprofRetrieval, https://huggingface.co/datasets/mteb/AngryTweetsClassification, https://huggingface.co/datasets/mteb/ArguAna-Fa, https://huggingface.co/datasets/mteb/ArguAna-NL, https://huggingface.co/datasets/mteb/ArmenianParaphrasePC, https://huggingface.co/datasets/mteb/BQ, https://huggingface.co/datasets/mteb/BSARDRetrieval, https://huggingface.co/datasets/mteb/BengaliSentimentAnalysis, https://huggingface.co/datasets/mteb/BeytooteClustering, https://huggingface.co/datasets/mteb/BlurbsClusteringP2P, https://huggingface.co/datasets/mteb/BlurbsClusteringS2S, https://huggingface.co/datasets/mteb/BornholmBitextMining, https://huggingface.co/datasets/mteb/BrightLongRetrieval, https://huggingface.co/datasets/mteb/BuiltBenchClusteringP2P, https://huggingface.co/datasets/mteb/BuiltBenchClusteringS2S, https://huggingface.co/datasets/mteb/BulgarianStoreReviewSentimentClassfication, https://huggingface.co/datasets/mteb/CBD, https://huggingface.co/datasets/mteb/CDSC-E, https://huggingface.co/datasets/mteb/CDSC-R, https://huggingface.co/datasets/mteb/CEDRClassification, https://huggingface.co/datasets/mteb/CLSClusteringP2P, https://huggingface.co/datasets/mteb/CLSClusteringP2P.v2, https://huggingface.co/datasets/mteb/CLSClusteringS2S, https://huggingface.co/datasets/mteb/CQADupstackGamingRetrieval-Fa, https://huggingface.co/datasets/mteb/CQADupstackGisRetrieval-Fa, https://huggingface.co/datasets/mteb/CQADupstackMathematicaRetrieval-Fa, https://huggingface.co/datasets/mteb/CQADupstackPhysicsRetrieval-Fa, https://huggingface.co/datasets/mteb/CQADupstackProgrammersRetrieval-Fa, https://huggingface.co/datasets/mteb/CQADupstackStatsRetrieval-Fa, https://huggingface.co/datasets/mteb/CQADupstackTexRetrieval-Fa, https://huggingface.co/datasets/mteb/CQADupstackUnixRetrieval-Fa, https://huggingface.co/datasets/mteb/CQADupstackWordpressRetrieval-Fa, https://huggingface.co/datasets/mteb/CSFDSKMovieReviewSentimentClassification, https://huggingface.co/datasets/mteb/CUREv1, https://huggingface.co/datasets/mteb/CataloniaTweetClassification, https://huggingface.co/datasets/mteb/ChemHotpotQARetrieval, https://huggingface.co/datasets/mteb/ChemNQRetrieval, https://huggingface.co/datasets/mteb/Cmnli",
    "datasets_detailed": "[{\"name\": \"mteb/sts22-crosslingual-sts\", \"link\": \"https://huggingface.co/datasets/mteb/sts22-crosslingual-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/sts12-sts\", \"link\": \"https://huggingface.co/datasets/mteb/sts12-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/sickr-sts\", \"link\": \"https://huggingface.co/datasets/mteb/sickr-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/sts13-sts\", \"link\": \"https://huggingface.co/datasets/mteb/sts13-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/stsbenchmark-sts\", \"link\": \"https://huggingface.co/datasets/mteb/stsbenchmark-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/arxiv-clustering-p2p\", \"link\": \"https://huggingface.co/datasets/mteb/arxiv-clustering-p2p\", \"task\": \"\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/amazon_counterfactual\", \"link\": \"https://huggingface.co/datasets/mteb/amazon_counterfactual\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\", \"size\": \"\"}, {\"name\": \"mteb/TurkicClassification\", \"link\": \"https://huggingface.co/datasets/mteb/TurkicClassification\", \"task\": \"\", \"likes\": \"48\", \"downloads\": \"\", \"updated\": \"Jun 20\", \"size\": \"\"}, {\"name\": \"mteb/reddit-clustering\", \"link\": \"https://huggingface.co/datasets/mteb/reddit-clustering\", \"task\": \"\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/stackexchange-clustering\", \"link\": \"https://huggingface.co/datasets/mteb/stackexchange-clustering\", \"task\": \"\", \"likes\": \"50\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/twentynewsgroups-clustering\", \"link\": \"https://huggingface.co/datasets/mteb/twentynewsgroups-clustering\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/twitterurlcorpus-pairclassification\", \"link\": \"https://huggingface.co/datasets/mteb/twitterurlcorpus-pairclassification\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/sprintduplicatequestions-pairclassification\", \"link\": \"https://huggingface.co/datasets/mteb/sprintduplicatequestions-pairclassification\", \"task\": \"\", \"likes\": \"2\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/twittersemeval2015-pairclassification\", \"link\": \"https://huggingface.co/datasets/mteb/twittersemeval2015-pairclassification\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/biosses-sts\", \"link\": \"https://huggingface.co/datasets/mteb/biosses-sts\", \"task\": \"\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/sts14-sts\", \"link\": \"https://huggingface.co/datasets/mteb/sts14-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/sts15-sts\", \"link\": \"https://huggingface.co/datasets/mteb/sts15-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/sts16-sts\", \"link\": \"https://huggingface.co/datasets/mteb/sts16-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/arxiv-clustering-s2s\", \"link\": \"https://huggingface.co/datasets/mteb/arxiv-clustering-s2s\", \"task\": \"\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/biorxiv-clustering-s2s\", \"link\": \"https://huggingface.co/datasets/mteb/biorxiv-clustering-s2s\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/biorxiv-clustering-p2p\", \"link\": \"https://huggingface.co/datasets/mteb/biorxiv-clustering-p2p\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/medrxiv-clustering-s2s\", \"link\": \"https://huggingface.co/datasets/mteb/medrxiv-clustering-s2s\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/medrxiv-clustering-p2p\", \"link\": \"https://huggingface.co/datasets/mteb/medrxiv-clustering-p2p\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/stackexchange-clustering-p2p\", \"link\": \"https://huggingface.co/datasets/mteb/stackexchange-clustering-p2p\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/reddit-clustering-p2p\", \"link\": \"https://huggingface.co/datasets/mteb/reddit-clustering-p2p\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/amazon_massive_scenario\", \"link\": \"https://huggingface.co/datasets/mteb/amazon_massive_scenario\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/amazon_massive_intent\", \"link\": \"https://huggingface.co/datasets/mteb/amazon_massive_intent\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/banking77\", \"link\": \"https://huggingface.co/datasets/mteb/banking77\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\", \"size\": \"\"}, {\"name\": \"mteb/sts17-crosslingual-sts\", \"link\": \"https://huggingface.co/datasets/mteb/sts17-crosslingual-sts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/mtop_intent\", \"link\": \"https://huggingface.co/datasets/mteb/mtop_intent\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\", \"size\": \"\"}, {\"name\": \"mteb/mtop_domain\", \"link\": \"https://huggingface.co/datasets/mteb/mtop_domain\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 19\", \"size\": \"\"}, {\"name\": \"mteb/tatoeba-bitext-mining\", \"link\": \"https://huggingface.co/datasets/mteb/tatoeba-bitext-mining\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/bucc-bitext-mining\", \"link\": \"https://huggingface.co/datasets/mteb/bucc-bitext-mining\", \"task\": \"\", \"likes\": \"470\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/emotion\", \"link\": \"https://huggingface.co/datasets/mteb/emotion\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\", \"size\": \"\"}, {\"name\": \"mteb/amazon_polarity\", \"link\": \"https://huggingface.co/datasets/mteb/amazon_polarity\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\", \"size\": \"\"}, {\"name\": \"mteb/imdb\", \"link\": \"https://huggingface.co/datasets/mteb/imdb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\", \"size\": \"\"}, {\"name\": \"mteb/toxic_conversations_50k\", \"link\": \"https://huggingface.co/datasets/mteb/toxic_conversations_50k\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/tweet_sentiment_extraction\", \"link\": \"https://huggingface.co/datasets/mteb/tweet_sentiment_extraction\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 6\", \"size\": \"\"}, {\"name\": \"mteb/summeval\", \"link\": \"https://huggingface.co/datasets/mteb/summeval\", \"task\": \"\", \"likes\": \"100\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/germanquad-retrieval\", \"link\": \"https://huggingface.co/datasets/mteb/germanquad-retrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/scifact\", \"link\": \"https://huggingface.co/datasets/mteb/scifact\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-android\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-android\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-english\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-english\", \"task\": \"\", \"likes\": \"781\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-gaming\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-gaming\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-gis\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-gis\", \"task\": \"\", \"likes\": \"679\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-mathematica\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-mathematica\", \"task\": \"\", \"likes\": \"654\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-physics\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-physics\", \"task\": \"\", \"likes\": \"687\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-programmers\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-programmers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-stats\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-stats\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-tex\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-tex\", \"task\": \"\", \"likes\": \"683\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-unix\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-unix\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-webmasters\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-webmasters\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/cqadupstack-wordpress\", \"link\": \"https://huggingface.co/datasets/mteb/cqadupstack-wordpress\", \"task\": \"\", \"likes\": \"665\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/msmarco\", \"link\": \"https://huggingface.co/datasets/mteb/msmarco\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/arguana\", \"link\": \"https://huggingface.co/datasets/mteb/arguana\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/dbpedia\", \"link\": \"https://huggingface.co/datasets/mteb/dbpedia\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/nq\", \"link\": \"https://huggingface.co/datasets/mteb/nq\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/hotpotqa\", \"link\": \"https://huggingface.co/datasets/mteb/hotpotqa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/trec-covid\", \"link\": \"https://huggingface.co/datasets/mteb/trec-covid\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/fever\", \"link\": \"https://huggingface.co/datasets/mteb/fever\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/fiqa\", \"link\": \"https://huggingface.co/datasets/mteb/fiqa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/climate-fever\", \"link\": \"https://huggingface.co/datasets/mteb/climate-fever\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 31\", \"size\": \"\"}, {\"name\": \"mteb/quora\", \"link\": \"https://huggingface.co/datasets/mteb/quora\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/msmarco-v2\", \"link\": \"https://huggingface.co/datasets/mteb/msmarco-v2\", \"task\": \"\", \"likes\": \"549\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/nfcorpus\", \"link\": \"https://huggingface.co/datasets/mteb/nfcorpus\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/legal_summarization\", \"link\": \"https://huggingface.co/datasets/mteb/legal_summarization\", \"task\": \"\", \"likes\": \"457\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/legalbench_consumer_contracts_qa\", \"link\": \"https://huggingface.co/datasets/mteb/legalbench_consumer_contracts_qa\", \"task\": \"\", \"likes\": \"946\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/legalbench_corporate_lobbying\", \"link\": \"https://huggingface.co/datasets/mteb/legalbench_corporate_lobbying\", \"task\": \"\", \"likes\": \"999\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/AILA_statutes\", \"link\": \"https://huggingface.co/datasets/mteb/AILA_statutes\", \"task\": \"\", \"likes\": \"349\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/LeCaRDv2\", \"link\": \"https://huggingface.co/datasets/mteb/LeCaRDv2\", \"task\": \"\", \"likes\": \"541\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/LegalQuAD\", \"link\": \"https://huggingface.co/datasets/mteb/LegalQuAD\", \"task\": \"\", \"likes\": \"600\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/GerDaLIRSmall\", \"link\": \"https://huggingface.co/datasets/mteb/GerDaLIRSmall\", \"task\": \"\", \"likes\": \"139\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/norec_classification\", \"link\": \"https://huggingface.co/datasets/mteb/norec_classification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/swerec_classification\", \"link\": \"https://huggingface.co/datasets/mteb/swerec_classification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/norquad_retrieval\", \"link\": \"https://huggingface.co/datasets/mteb/norquad_retrieval\", \"task\": \"\", \"likes\": \"194\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/medical_qa\", \"link\": \"https://huggingface.co/datasets/mteb/medical_qa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/neuclir-2022\", \"link\": \"https://huggingface.co/datasets/mteb/neuclir-2022\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/neuclir-2023\", \"link\": \"https://huggingface.co/datasets/mteb/neuclir-2023\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/multi-hatecheck\", \"link\": \"https://huggingface.co/datasets/mteb/multi-hatecheck\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/eurlex-multilingual\", \"link\": \"https://huggingface.co/datasets/mteb/eurlex-multilingual\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/multilingual-sentiment-classification\", \"link\": \"https://huggingface.co/datasets/mteb/multilingual-sentiment-classification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/multilingual-scala-classification\", \"link\": \"https://huggingface.co/datasets/mteb/multilingual-scala-classification\", \"task\": \"\", \"likes\": \"423\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/biblenlp-corpus-mmteb\", \"link\": \"https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\", \"size\": \"\"}, {\"name\": \"mteb/tweet_sentiment_multilingual\", \"link\": \"https://huggingface.co/datasets/mteb/tweet_sentiment_multilingual\", \"task\": \"\", \"likes\": \"480\", \"downloads\": \"\", \"updated\": \"Jul 23\", \"size\": \"\"}, {\"name\": \"mteb/IndicSentiment\", \"link\": \"https://huggingface.co/datasets/mteb/IndicSentiment\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 20\", \"size\": \"\"}, {\"name\": \"mteb/stsb_multi_mt\", \"link\": \"https://huggingface.co/datasets/mteb/stsb_multi_mt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/xnli\", \"link\": \"https://huggingface.co/datasets/mteb/xnli\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/sib200\", \"link\": \"https://huggingface.co/datasets/mteb/sib200\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 20\", \"size\": \"\"}, {\"name\": \"mteb/masakhanews\", \"link\": \"https://huggingface.co/datasets/mteb/masakhanews\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/flores\", \"link\": \"https://huggingface.co/datasets/mteb/flores\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/NTREX\", \"link\": \"https://huggingface.co/datasets/mteb/NTREX\", \"task\": \"\", \"likes\": \"247\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/IN22-Conv\", \"link\": \"https://huggingface.co/datasets/mteb/IN22-Conv\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/IN22-Gen\", \"link\": \"https://huggingface.co/datasets/mteb/IN22-Gen\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/big-patent\", \"link\": \"https://huggingface.co/datasets/mteb/big-patent\", \"task\": \"\", \"likes\": \"238\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/wit\", \"link\": \"https://huggingface.co/datasets/mteb/wit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/miracl-hard-negatives\", \"link\": \"https://huggingface.co/datasets/mteb/miracl-hard-negatives\", \"task\": \"\", \"likes\": \"583\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/Quora_PL_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/Quora_PL_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/QuoraRetrieval_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/QuoraRetrieval_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/DBPedia_PL_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/DBPedia_PL_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/MSMARCO_PL_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/MSMARCO_PL_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/DBPedia_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/DBPedia_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/ClimateFEVER_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/ClimateFEVER_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/NQ_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/NQ_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/NQ_PL_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/NQ_PL_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/HotpotQA_PL_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/HotpotQA_PL_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/FEVER_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/FEVER_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/HotpotQA_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/HotpotQA_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/TopiOCQA_validation_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/TopiOCQA_validation_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/neuclir-2023-hard-negatives\", \"link\": \"https://huggingface.co/datasets/mteb/neuclir-2023-hard-negatives\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/MSMARCO_test_top_250_only_w_correct-v2\", \"link\": \"https://huggingface.co/datasets/mteb/MSMARCO_test_top_250_only_w_correct-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/neuclir-2022-hard-negatives\", \"link\": \"https://huggingface.co/datasets/mteb/neuclir-2022-hard-negatives\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/jaqket\", \"link\": \"https://huggingface.co/datasets/mteb/jaqket\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/mrtidy\", \"link\": \"https://huggingface.co/datasets/mteb/mrtidy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/webis-touche2020-v3\", \"link\": \"https://huggingface.co/datasets/mteb/webis-touche2020-v3\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/mlsum\", \"link\": \"https://huggingface.co/datasets/mteb/mlsum\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/told-br\", \"link\": \"https://huggingface.co/datasets/mteb/told-br\", \"task\": \"\", \"likes\": \"211\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/InstructIR-mteb\", \"link\": \"https://huggingface.co/datasets/mteb/InstructIR-mteb\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/WikipediaRetrievalMultilingual\", \"link\": \"https://huggingface.co/datasets/mteb/WikipediaRetrievalMultilingual\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/T2Retrieval\", \"link\": \"https://huggingface.co/datasets/mteb/T2Retrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/MMarcoRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/MMarcoRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/DuRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/DuRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CovidRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/CovidRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/CmedqaRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/CmedqaRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/EcomRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/EcomRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/MedicalRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/MedicalRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/VideoRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/VideoRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/AskUbuntuDupQuestions\", \"link\": \"https://huggingface.co/datasets/mteb/AskUbuntuDupQuestions\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/IWSLT2017BitextMining\", \"link\": \"https://huggingface.co/datasets/mteb/IWSLT2017BitextMining\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/AmazonReviewsClassification\", \"link\": \"https://huggingface.co/datasets/mteb/AmazonReviewsClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/IndicReviewsClusteringP2P\", \"link\": \"https://huggingface.co/datasets/mteb/IndicReviewsClusteringP2P\", \"task\": \"\", \"likes\": \"65\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/XNLIV2\", \"link\": \"https://huggingface.co/datasets/mteb/XNLIV2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/IndicCrosslingualSTS\", \"link\": \"https://huggingface.co/datasets/mteb/IndicCrosslingualSTS\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/HotelReviewSentimentClassification\", \"link\": \"https://huggingface.co/datasets/mteb/HotelReviewSentimentClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\", \"size\": \"\"}, {\"name\": \"mteb/TweetEmotionClassification\", \"link\": \"https://huggingface.co/datasets/mteb/TweetEmotionClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 21\", \"size\": \"\"}, {\"name\": \"mteb/TenKGnadClassification\", \"link\": \"https://huggingface.co/datasets/mteb/TenKGnadClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/IndicQARetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/IndicQARetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/ArxivClassification\", \"link\": \"https://huggingface.co/datasets/mteb/ArxivClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/PatentClassification\", \"link\": \"https://huggingface.co/datasets/mteb/PatentClassification\", \"task\": \"\", \"likes\": \"61\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/FilipinoHateSpeechClassification\", \"link\": \"https://huggingface.co/datasets/mteb/FilipinoHateSpeechClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/MyanmarNews\", \"link\": \"https://huggingface.co/datasets/mteb/MyanmarNews\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/DutchBookReviewSentimentClassification\", \"link\": \"https://huggingface.co/datasets/mteb/DutchBookReviewSentimentClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/SwedishSentimentClassification\", \"link\": \"https://huggingface.co/datasets/mteb/SwedishSentimentClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/WisesightSentimentClassification\", \"link\": \"https://huggingface.co/datasets/mteb/WisesightSentimentClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/UrduRomanSentimentClassification\", \"link\": \"https://huggingface.co/datasets/mteb/UrduRomanSentimentClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/JSTS\", \"link\": \"https://huggingface.co/datasets/mteb/JSTS\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/climate-fever-v2\", \"link\": \"https://huggingface.co/datasets/mteb/climate-fever-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Wordpress-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Wordpress-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/MSMARCO-PL\", \"link\": \"https://huggingface.co/datasets/mteb/MSMARCO-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\", \"size\": \"\"}, {\"name\": \"mteb/TRECCOVID-PL\", \"link\": \"https://huggingface.co/datasets/mteb/TRECCOVID-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/NFCorpus-PL\", \"link\": \"https://huggingface.co/datasets/mteb/NFCorpus-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/NQ-PL\", \"link\": \"https://huggingface.co/datasets/mteb/NQ-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/HotpotQA-PL\", \"link\": \"https://huggingface.co/datasets/mteb/HotpotQA-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/FiQA-PL\", \"link\": \"https://huggingface.co/datasets/mteb/FiQA-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/ArguAna-PL\", \"link\": \"https://huggingface.co/datasets/mteb/ArguAna-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/Quora-PL\", \"link\": \"https://huggingface.co/datasets/mteb/Quora-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Android-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Android-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/DBPedia-PL\", \"link\": \"https://huggingface.co/datasets/mteb/DBPedia-PL\", \"task\": \"\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/SciFact-PL\", \"link\": \"https://huggingface.co/datasets/mteb/SciFact-PL\", \"task\": \"\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-English-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-English-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Gaming-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Gaming-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Gis-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Gis-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Mathematica-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Mathematica-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Physics-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Physics-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Programmers-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Programmers-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Stats-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Stats-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Tex-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Tex-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Unix-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Unix-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstack-Webmasters-PL\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstack-Webmasters-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/Touche2020-PL\", \"link\": \"https://huggingface.co/datasets/mteb/Touche2020-PL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/LoTTE\", \"link\": \"https://huggingface.co/datasets/mteb/LoTTE\", \"task\": \"\", \"likes\": \"227\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/MindSmallReranking\", \"link\": \"https://huggingface.co/datasets/mteb/MindSmallReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\", \"size\": \"\"}, {\"name\": \"mteb/BIRCO-DorisMae-Test\", \"link\": \"https://huggingface.co/datasets/mteb/BIRCO-DorisMae-Test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/BIRCO-Arguana-Test\", \"link\": \"https://huggingface.co/datasets/mteb/BIRCO-Arguana-Test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/BIRCO-ClinicalTrial-Test\", \"link\": \"https://huggingface.co/datasets/mteb/BIRCO-ClinicalTrial-Test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/BIRCO-WTB-Test\", \"link\": \"https://huggingface.co/datasets/mteb/BIRCO-WTB-Test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/BIRCO-Relic-Test\", \"link\": \"https://huggingface.co/datasets/mteb/BIRCO-Relic-Test\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/NamaaMrTydiReranking\", \"link\": \"https://huggingface.co/datasets/mteb/NamaaMrTydiReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/StackOverflowDupQuestions\", \"link\": \"https://huggingface.co/datasets/mteb/StackOverflowDupQuestions\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/WebLINXCandidatesReranking\", \"link\": \"https://huggingface.co/datasets/mteb/WebLINXCandidatesReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/AlloprofReranking\", \"link\": \"https://huggingface.co/datasets/mteb/AlloprofReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/SyntecReranking\", \"link\": \"https://huggingface.co/datasets/mteb/SyntecReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/VoyageMMarcoReranking\", \"link\": \"https://huggingface.co/datasets/mteb/VoyageMMarcoReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/ESCIReranking\", \"link\": \"https://huggingface.co/datasets/mteb/ESCIReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/WikipediaRerankingMultilingual\", \"link\": \"https://huggingface.co/datasets/mteb/WikipediaRerankingMultilingual\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/RuBQReranking\", \"link\": \"https://huggingface.co/datasets/mteb/RuBQReranking\", \"task\": \"\", \"likes\": \"227\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/T2Reranking\", \"link\": \"https://huggingface.co/datasets/mteb/T2Reranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 23\", \"size\": \"\"}, {\"name\": \"mteb/MMarcoReranking\", \"link\": \"https://huggingface.co/datasets/mteb/MMarcoReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/CMedQAv1-reranking\", \"link\": \"https://huggingface.co/datasets/mteb/CMedQAv1-reranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"mteb/CMedQAv2-reranking\", \"link\": \"https://huggingface.co/datasets/mteb/CMedQAv2-reranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/MIRACLReranking\", \"link\": \"https://huggingface.co/datasets/mteb/MIRACLReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/BuiltBenchRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/BuiltBenchRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/BuiltBenchReranking\", \"link\": \"https://huggingface.co/datasets/mteb/BuiltBenchReranking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 3\", \"size\": \"\"}, {\"name\": \"GreenNode/stsbenchmark-sts-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/stsbenchmark-sts-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/biosses-sts-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/biosses-sts-vn\", \"task\": \"\", \"likes\": \"200\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/sickr-sts-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/sickr-sts-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/arguana-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/arguana-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/climate-fever-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/climate-fever-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-android-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-android-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-gaming-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-gaming-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-gis-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-gis-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-mathematica-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-mathematica-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-stats-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-stats-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-tex-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-tex-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-unix-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-unix-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-webmasters-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-webmasters-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-wordpress-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-wordpress-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/dbpedia-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/dbpedia-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/fever-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/fever-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/scifact-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/scifact-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/webis-touche2020-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/webis-touche2020-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/trec-covid-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/trec-covid-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/fiqa-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/fiqa-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/hotpotqa-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/hotpotqa-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/msmarco-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/msmarco-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/nfcorpus-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/nfcorpus-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/nq-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/nq-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/quora-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/quora-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/emotion-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/emotion-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/banking77-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/banking77-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/toxic-conversations-50k-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/toxic-conversations-50k-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/imdb-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/imdb-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/tweet-sentiment-extraction-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/tweet-sentiment-extraction-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/amazon-counterfactual-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/amazon-counterfactual-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/mtop-domain-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/mtop-domain-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/mtop-intent-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/mtop-intent-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/amazon-reviews-multi-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/amazon-reviews-multi-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/amazon-massive-intent-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/amazon-massive-intent-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/amazon-massive-scenario-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/amazon-massive-scenario-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/amazon-polarity-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/amazon-polarity-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/sprintduplicatequestions-pairclassification-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/sprintduplicatequestions-pairclassification-vn\", \"task\": \"\", \"likes\": \"2\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/twittersemeval2015-pairclassification-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/twittersemeval2015-pairclassification-vn\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/twitterurlcorpus-pairclassification-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/twitterurlcorpus-pairclassification-vn\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-physics-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-physics-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/askubuntudupquestions-reranking-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/askubuntudupquestions-reranking-vn\", \"task\": \"\", \"likes\": \"305\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/stackoverflowdupquestions-reranking-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/stackoverflowdupquestions-reranking-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/cqadupstack-programmers-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/cqadupstack-programmers-vn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/twentynewsgroups-clustering-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/twentynewsgroups-clustering-vn\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/reddit-clustering-p2p-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/reddit-clustering-p2p-vn\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/stackexchange-clustering-p2p-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/stackexchange-clustering-p2p-vn\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/stackexchange-clustering-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/stackexchange-clustering-vn\", \"task\": \"\", \"likes\": \"4\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"GreenNode/reddit-clustering-vn\", \"link\": \"https://huggingface.co/datasets/GreenNode/reddit-clustering-vn\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Jul 31\", \"size\": \"\"}, {\"name\": \"mteb/SentiRuEval2016\", \"link\": \"https://huggingface.co/datasets/mteb/SentiRuEval2016\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/RuToxicOKMLCUPClassification\", \"link\": \"https://huggingface.co/datasets/mteb/RuToxicOKMLCUPClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/InappropriatenessClassificationv2\", \"link\": \"https://huggingface.co/datasets/mteb/InappropriatenessClassificationv2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/RuNLUIntentClassification\", \"link\": \"https://huggingface.co/datasets/mteb/RuNLUIntentClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/talemaader_pc\", \"link\": \"https://huggingface.co/datasets/mteb/talemaader_pc\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/english-danish-parallel-corpus\", \"link\": \"https://huggingface.co/datasets/mteb/english-danish-parallel-corpus\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 4\", \"size\": \"\"}, {\"name\": \"mteb/VieQuADRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/VieQuADRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 2\", \"size\": \"\"}, {\"name\": \"mteb/AFQMC\", \"link\": \"https://huggingface.co/datasets/mteb/AFQMC\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"mteb/ATEC\", \"link\": \"https://huggingface.co/datasets/mteb/ATEC\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\", \"size\": \"\"}, {\"name\": \"mteb/AfriSentiClassification\", \"link\": \"https://huggingface.co/datasets/mteb/AfriSentiClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/AllegroReviews\", \"link\": \"https://huggingface.co/datasets/mteb/AllegroReviews\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/AlloProfClusteringP2P\", \"link\": \"https://huggingface.co/datasets/mteb/AlloProfClusteringP2P\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/AlloProfClusteringS2S\", \"link\": \"https://huggingface.co/datasets/mteb/AlloProfClusteringS2S\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/AlloProfClusteringS2S.v2\", \"link\": \"https://huggingface.co/datasets/mteb/AlloProfClusteringS2S.v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/AlloprofRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/AlloprofRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/AngryTweetsClassification\", \"link\": \"https://huggingface.co/datasets/mteb/AngryTweetsClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/ArguAna-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/ArguAna-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/ArguAna-NL\", \"link\": \"https://huggingface.co/datasets/mteb/ArguAna-NL\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/ArmenianParaphrasePC\", \"link\": \"https://huggingface.co/datasets/mteb/ArmenianParaphrasePC\", \"task\": \"\", \"likes\": \"2\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BQ\", \"link\": \"https://huggingface.co/datasets/mteb/BQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BSARDRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/BSARDRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BengaliSentimentAnalysis\", \"link\": \"https://huggingface.co/datasets/mteb/BengaliSentimentAnalysis\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BeytooteClustering\", \"link\": \"https://huggingface.co/datasets/mteb/BeytooteClustering\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BlurbsClusteringP2P\", \"link\": \"https://huggingface.co/datasets/mteb/BlurbsClusteringP2P\", \"task\": \"\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BlurbsClusteringS2S\", \"link\": \"https://huggingface.co/datasets/mteb/BlurbsClusteringS2S\", \"task\": \"\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BornholmBitextMining\", \"link\": \"https://huggingface.co/datasets/mteb/BornholmBitextMining\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 10\", \"size\": \"\"}, {\"name\": \"mteb/BrightLongRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/BrightLongRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\", \"size\": \"\"}, {\"name\": \"mteb/BuiltBenchClusteringP2P\", \"link\": \"https://huggingface.co/datasets/mteb/BuiltBenchClusteringP2P\", \"task\": \"\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BuiltBenchClusteringS2S\", \"link\": \"https://huggingface.co/datasets/mteb/BuiltBenchClusteringS2S\", \"task\": \"\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/BulgarianStoreReviewSentimentClassfication\", \"link\": \"https://huggingface.co/datasets/mteb/BulgarianStoreReviewSentimentClassfication\", \"task\": \"\", \"likes\": \"906\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CBD\", \"link\": \"https://huggingface.co/datasets/mteb/CBD\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 5\", \"size\": \"\"}, {\"name\": \"mteb/CDSC-E\", \"link\": \"https://huggingface.co/datasets/mteb/CDSC-E\", \"task\": \"\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CDSC-R\", \"link\": \"https://huggingface.co/datasets/mteb/CDSC-R\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CEDRClassification\", \"link\": \"https://huggingface.co/datasets/mteb/CEDRClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CLSClusteringP2P\", \"link\": \"https://huggingface.co/datasets/mteb/CLSClusteringP2P\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CLSClusteringP2P.v2\", \"link\": \"https://huggingface.co/datasets/mteb/CLSClusteringP2P.v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CLSClusteringS2S\", \"link\": \"https://huggingface.co/datasets/mteb/CLSClusteringS2S\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackGamingRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackGamingRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackGisRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackGisRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackMathematicaRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackMathematicaRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackPhysicsRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackPhysicsRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackProgrammersRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackProgrammersRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackStatsRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackStatsRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackTexRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackTexRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackUnixRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackUnixRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CQADupstackWordpressRetrieval-Fa\", \"link\": \"https://huggingface.co/datasets/mteb/CQADupstackWordpressRetrieval-Fa\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CSFDSKMovieReviewSentimentClassification\", \"link\": \"https://huggingface.co/datasets/mteb/CSFDSKMovieReviewSentimentClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CUREv1\", \"link\": \"https://huggingface.co/datasets/mteb/CUREv1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/CataloniaTweetClassification\", \"link\": \"https://huggingface.co/datasets/mteb/CataloniaTweetClassification\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/ChemHotpotQARetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/ChemHotpotQARetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/ChemNQRetrieval\", \"link\": \"https://huggingface.co/datasets/mteb/ChemNQRetrieval\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}, {\"name\": \"mteb/Cmnli\", \"link\": \"https://huggingface.co/datasets/mteb/Cmnli\", \"task\": \"\", \"likes\": \"1\", \"downloads\": \"\", \"updated\": \"May 6\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.13917",
    "first_seen_date": "2025-02-20",
    "title": "TESS 2: A Large-Scale Generalist Diffusion Language Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.13917TESS 2: A Large-Scale Generalist Diffusion Language ModelPublished on Feb 19\u00b7Submitted byHamish Ivisonon Feb 20Upvote6Authors:Jaesung Tae,Hamish Ivison,Sachin Kumar,Arman CohanAbstractTESS 2, an instruction-following diffusion language model, surpasses contemporary diffusion models and matches or exceeds strong autoregressive models through adaptation training and reward guidance at inference.AI-generated summaryWe introduce TESS 2, a generalinstruction-following diffusion language modelthat outperforms contemporary instruction-tuned diffusion models, as well as\nmatches and sometimes exceeds strong autoregressive (AR) models. We train TESS\n2 by first adapting a strong AR model via continued pretraining with the usualcross-entropyasdiffusion loss, and then performing further instruction\ntuning. We find that adaptation training as well as the choice of the base\nmodel is crucial for training good instruction-following diffusion models. We\nfurther proposereward guidance, a novel and modularinference-time guidanceprocedure to align model outputs without needing to train the underlying model.\nFinally, we show that TESS 2 further improves with increased inference-time\ncompute, highlighting the utility of diffusion LMs in having fine-grained\ncontrollability over the amount of compute used at inference time. Code and\nmodels are available at https://github.com/hamishivi/tess-2.View arXiv pageView PDFGitHub53autoAdd to collectionCommunityhamishiviPaper authorPaper submitterFeb 20code:https://github.com/hamishivi/tess-2, models:https://huggingface.co/collections/hamishivi/tess-2-677ea36894e38f96dfc7b590See translationReplyhamishiviPaper authorPaper submitterFeb 20Demo:https://huggingface.co/spaces/hamishivi/tess-2-demoSee translationReplylibrarian-botFeb 21This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/hamishivi/tess-2",
    "hf_paper_url": "https://huggingface.co/papers/2502.13917",
    "arxiv_url": "https://arxiv.org/abs/2502.13917",
    "num_models": 7,
    "models_list": "hamishivi/tess2-v0.3, hamishivi/tess2-v0.1, hamishivi/tess2-v0.1-base, hamishivi/tess2-v0.1-symbolic, hamishivi/mistral_tess2_rm, hamishivi/tess2-v0.3-base, hamishivi/tess2-v0.3-symbolic",
    "models_links": "https://huggingface.co/hamishivi/tess2-v0.3, https://huggingface.co/hamishivi/tess2-v0.1, https://huggingface.co/hamishivi/tess2-v0.1-base, https://huggingface.co/hamishivi/tess2-v0.1-symbolic, https://huggingface.co/hamishivi/mistral_tess2_rm, https://huggingface.co/hamishivi/tess2-v0.3-base, https://huggingface.co/hamishivi/tess2-v0.3-symbolic",
    "models_detailed": "[{\"name\": \"hamishivi/tess2-v0.3\", \"link\": \"https://huggingface.co/hamishivi/tess2-v0.3\", \"task\": \"\", \"likes\": \"50\", \"downloads\": \"\", \"updated\": \"Feb 20\"}, {\"name\": \"hamishivi/tess2-v0.1\", \"link\": \"https://huggingface.co/hamishivi/tess2-v0.1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 20\"}, {\"name\": \"hamishivi/tess2-v0.1-base\", \"link\": \"https://huggingface.co/hamishivi/tess2-v0.1-base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 20\"}, {\"name\": \"hamishivi/tess2-v0.1-symbolic\", \"link\": \"https://huggingface.co/hamishivi/tess2-v0.1-symbolic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 20\"}, {\"name\": \"hamishivi/mistral_tess2_rm\", \"link\": \"https://huggingface.co/hamishivi/mistral_tess2_rm\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 20\"}, {\"name\": \"hamishivi/tess2-v0.3-base\", \"link\": \"https://huggingface.co/hamishivi/tess2-v0.3-base\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 20\"}, {\"name\": \"hamishivi/tess2-v0.3-symbolic\", \"link\": \"https://huggingface.co/hamishivi/tess2-v0.3-symbolic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 20\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.13922",
    "first_seen_date": "2025-02-20",
    "title": "LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.13922LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference OptimizationPublished on Feb 19\u00b7Submitted byChenon Feb 20Upvote28+20Authors:Guanzheng Chen,Xin Li,Michael Qizhe Shieh,Lidong BingAbstractLongPO enables short-context LLMs to adapt to long-context tasks by self-learning from preference data and maintaining short-context performance.AI-generated summaryLarge Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superiorshort-contextLLMsmay\nunderperform inlong-contextscenarios due to insufficientlong-contextalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- andlong-contextperformance. To address these challenges, we introduceLongPO, that enablesshort-contextLLMstoself-evolveto excel onlong-contexttasks by internally transferringshort-contextcapabilities.LongPOharnessesLLMsto learn fromself-generated short-to-long preference data, comprising\npaired responses generated for identical instructions withlong-contextinputs\nand their compressedshort-contextcounterparts, respectively. This preference\nreveals capabilities and potentials ofLLMscultivated duringshort-contextalignment that may be diminished in under-alignedlong-contextscenarios.\nAdditionally,LongPOincorporates a short-to-longKL constraintto mitigateshort-contextperformance decline duringlong-contextalignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512Kcontext lengths,LongPOfully\nretainsshort-contextperformance and largely outperforms naiveSFTandDPOin\nboth long- andshort-contexttasks. Specifically, \\ourMethod-trained models can\nachieve results onlong-context benchmarkscomparable to, or even surpassing,\nthose of superiorLLMs(e.g.,GPT-4-128K) that involve extensivelong-contextannotation and larger para",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/DAMO-NLP-SG/LongPO",
    "hf_paper_url": "https://huggingface.co/papers/2502.13922",
    "arxiv_url": "https://arxiv.org/abs/2502.13922",
    "num_models": 4,
    "models_list": "DAMO-NLP-SG/Qwen2.5-7B-LongPO-128K, DAMO-NLP-SG/Mistral-7B-LongPO-128K, DAMO-NLP-SG/Mistral-7B-LongPO-256K-EXP, DAMO-NLP-SG/Mistral-7B-LongPO-512K-EXP",
    "models_links": "https://huggingface.co/DAMO-NLP-SG/Qwen2.5-7B-LongPO-128K, https://huggingface.co/DAMO-NLP-SG/Mistral-7B-LongPO-128K, https://huggingface.co/DAMO-NLP-SG/Mistral-7B-LongPO-256K-EXP, https://huggingface.co/DAMO-NLP-SG/Mistral-7B-LongPO-512K-EXP",
    "models_detailed": "[{\"name\": \"DAMO-NLP-SG/Qwen2.5-7B-LongPO-128K\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/Qwen2.5-7B-LongPO-128K\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Feb 22\"}, {\"name\": \"DAMO-NLP-SG/Mistral-7B-LongPO-128K\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/Mistral-7B-LongPO-128K\", \"task\": \"Text Generation\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Feb 22\"}, {\"name\": \"DAMO-NLP-SG/Mistral-7B-LongPO-256K-EXP\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/Mistral-7B-LongPO-256K-EXP\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 22\"}, {\"name\": \"DAMO-NLP-SG/Mistral-7B-LongPO-512K-EXP\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/Mistral-7B-LongPO-512K-EXP\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 22\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.13923",
    "first_seen_date": "2025-02-20",
    "title": "Qwen2.5-VL Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.13923Qwen2.5-VL Technical ReportPublished on Feb 19\u00b7Submitted byshuai baion Feb 20#1 Paper of the dayUpvote212+204Authors:Shuai Bai,Keqin Chen,Xuejing Liu,Jialin Wang,Wenbin Ge,Sibo Song,Kai Dang,Peng Wang,Shijie Wang,Jun Tang,Humen Zhong,Yuanzhi Zhu,Mingkun Yang,Zhaohai Li,Jianqiang Wan,Pengfei Wang,Wei Ding,Zheren Fu,Yiheng Xu,Jiabo Ye,Xi Zhang,Tianbao Xie+5 authorsAbstractQwen2.5-VL, the latest vision-language model, advances visual recognition, document parsing, and video comprehension through dynamic resolution processing, Window Attention, and a native Vision Transformer.AI-generated summaryWe introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robustdocument parsing, andlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects usingbounding boxesor points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introducesdynamic resolution processingand absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceivespatial scalesandtemporal dynamicswithout relying on\ntraditional normalization techniques. By training a nativedynamic-resolutionVision Transformer(ViT) from scratch and incorporatingWindow Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an inte",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/QwenLM/Qwen2.5-VL",
    "hf_paper_url": "https://huggingface.co/papers/2502.13923",
    "arxiv_url": "https://arxiv.org/abs/2502.13923",
    "num_models": 219,
    "models_list": "Qwen/Qwen3-VL-8B-Instruct, Qwen/Qwen2.5-VL-32B-Instruct, Qwen/Qwen3-VL-30B-A3B-Instruct, Qwen/Qwen3-VL-235B-A22B-Thinking, Qwen/Qwen3-VL-8B-Thinking, Qwen/Qwen3-VL-30B-A3B-Thinking, Qwen/Qwen3-VL-8B-Instruct-GGUF, Qwen/Qwen3-VL-235B-A22B-Instruct, Qwen/Qwen3-VL-4B-Instruct, Qwen/Qwen3-VL-32B-Instruct, Qwen/Qwen3-VL-32B-Thinking, Qwen/Qwen3-VL-2B-Instruct, unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF, Qwen/Qwen3-VL-4B-Instruct-GGUF, coder3101/Qwen3-VL-32B-Instruct-heretic-v2, Qwen/Qwen3-VL-8B-Instruct-FP8, unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit, unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF, unsloth/Qwen3-VL-8B-Thinking-1M-GGUF, unsloth/Qwen3-VL-32B-Thinking-1M-GGUF, coder3101/Qwen3-VL-4B-Instruct-heretic, unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF, Qwen/Qwen3-VL-235B-A22B-Instruct-FP8, Qwen/Qwen3-VL-30B-A3B-Instruct-FP8, Qwen/Qwen3-VL-30B-A3B-Thinking-FP8, QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ, Qwen/Qwen3-VL-4B-Thinking, Qwen/Qwen3-VL-4B-Instruct-FP8, unsloth/Qwen3-VL-8B-Thinking, cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit, cpatonn/Qwen3-VL-4B-Instruct-AWQ-8bit, Qwen/Qwen3-VL-2B-Instruct-FP8, Qwen/Qwen3-VL-2B-Thinking, unsloth/Qwen3-VL-4B-Instruct-GGUF, unsloth/Qwen3-VL-32B-Instruct-GGUF, Qwen/Qwen3-VL-2B-Instruct-GGUF, Qwen/Qwen3-VL-4B-Thinking-GGUF, Qwen/Qwen3-VL-30B-A3B-Instruct-GGUF, Qwen/Qwen3-VL-235B-A22B-Instruct-GGUF, unsloth/Qwen3-VL-8B-Instruct-1M-GGUF, svjack/Qwen3-VL-4B-Instruct-heretic-7refusal, prithivMLmods/proxima-ocr-d.markdown-post3.0.l, prithivMLmods/Gliese-CUA-Tool-Call-8B, coder3101/Qwen3-VL-32B-Thinking-heretic-v2, unsloth/Qwen2.5-VL-32B-Instruct, unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit, unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit, christopherthompson81/Qwen2.5-VL-32B-Instruct-exl2-4_25bpw, Qwen/Qwen2.5-VL-32B-Instruct-AWQ, remichu/Qwen2.5-VL-32B-Instruct-exl2-5.5bpw, remichu/Qwen2.5-VL-32B-Instruct-exl2-5.0bpw, remichu/Qwen2.5-VL-32B-Instruct-exl2-6.5bpw, Mungert/Qwen2.5-VL-32B-Instruct-GGUF, Alhdrawi/Space_model, IntelligenceLab/RewardPreferenceBert, BiliSakura/RSCCM, unsloth/Qwen2.5-VL-32B-Instruct-GGUF, PF94/Qwen2.5-VL-32B-Instruct-4.0bpw-exl2, PF94/Qwen2.5-VL-32B-Instruct-6.0bpw-exl2, Meosiuuubeo/qwen2.5-vl-32b-it-fn, unsloth/Qwen3-VL-235B-A22B-Thinking, Qwen/Qwen3-VL-235B-A22B-Thinking-FP8, QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ, yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF, yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF, cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit, cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-4bit, cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-8bit, cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-8bit, Qwen/Qwen3-VL-4B-Thinking-FP8, Qwen/Qwen3-VL-8B-Thinking-FP8, unsloth/Qwen3-VL-8B-Instruct, unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit, unsloth/Qwen3-VL-8B-Instruct-bnb-4bit, unsloth/Qwen3-VL-4B-Instruct, unsloth/Qwen3-VL-4B-Instruct-bnb-4bit, unsloth/Qwen3-VL-4B-Thinking, unsloth/Qwen3-VL-4B-Thinking-unsloth-bnb-4bit, unsloth/Qwen3-VL-4B-Thinking-bnb-4bit, unsloth/Qwen3-VL-8B-Thinking-unsloth-bnb-4bit, unsloth/Qwen3-VL-8B-Thinking-bnb-4bit, unsloth/Qwen3-VL-8B-Thinking-FP8, unsloth/Qwen3-VL-4B-Thinking-FP8, unsloth/Qwen3-VL-4B-Instruct-FP8, unsloth/Qwen3-VL-8B-Instruct-FP8, cpatonn/Qwen3-VL-8B-Instruct-AWQ-4bit, cpatonn/Qwen3-VL-8B-Instruct-AWQ-8bit, cpatonn/Qwen3-VL-8B-Thinking-AWQ-4bit, cpatonn/Qwen3-VL-8B-Thinking-AWQ-8bit, cpatonn/Qwen3-VL-4B-Thinking-AWQ-4bit, cpatonn/Qwen3-VL-4B-Thinking-AWQ-8bit, toughcent/Qwen3-VL-8B-Thinking-bnb-8bit, ticoAg/Qwen3-VL-30B-A3B-Instruct-AWQ, Echo9Zulu/Nanonets-OCR2-3B-LM-INT4_ASYM-VE-FP16-ov, vocaela/Vocaela-500M, Qwen/Qwen3-VL-32B-Thinking-FP8, Qwen/Qwen3-VL-32B-Instruct-FP8, gary109/Qwen3-VL-4B-Instruct, Qwen/Qwen3-VL-2B-Thinking-FP8, gary109/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit, unsloth/Qwen3-VL-2B-Instruct, unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit, unsloth/Qwen3-VL-2B-Instruct-bnb-4bit, unsloth/Qwen3-VL-32B-Thinking, unsloth/Qwen3-VL-32B-Thinking-unsloth-bnb-4bit, unsloth/Qwen3-VL-32B-Thinking-bnb-4bit, unsloth/Qwen3-VL-32B-Instruct, unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit, unsloth/Qwen3-VL-32B-Instruct-bnb-4bit, cpatonn/Qwen3-VL-32B-Instruct-AWQ-4bit, cpatonn/Qwen3-VL-32B-Thinking-AWQ-4bit, wealthcoders/qwen3-vl, yairpatch/Qwen3-VL-32B-Instruct-GGUF, sunemo/dawgs, QuantTrio/Qwen3-VL-32B-Instruct-AWQ, QuantTrio/Qwen3-VL-32B-Thinking-AWQ, cpatonn/Qwen3-VL-32B-Instruct-AWQ-8bit, cpatonn/Qwen3-VL-32B-Thinking-AWQ-8bit, yairzar/Qwen3-VL-4B-Instruct-GGUF, abhishekchohan/Qwen3-VL-32B-Instruct-AWQ, FastFlowLM/Qwen3-VL-4B-Instruct-NPU2, wangkanai/qwen2.5-vl-32b-instruct, cyankiwi/Qwen3-VL-235B-A22B-Instruct-AWQ-4bit, ahmed-20033/my-ai-model, cyankiwi/Qwen3-VL-235B-A22B-Thinking-AWQ-4bit, abhishekchohan/Qwen3-VL-32B-Thinking-AWQ, unsloth/Qwen3-VL-2B-Thinking-GGUF, unsloth/Qwen3-VL-2B-Thinking, unsloth/Qwen3-VL-2B-Instruct-GGUF, unsloth/Qwen3-VL-2B-Thinking-unsloth-bnb-4bit, unsloth/Qwen3-VL-2B-Thinking-bnb-4bit, bullerwins/Qwen3-VL-32B-Instruct-GGUF, unsloth/Qwen3-VL-4B-Thinking-GGUF, unsloth/Qwen3-VL-235B-A22B-Instruct-GGUF, unsloth/Qwen3-VL-235B-A22B-Instruct, unsloth/Qwen3-VL-8B-Thinking-GGUF, unsloth/Qwen3-VL-8B-Instruct-GGUF, unsloth/Qwen3-VL-30B-A3B-Thinking, unsloth/Qwen3-VL-30B-A3B-Instruct, unsloth/Qwen3-VL-32B-Thinking-GGUF, Qwen/Qwen3-VL-32B-Instruct-GGUF, redponike/Qwen3-VL-2B-Thinking-GGUF, Qwen/Qwen3-VL-32B-Thinking-GGUF, Qwen/Qwen3-VL-8B-Thinking-GGUF, Qwen/Qwen3-VL-2B-Thinking-GGUF, redponike/Qwen3-VL-8B-Thinking-GGUF, Mungert/Qwen3-VL-2B-Instruct-GGUF, redponike/Qwen3-VL-30B-A3B-Thinking-GGUF, Qwen/Qwen3-VL-30B-A3B-Thinking-GGUF, Qwen/Qwen3-VL-235B-A22B-Thinking-GGUF, Infraizoo/Qwen3-VL-8B-Instruct-custom, unsloth/Qwen3-VL-2B-Thinking-1M-GGUF, Mungert/Qwen3-VL-8B-Instruct-GGUF, redponike/Qwen3-VL-32B-Thinking-GGUF, redponike/Qwen3-VL-4B-Thinking-GGUF, unsloth/Qwen3-VL-235B-A22B-Thinking-1M-GGUF, unsloth/Qwen3-VL-235B-A22B-Instruct-1M-GGUF, unsloth/Qwen3-VL-2B-Instruct-1M-GGUF, unsloth/Qwen3-VL-4B-Instruct-1M-GGUF, unsloth/Qwen3-VL-4B-Thinking-1M-GGUF, unsloth/Qwen3-VL-32B-Instruct-1M-GGUF, unsloth/Qwen3-VL-30B-A3B-Thinking-1M-GGUF, unsloth/Qwen3-VL-30B-A3B-Instruct-1M-GGUF, redponike/Qwen3-VL-30B-A3B-Instruct-GGUF, redponike/Qwen3-VL-32B-Instruct-GGUF, redponike/Qwen3-VL-4B-Instruct, Robertp423/Qwen3-VL-4B-Construct, Mungert/Qwen3-VL-4B-Instruct-GGUF, Mungert/Qwen3-VL-30B-A3B-Instruct-GGUF, Robertp423/Qwen3-VL-32B-Eevum-Merged, Userb1az/Qwen3-VL-30B-A3B-Thinking-GGUF, Userb1az/Qwen3-VL-30B-A3B-Instruct-GGUF, Robertp423/Qwen3-VL-32B-Fevum-Merged, Robertp423/Qwen3-VL-32B-Gevum-Merged-25pct, Robertp423/Qwen3-VL-32B-Gevum-Merged-100pct, khairul5/BINI-Qwen3-VL-MyMirror, OpenMOSE/qwen3-vl-30b-text, OpenMOSE/Qwen3-25B-A3B-REAP-Instruct, georgehenney/Qwen3-VL-4B-Instruct-heretic-7refusal, Kizzington/Qwen3-VL-8B-Thinking-heretic, Kizzington/Qwen3-VL-8B-Thinking-heretic-1refusal, JSon-AI/Qwen3-VL-4B-Instruct-Abliterated-heretic, Kizzington/Qwen3-VL-8B-Instruct-heretic, georgehenney/Qwen3-VL-4B-Instruct-heretic-5refusal, georgehenney/Qwen3-VL-4B-Instruct-heretic-8refusal, kurtinau/Qwen3-VL-2B-Instruct-GGUF, hainguyen306201/bank-model-2b, Userb1az/Qwen3-VL-235B-A22B-Thinking-GGUF, remodlai/Qwen3-VL-30B-A3B-Instruct-AWQ, coder3101/Qwen3-VL-32B-Instruct-Heretic, coder3101/Qwen3-VL-2B-Instruct-heretic, coder3101/Qwen3-VL-2B-Thinking-heretic, coder3101/Qwen3-VL-32B-Thinking-heretic, coder3101/Qwen3-VL-8B-Thinking-heretic, KalvinPhan/MathCoder-VL-34bit, KalvinPhan/MathCoder-VL-3-4bit, pranavvmurthy26/Qwen3-VL-2B-Instruct-Docling-5K-30perc-11ep, unsloth/Qwen3-VL-2B-Instruct-FP8, unsloth/Qwen3-VL-2B-Thinking-FP8, unsloth/Qwen3-VL-32B-Instruct-FP8, unsloth/Qwen3-VL-32B-Thinking-FP8, unsloth/Qwen3-VL-30B-A3B-Thinking-FP8, unsloth/Qwen3-VL-30B-A3B-Instruct-FP8, unsloth/Qwen3-VL-235B-A22B-Thinking-FP8, unsloth/Qwen3-VL-235B-A22B-Instruct-FP8, Userb1az/Qwen3-VL-32B-Thinking-GGUF, minpeter/Qwen3-VL-32B-Thinking, wealthcoders/qwen3-vl-2B, ZuzeTt/Qwen3-VL-4B-Instruct-heretic-GGUF, ZuzeTt/Qwen3-VL-2B-Thinking-heretic-GGUF, prithivMLmods/epsilon-ocr-d.markdown-post3.0.m, ZuzeTt/Qwen3-VL-8B-Thinking-heretic, Robertp423/Qwen3-VL-32B-Ievum-Merged, ZuzeTt/Qwen3-VL-2B-Thinking-heretic-Imatrix-GGUF, ZuzeTt/Qwen3-VL-4B-Instruct-heretic-Imatrix-GGUF, ZuzeTt/Qwen3-VL-8B-Thinking-Imatrix-heretic, drgary/Qwen3-VL-2B-Instruct, n0kovo/Qwen3-VL-32B-Instruct-heretic-v2, AuricAlgos/qwen3-v1-8b-custom1",
    "models_links": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct, https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF, https://huggingface.co/coder3101/Qwen3-VL-32B-Instruct-heretic-v2, https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF, https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-1M-GGUF, https://huggingface.co/coder3101/Qwen3-VL-4B-Instruct-heretic, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct-FP8, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct-FP8, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking-FP8, https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ, https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking, https://huggingface.co/cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit, https://huggingface.co/cpatonn/Qwen3-VL-4B-Instruct-AWQ-8bit, https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-FP8, https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking, https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-1M-GGUF, https://huggingface.co/svjack/Qwen3-VL-4B-Instruct-heretic-7refusal, https://huggingface.co/prithivMLmods/proxima-ocr-d.markdown-post3.0.l, https://huggingface.co/prithivMLmods/Gliese-CUA-Tool-Call-8B, https://huggingface.co/coder3101/Qwen3-VL-32B-Thinking-heretic-v2, https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct, https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit, https://huggingface.co/christopherthompson81/Qwen2.5-VL-32B-Instruct-exl2-4_25bpw, https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct-AWQ, https://huggingface.co/remichu/Qwen2.5-VL-32B-Instruct-exl2-5.5bpw, https://huggingface.co/remichu/Qwen2.5-VL-32B-Instruct-exl2-5.0bpw, https://huggingface.co/remichu/Qwen2.5-VL-32B-Instruct-exl2-6.5bpw, https://huggingface.co/Mungert/Qwen2.5-VL-32B-Instruct-GGUF, https://huggingface.co/Alhdrawi/Space_model, https://huggingface.co/IntelligenceLab/RewardPreferenceBert, https://huggingface.co/BiliSakura/RSCCM, https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-GGUF, https://huggingface.co/PF94/Qwen2.5-VL-32B-Instruct-4.0bpw-exl2, https://huggingface.co/PF94/Qwen2.5-VL-32B-Instruct-6.0bpw-exl2, https://huggingface.co/Meosiuuubeo/qwen2.5-vl-32b-it-fn, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking-FP8, https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ, https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF, https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF, https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit, https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-4bit, https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-8bit, https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-8bit, https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking-FP8, https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking-FP8, https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct, https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct, https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking, https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-FP8, https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-FP8, https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-FP8, https://huggingface.co/cpatonn/Qwen3-VL-8B-Instruct-AWQ-4bit, https://huggingface.co/cpatonn/Qwen3-VL-8B-Instruct-AWQ-8bit, https://huggingface.co/cpatonn/Qwen3-VL-8B-Thinking-AWQ-4bit, https://huggingface.co/cpatonn/Qwen3-VL-8B-Thinking-AWQ-8bit, https://huggingface.co/cpatonn/Qwen3-VL-4B-Thinking-AWQ-4bit, https://huggingface.co/cpatonn/Qwen3-VL-4B-Thinking-AWQ-8bit, https://huggingface.co/toughcent/Qwen3-VL-8B-Thinking-bnb-8bit, https://huggingface.co/ticoAg/Qwen3-VL-30B-A3B-Instruct-AWQ, https://huggingface.co/Echo9Zulu/Nanonets-OCR2-3B-LM-INT4_ASYM-VE-FP16-ov, https://huggingface.co/vocaela/Vocaela-500M, https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking-FP8, https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct-FP8, https://huggingface.co/gary109/Qwen3-VL-4B-Instruct, https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-FP8, https://huggingface.co/gary109/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct, https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking, https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct, https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-bnb-4bit, https://huggingface.co/cpatonn/Qwen3-VL-32B-Instruct-AWQ-4bit, https://huggingface.co/cpatonn/Qwen3-VL-32B-Thinking-AWQ-4bit, https://huggingface.co/wealthcoders/qwen3-vl, https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF, https://huggingface.co/sunemo/dawgs, https://huggingface.co/QuantTrio/Qwen3-VL-32B-Instruct-AWQ, https://huggingface.co/QuantTrio/Qwen3-VL-32B-Thinking-AWQ, https://huggingface.co/cpatonn/Qwen3-VL-32B-Instruct-AWQ-8bit, https://huggingface.co/cpatonn/Qwen3-VL-32B-Thinking-AWQ-8bit, https://huggingface.co/yairzar/Qwen3-VL-4B-Instruct-GGUF, https://huggingface.co/abhishekchohan/Qwen3-VL-32B-Instruct-AWQ, https://huggingface.co/FastFlowLM/Qwen3-VL-4B-Instruct-NPU2, https://huggingface.co/wangkanai/qwen2.5-vl-32b-instruct, https://huggingface.co/cyankiwi/Qwen3-VL-235B-A22B-Instruct-AWQ-4bit, https://huggingface.co/ahmed-20033/my-ai-model, https://huggingface.co/cyankiwi/Qwen3-VL-235B-A22B-Thinking-AWQ-4bit, https://huggingface.co/abhishekchohan/Qwen3-VL-32B-Thinking-AWQ, https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-GGUF, https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking, https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-unsloth-bnb-4bit, https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-bnb-4bit, https://huggingface.co/bullerwins/Qwen3-VL-32B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-GGUF, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct, https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-GGUF, https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct, https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct-GGUF, https://huggingface.co/redponike/Qwen3-VL-2B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF, https://huggingface.co/redponike/Qwen3-VL-8B-Thinking-GGUF, https://huggingface.co/Mungert/Qwen3-VL-2B-Instruct-GGUF, https://huggingface.co/redponike/Qwen3-VL-30B-A3B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking-GGUF, https://huggingface.co/Infraizoo/Qwen3-VL-8B-Instruct-custom, https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-1M-GGUF, https://huggingface.co/Mungert/Qwen3-VL-8B-Instruct-GGUF, https://huggingface.co/redponike/Qwen3-VL-32B-Thinking-GGUF, https://huggingface.co/redponike/Qwen3-VL-4B-Thinking-GGUF, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-1M-GGUF, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-1M-GGUF, https://huggingface.co/redponike/Qwen3-VL-30B-A3B-Instruct-GGUF, https://huggingface.co/redponike/Qwen3-VL-32B-Instruct-GGUF, https://huggingface.co/redponike/Qwen3-VL-4B-Instruct, https://huggingface.co/Robertp423/Qwen3-VL-4B-Construct, https://huggingface.co/Mungert/Qwen3-VL-4B-Instruct-GGUF, https://huggingface.co/Mungert/Qwen3-VL-30B-A3B-Instruct-GGUF, https://huggingface.co/Robertp423/Qwen3-VL-32B-Eevum-Merged, https://huggingface.co/Userb1az/Qwen3-VL-30B-A3B-Thinking-GGUF, https://huggingface.co/Userb1az/Qwen3-VL-30B-A3B-Instruct-GGUF, https://huggingface.co/Robertp423/Qwen3-VL-32B-Fevum-Merged, https://huggingface.co/Robertp423/Qwen3-VL-32B-Gevum-Merged-25pct, https://huggingface.co/Robertp423/Qwen3-VL-32B-Gevum-Merged-100pct, https://huggingface.co/khairul5/BINI-Qwen3-VL-MyMirror, https://huggingface.co/OpenMOSE/qwen3-vl-30b-text, https://huggingface.co/OpenMOSE/Qwen3-25B-A3B-REAP-Instruct, https://huggingface.co/georgehenney/Qwen3-VL-4B-Instruct-heretic-7refusal, https://huggingface.co/Kizzington/Qwen3-VL-8B-Thinking-heretic, https://huggingface.co/Kizzington/Qwen3-VL-8B-Thinking-heretic-1refusal, https://huggingface.co/JSon-AI/Qwen3-VL-4B-Instruct-Abliterated-heretic, https://huggingface.co/Kizzington/Qwen3-VL-8B-Instruct-heretic, https://huggingface.co/georgehenney/Qwen3-VL-4B-Instruct-heretic-5refusal, https://huggingface.co/georgehenney/Qwen3-VL-4B-Instruct-heretic-8refusal, https://huggingface.co/kurtinau/Qwen3-VL-2B-Instruct-GGUF, https://huggingface.co/hainguyen306201/bank-model-2b, https://huggingface.co/Userb1az/Qwen3-VL-235B-A22B-Thinking-GGUF, https://huggingface.co/remodlai/Qwen3-VL-30B-A3B-Instruct-AWQ, https://huggingface.co/coder3101/Qwen3-VL-32B-Instruct-Heretic, https://huggingface.co/coder3101/Qwen3-VL-2B-Instruct-heretic, https://huggingface.co/coder3101/Qwen3-VL-2B-Thinking-heretic, https://huggingface.co/coder3101/Qwen3-VL-32B-Thinking-heretic, https://huggingface.co/coder3101/Qwen3-VL-8B-Thinking-heretic, https://huggingface.co/KalvinPhan/MathCoder-VL-34bit, https://huggingface.co/KalvinPhan/MathCoder-VL-3-4bit, https://huggingface.co/pranavvmurthy26/Qwen3-VL-2B-Instruct-Docling-5K-30perc-11ep, https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-FP8, https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-FP8, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-FP8, https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-FP8, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-FP8, https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct-FP8, https://huggingface.co/Userb1az/Qwen3-VL-32B-Thinking-GGUF, https://huggingface.co/minpeter/Qwen3-VL-32B-Thinking, https://huggingface.co/wealthcoders/qwen3-vl-2B, https://huggingface.co/ZuzeTt/Qwen3-VL-4B-Instruct-heretic-GGUF, https://huggingface.co/ZuzeTt/Qwen3-VL-2B-Thinking-heretic-GGUF, https://huggingface.co/prithivMLmods/epsilon-ocr-d.markdown-post3.0.m, https://huggingface.co/ZuzeTt/Qwen3-VL-8B-Thinking-heretic, https://huggingface.co/Robertp423/Qwen3-VL-32B-Ievum-Merged, https://huggingface.co/ZuzeTt/Qwen3-VL-2B-Thinking-heretic-Imatrix-GGUF, https://huggingface.co/ZuzeTt/Qwen3-VL-4B-Instruct-heretic-Imatrix-GGUF, https://huggingface.co/ZuzeTt/Qwen3-VL-8B-Thinking-Imatrix-heretic, https://huggingface.co/drgary/Qwen3-VL-2B-Instruct, https://huggingface.co/n0kovo/Qwen3-VL-32B-Instruct-heretic-v2, https://huggingface.co/AuricAlgos/qwen3-v1-8b-custom1",
    "models_detailed": "[{\"name\": \"Qwen/Qwen3-VL-8B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Qwen/Qwen2.5-VL-32B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Qwen/Qwen3-VL-32B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"Qwen/Qwen3-VL-32B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 23\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"coder3101/Qwen3-VL-32B-Instruct-heretic-v2\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-32B-Instruct-heretic-v2\", \"task\": \"\", \"likes\": \"181\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Thinking-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Thinking-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"coder3101/Qwen3-VL-4B-Instruct-heretic\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-4B-Instruct-heretic\", \"task\": \"\", \"likes\": \"262\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Thinking\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-4B-Instruct-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"cpatonn/Qwen3-VL-4B-Instruct-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-4B-Instruct-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"svjack/Qwen3-VL-4B-Instruct-heretic-7refusal\", \"link\": \"https://huggingface.co/svjack/Qwen3-VL-4B-Instruct-heretic-7refusal\", \"task\": \"\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"about 1\"}, {\"name\": \"prithivMLmods/proxima-ocr-d.markdown-post3.0.l\", \"link\": \"https://huggingface.co/prithivMLmods/proxima-ocr-d.markdown-post3.0.l\", \"task\": \"\", \"likes\": \"180\", \"downloads\": \"\", \"updated\": \"12 days ago\"}, {\"name\": \"prithivMLmods/Gliese-CUA-Tool-Call-8B\", \"link\": \"https://huggingface.co/prithivMLmods/Gliese-CUA-Tool-Call-8B\", \"task\": \"\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"1 day ago\"}, {\"name\": \"coder3101/Qwen3-VL-32B-Thinking-heretic-v2\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-32B-Thinking-heretic-v2\", \"task\": \"\", \"likes\": \"99\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"unsloth/Qwen2.5-VL-32B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-bnb-4bit\", \"task\": \"\", \"likes\": \"618\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"christopherthompson81/Qwen2.5-VL-32B-Instruct-exl2-4_25bpw\", \"link\": \"https://huggingface.co/christopherthompson81/Qwen2.5-VL-32B-Instruct-exl2-4_25bpw\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Mar 25\"}, {\"name\": \"Qwen/Qwen2.5-VL-32B-Instruct-AWQ\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct-AWQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 6\"}, {\"name\": \"remichu/Qwen2.5-VL-32B-Instruct-exl2-5.5bpw\", \"link\": \"https://huggingface.co/remichu/Qwen2.5-VL-32B-Instruct-exl2-5.5bpw\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"remichu/Qwen2.5-VL-32B-Instruct-exl2-5.0bpw\", \"link\": \"https://huggingface.co/remichu/Qwen2.5-VL-32B-Instruct-exl2-5.0bpw\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"remichu/Qwen2.5-VL-32B-Instruct-exl2-6.5bpw\", \"link\": \"https://huggingface.co/remichu/Qwen2.5-VL-32B-Instruct-exl2-6.5bpw\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"Mungert/Qwen2.5-VL-32B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen2.5-VL-32B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Alhdrawi/Space_model\", \"link\": \"https://huggingface.co/Alhdrawi/Space_model\", \"task\": \"Question Answering\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"Apr 4\"}, {\"name\": \"IntelligenceLab/RewardPreferenceBert\", \"link\": \"https://huggingface.co/IntelligenceLab/RewardPreferenceBert\", \"task\": \"\", \"likes\": \"94\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"BiliSakura/RSCCM\", \"link\": \"https://huggingface.co/BiliSakura/RSCCM\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 20\"}, {\"name\": \"unsloth/Qwen2.5-VL-32B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen2.5-VL-32B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"PF94/Qwen2.5-VL-32B-Instruct-4.0bpw-exl2\", \"link\": \"https://huggingface.co/PF94/Qwen2.5-VL-32B-Instruct-4.0bpw-exl2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"PF94/Qwen2.5-VL-32B-Instruct-6.0bpw-exl2\", \"link\": \"https://huggingface.co/PF94/Qwen2.5-VL-32B-Instruct-6.0bpw-exl2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 2\"}, {\"name\": \"Meosiuuubeo/qwen2.5-vl-32b-it-fn\", \"link\": \"https://huggingface.co/Meosiuuubeo/qwen2.5-vl-32b-it-fn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Thinking\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking\", \"task\": \"\", \"likes\": \"43\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"30\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Instruct-AWQ-8bit\", \"task\": \"\", \"likes\": \"903\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-30B-A3B-Thinking-AWQ-8bit\", \"task\": \"\", \"likes\": \"186\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-4B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Thinking\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Thinking-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Thinking-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-bnb-4bit\", \"task\": \"\", \"likes\": \"361\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Thinking-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\", \"task\": \"\", \"likes\": \"393\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Thinking-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-FP8\", \"task\": \"\", \"likes\": \"261\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Thinking-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Instruct-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Instruct-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-FP8\", \"task\": \"\", \"likes\": \"896\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"cpatonn/Qwen3-VL-8B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-8B-Instruct-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"cpatonn/Qwen3-VL-8B-Instruct-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-8B-Instruct-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"cpatonn/Qwen3-VL-8B-Thinking-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-8B-Thinking-AWQ-4bit\", \"task\": \"\", \"likes\": \"773\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"cpatonn/Qwen3-VL-8B-Thinking-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-8B-Thinking-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 14\"}, {\"name\": \"cpatonn/Qwen3-VL-4B-Thinking-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-4B-Thinking-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"cpatonn/Qwen3-VL-4B-Thinking-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-4B-Thinking-AWQ-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"toughcent/Qwen3-VL-8B-Thinking-bnb-8bit\", \"link\": \"https://huggingface.co/toughcent/Qwen3-VL-8B-Thinking-bnb-8bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 15\"}, {\"name\": \"ticoAg/Qwen3-VL-30B-A3B-Instruct-AWQ\", \"link\": \"https://huggingface.co/ticoAg/Qwen3-VL-30B-A3B-Instruct-AWQ\", \"task\": \"Text Generation\", \"likes\": \"89\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"Echo9Zulu/Nanonets-OCR2-3B-LM-INT4_ASYM-VE-FP16-ov\", \"link\": \"https://huggingface.co/Echo9Zulu/Nanonets-OCR2-3B-LM-INT4_ASYM-VE-FP16-ov\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 17\"}, {\"name\": \"vocaela/Vocaela-500M\", \"link\": \"https://huggingface.co/vocaela/Vocaela-500M\", \"task\": \"\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Oct 19\"}, {\"name\": \"Qwen/Qwen3-VL-32B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"Qwen/Qwen3-VL-32B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 22\"}, {\"name\": \"gary109/Qwen3-VL-4B-Instruct\", \"link\": \"https://huggingface.co/gary109/Qwen3-VL-4B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"gary109/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/gary109/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Thinking\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Thinking-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"957\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"cpatonn/Qwen3-VL-32B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-32B-Instruct-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"cpatonn/Qwen3-VL-32B-Thinking-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-32B-Thinking-AWQ-4bit\", \"task\": \"\", \"likes\": \"865\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"wealthcoders/qwen3-vl\", \"link\": \"https://huggingface.co/wealthcoders/qwen3-vl\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"yairpatch/Qwen3-VL-32B-Instruct-GGUF\", \"link\": \"https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"sunemo/dawgs\", \"link\": \"https://huggingface.co/sunemo/dawgs\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 22\"}, {\"name\": \"QuantTrio/Qwen3-VL-32B-Instruct-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-VL-32B-Instruct-AWQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 22\"}, {\"name\": \"QuantTrio/Qwen3-VL-32B-Thinking-AWQ\", \"link\": \"https://huggingface.co/QuantTrio/Qwen3-VL-32B-Thinking-AWQ\", \"task\": \"\", \"likes\": \"914\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"cpatonn/Qwen3-VL-32B-Instruct-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-32B-Instruct-AWQ-8bit\", \"task\": \"\", \"likes\": \"370\", \"downloads\": \"\", \"updated\": \"Oct 22\"}, {\"name\": \"cpatonn/Qwen3-VL-32B-Thinking-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-VL-32B-Thinking-AWQ-8bit\", \"task\": \"\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Oct 22\"}, {\"name\": \"yairzar/Qwen3-VL-4B-Instruct-GGUF\", \"link\": \"https://huggingface.co/yairzar/Qwen3-VL-4B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"abhishekchohan/Qwen3-VL-32B-Instruct-AWQ\", \"link\": \"https://huggingface.co/abhishekchohan/Qwen3-VL-32B-Instruct-AWQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"FastFlowLM/Qwen3-VL-4B-Instruct-NPU2\", \"link\": \"https://huggingface.co/FastFlowLM/Qwen3-VL-4B-Instruct-NPU2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"wangkanai/qwen2.5-vl-32b-instruct\", \"link\": \"https://huggingface.co/wangkanai/qwen2.5-vl-32b-instruct\", \"task\": \"\", \"likes\": \"152\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"cyankiwi/Qwen3-VL-235B-A22B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Qwen3-VL-235B-A22B-Instruct-AWQ-4bit\", \"task\": \"\", \"likes\": \"354\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"ahmed-20033/my-ai-model\", \"link\": \"https://huggingface.co/ahmed-20033/my-ai-model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"cyankiwi/Qwen3-VL-235B-A22B-Thinking-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Qwen3-VL-235B-A22B-Thinking-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"abhishekchohan/Qwen3-VL-32B-Thinking-AWQ\", \"link\": \"https://huggingface.co/abhishekchohan/Qwen3-VL-32B-Thinking-AWQ\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Thinking\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking\", \"task\": \"\", \"likes\": \"273\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Thinking-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Thinking-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"bullerwins/Qwen3-VL-32B-Instruct-GGUF\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-VL-32B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"136\", \"downloads\": \"\", \"updated\": \"Oct 30\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-8B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Thinking\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 11\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"Qwen/Qwen3-VL-32B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"redponike/Qwen3-VL-2B-Thinking-GGUF\", \"link\": \"https://huggingface.co/redponike/Qwen3-VL-2B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"Qwen/Qwen3-VL-32B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-8B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-2B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"redponike/Qwen3-VL-8B-Thinking-GGUF\", \"link\": \"https://huggingface.co/redponike/Qwen3-VL-8B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"Mungert/Qwen3-VL-2B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-VL-2B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"redponike/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/redponike/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"Qwen/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Qwen/Qwen3-VL-235B-A22B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Infraizoo/Qwen3-VL-8B-Instruct-custom\", \"link\": \"https://huggingface.co/Infraizoo/Qwen3-VL-8B-Instruct-custom\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Thinking-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"Mungert/Qwen3-VL-8B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-VL-8B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"redponike/Qwen3-VL-32B-Thinking-GGUF\", \"link\": \"https://huggingface.co/redponike/Qwen3-VL-32B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"redponike/Qwen3-VL-4B-Thinking-GGUF\", \"link\": \"https://huggingface.co/redponike/Qwen3-VL-4B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 31\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Thinking-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-4B-Thinking-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 1\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Thinking-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-1M-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"redponike/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/redponike/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"redponike/Qwen3-VL-32B-Instruct-GGUF\", \"link\": \"https://huggingface.co/redponike/Qwen3-VL-32B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"redponike/Qwen3-VL-4B-Instruct\", \"link\": \"https://huggingface.co/redponike/Qwen3-VL-4B-Instruct\", \"task\": \"\", \"likes\": \"277\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"Robertp423/Qwen3-VL-4B-Construct\", \"link\": \"https://huggingface.co/Robertp423/Qwen3-VL-4B-Construct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 3\"}, {\"name\": \"Mungert/Qwen3-VL-4B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-VL-4B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"308\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"Mungert/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"Robertp423/Qwen3-VL-32B-Eevum-Merged\", \"link\": \"https://huggingface.co/Robertp423/Qwen3-VL-32B-Eevum-Merged\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 4\"}, {\"name\": \"Userb1az/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Userb1az/Qwen3-VL-30B-A3B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"Userb1az/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Userb1az/Qwen3-VL-30B-A3B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 5\"}, {\"name\": \"Robertp423/Qwen3-VL-32B-Fevum-Merged\", \"link\": \"https://huggingface.co/Robertp423/Qwen3-VL-32B-Fevum-Merged\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"Robertp423/Qwen3-VL-32B-Gevum-Merged-25pct\", \"link\": \"https://huggingface.co/Robertp423/Qwen3-VL-32B-Gevum-Merged-25pct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"Robertp423/Qwen3-VL-32B-Gevum-Merged-100pct\", \"link\": \"https://huggingface.co/Robertp423/Qwen3-VL-32B-Gevum-Merged-100pct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 8\"}, {\"name\": \"khairul5/BINI-Qwen3-VL-MyMirror\", \"link\": \"https://huggingface.co/khairul5/BINI-Qwen3-VL-MyMirror\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 10\"}, {\"name\": \"OpenMOSE/qwen3-vl-30b-text\", \"link\": \"https://huggingface.co/OpenMOSE/qwen3-vl-30b-text\", \"task\": \"\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"OpenMOSE/Qwen3-25B-A3B-REAP-Instruct\", \"link\": \"https://huggingface.co/OpenMOSE/Qwen3-25B-A3B-REAP-Instruct\", \"task\": \"\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"georgehenney/Qwen3-VL-4B-Instruct-heretic-7refusal\", \"link\": \"https://huggingface.co/georgehenney/Qwen3-VL-4B-Instruct-heretic-7refusal\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 18\"}, {\"name\": \"Kizzington/Qwen3-VL-8B-Thinking-heretic\", \"link\": \"https://huggingface.co/Kizzington/Qwen3-VL-8B-Thinking-heretic\", \"task\": \"\", \"likes\": \"45\", \"downloads\": \"\", \"updated\": \"Nov 18\"}, {\"name\": \"Kizzington/Qwen3-VL-8B-Thinking-heretic-1refusal\", \"link\": \"https://huggingface.co/Kizzington/Qwen3-VL-8B-Thinking-heretic-1refusal\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 18\"}, {\"name\": \"JSon-AI/Qwen3-VL-4B-Instruct-Abliterated-heretic\", \"link\": \"https://huggingface.co/JSon-AI/Qwen3-VL-4B-Instruct-Abliterated-heretic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"Kizzington/Qwen3-VL-8B-Instruct-heretic\", \"link\": \"https://huggingface.co/Kizzington/Qwen3-VL-8B-Instruct-heretic\", \"task\": \"\", \"likes\": \"51\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"georgehenney/Qwen3-VL-4B-Instruct-heretic-5refusal\", \"link\": \"https://huggingface.co/georgehenney/Qwen3-VL-4B-Instruct-heretic-5refusal\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"georgehenney/Qwen3-VL-4B-Instruct-heretic-8refusal\", \"link\": \"https://huggingface.co/georgehenney/Qwen3-VL-4B-Instruct-heretic-8refusal\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"kurtinau/Qwen3-VL-2B-Instruct-GGUF\", \"link\": \"https://huggingface.co/kurtinau/Qwen3-VL-2B-Instruct-GGUF\", \"task\": \"\", \"likes\": \"73\", \"downloads\": \"\", \"updated\": \"Nov 19\"}, {\"name\": \"hainguyen306201/bank-model-2b\", \"link\": \"https://huggingface.co/hainguyen306201/bank-model-2b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\"}, {\"name\": \"Userb1az/Qwen3-VL-235B-A22B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Userb1az/Qwen3-VL-235B-A22B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"23 days ago\"}, {\"name\": \"remodlai/Qwen3-VL-30B-A3B-Instruct-AWQ\", \"link\": \"https://huggingface.co/remodlai/Qwen3-VL-30B-A3B-Instruct-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"30 days ago\"}, {\"name\": \"coder3101/Qwen3-VL-32B-Instruct-Heretic\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-32B-Instruct-Heretic\", \"task\": \"\", \"likes\": \"597\", \"downloads\": \"\", \"updated\": \"29 days ago\"}, {\"name\": \"coder3101/Qwen3-VL-2B-Instruct-heretic\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-2B-Instruct-heretic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"coder3101/Qwen3-VL-2B-Thinking-heretic\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-2B-Thinking-heretic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"coder3101/Qwen3-VL-32B-Thinking-heretic\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-32B-Thinking-heretic\", \"task\": \"\", \"likes\": \"658\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"coder3101/Qwen3-VL-8B-Thinking-heretic\", \"link\": \"https://huggingface.co/coder3101/Qwen3-VL-8B-Thinking-heretic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"KalvinPhan/MathCoder-VL-34bit\", \"link\": \"https://huggingface.co/KalvinPhan/MathCoder-VL-34bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"KalvinPhan/MathCoder-VL-3-4bit\", \"link\": \"https://huggingface.co/KalvinPhan/MathCoder-VL-3-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"pranavvmurthy26/Qwen3-VL-2B-Instruct-Docling-5K-30perc-11ep\", \"link\": \"https://huggingface.co/pranavvmurthy26/Qwen3-VL-2B-Instruct-Docling-5K-30perc-11ep\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Instruct-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-2B-Thinking-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Instruct-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-32B-Thinking-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-FP8\", \"task\": \"\", \"likes\": \"75\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Thinking-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-30B-A3B-Instruct-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Thinking-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"unsloth/Qwen3-VL-235B-A22B-Instruct-FP8\", \"link\": \"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct-FP8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"Userb1az/Qwen3-VL-32B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Userb1az/Qwen3-VL-32B-Thinking-GGUF\", \"task\": \"\", \"likes\": \"237\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"minpeter/Qwen3-VL-32B-Thinking\", \"link\": \"https://huggingface.co/minpeter/Qwen3-VL-32B-Thinking\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"25 days ago\"}, {\"name\": \"wealthcoders/qwen3-vl-2B\", \"link\": \"https://huggingface.co/wealthcoders/qwen3-vl-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"ZuzeTt/Qwen3-VL-4B-Instruct-heretic-GGUF\", \"link\": \"https://huggingface.co/ZuzeTt/Qwen3-VL-4B-Instruct-heretic-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ZuzeTt/Qwen3-VL-2B-Thinking-heretic-GGUF\", \"link\": \"https://huggingface.co/ZuzeTt/Qwen3-VL-2B-Thinking-heretic-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"prithivMLmods/epsilon-ocr-d.markdown-post3.0.m\", \"link\": \"https://huggingface.co/prithivMLmods/epsilon-ocr-d.markdown-post3.0.m\", \"task\": \"\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"ZuzeTt/Qwen3-VL-8B-Thinking-heretic\", \"link\": \"https://huggingface.co/ZuzeTt/Qwen3-VL-8B-Thinking-heretic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"Robertp423/Qwen3-VL-32B-Ievum-Merged\", \"link\": \"https://huggingface.co/Robertp423/Qwen3-VL-32B-Ievum-Merged\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"ZuzeTt/Qwen3-VL-2B-Thinking-heretic-Imatrix-GGUF\", \"link\": \"https://huggingface.co/ZuzeTt/Qwen3-VL-2B-Thinking-heretic-Imatrix-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\"}, {\"name\": \"ZuzeTt/Qwen3-VL-4B-Instruct-heretic-Imatrix-GGUF\", \"link\": \"https://huggingface.co/ZuzeTt/Qwen3-VL-4B-Instruct-heretic-Imatrix-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"ZuzeTt/Qwen3-VL-8B-Thinking-Imatrix-heretic\", \"link\": \"https://huggingface.co/ZuzeTt/Qwen3-VL-8B-Thinking-Imatrix-heretic\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"drgary/Qwen3-VL-2B-Instruct\", \"link\": \"https://huggingface.co/drgary/Qwen3-VL-2B-Instruct\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"n0kovo/Qwen3-VL-32B-Instruct-heretic-v2\", \"link\": \"https://huggingface.co/n0kovo/Qwen3-VL-32B-Instruct-heretic-v2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"AuricAlgos/qwen3-v1-8b-custom1\", \"link\": \"https://huggingface.co/AuricAlgos/qwen3-v1-8b-custom1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 16\"}]",
    "num_datasets": 5,
    "datasets_list": "xlangai/Jedi, IntelligenceLab/VideoHallu, turing-motors/MOMIJI, Hcompany/WebClick, osunlp/GUI-Drag-dataset",
    "datasets_links": "https://huggingface.co/datasets/xlangai/Jedi, https://huggingface.co/datasets/IntelligenceLab/VideoHallu, https://huggingface.co/datasets/turing-motors/MOMIJI, https://huggingface.co/datasets/Hcompany/WebClick, https://huggingface.co/datasets/osunlp/GUI-Drag-dataset",
    "datasets_detailed": "[{\"name\": \"xlangai/Jedi\", \"link\": \"https://huggingface.co/datasets/xlangai/Jedi\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 27\", \"size\": \"\"}, {\"name\": \"IntelligenceLab/VideoHallu\", \"link\": \"https://huggingface.co/datasets/IntelligenceLab/VideoHallu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\", \"size\": \"\"}, {\"name\": \"turing-motors/MOMIJI\", \"link\": \"https://huggingface.co/datasets/turing-motors/MOMIJI\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\", \"size\": \"\"}, {\"name\": \"Hcompany/WebClick\", \"link\": \"https://huggingface.co/datasets/Hcompany/WebClick\", \"task\": \"\", \"likes\": \"969\", \"downloads\": \"\", \"updated\": \"Jun 9\", \"size\": \"\"}, {\"name\": \"osunlp/GUI-Drag-dataset\", \"link\": \"https://huggingface.co/datasets/osunlp/GUI-Drag-dataset\", \"task\": \"\", \"likes\": \"26\", \"downloads\": \"\", \"updated\": \"Oct 16\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.11079",
    "first_seen_date": "2025-02-19",
    "title": "Phantom: Subject-consistent video generation via cross-modal alignment",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.11079Phantom: Subject-consistent video generation via cross-modal alignmentPublished on Feb 16\u00b7Submitted byLijie Liuon Feb 19#3 Paper of the dayUpvote59+51Authors:Lijie Liu,Tianxiang Ma,Bingchuan Li,Zhuowei Chen,Jiawei Liu,Qian He,Xinglong WuAbstractA unified video generation framework called Phantom integrates text and image inputs to produce subject-consistent videos, focusing on human generation and leveraging cross-modal alignment.AI-generated summaryThe continuous development of foundational models for video generation is\nevolving into various applications, with subject-consistent video generation\nstill in the exploratory stage. We refer to this as Subject-to-Video, which\nextracts subject elements from reference images and generates\nsubject-consistent video through textual instructions. We believe that the\nessence of subject-to-video lies in balancing the dual-modal prompts of text\nand image, thereby deeply and simultaneously aligning both text and visual\ncontent. To this end, we propose Phantom, a unified video generation framework\nfor both single and multi-subject references. Building on existingtext-to-videoandimage-to-videoarchitectures, we redesign the joint\ntext-image injection model and drive it to learncross-modal alignmentviatext-image-video triplet data. In particular, we emphasizesubject consistencyin human generation, covering existingID-preserving video generationwhile\noffering enhanced advantages. The project homepage is here\nhttps://phantom-video.github.io/Phantom/.View arXiv pageView PDFProject pageGitHub1.47kAdd to collectionCommunityliulj13Paper authorPaper submitterFeb 19\u2022edited Feb 19more demos can be found here:https://phantom-video.github.io/Phantom/The reference images in the above video are not complete, the modified version:See translation\ud83d\udc4d22\ud83d\udc4011+Replylibrarian-botFeb 20This is an automated message from theLibrarian Bot. I found the following pape",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/Phantom-video/Phantom",
    "hf_paper_url": "https://huggingface.co/papers/2502.11079",
    "arxiv_url": "https://arxiv.org/abs/2502.11079",
    "num_models": 2,
    "models_list": "ByteDance/BindWeave, vantagewithai/BindWeave-GGUF",
    "models_links": "https://huggingface.co/ByteDance/BindWeave, https://huggingface.co/vantagewithai/BindWeave-GGUF",
    "models_detailed": "[{\"name\": \"ByteDance/BindWeave\", \"link\": \"https://huggingface.co/ByteDance/BindWeave\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"vantagewithai/BindWeave-GGUF\", \"link\": \"https://huggingface.co/vantagewithai/BindWeave-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"about 1\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.13130",
    "first_seen_date": "2025-02-19",
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.13130Magma: A Foundation Model for Multimodal AI AgentsPublished on Feb 18\u00b7Submitted byAKon Feb 19Upvote58+50Authors:Jianwei Yang,Reuben Tan,Qianhui Wu,Ruijie Zheng,Baolin Peng,Yongyuan Liang,Yu Gu,Mu Cai,Seonghyeon Ye,Joel Jang,Yuquan Deng,Lars Liden,Jianfeng GaoAbstractMagma is a multimodal foundation model with both verbal intelligence and spatial-temporal intelligence, trained on diverse datasets to perform agentic tasks like UI navigation and robotic manipulation, outperforming specialized models.AI-generated summaryWe present Magma, a foundation model that serves multimodal AI agentic tasks\nin both the digital and physical worlds. Magma is a significant extension of\nvision-language (VL) models in that it not only retains the VL understanding\nability (verbal intelligence) of the latter, but is also equipped with the\nability to plan and act in the visual-spatial world (spatial-temporal\nintelligence) and complete agentic tasks ranging fromUI navigationto robot\nmanipulation. To endow the agentic capabilities, Magma is pretrained on large\namounts of heterogeneous datasets spanning from images, videos to robotics\ndata, where theactionable visual objects(e.g., clickable buttons in GUI) in\nimages are labeled bySet-of-Mark(SoM) foraction grounding, and the object\nmovements (e.g., the trace of human hands or robotic arms) in videos are\nlabeled byTrace-of-Mark(ToM) foraction planning. Extensive experiments show\nthat SoM and ToM reach great synergy and facilitate the acquisition ofspatial-temporal intelligencefor our Magma model, which is fundamental to a\nwide range of tasks as shown in Fig.1. In particular, Magma creates new\nstate-of-the-art results onUI navigationandrobotic manipulationtasks,\noutperforming previous models that are specifically tailored to these tasks. On\nimage and video-related multimodal tasks, Magma also compares favorably to\npopular large multimodal models tha",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/microsoft/Magma",
    "hf_paper_url": "https://huggingface.co/papers/2502.13130",
    "arxiv_url": "https://arxiv.org/abs/2502.13130",
    "num_models": 5,
    "models_list": "microsoft/Magma-8B, z-coder/Magma-8B-modified, Mungert/Magma-8B-GGUF, xuanzhaopeng/Magma-8B, alvarobartt/Magma-8B",
    "models_links": "https://huggingface.co/microsoft/Magma-8B, https://huggingface.co/z-coder/Magma-8B-modified, https://huggingface.co/Mungert/Magma-8B-GGUF, https://huggingface.co/xuanzhaopeng/Magma-8B, https://huggingface.co/alvarobartt/Magma-8B",
    "models_detailed": "[{\"name\": \"microsoft/Magma-8B\", \"link\": \"https://huggingface.co/microsoft/Magma-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"z-coder/Magma-8B-modified\", \"link\": \"https://huggingface.co/z-coder/Magma-8B-modified\", \"task\": \"\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Jun 3\"}, {\"name\": \"Mungert/Magma-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/Magma-8B-GGUF\", \"task\": \"\", \"likes\": \"446\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"xuanzhaopeng/Magma-8B\", \"link\": \"https://huggingface.co/xuanzhaopeng/Magma-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 28\"}, {\"name\": \"alvarobartt/Magma-8B\", \"link\": \"https://huggingface.co/alvarobartt/Magma-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\"}]",
    "num_datasets": 5,
    "datasets_list": "MagmaAI/Magma-OXE-ToM, MagmaAI/Magma-Mind2Web-SoM, MagmaAI/Magma-Video-ToM, MagmaAI/Magma-AITW-SoM, MagmaAI/Magma-820K",
    "datasets_links": "https://huggingface.co/datasets/MagmaAI/Magma-OXE-ToM, https://huggingface.co/datasets/MagmaAI/Magma-Mind2Web-SoM, https://huggingface.co/datasets/MagmaAI/Magma-Video-ToM, https://huggingface.co/datasets/MagmaAI/Magma-AITW-SoM, https://huggingface.co/datasets/MagmaAI/Magma-820K",
    "datasets_detailed": "[{\"name\": \"MagmaAI/Magma-OXE-ToM\", \"link\": \"https://huggingface.co/datasets/MagmaAI/Magma-OXE-ToM\", \"task\": \"\", \"likes\": \"525\", \"downloads\": \"\", \"updated\": \"Apr 6\", \"size\": \"\"}, {\"name\": \"MagmaAI/Magma-Mind2Web-SoM\", \"link\": \"https://huggingface.co/datasets/MagmaAI/Magma-Mind2Web-SoM\", \"task\": \"\", \"likes\": \"225\", \"downloads\": \"\", \"updated\": \"Apr 29\", \"size\": \"\"}, {\"name\": \"MagmaAI/Magma-Video-ToM\", \"link\": \"https://huggingface.co/datasets/MagmaAI/Magma-Video-ToM\", \"task\": \"\", \"likes\": \"171\", \"downloads\": \"\", \"updated\": \"Apr 12\", \"size\": \"\"}, {\"name\": \"MagmaAI/Magma-AITW-SoM\", \"link\": \"https://huggingface.co/datasets/MagmaAI/Magma-AITW-SoM\", \"task\": \"\", \"likes\": \"96\", \"downloads\": \"\", \"updated\": \"Apr 29\", \"size\": \"\"}, {\"name\": \"MagmaAI/Magma-820K\", \"link\": \"https://huggingface.co/datasets/MagmaAI/Magma-820K\", \"task\": \"\", \"likes\": \"28\", \"downloads\": \"\", \"updated\": \"Mar 9\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.12148",
    "first_seen_date": "2025-02-18",
    "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and\n  Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.12148HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and\n  GenerationPublished on Feb 17\u00b7Submitted byXinchen Zhangon Feb 18Upvote17+9Authors:Ling Yang,Xinchen Zhang,Ye Tian,Chenming Shang,Minghao Xu,Wentao Zhang,Bin CuiAbstractHermesFlow aligns multimodal understanding and generation through homologous preference data, demonstrating superior performance compared to existing methods.AI-generated summaryThe remarkable success of theautoregressive paradigmhas made significant\nadvancement inMultimodal Large Language Models (MLLMs), with powerful models\nlikeShow-o,TransfusionandEmu3achieving notable progress in unified image\nunderstanding and generation. For the first time, we uncover a common\nphenomenon: the understanding capabilities of MLLMs are typically stronger than\ntheir generative capabilities, with a significant gap between the two. Building\non this insight, we proposeHermesFlow, a simple yet general framework designed\nto seamlessly bridge the gap between understanding and generation in MLLMs.\nSpecifically, we take thehomologous dataas input to curate homologous\npreference data of both understanding and generation. ThroughPair-DPOandself-play iterative optimization,HermesFloweffectively aligns multimodal\nunderstanding and generation usinghomologous preference data. Extensive\nexperiments demonstrate the significant superiority of our approach over prior\nmethods, particularly in narrowing the gap between multimodal understanding and\ngeneration. These findings highlight the potential ofHermesFlowas a generalalignment frameworkfornext-generation multimodal foundation models. Code:\nhttps://github.com/Gen-Verse/HermesFlowView arXiv pageView PDFGitHub73Add to collectionCommunitycominPaper authorPaper submitterFeb 18code:https://github.com/Gen-Verse/HermesFlowSee translationReplylibrarian-botFeb 20This is an automated message from theLibrarian Bot. I found the ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/Gen-Verse/HermesFlow",
    "hf_paper_url": "https://huggingface.co/papers/2502.12148",
    "arxiv_url": "https://arxiv.org/abs/2502.12148",
    "num_models": 1,
    "models_list": "Gen-Verse/HermesFlow",
    "models_links": "https://huggingface.co/Gen-Verse/HermesFlow",
    "models_detailed": "[{\"name\": \"Gen-Verse/HermesFlow\", \"link\": \"https://huggingface.co/Gen-Verse/HermesFlow\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Feb 22\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.10140",
    "first_seen_date": "2025-02-17",
    "title": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource Languages",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.10140Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource LanguagesPublished on Feb 14\u00b7Submitted byDaniil Gurgurovon Feb 17Upvote9+1Authors:Daniil Gurgurov,Ivan Vykopal,Josef van Genabith,Simon OstermannAbstractAdapter-based methods improve multilingual models' performance on low-resource languages using minimal adaptation data, outperforming full fine-tuning with fewer parameters.AI-generated summaryLow-resource languages(LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smallermultilingual models(mLMs) such asmBERTandXLM-Roffer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigatesparameter-efficient adapter-based methodsfor adapting mLMs to\nLRLs, evaluating three architectures:Sequential Bottleneck, Invertible\nBottleneck, andLow-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification,sentiment analysis, andnamed entity recognition). We find thatSequential Bottleneckadapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto betterembedding alignmentand larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs likeLLaMA-3,GPT-4, andDeepSeek-R1-based distilled models. While adaptation improves\nperformance,pre-training data sizeremains the dominant factor, especially for\nlanguages ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/d-gurgurov/Knowledge-Driven-Adaptation-LLMs",
    "hf_paper_url": "https://huggingface.co/papers/2502.10140",
    "arxiv_url": "https://arxiv.org/abs/2502.10140",
    "num_models": 60,
    "models_list": "DGurgurov/mbert_slv-latn, DGurgurov/mbert_mlt-latn, DGurgurov/mbert_uzn-latn, DGurgurov/mbert_mar-deva, DGurgurov/mbert_lvs-latn, DGurgurov/mbert_mkd-cyrl, DGurgurov/mbert_ben-beng, DGurgurov/mbert_bod-tibt, DGurgurov/mbert_uig-arab, DGurgurov/mbert_yor-latn, DGurgurov/mbert_swh-latn, DGurgurov/mbert_dan-latn, DGurgurov/mbert_urd-arab, DGurgurov/mbert_azj-latn, DGurgurov/mbert_ckb-arab, DGurgurov/mbert_cym-latn, DGurgurov/mbert_jav-latn, DGurgurov/mbert_ell-grek, DGurgurov/mbert_amh-ethi, DGurgurov/mbert_bul-cyrl, DGurgurov/mbert_heb-hebr, DGurgurov/mbert_sun-latn, DGurgurov/mbert_kat-geor, DGurgurov/mbert_sin-sinh, DGurgurov/mbert_tel-telu, DGurgurov/mbert_slk-latn, DGurgurov/mbert_zsm-latn, DGurgurov/mbert_npi-deva, DGurgurov/mbert_tha-thai, DGurgurov/mbert_ron-latn, DGurgurov/xlm-r_slv-latn, DGurgurov/xlm-r_mlt-latn, DGurgurov/xlm-r_uzn-latn, DGurgurov/xlm-r_mar-deva, DGurgurov/xlm-r_lvs-latn, DGurgurov/xlm-r_mkd-cyrl, DGurgurov/xlm-r_ben-beng, DGurgurov/xlm-r_bod-tibt, DGurgurov/xlm-r_uig-arab, DGurgurov/xlm-r_yor-latn, DGurgurov/xlm-r_swh-latn, DGurgurov/xlm-r_dan-latn, DGurgurov/xlm-r_urd-arab, DGurgurov/xlm-r_azj-latn, DGurgurov/xlm-r_ckb-arab, DGurgurov/xlm-r_cym-latn, DGurgurov/xlm-r_jav-latn, DGurgurov/xlm-r_ell-grek, DGurgurov/xlm-r_amh-ethi, DGurgurov/xlm-r_bul-cyrl, DGurgurov/xlm-r_heb-hebr, DGurgurov/xlm-r_sun-latn, DGurgurov/xlm-r_kat-geor, DGurgurov/xlm-r_sin-sinh, DGurgurov/xlm-r_tel-telu, DGurgurov/xlm-r_slk-latn, DGurgurov/xlm-r_zsm-latn, DGurgurov/xlm-r_npi-deva, DGurgurov/xlm-r_tha-thai, DGurgurov/xlm-r_ron-latn",
    "models_links": "https://huggingface.co/DGurgurov/mbert_slv-latn, https://huggingface.co/DGurgurov/mbert_mlt-latn, https://huggingface.co/DGurgurov/mbert_uzn-latn, https://huggingface.co/DGurgurov/mbert_mar-deva, https://huggingface.co/DGurgurov/mbert_lvs-latn, https://huggingface.co/DGurgurov/mbert_mkd-cyrl, https://huggingface.co/DGurgurov/mbert_ben-beng, https://huggingface.co/DGurgurov/mbert_bod-tibt, https://huggingface.co/DGurgurov/mbert_uig-arab, https://huggingface.co/DGurgurov/mbert_yor-latn, https://huggingface.co/DGurgurov/mbert_swh-latn, https://huggingface.co/DGurgurov/mbert_dan-latn, https://huggingface.co/DGurgurov/mbert_urd-arab, https://huggingface.co/DGurgurov/mbert_azj-latn, https://huggingface.co/DGurgurov/mbert_ckb-arab, https://huggingface.co/DGurgurov/mbert_cym-latn, https://huggingface.co/DGurgurov/mbert_jav-latn, https://huggingface.co/DGurgurov/mbert_ell-grek, https://huggingface.co/DGurgurov/mbert_amh-ethi, https://huggingface.co/DGurgurov/mbert_bul-cyrl, https://huggingface.co/DGurgurov/mbert_heb-hebr, https://huggingface.co/DGurgurov/mbert_sun-latn, https://huggingface.co/DGurgurov/mbert_kat-geor, https://huggingface.co/DGurgurov/mbert_sin-sinh, https://huggingface.co/DGurgurov/mbert_tel-telu, https://huggingface.co/DGurgurov/mbert_slk-latn, https://huggingface.co/DGurgurov/mbert_zsm-latn, https://huggingface.co/DGurgurov/mbert_npi-deva, https://huggingface.co/DGurgurov/mbert_tha-thai, https://huggingface.co/DGurgurov/mbert_ron-latn, https://huggingface.co/DGurgurov/xlm-r_slv-latn, https://huggingface.co/DGurgurov/xlm-r_mlt-latn, https://huggingface.co/DGurgurov/xlm-r_uzn-latn, https://huggingface.co/DGurgurov/xlm-r_mar-deva, https://huggingface.co/DGurgurov/xlm-r_lvs-latn, https://huggingface.co/DGurgurov/xlm-r_mkd-cyrl, https://huggingface.co/DGurgurov/xlm-r_ben-beng, https://huggingface.co/DGurgurov/xlm-r_bod-tibt, https://huggingface.co/DGurgurov/xlm-r_uig-arab, https://huggingface.co/DGurgurov/xlm-r_yor-latn, https://huggingface.co/DGurgurov/xlm-r_swh-latn, https://huggingface.co/DGurgurov/xlm-r_dan-latn, https://huggingface.co/DGurgurov/xlm-r_urd-arab, https://huggingface.co/DGurgurov/xlm-r_azj-latn, https://huggingface.co/DGurgurov/xlm-r_ckb-arab, https://huggingface.co/DGurgurov/xlm-r_cym-latn, https://huggingface.co/DGurgurov/xlm-r_jav-latn, https://huggingface.co/DGurgurov/xlm-r_ell-grek, https://huggingface.co/DGurgurov/xlm-r_amh-ethi, https://huggingface.co/DGurgurov/xlm-r_bul-cyrl, https://huggingface.co/DGurgurov/xlm-r_heb-hebr, https://huggingface.co/DGurgurov/xlm-r_sun-latn, https://huggingface.co/DGurgurov/xlm-r_kat-geor, https://huggingface.co/DGurgurov/xlm-r_sin-sinh, https://huggingface.co/DGurgurov/xlm-r_tel-telu, https://huggingface.co/DGurgurov/xlm-r_slk-latn, https://huggingface.co/DGurgurov/xlm-r_zsm-latn, https://huggingface.co/DGurgurov/xlm-r_npi-deva, https://huggingface.co/DGurgurov/xlm-r_tha-thai, https://huggingface.co/DGurgurov/xlm-r_ron-latn",
    "models_detailed": "[{\"name\": \"DGurgurov/mbert_slv-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_slv-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_mlt-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_mlt-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_uzn-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_uzn-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_mar-deva\", \"link\": \"https://huggingface.co/DGurgurov/mbert_mar-deva\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_lvs-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_lvs-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_mkd-cyrl\", \"link\": \"https://huggingface.co/DGurgurov/mbert_mkd-cyrl\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_ben-beng\", \"link\": \"https://huggingface.co/DGurgurov/mbert_ben-beng\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_bod-tibt\", \"link\": \"https://huggingface.co/DGurgurov/mbert_bod-tibt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_uig-arab\", \"link\": \"https://huggingface.co/DGurgurov/mbert_uig-arab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_yor-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_yor-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_swh-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_swh-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_dan-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_dan-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_urd-arab\", \"link\": \"https://huggingface.co/DGurgurov/mbert_urd-arab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_azj-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_azj-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_ckb-arab\", \"link\": \"https://huggingface.co/DGurgurov/mbert_ckb-arab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_cym-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_cym-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_jav-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_jav-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_ell-grek\", \"link\": \"https://huggingface.co/DGurgurov/mbert_ell-grek\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_amh-ethi\", \"link\": \"https://huggingface.co/DGurgurov/mbert_amh-ethi\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_bul-cyrl\", \"link\": \"https://huggingface.co/DGurgurov/mbert_bul-cyrl\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_heb-hebr\", \"link\": \"https://huggingface.co/DGurgurov/mbert_heb-hebr\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_sun-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_sun-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_kat-geor\", \"link\": \"https://huggingface.co/DGurgurov/mbert_kat-geor\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_sin-sinh\", \"link\": \"https://huggingface.co/DGurgurov/mbert_sin-sinh\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_tel-telu\", \"link\": \"https://huggingface.co/DGurgurov/mbert_tel-telu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_slk-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_slk-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_zsm-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_zsm-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_npi-deva\", \"link\": \"https://huggingface.co/DGurgurov/mbert_npi-deva\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_tha-thai\", \"link\": \"https://huggingface.co/DGurgurov/mbert_tha-thai\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/mbert_ron-latn\", \"link\": \"https://huggingface.co/DGurgurov/mbert_ron-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_slv-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_slv-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_mlt-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_mlt-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_uzn-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_uzn-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_mar-deva\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_mar-deva\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_lvs-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_lvs-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_mkd-cyrl\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_mkd-cyrl\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_ben-beng\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_ben-beng\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_bod-tibt\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_bod-tibt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_uig-arab\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_uig-arab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_yor-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_yor-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_swh-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_swh-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_dan-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_dan-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_urd-arab\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_urd-arab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_azj-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_azj-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_ckb-arab\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_ckb-arab\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_cym-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_cym-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_jav-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_jav-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_ell-grek\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_ell-grek\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_amh-ethi\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_amh-ethi\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_bul-cyrl\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_bul-cyrl\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_heb-hebr\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_heb-hebr\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_sun-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_sun-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_kat-geor\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_kat-geor\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_sin-sinh\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_sin-sinh\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_tel-telu\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_tel-telu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_slk-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_slk-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_zsm-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_zsm-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_npi-deva\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_npi-deva\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_tha-thai\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_tha-thai\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"DGurgurov/xlm-r_ron-latn\", \"link\": \"https://huggingface.co/DGurgurov/xlm-r_ron-latn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.08127",
    "first_seen_date": "2025-02-13",
    "title": "Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.08127Fino1: On the Transferability of Reasoning Enhanced LLMs to FinancePublished on Feb 12\u00b7Submitted byJimin Huangon Feb 13#2 Paper of the day\u00b7The Fin AIUpvote58+50Authors:Lingfei Qian,Weipeng Zhou,Yan Wang,Xueqing Peng,Jimin Huang,Qianqian XieAbstractA study evaluates 16 large language models on complex financial tasks, finding that domain-specific CoT fine-tuning and reinforcement learning improve performance and highlight the need for further research on long-context and multi-table reasoning.AI-generated summaryRecent advancements inlarge language models(LLMs) have shown strong general\nreasoning abilities, yet their effectiveness infinancial reasoningremains\nunderexplored. In this study, we comprehensively evaluate 16 powerful reasoning\nand general LLMs on three complex financial tasks involving financial text,\ntabular data, and equations, assessing numerical reasoning, tabular\ninterpretation, financial terminology comprehension,long-context processing,\nand equation-based problem solving. Our results show that while better datasets\nand pretraining improvefinancial reasoning, general enhancements like CoT\nfine-tuning do not always yield consistent gains. Moreover, all reasoning\nstrategies face challenges in improving performance on long-context and\nmulti-table tasks. To address these limitations, we develop a financial\nreasoning-enhanced model based on Llama-3.1-8B-Instruct, byCoT fine-tuningandreinforcement learningwithdomain-specific reasoning paths. Even with simple\nfine-tuning with one financial dataset, our model achieves a consistent 10%\nperformance improvement across tasks, surpassing all 8B models and even\nLlama3-70B-Instruct and Llama3.1-70B-Instruct on average. Our results highlight\nthe need for domain-specific adaptations in financial tasks, emphasizing future\ndirections such asmulti-table reasoning,long-context processing, and\nfinancial terminology comprehensi",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/the-finai/fino1",
    "hf_paper_url": "https://huggingface.co/papers/2502.08127",
    "arxiv_url": "https://arxiv.org/abs/2502.08127",
    "num_models": 8,
    "models_list": "TheFinAI/Fino1-8B, TheFinAI/Fin-o1-8B, TheFinAI/Fin-o1-14B, ThinkTim21/FinPlan-1, SandLogicTechnologies/Fino1-8B-GGUF, TheFinAI/Fino1-14B, dinalad0/my-fino1-model, khazarai/Fino1-4B",
    "models_links": "https://huggingface.co/TheFinAI/Fino1-8B, https://huggingface.co/TheFinAI/Fin-o1-8B, https://huggingface.co/TheFinAI/Fin-o1-14B, https://huggingface.co/ThinkTim21/FinPlan-1, https://huggingface.co/SandLogicTechnologies/Fino1-8B-GGUF, https://huggingface.co/TheFinAI/Fino1-14B, https://huggingface.co/dinalad0/my-fino1-model, https://huggingface.co/khazarai/Fino1-4B",
    "models_detailed": "[{\"name\": \"TheFinAI/Fino1-8B\", \"link\": \"https://huggingface.co/TheFinAI/Fino1-8B\", \"task\": \"Text Generation\", \"likes\": \"128\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"TheFinAI/Fin-o1-8B\", \"link\": \"https://huggingface.co/TheFinAI/Fin-o1-8B\", \"task\": \"Text Generation\", \"likes\": \"444\", \"downloads\": \"\", \"updated\": \"May 16\"}, {\"name\": \"TheFinAI/Fin-o1-14B\", \"link\": \"https://huggingface.co/TheFinAI/Fin-o1-14B\", \"task\": \"Text Generation\", \"likes\": \"197\", \"downloads\": \"\", \"updated\": \"May 16\"}, {\"name\": \"ThinkTim21/FinPlan-1\", \"link\": \"https://huggingface.co/ThinkTim21/FinPlan-1\", \"task\": \"Text Generation\", \"likes\": \"101\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"SandLogicTechnologies/Fino1-8B-GGUF\", \"link\": \"https://huggingface.co/SandLogicTechnologies/Fino1-8B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"TheFinAI/Fino1-14B\", \"link\": \"https://huggingface.co/TheFinAI/Fino1-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 28\"}, {\"name\": \"dinalad0/my-fino1-model\", \"link\": \"https://huggingface.co/dinalad0/my-fino1-model\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 2\"}, {\"name\": \"khazarai/Fino1-4B\", \"link\": \"https://huggingface.co/khazarai/Fino1-4B\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Sep 25\"}]",
    "num_datasets": 3,
    "datasets_list": "TheFinAI/FinCoT, TheFinAI/Fino1_Reasoning_Path_FinQA, TheFinAI/Fino1_Reasoning_Path_FinQA_v2",
    "datasets_links": "https://huggingface.co/datasets/TheFinAI/FinCoT, https://huggingface.co/datasets/TheFinAI/Fino1_Reasoning_Path_FinQA, https://huggingface.co/datasets/TheFinAI/Fino1_Reasoning_Path_FinQA_v2",
    "datasets_detailed": "[{\"name\": \"TheFinAI/FinCoT\", \"link\": \"https://huggingface.co/datasets/TheFinAI/FinCoT\", \"task\": \"\", \"likes\": \"920\", \"downloads\": \"\", \"updated\": \"Jul 27\", \"size\": \"\"}, {\"name\": \"TheFinAI/Fino1_Reasoning_Path_FinQA\", \"link\": \"https://huggingface.co/datasets/TheFinAI/Fino1_Reasoning_Path_FinQA\", \"task\": \"\", \"likes\": \"490\", \"downloads\": \"\", \"updated\": \"Feb 26\", \"size\": \"\"}, {\"name\": \"TheFinAI/Fino1_Reasoning_Path_FinQA_v2\", \"link\": \"https://huggingface.co/datasets/TheFinAI/Fino1_Reasoning_Path_FinQA_v2\", \"task\": \"\", \"likes\": \"176\", \"downloads\": \"\", \"updated\": \"Mar 28\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.04223",
    "first_seen_date": "2025-02-12",
    "title": "\u00c9clair -- Extracting Content and Layout with Integrated Reading Order\n  for Documents",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.04223\u00c9clair -- Extracting Content and Layout with Integrated Reading Order\n  for DocumentsPublished on Feb 6\u00b7Submitted byJarno Sepp\u00e4nenon Feb 12Upvote10+2Authors:Ilia Karmanov,Amala Sanjay Deshmukh,Lukas Voegtle,Philipp Fischer,Kateryna Chumachenko,Timo Roman,Jarno Sepp\u00e4nen,Jupinder Parmar,Joseph Jennings,Andrew Tao,Karan SapraAbstractEclair is a general-purpose OCR tool designed to extract formatted text with semantic information for document-level understanding, achieving state-of-the-art accuracy on diverse benchmarks.AI-generated summaryOptical Character Recognition(OCR) technology is widely used to extract text\nfrom images of documents, facilitating efficient digitization and data\nretrieval. However, merely extracting text is insufficient when dealing with\ncomplex documents. Fully comprehending such documents requires an understanding\nof their structure -- including formatting, formulas, tables, and the reading\norder of multiple blocks and columns across multiple pages -- as well as\nsemantic information for detecting elements like footnotes and image captions.\nThis comprehensive understanding is crucial for downstream tasks such as\nretrieval, document question answering, and data curation for training Large\nLanguage Models (LLMs) andVision Language Models(VLMs). To address this, we\nintroduce \\'Eclair, a general-purpose text-extraction tool specifically\ndesigned to process a wide range of document types. Given an image, \\'Eclair is\nable to extractformatted textinreading order, along withbounding boxesand\ntheir correspondingsemantic classes. To thoroughly evaluate these novel\ncapabilities, we introduce our diverse human-annotated benchmark fordocument-level OCRandsemantic classification. \\'Eclair achieves\nstate-of-the-art accuracy on this benchmark, outperforming other methods across\nkey metrics. Additionally, we evaluate \\'Eclair on established benchmarks,\ndemonstrating i",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.04223",
    "arxiv_url": "https://arxiv.org/abs/2502.04223",
    "num_models": 1,
    "models_list": "Edens-Gate/VLM_Nvidia-backup",
    "models_links": "https://huggingface.co/Edens-Gate/VLM_Nvidia-backup",
    "models_detailed": "[{\"name\": \"Edens-Gate/VLM_Nvidia-backup\", \"link\": \"https://huggingface.co/Edens-Gate/VLM_Nvidia-backup\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}]",
    "num_datasets": 1,
    "datasets_list": "nvidia/Llama-Nemotron-VLM-Dataset-v1",
    "datasets_links": "https://huggingface.co/datasets/nvidia/Llama-Nemotron-VLM-Dataset-v1",
    "datasets_detailed": "[{\"name\": \"nvidia/Llama-Nemotron-VLM-Dataset-v1\", \"link\": \"https://huggingface.co/datasets/nvidia/Llama-Nemotron-VLM-Dataset-v1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 22\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.05878",
    "first_seen_date": "2025-02-12",
    "title": "Retrieval-augmented Large Language Models for Financial Time Series\n  Forecasting",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.05878Retrieval-augmented Large Language Models for Financial Time Series\n  ForecastingPublished on Feb 9\u00b7Submitted byJimin Huangon Feb 12\u00b7The Fin AIUpvote41+33Authors:Mengxi Xiao,Zihao Jiang,Lingfei Qian,Zhengyu Chen,Yueru He,Yijing Xu,Yuecheng Jiang,Dong Li,Ruey-Ling Weng,Min Peng,Jimin Huang,Sophia Ananiadou,Qianqian XieAbstractA retrieval-augmented generation framework enhances stock prediction accuracy by fine-tuning a large language model and integrating it with a novel retrieval method on specialized datasets.AI-generated summaryStock movement prediction, a fundamental task in financial time-series\nforecasting, requires identifying and retrieving critical influencing factors\nfrom vast amounts of time-series data. However, existing text-trained or\nnumericsimilarity-based retrieval methods fall short in handling complex\nfinancial analysis. To address this, we propose the first retrieval-augmented\ngeneration (RAG) framework for financial time-series forecasting, featuring\nthree key innovations: afine-tuned1B parameterlarge language model(StockLLM) as the backbone, a novelcandidate selectionmethod leveraging LLM\nfeedback, and atraining objectivethat maximizessimilaritybetween queries\nand historically significant sequences. This enables ourretriever,FinSeer, to\nuncover meaningful patterns while minimizing noise in complex financial data.\nWe also construct new datasets integrating financial indicators and historical\nstock prices to trainFinSeerand ensure robust evaluation. Experimental\nresults demonstrate that our RAG framework outperforms bare StockLLM and random\nretrieval, highlighting its effectiveness, whileFinSeersurpasses existing\nretrieval methods, achieving an 8\\% higher accuracy onBIGDATA22and retrieving\nmore impactful sequences. This work underscores the importance of tailored\nretrieval models infinancial forecastingand provides a novel framework for\nfuture research",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.05878",
    "arxiv_url": "https://arxiv.org/abs/2502.05878",
    "num_models": 3,
    "models_list": "TheFinAI/StockLLM, TheFinAI/FinSeer, dinalad0/my-LLM_RAG-model",
    "models_links": "https://huggingface.co/TheFinAI/StockLLM, https://huggingface.co/TheFinAI/FinSeer, https://huggingface.co/dinalad0/my-LLM_RAG-model",
    "models_detailed": "[{\"name\": \"TheFinAI/StockLLM\", \"link\": \"https://huggingface.co/TheFinAI/StockLLM\", \"task\": \"\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Mar 15\"}, {\"name\": \"TheFinAI/FinSeer\", \"link\": \"https://huggingface.co/TheFinAI/FinSeer\", \"task\": \"\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Mar 15\"}, {\"name\": \"dinalad0/my-LLM_RAG-model\", \"link\": \"https://huggingface.co/dinalad0/my-LLM_RAG-model\", \"task\": \"\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"May 2\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.06807",
    "first_seen_date": "2025-02-12",
    "title": "Competitive Programming with Large Reasoning Models",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.06807Competitive Programming with Large Reasoning ModelsPublished on Feb 3\u00b7Submitted byAKon Feb 12#2 Paper of the dayUpvote68+60Authors:OpenAI,Ahmed El-Kishky,Alexander Wei,Andre Saraiva,Borys Minaev,Daniel Selsam,David Dohan,Francis Song,Hunter Lightman,Ignasi Clavera,Jakub Pachocki,Jerry Tworek,Lorenz Kuhn,Lukasz Kaiser,Mark Chen,Max Schwarzer,Mostafa Rohaninejad,Nat McAleese,o3 contributors,Oleg M\u00fcrk,Rhythm Garg,Rui Shu+3 authorsAbstractGeneral-purpose reinforcement learning applied to large language models outperforms domain-specific systems in complex coding and reasoning tasks, achieving top results in competitions without hand-crafted strategies.AI-generated summaryWe show thatreinforcement learningapplied tolarge language models(LLMs)\nsignificantly boosts performance on complex coding and reasoning tasks.\nAdditionally, we compare twogeneral-purpose reasoning models- OpenAI o1 and\nan early checkpoint of o3 - with adomain-specific system, o1-ioi, which useshand-engineered inference strategiesdesigned for competing in the 2024International Olympiad in Informatics(IOI). We competed live atIOI2024 with\no1-ioiand, using hand-crafted test-time strategies, placed in the 49th\npercentile. Under relaxed competition constraints, o1-ioiachieved a gold\nmedal. However, when evaluating later models such as o3, we find that o3\nachieves gold without hand-crafted domain-specific strategies or relaxed\nconstraints. Our findings show that although specialized pipelines such as\no1-ioiyield solid improvements, the scaled-up, general-purpose o3 model\nsurpasses those results without relying on hand-crafted inference heuristics.\nNotably, o3 achieves agold medalat the 2024IOIand obtains a Codeforces\nrating on par with elite human competitors. Overall, these results indicate\nthat scaling general-purposereinforcement learning, rather than relying on\ndomain-specific techniques, offers a robust path",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.06807",
    "arxiv_url": "https://arxiv.org/abs/2502.06807",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "open-r1/ioi",
    "datasets_links": "https://huggingface.co/datasets/open-r1/ioi",
    "datasets_detailed": "[{\"name\": \"open-r1/ioi\", \"link\": \"https://huggingface.co/datasets/open-r1/ioi\", \"task\": \"\", \"likes\": \"270\", \"downloads\": \"\", \"updated\": \"Mar 12\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.06772",
    "first_seen_date": "2025-02-11",
    "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.06772ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought TemplatesPublished on Feb 10\u00b7Submitted byLing Yangon Feb 11Upvote22+14Authors:Ling Yang,Zhaochen Yu,Bin Cui,Mengdi WangAbstractHierarchical reasoning with LLMs using scaled thought templates improves mathematical reasoning and outperforms existing models on benchmarks like MATH and AIME.AI-generated summaryWe present thathierarchical LLM reasoningvia scalingthought templatescan\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train ourReasonFlux-32Bmodel with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-levelthought templatescapable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence ofthought templatesinstead of long CoTs, optimizing a\nbase LLM to plan out an optimaltemplate trajectoryfor gradually handling\ncomplex problems; (iii) a brand newinference scaling systemthat enableshierarchical LLM reasoningby adaptively scalingthought templatesat inference\ntime. With atemplate trajectorycontaining sequentialthought templates, ourReasonFlux-32Bsignificantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on theMATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On theUSA Math Olympiad(AIME) benchmark,ReasonFlux-32Bsolves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFluxView arXiv pageView PDFGitHub510autoAdd to collectionCommunityLingaaaaaaaPaper authorPaper submitterFeb 11Code:https://github.com/Gen-Verse/ReasonFluxSee translationReplyLingaaaaaaaPaper authorPaper submitterFeb 11See translationReplyli",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/gen-verse/reasonflux",
    "hf_paper_url": "https://huggingface.co/papers/2502.06772",
    "arxiv_url": "https://arxiv.org/abs/2502.06772",
    "num_models": 3,
    "models_list": "Gen-Verse/ReasonFlux-F1, Gen-Verse/ReasonFlux-F1-14B, Gen-Verse/ReasonFlux-F1-7B",
    "models_links": "https://huggingface.co/Gen-Verse/ReasonFlux-F1, https://huggingface.co/Gen-Verse/ReasonFlux-F1-14B, https://huggingface.co/Gen-Verse/ReasonFlux-F1-7B",
    "models_detailed": "[{\"name\": \"Gen-Verse/ReasonFlux-F1\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-F1\", \"task\": \"Text Generation\", \"likes\": \"45\", \"downloads\": \"\", \"updated\": \"Mar 22\"}, {\"name\": \"Gen-Verse/ReasonFlux-F1-14B\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-F1-14B\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Mar 22\"}, {\"name\": \"Gen-Verse/ReasonFlux-F1-7B\", \"link\": \"https://huggingface.co/Gen-Verse/ReasonFlux-F1-7B\", \"task\": \"Text Generation\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Mar 22\"}]",
    "num_datasets": 5,
    "datasets_list": "Gen-Verse/ReasonFlux_SFT_15k, Gen-Verse/ReasonFlux-V2-SFT, Gen-Verse/ReasonFlux-V2-Reasoner-DPO, Gen-Verse/ReasonFlux-V2-DPO, Gen-Verse/ReasonFlux-F1-SFT",
    "datasets_links": "https://huggingface.co/datasets/Gen-Verse/ReasonFlux_SFT_15k, https://huggingface.co/datasets/Gen-Verse/ReasonFlux-V2-SFT, https://huggingface.co/datasets/Gen-Verse/ReasonFlux-V2-Reasoner-DPO, https://huggingface.co/datasets/Gen-Verse/ReasonFlux-V2-DPO, https://huggingface.co/datasets/Gen-Verse/ReasonFlux-F1-SFT",
    "datasets_detailed": "[{\"name\": \"Gen-Verse/ReasonFlux_SFT_15k\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/ReasonFlux_SFT_15k\", \"task\": \"\", \"likes\": \"115\", \"downloads\": \"\", \"updated\": \"Mar 23\", \"size\": \"\"}, {\"name\": \"Gen-Verse/ReasonFlux-V2-SFT\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/ReasonFlux-V2-SFT\", \"task\": \"\", \"likes\": \"60\", \"downloads\": \"\", \"updated\": \"Aug 7\", \"size\": \"\"}, {\"name\": \"Gen-Verse/ReasonFlux-V2-Reasoner-DPO\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/ReasonFlux-V2-Reasoner-DPO\", \"task\": \"\", \"likes\": \"27\", \"downloads\": \"\", \"updated\": \"Aug 7\", \"size\": \"\"}, {\"name\": \"Gen-Verse/ReasonFlux-V2-DPO\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/ReasonFlux-V2-DPO\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Aug 7\", \"size\": \"\"}, {\"name\": \"Gen-Verse/ReasonFlux-F1-SFT\", \"link\": \"https://huggingface.co/datasets/Gen-Verse/ReasonFlux-F1-SFT\", \"task\": \"\", \"likes\": \"21\", \"downloads\": \"\", \"updated\": \"Mar 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.06781",
    "first_seen_date": "2025-02-11",
    "title": "Exploring the Limit of Outcome Reward for Learning Mathematical\n  Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.06781Exploring the Limit of Outcome Reward for Learning Mathematical\n  ReasoningPublished on Feb 10\u00b7Submitted byYuzhe Guon Feb 11#3 Paper of the dayUpvote59+51Authors:Chengqi Lyu,Songyang Gao,Yuzhe Gu,Wenwei Zhang,Jianfei Gao,Kuikun Liu,Ziyi Wang,Shuaibin Li,Qian Zhao,Haian Huang,Weihan Cao,Jiangning Liu,Hongwei Liu,Junnan Liu,Songyang Zhang,Dahua Lin,Kai ChenAbstractA novel reinforcement learning framework, OREAL, using outcome rewards achieves high accuracy in mathematical reasoning tasks with smaller models compared to larger models trained using distillation.AI-generated summaryReasoning abilities, especially those for solving complex math problems, are\ncrucial components of general intelligence. Recent advances by proprietary\ncompanies, such as o-series models of OpenAI, have made remarkable progress on\nreasoning tasks. However, the complete technical details remain unrevealed, and\nthe techniques that are believed certainly to be adopted are only reinforcement\nlearning (RL) and the long chain of thoughts. This paper proposes a new RL\nframework, termedOREAL, to pursue the performance limit that can be achieved\nthrough Outcome REwArd-based reinforcement\nLearning for mathematical reasoning tasks, where only binary outcome\nrewards are easily accessible. We theoretically prove thatbehavior cloningon\npositive trajectories frombest-of-N(BoN) sampling is sufficient to learn theKL-regularized optimal policyin binary feedback environments. This formulation\nfurther implies that the rewards of negative samples should be reshaped to\nensure the gradient consistency between positive and negative samples. To\nalleviate the long-existing difficulties brought by sparse rewards in RL, which\nare even exacerbated by the partial correctness of the long chain of thought\nfor reasoning tasks, we further apply atoken-level reward modelto sample\nimportant tokens in reasoning trajectories for learni",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/internlm/oreal",
    "hf_paper_url": "https://huggingface.co/papers/2502.06781",
    "arxiv_url": "https://arxiv.org/abs/2502.06781",
    "num_models": 6,
    "models_list": "internlm/OREAL-32B, internlm/OREAL-7B, internlm/OREAL-DeepSeek-R1-Distill-Qwen-7B, internlm/OREAL-32B-SFT, internlm/OREAL-7B-SFT, Apel-sin/OREAL-32B-exl2",
    "models_links": "https://huggingface.co/internlm/OREAL-32B, https://huggingface.co/internlm/OREAL-7B, https://huggingface.co/internlm/OREAL-DeepSeek-R1-Distill-Qwen-7B, https://huggingface.co/internlm/OREAL-32B-SFT, https://huggingface.co/internlm/OREAL-7B-SFT, https://huggingface.co/Apel-sin/OREAL-32B-exl2",
    "models_detailed": "[{\"name\": \"internlm/OREAL-32B\", \"link\": \"https://huggingface.co/internlm/OREAL-32B\", \"task\": \"Text Generation\", \"likes\": \"158\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"internlm/OREAL-7B\", \"link\": \"https://huggingface.co/internlm/OREAL-7B\", \"task\": \"Text Generation\", \"likes\": \"85\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"internlm/OREAL-DeepSeek-R1-Distill-Qwen-7B\", \"link\": \"https://huggingface.co/internlm/OREAL-DeepSeek-R1-Distill-Qwen-7B\", \"task\": \"Text Generation\", \"likes\": \"62\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"internlm/OREAL-32B-SFT\", \"link\": \"https://huggingface.co/internlm/OREAL-32B-SFT\", \"task\": \"Question Answering\", \"likes\": \"133\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"internlm/OREAL-7B-SFT\", \"link\": \"https://huggingface.co/internlm/OREAL-7B-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"Apel-sin/OREAL-32B-exl2\", \"link\": \"https://huggingface.co/Apel-sin/OREAL-32B-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 26\"}]",
    "num_datasets": 1,
    "datasets_list": "internlm/OREAL-RL-Prompts",
    "datasets_links": "https://huggingface.co/datasets/internlm/OREAL-RL-Prompts",
    "datasets_detailed": "[{\"name\": \"internlm/OREAL-RL-Prompts\", \"link\": \"https://huggingface.co/datasets/internlm/OREAL-RL-Prompts\", \"task\": \"\", \"likes\": \"209\", \"downloads\": \"\", \"updated\": \"Feb 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.05178",
    "first_seen_date": "2025-02-10",
    "title": "QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive\n  Multimodal Understanding and Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.05178QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive\n  Multimodal Understanding and GenerationPublished on Feb 7\u00b7Submitted byYue Zhaoon Feb 10Upvote10+2Authors:Yue Zhao,Fuzhao Xue,Scott Reed,Linxi Fan,Yuke Zhu,Jan Kautz,Zhiding Yu,Philipp Kr\u00e4henb\u00fchl,De-An HuangAbstractQuantized Language-Image Pretraining (QLIP) integrates binary-spherical-quantization for image reconstructions and language-image alignment, enhancing multimodal understanding and text-conditioned image generation.AI-generated summaryWe introduce Quantized Language-Image Pretraining (QLIP), a visual\ntokenization method that combines state-of-the-artreconstructionquality with\nstate-of-the-art zero-shot image understanding. QLIP trains abinary-spherical-quantization-basedautoencoderwithreconstructionandlanguage-image alignmentobjectives. We are the first to show that the two\nobjectives do not need to be at odds. We balance the two loss terms dynamically\nduring training and show that a two-stage training pipeline effectively mixes\nthe large-batch requirements of image-language pre-training with the memory\nbottleneck imposed by thereconstructionobjective. We validate the\neffectiveness of QLIP formultimodal understandingand text-conditioned image\ngeneration with a single model. Specifically, QLIP serves as a drop-in\nreplacement for the visual encoder forLLaVAand the image tokenizer forLlamaGenwith comparable or even better performance. Finally, we demonstrate\nthat QLIP enables a unifiedmixed-modality auto-regressive modelfor\nunderstanding and generation.View arXiv pageView PDFProject pageGitHub94autoAdd to collectionCommunityzhaoyue-zephyrusPaper authorPaper submitterFeb 10Project page:https://nvlabs.github.io/QLIP/. Models and checkpoints:https://huggingface.co/collections/nvidia/qlip-67a478054fce07a7be99d5cd.See translationReplylibrarian-botFeb 11This is an automated message from theLibrarian Bot. I ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/NVlabs/QLIP",
    "hf_paper_url": "https://huggingface.co/papers/2502.05178",
    "arxiv_url": "https://arxiv.org/abs/2502.05178",
    "num_models": 6,
    "models_list": "nvidia/QLIP-L-14-392, nvidia/QLIP-B-8-256, nvidia/QLIP-B-16-256, lavinal712/QLIP-B-16-256, lavinal712/QLIP-B-8-256, lavinal712/QLIP-L-14-392",
    "models_links": "https://huggingface.co/nvidia/QLIP-L-14-392, https://huggingface.co/nvidia/QLIP-B-8-256, https://huggingface.co/nvidia/QLIP-B-16-256, https://huggingface.co/lavinal712/QLIP-B-16-256, https://huggingface.co/lavinal712/QLIP-B-8-256, https://huggingface.co/lavinal712/QLIP-L-14-392",
    "models_detailed": "[{\"name\": \"nvidia/QLIP-L-14-392\", \"link\": \"https://huggingface.co/nvidia/QLIP-L-14-392\", \"task\": \"\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Feb 10\"}, {\"name\": \"nvidia/QLIP-B-8-256\", \"link\": \"https://huggingface.co/nvidia/QLIP-B-8-256\", \"task\": \"\", \"likes\": \"40\", \"downloads\": \"\", \"updated\": \"Feb 10\"}, {\"name\": \"nvidia/QLIP-B-16-256\", \"link\": \"https://huggingface.co/nvidia/QLIP-B-16-256\", \"task\": \"\", \"likes\": \"45\", \"downloads\": \"\", \"updated\": \"Feb 10\"}, {\"name\": \"lavinal712/QLIP-B-16-256\", \"link\": \"https://huggingface.co/lavinal712/QLIP-B-16-256\", \"task\": \"\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Apr 1\"}, {\"name\": \"lavinal712/QLIP-B-8-256\", \"link\": \"https://huggingface.co/lavinal712/QLIP-B-8-256\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 1\"}, {\"name\": \"lavinal712/QLIP-L-14-392\", \"link\": \"https://huggingface.co/lavinal712/QLIP-L-14-392\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 1\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2502.04128",
    "first_seen_date": "2025-02-07",
    "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.04128Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech SynthesisPublished on Feb 6\u00b7Submitted byAKon Feb 7Upvote27+19Authors:Zhen Ye,Xinfa Zhu,Chi-Min Chan,Xinsheng Wang,Xu Tan,Jiahe Lei,Yi Peng,Haohe Liu,Yizhu Jin,Zheqi DAI,Hongzhan Lin,Jianyi Chen,Xingjian Du,Liumeng Xue,Yunlin Chen,Zhifei Li,Lei Xie,Qiuqiang Kong,Yike Guo,Wei XueAbstractA single-layer vector quantizer codec and Transformer-based framework for text-to-speech synthesis improves speech naturalness and emotional expressiveness with scalable compute resources.AI-generated summaryRecent advances in text-basedlarge language models(LLMs), particularly in\ntheGPT seriesand the o1 model, have demonstrated the effectiveness of scaling\nboth training-time andinference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g.,diffusion modelsafter LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nandinference-time computeforspeech synthesis. Second, we propose a simple\nframework Llasa forspeech synthesisthat employs a single-layer vector\nquantizer (VQ) codec and a singleTransformer architectureto fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurateprosody patterns.\nFurthermore, from the perspective of scalinginference-time compute, we employspeech understanding modelsasverifiersduring the search, finding that\nscalinginference-time computeshifts thesampling modestoward the preferences\nof specificverifiers, thereby improvingemotional expressiveness, timbre\nconsistency, andcontent accuracy. In addition, we released the ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/zhenye234/LLaSA_training",
    "hf_paper_url": "https://huggingface.co/papers/2502.04128",
    "arxiv_url": "https://arxiv.org/abs/2502.04128",
    "num_models": 18,
    "models_list": "HKUSTAudio/Llasa-3B, HKUSTAudio/Llasa-1B, HKUSTAudio/Llasa-8B, HKUSTAudio/xcodec2, HKUSTAudio/Llasa-1B-Multilingual, bosonai/higgs-audio-v2-tokenizer, hf-audio/xcodec2, GameRuiner/Llasa-1B, HKUSTAudio/Llasa-1B-Preserve-TextChat, HKUSTAudio/Llasa-3B-Preserve-TextChat, HKUSTAudio/Llasa-1B-two-speakers-kore-puck, HKUSTAudio/Llasa-1B-multi-speakers-genshin-zh-en-ja-ko, Gapeleon/llasa-3b, unsloth/Llasa-3B, unsloth/Llasa-1B, litagin/xcodec2, Prince-1/Llasa-3B, PierrunoYT/higgs-audio-v2-tokenizer",
    "models_links": "https://huggingface.co/HKUSTAudio/Llasa-3B, https://huggingface.co/HKUSTAudio/Llasa-1B, https://huggingface.co/HKUSTAudio/Llasa-8B, https://huggingface.co/HKUSTAudio/xcodec2, https://huggingface.co/HKUSTAudio/Llasa-1B-Multilingual, https://huggingface.co/bosonai/higgs-audio-v2-tokenizer, https://huggingface.co/hf-audio/xcodec2, https://huggingface.co/GameRuiner/Llasa-1B, https://huggingface.co/HKUSTAudio/Llasa-1B-Preserve-TextChat, https://huggingface.co/HKUSTAudio/Llasa-3B-Preserve-TextChat, https://huggingface.co/HKUSTAudio/Llasa-1B-two-speakers-kore-puck, https://huggingface.co/HKUSTAudio/Llasa-1B-multi-speakers-genshin-zh-en-ja-ko, https://huggingface.co/Gapeleon/llasa-3b, https://huggingface.co/unsloth/Llasa-3B, https://huggingface.co/unsloth/Llasa-1B, https://huggingface.co/litagin/xcodec2, https://huggingface.co/Prince-1/Llasa-3B, https://huggingface.co/PierrunoYT/higgs-audio-v2-tokenizer",
    "models_detailed": "[{\"name\": \"HKUSTAudio/Llasa-3B\", \"link\": \"https://huggingface.co/HKUSTAudio/Llasa-3B\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"HKUSTAudio/Llasa-1B\", \"link\": \"https://huggingface.co/HKUSTAudio/Llasa-1B\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"HKUSTAudio/Llasa-8B\", \"link\": \"https://huggingface.co/HKUSTAudio/Llasa-8B\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 9\"}, {\"name\": \"HKUSTAudio/xcodec2\", \"link\": \"https://huggingface.co/HKUSTAudio/xcodec2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 23\"}, {\"name\": \"HKUSTAudio/Llasa-1B-Multilingual\", \"link\": \"https://huggingface.co/HKUSTAudio/Llasa-1B-Multilingual\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 5\"}, {\"name\": \"bosonai/higgs-audio-v2-tokenizer\", \"link\": \"https://huggingface.co/bosonai/higgs-audio-v2-tokenizer\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 22\"}, {\"name\": \"hf-audio/xcodec2\", \"link\": \"https://huggingface.co/hf-audio/xcodec2\", \"task\": \"\", \"likes\": \"111\", \"downloads\": \"\", \"updated\": \"Nov 18\"}, {\"name\": \"GameRuiner/Llasa-1B\", \"link\": \"https://huggingface.co/GameRuiner/Llasa-1B\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 7\"}, {\"name\": \"HKUSTAudio/Llasa-1B-Preserve-TextChat\", \"link\": \"https://huggingface.co/HKUSTAudio/Llasa-1B-Preserve-TextChat\", \"task\": \"Text-to-Speech\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Feb 13\"}, {\"name\": \"HKUSTAudio/Llasa-3B-Preserve-TextChat\", \"link\": \"https://huggingface.co/HKUSTAudio/Llasa-3B-Preserve-TextChat\", \"task\": \"Text-to-Speech\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Feb 13\"}, {\"name\": \"HKUSTAudio/Llasa-1B-two-speakers-kore-puck\", \"link\": \"https://huggingface.co/HKUSTAudio/Llasa-1B-two-speakers-kore-puck\", \"task\": \"Text-to-Speech\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"Feb 13\"}, {\"name\": \"HKUSTAudio/Llasa-1B-multi-speakers-genshin-zh-en-ja-ko\", \"link\": \"https://huggingface.co/HKUSTAudio/Llasa-1B-multi-speakers-genshin-zh-en-ja-ko\", \"task\": \"Text-to-Speech\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Feb 13\"}, {\"name\": \"Gapeleon/llasa-3b\", \"link\": \"https://huggingface.co/Gapeleon/llasa-3b\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 29\"}, {\"name\": \"unsloth/Llasa-3B\", \"link\": \"https://huggingface.co/unsloth/Llasa-3B\", \"task\": \"Text-to-Speech\", \"likes\": \"194\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"unsloth/Llasa-1B\", \"link\": \"https://huggingface.co/unsloth/Llasa-1B\", \"task\": \"Text-to-Speech\", \"likes\": \"495\", \"downloads\": \"\", \"updated\": \"May 17\"}, {\"name\": \"litagin/xcodec2\", \"link\": \"https://huggingface.co/litagin/xcodec2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 27\"}, {\"name\": \"Prince-1/Llasa-3B\", \"link\": \"https://huggingface.co/Prince-1/Llasa-3B\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 4\"}, {\"name\": \"PierrunoYT/higgs-audio-v2-tokenizer\", \"link\": \"https://huggingface.co/PierrunoYT/higgs-audio-v2-tokenizer\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}]",
    "num_datasets": 2,
    "datasets_list": "HKUSTAudio/Llasa_opensource_speech_data_160k_hours_tokenized, Pendrokar/open_tts_tracker",
    "datasets_links": "https://huggingface.co/datasets/HKUSTAudio/Llasa_opensource_speech_data_160k_hours_tokenized, https://huggingface.co/datasets/Pendrokar/open_tts_tracker",
    "datasets_detailed": "[{\"name\": \"HKUSTAudio/Llasa_opensource_speech_data_160k_hours_tokenized\", \"link\": \"https://huggingface.co/datasets/HKUSTAudio/Llasa_opensource_speech_data_160k_hours_tokenized\", \"task\": \"\", \"likes\": \"565\", \"downloads\": \"\", \"updated\": \"Feb 13\", \"size\": \"\"}, {\"name\": \"Pendrokar/open_tts_tracker\", \"link\": \"https://huggingface.co/datasets/Pendrokar/open_tts_tracker\", \"task\": \"\", \"likes\": \"19\", \"downloads\": \"\", \"updated\": \"Aug 29\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.04235",
    "first_seen_date": "2025-02-07",
    "title": "MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus\n  Expansion",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.04235MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus\n  ExpansionPublished on Feb 6\u00b7Submitted byhaoxintongon Feb 7\u00b7ByteDance SeedUpvote23+15Authors:Xintong Hao,Ke Shen,Chenggang LiAbstractMAGA reformulation method expands pretraining datasets by synthesizing diverse and contextually-rich data, improving model performance across various sizes and highlighting limitations in collapse detection metrics.AI-generated summaryDespite the remarkable capabilities of large language models across various\ntasks, their continued scaling faces a critical challenge: the scarcity of\nhigh-quality pretraining data. While model architectures continue to evolve,\nthe natural language data struggles to scale up. To tackle this bottleneck, we\nproposeMAssive Genre-Audience~(MAGA) reformulation\nmethod, which systematic synthesizes diverse, contextually-rich pretraining\ndata from existing corpus. This work makes three main contributions: (1) We\nproposeMAGA reformulation method, a lightweight and scalable approach forpretraining corpus expansion, and build a 770B tokensMAGACorpus. (2) We\nevaluateMAGACorpuswith differentdata budget scaling strategies,\ndemonstrating consistent improvements across various model sizes (134M-13B),\nestablishing the necessity for next-generation large-scale synthetic\npretraining language models. (3) Through comprehensive analysis, we investigateprompt engineering's impact onsynthetic training collapseand reveal\nlimitations in conventionalcollapse detection metricsusing validation losses.\nOur work shows that MAGA can substantially expand training datasets while\nmaintaining quality, offering a reliably pathway for scaling models beyond data\nlimitations.View arXiv pageView PDFAdd to collectionCommunityhaoxintongPaper authorPaper submitterFeb 7We build MAGACorpus based on [SmolLM Corpus], expanding fineweb-edu-dedup source from 195B tokens to 770B tokens, showing ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.04235",
    "arxiv_url": "https://arxiv.org/abs/2502.04235",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 3,
    "datasets_list": "ByteDance-Seed/mga-fineweb-edu, Tiiny/PowerCoding, Tiiny/PowerMath",
    "datasets_links": "https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu, https://huggingface.co/datasets/Tiiny/PowerCoding, https://huggingface.co/datasets/Tiiny/PowerMath",
    "datasets_detailed": "[{\"name\": \"ByteDance-Seed/mga-fineweb-edu\", \"link\": \"https://huggingface.co/datasets/ByteDance-Seed/mga-fineweb-edu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 19\", \"size\": \"\"}, {\"name\": \"Tiiny/PowerCoding\", \"link\": \"https://huggingface.co/datasets/Tiiny/PowerCoding\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\", \"size\": \"\"}, {\"name\": \"Tiiny/PowerMath\", \"link\": \"https://huggingface.co/datasets/Tiiny/PowerMath\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Jul 27\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.02737",
    "first_seen_date": "2025-02-06",
    "title": "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.02737SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language ModelPublished on Feb 4\u00b7Submitted byAKon Feb 6#1 Paper of the day\u00b7Hugging Face Smol Models ResearchUpvote251+243Authors:Loubna Ben Allal,Anton Lozhkov,Elie Bakouch,Gabriel Mart\u00edn Bl\u00e1zquez,Guilherme Penedo,Lewis Tunstall,Andr\u00e9s Marafioti,Hynek Kydl\u00ed\u010dek,Agust\u00edn Piqueres Lajar\u00edn,Vaibhav Srivastav,Joshua Lochner,Caleb Fahlgren,Xuan-Son Nguyen,Cl\u00e9mentine Fourrier,Ben Burtenshaw,Hugo Larcher,Haojun Zhao,Cyril Zakka,Mathieu Morlon,Colin Raffel,Leandro von Werra,Thomas WolfAbstractSmolLM2, a small language model with 1.7 billion parameters, achieves strong performance through overtraining on diverse datasets, outperforming other recent small models.AI-generated summaryWhilelarge language modelshave facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath,Stack-Edu, andSmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scaleablationsas well as a manual refinement process that updates thedataset mixingrates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as w",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/huggingface/smollm",
    "hf_paper_url": "https://huggingface.co/papers/2502.02737",
    "arxiv_url": "https://arxiv.org/abs/2502.02737",
    "num_models": 50,
    "models_list": "HuggingFaceTB/SmolLM2-1.7B-Instruct, HuggingFaceTB/SmolLM2-135M-Instruct, HuggingFaceTB/SmolLM2-360M-Instruct, HuggingFaceTB/SmolLM2-135M, HuggingFaceTB/SmolLM2-360M, HuggingFaceTB/SmolLM2-1.7B, HuggingFaceTB/FineMath-Llama-3B, lvwerra/science-timeline, HuggingFaceTB/stack-edu-classifier-cpp, HuggingFaceTB/stack-edu-classifier-c, HuggingFaceTB/stack-edu-classifier-ruby, HuggingFaceTB/stack-edu-classifier-javascript, HuggingFaceTB/stack-edu-classifier-go, HuggingFaceTB/stack-edu-classifier-rust, HuggingFaceTB/stack-edu-classifier-shell, HuggingFaceTB/stack-edu-classifier-typescript, HuggingFaceTB/stack-edu-classifier-java, HuggingFaceTB/stack-edu-classifier-swift, HuggingFaceTB/stack-edu-classifier-markdown, HuggingFaceTB/stack-edu-classifier-python, HuggingFaceTB/stack-edu-classifier-sql, HuggingFaceTB/stack-edu-classifier-php, HuggingFaceTB/stack-edu-classifier-csharp, cortexso/smollm2, jncraton/SmolLM2-1.7B-Instruct-ct2-int8, Unseen1980/fiori-tools-ga, loubnabnl/SmolLM2-135M-Instruct-template2, HuggingFaceTB/SmolLM2-1.7B-Instruct-16k, HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints, HuggingFaceTB/SmolLM2-360M-intermediate-checkpoints, HuggingFaceTB/SmolLM2-135M-intermediate-checkpoints, real-jiakai/SmolLM2-1.7B-TLDR, RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-4bits, RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-8bits, RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-Instruct-4bits, RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-Instruct-8bits, defnic/ShittyTranslator-GGUF, AngelRaychev/policy_iteration_0, FM-1976/SmolLM2-360M-it-llamafile, yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview, Mungert/SmolLM2-135M-Instruct-GGUF, Mungert/SmolLM2-360M-Instruct-GGUF, Mungert/SmolLM2-1.7B-Instruct-GGUF, schmuell/SmolLM2-1.7B-Instruct, jhall0310/SmolLM2-135M, ronx-labs/affine-smollm2-135b, jg940101/Babelbit-5F9k8PGRsgVpJF2tGDqa31pgUTUb5HVDENfmm1xRdansADJT, viktoroo/SmolLM2-360M-Tools, isotnek/SmolLM2-360M-Instruct-heretic, onnx-community/SmolLM2-360M-ONNX",
    "models_links": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct, https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct, https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct, https://huggingface.co/HuggingFaceTB/SmolLM2-135M, https://huggingface.co/HuggingFaceTB/SmolLM2-360M, https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B, https://huggingface.co/HuggingFaceTB/FineMath-Llama-3B, https://huggingface.co/lvwerra/science-timeline, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-cpp, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-c, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-ruby, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-javascript, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-go, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-rust, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-shell, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-typescript, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-java, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-swift, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-markdown, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-python, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-sql, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-php, https://huggingface.co/HuggingFaceTB/stack-edu-classifier-csharp, https://huggingface.co/cortexso/smollm2, https://huggingface.co/jncraton/SmolLM2-1.7B-Instruct-ct2-int8, https://huggingface.co/Unseen1980/fiori-tools-ga, https://huggingface.co/loubnabnl/SmolLM2-135M-Instruct-template2, https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-16k, https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints, https://huggingface.co/HuggingFaceTB/SmolLM2-360M-intermediate-checkpoints, https://huggingface.co/HuggingFaceTB/SmolLM2-135M-intermediate-checkpoints, https://huggingface.co/real-jiakai/SmolLM2-1.7B-TLDR, https://huggingface.co/RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-4bits, https://huggingface.co/RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-8bits, https://huggingface.co/RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-Instruct-4bits, https://huggingface.co/RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-Instruct-8bits, https://huggingface.co/defnic/ShittyTranslator-GGUF, https://huggingface.co/AngelRaychev/policy_iteration_0, https://huggingface.co/FM-1976/SmolLM2-360M-it-llamafile, https://huggingface.co/yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview, https://huggingface.co/Mungert/SmolLM2-135M-Instruct-GGUF, https://huggingface.co/Mungert/SmolLM2-360M-Instruct-GGUF, https://huggingface.co/Mungert/SmolLM2-1.7B-Instruct-GGUF, https://huggingface.co/schmuell/SmolLM2-1.7B-Instruct, https://huggingface.co/jhall0310/SmolLM2-135M, https://huggingface.co/ronx-labs/affine-smollm2-135b, https://huggingface.co/jg940101/Babelbit-5F9k8PGRsgVpJF2tGDqa31pgUTUb5HVDENfmm1xRdansADJT, https://huggingface.co/viktoroo/SmolLM2-360M-Tools, https://huggingface.co/isotnek/SmolLM2-360M-Instruct-heretic, https://huggingface.co/onnx-community/SmolLM2-360M-ONNX",
    "models_detailed": "[{\"name\": \"HuggingFaceTB/SmolLM2-1.7B-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"HuggingFaceTB/SmolLM2-135M-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\"}, {\"name\": \"HuggingFaceTB/SmolLM2-360M-Instruct\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\"}, {\"name\": \"HuggingFaceTB/SmolLM2-135M\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-135M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 6\"}, {\"name\": \"HuggingFaceTB/SmolLM2-360M\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-360M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 6\"}, {\"name\": \"HuggingFaceTB/SmolLM2-1.7B\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 6\"}, {\"name\": \"HuggingFaceTB/FineMath-Llama-3B\", \"link\": \"https://huggingface.co/HuggingFaceTB/FineMath-Llama-3B\", \"task\": \"\", \"likes\": \"90\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"lvwerra/science-timeline\", \"link\": \"https://huggingface.co/lvwerra/science-timeline\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 13\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-cpp\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-cpp\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-c\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-c\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-ruby\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-ruby\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-javascript\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-javascript\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-go\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-go\", \"task\": \"\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-rust\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-rust\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-shell\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-shell\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-typescript\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-typescript\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-java\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-java\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-swift\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-swift\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-markdown\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-markdown\", \"task\": \"\", \"likes\": \"80\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-python\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-python\", \"task\": \"\", \"likes\": \"727\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-sql\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-sql\", \"task\": \"\", \"likes\": \"55\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-php\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-php\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"HuggingFaceTB/stack-edu-classifier-csharp\", \"link\": \"https://huggingface.co/HuggingFaceTB/stack-edu-classifier-csharp\", \"task\": \"\", \"likes\": \"116\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"cortexso/smollm2\", \"link\": \"https://huggingface.co/cortexso/smollm2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 3\"}, {\"name\": \"jncraton/SmolLM2-1.7B-Instruct-ct2-int8\", \"link\": \"https://huggingface.co/jncraton/SmolLM2-1.7B-Instruct-ct2-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 9\"}, {\"name\": \"Unseen1980/fiori-tools-ga\", \"link\": \"https://huggingface.co/Unseen1980/fiori-tools-ga\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"loubnabnl/SmolLM2-135M-Instruct-template2\", \"link\": \"https://huggingface.co/loubnabnl/SmolLM2-135M-Instruct-template2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 18\"}, {\"name\": \"HuggingFaceTB/SmolLM2-1.7B-Instruct-16k\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-16k\", \"task\": \"Text Generation\", \"likes\": \"558\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints\", \"task\": \"\", \"likes\": \"58\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"HuggingFaceTB/SmolLM2-360M-intermediate-checkpoints\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-360M-intermediate-checkpoints\", \"task\": \"\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"HuggingFaceTB/SmolLM2-135M-intermediate-checkpoints\", \"link\": \"https://huggingface.co/HuggingFaceTB/SmolLM2-135M-intermediate-checkpoints\", \"task\": \"\", \"likes\": \"275\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"real-jiakai/SmolLM2-1.7B-TLDR\", \"link\": \"https://huggingface.co/real-jiakai/SmolLM2-1.7B-TLDR\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 9\"}, {\"name\": \"RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-4bits\", \"link\": \"https://huggingface.co/RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-4bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 21\"}, {\"name\": \"RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-8bits\", \"link\": \"https://huggingface.co/RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-8bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 21\"}, {\"name\": \"RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-Instruct-4bits\", \"link\": \"https://huggingface.co/RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-Instruct-4bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 21\"}, {\"name\": \"RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-Instruct-8bits\", \"link\": \"https://huggingface.co/RichardErkhov/HuggingFaceTB_-_SmolLM2-135M-Instruct-8bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 21\"}, {\"name\": \"defnic/ShittyTranslator-GGUF\", \"link\": \"https://huggingface.co/defnic/ShittyTranslator-GGUF\", \"task\": \"\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Mar 25\"}, {\"name\": \"AngelRaychev/policy_iteration_0\", \"link\": \"https://huggingface.co/AngelRaychev/policy_iteration_0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"FM-1976/SmolLM2-360M-it-llamafile\", \"link\": \"https://huggingface.co/FM-1976/SmolLM2-360M-it-llamafile\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 15\"}, {\"name\": \"yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview\", \"task\": \"\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"Mungert/SmolLM2-135M-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/SmolLM2-135M-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/SmolLM2-360M-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/SmolLM2-360M-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/SmolLM2-1.7B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Mungert/SmolLM2-1.7B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"schmuell/SmolLM2-1.7B-Instruct\", \"link\": \"https://huggingface.co/schmuell/SmolLM2-1.7B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"jhall0310/SmolLM2-135M\", \"link\": \"https://huggingface.co/jhall0310/SmolLM2-135M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 15\"}, {\"name\": \"ronx-labs/affine-smollm2-135b\", \"link\": \"https://huggingface.co/ronx-labs/affine-smollm2-135b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"jg940101/Babelbit-5F9k8PGRsgVpJF2tGDqa31pgUTUb5HVDENfmm1xRdansADJT\", \"link\": \"https://huggingface.co/jg940101/Babelbit-5F9k8PGRsgVpJF2tGDqa31pgUTUb5HVDENfmm1xRdansADJT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 29\"}, {\"name\": \"viktoroo/SmolLM2-360M-Tools\", \"link\": \"https://huggingface.co/viktoroo/SmolLM2-360M-Tools\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"isotnek/SmolLM2-360M-Instruct-heretic\", \"link\": \"https://huggingface.co/isotnek/SmolLM2-360M-Instruct-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"onnx-community/SmolLM2-360M-ONNX\", \"link\": \"https://huggingface.co/onnx-community/SmolLM2-360M-ONNX\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 19,
    "datasets_list": "HuggingFaceTB/dclm-edu, HuggingFaceTB/smol-smoltalk, HuggingFaceTB/finemath, HuggingFaceTB/smoltalk, Leon-Leee/unofficial-pyedu, maharnab/smol-smoltalk-10k, HuggingFaceTB/stack-edu, EleutherAI/SmolLM2-135M-10B, ReactiveAI/smol-smoltalk-mini-Interaction-SFT, ReactiveAI/smol-smoltalk-Interaction-SFT, vanek-epfl/MNLP_M2_mcqa_dataset, ushakov15/MNLP_M2_rag_dataset, meryyllebr543/stack-edu-huggingface, aladinDJ/smoltalk-annotated, IVUL-KAUST/MOLE-plus, apart/SmolLM2-135M-1M-rows, tanganke/smol-smoltalk-Interaction-SFT-formatted, HayatoHongo/smoltalk, Fhrozen/stack-prompts",
    "datasets_links": "https://huggingface.co/datasets/HuggingFaceTB/dclm-edu, https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk, https://huggingface.co/datasets/HuggingFaceTB/finemath, https://huggingface.co/datasets/HuggingFaceTB/smoltalk, https://huggingface.co/datasets/Leon-Leee/unofficial-pyedu, https://huggingface.co/datasets/maharnab/smol-smoltalk-10k, https://huggingface.co/datasets/HuggingFaceTB/stack-edu, https://huggingface.co/datasets/EleutherAI/SmolLM2-135M-10B, https://huggingface.co/datasets/ReactiveAI/smol-smoltalk-mini-Interaction-SFT, https://huggingface.co/datasets/ReactiveAI/smol-smoltalk-Interaction-SFT, https://huggingface.co/datasets/vanek-epfl/MNLP_M2_mcqa_dataset, https://huggingface.co/datasets/ushakov15/MNLP_M2_rag_dataset, https://huggingface.co/datasets/meryyllebr543/stack-edu-huggingface, https://huggingface.co/datasets/aladinDJ/smoltalk-annotated, https://huggingface.co/datasets/IVUL-KAUST/MOLE-plus, https://huggingface.co/datasets/apart/SmolLM2-135M-1M-rows, https://huggingface.co/datasets/tanganke/smol-smoltalk-Interaction-SFT-formatted, https://huggingface.co/datasets/HayatoHongo/smoltalk, https://huggingface.co/datasets/Fhrozen/stack-prompts",
    "datasets_detailed": "[{\"name\": \"HuggingFaceTB/dclm-edu\", \"link\": \"https://huggingface.co/datasets/HuggingFaceTB/dclm-edu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 7\", \"size\": \"\"}, {\"name\": \"HuggingFaceTB/smol-smoltalk\", \"link\": \"https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 6\", \"size\": \"\"}, {\"name\": \"HuggingFaceTB/finemath\", \"link\": \"https://huggingface.co/datasets/HuggingFaceTB/finemath\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 6\", \"size\": \"\"}, {\"name\": \"HuggingFaceTB/smoltalk\", \"link\": \"https://huggingface.co/datasets/HuggingFaceTB/smoltalk\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 10\", \"size\": \"\"}, {\"name\": \"Leon-Leee/unofficial-pyedu\", \"link\": \"https://huggingface.co/datasets/Leon-Leee/unofficial-pyedu\", \"task\": \"\", \"likes\": \"122\", \"downloads\": \"\", \"updated\": \"Mar 12\", \"size\": \"\"}, {\"name\": \"maharnab/smol-smoltalk-10k\", \"link\": \"https://huggingface.co/datasets/maharnab/smol-smoltalk-10k\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Apr 6\", \"size\": \"\"}, {\"name\": \"HuggingFaceTB/stack-edu\", \"link\": \"https://huggingface.co/datasets/HuggingFaceTB/stack-edu\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\", \"size\": \"\"}, {\"name\": \"EleutherAI/SmolLM2-135M-10B\", \"link\": \"https://huggingface.co/datasets/EleutherAI/SmolLM2-135M-10B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 15\", \"size\": \"\"}, {\"name\": \"ReactiveAI/smol-smoltalk-mini-Interaction-SFT\", \"link\": \"https://huggingface.co/datasets/ReactiveAI/smol-smoltalk-mini-Interaction-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 17\", \"size\": \"\"}, {\"name\": \"ReactiveAI/smol-smoltalk-Interaction-SFT\", \"link\": \"https://huggingface.co/datasets/ReactiveAI/smol-smoltalk-Interaction-SFT\", \"task\": \"\", \"likes\": \"338\", \"downloads\": \"\", \"updated\": \"Oct 8\", \"size\": \"\"}, {\"name\": \"vanek-epfl/MNLP_M2_mcqa_dataset\", \"link\": \"https://huggingface.co/datasets/vanek-epfl/MNLP_M2_mcqa_dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 27\", \"size\": \"\"}, {\"name\": \"ushakov15/MNLP_M2_rag_dataset\", \"link\": \"https://huggingface.co/datasets/ushakov15/MNLP_M2_rag_dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 27\", \"size\": \"\"}, {\"name\": \"meryyllebr543/stack-edu-huggingface\", \"link\": \"https://huggingface.co/datasets/meryyllebr543/stack-edu-huggingface\", \"task\": \"\", \"likes\": \"137\", \"downloads\": \"\", \"updated\": \"Jun 29\", \"size\": \"\"}, {\"name\": \"aladinDJ/smoltalk-annotated\", \"link\": \"https://huggingface.co/datasets/aladinDJ/smoltalk-annotated\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 16\", \"size\": \"\"}, {\"name\": \"IVUL-KAUST/MOLE-plus\", \"link\": \"https://huggingface.co/datasets/IVUL-KAUST/MOLE-plus\", \"task\": \"\", \"likes\": \"147\", \"downloads\": \"\", \"updated\": \"Oct 9\", \"size\": \"\"}, {\"name\": \"apart/SmolLM2-135M-1M-rows\", \"link\": \"https://huggingface.co/datasets/apart/SmolLM2-135M-1M-rows\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\", \"size\": \"\"}, {\"name\": \"tanganke/smol-smoltalk-Interaction-SFT-formatted\", \"link\": \"https://huggingface.co/datasets/tanganke/smol-smoltalk-Interaction-SFT-formatted\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\", \"size\": \"\"}, {\"name\": \"HayatoHongo/smoltalk\", \"link\": \"https://huggingface.co/datasets/HayatoHongo/smoltalk\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 18\", \"size\": \"\"}, {\"name\": \"Fhrozen/stack-prompts\", \"link\": \"https://huggingface.co/datasets/Fhrozen/stack-prompts\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.03387",
    "first_seen_date": "2025-02-06",
    "title": "LIMO: Less is More for Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.03387LIMO: Less is More for ReasoningPublished on Feb 5\u00b7Submitted byAKon Feb 6#2 Paper of the dayUpvote62+54Authors:Yixin Ye,Zhen Huang,Yang Xiao,Ethan Chern,Shijie Xia,Pengfei LiuAbstractLIMO, a new model, achieves high mathematical reasoning performance using minimal training data, challenging the notion that extensive datasets are necessary for complex reasoning.AI-generated summaryWe present a fundamental discovery that challenges our understanding of how\ncomplex reasoning emerges in large language models. While conventional wisdom\nsuggests that sophisticated reasoning tasks demand extensive training data\n(>100,000 examples), we demonstrate that complex mathematical reasoning\nabilities can be effectively elicited with surprisingly few examples. Through\ncomprehensive experiments, our proposed modelLIMOdemonstrates unprecedented\nperformance in mathematical reasoning. With merely 817 curated training\nsamples,LIMOachieves 57.1% accuracy on AIME and 94.8% on MATH, improving from\nprevious SFT-based models' 6.5% and 59.2% respectively, while only using 1% of\nthe training data required by previous approaches.LIMOdemonstrates\nexceptional out-of-distribution generalization, achieving 40.5% absolute\nimprovement across 10 diverse benchmarks, outperforming models trained on 100x\nmore data, challenging the notion that SFT leads to memorization rather than\ngeneralization. Based on these results, we propose the Less-Is-More Reasoning\nHypothesis (LIMO Hypothesis): Infoundation modelswhere domain knowledge has\nbeen comprehensively encoded duringpre-training, sophisticated reasoning\ncapabilities can emerge through minimal but precisely orchestrated\ndemonstrations of cognitive processes. This hypothesis posits that the\nelicitation threshold for complex reasoning is determined by two key factors:\n(1) the completeness of the model'sencoded knowledgefoundation duringpre-training, and (2) the ef",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/gair-nlp/limo",
    "hf_paper_url": "https://huggingface.co/papers/2502.03387",
    "arxiv_url": "https://arxiv.org/abs/2502.03387",
    "num_models": 14,
    "models_list": "GAIR/LIMO, Josephgflowers/DeepSeek-R1-Distill-Qwen-1.5B-LIMO, GAIR/LIMO-v2, Josephgflowers/TinyLlama-R1-LIMO, BlackBeenie/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-LIMO, t83714/llama-3.1-8b-instruct-limo, t83714/llama-3.1-8b-instruct-limo-lora-adapter, t83714/qwen2.5-32b-instruct-limo-lora-adapter, loenma/conmemay, prithivMLmods/Hatshepsut-Qwen3_QWQ-LCoT-4B, Cbgcbg/limo-qwen3-8b-math, Cbgcbg/limo-qwen3-8b-math-merged, Cbgcbg/limo-qwen3-8b-math-full-precision_v2, Cbgcbg/limo-qwen3-8b-math-full-precision_v3",
    "models_links": "https://huggingface.co/GAIR/LIMO, https://huggingface.co/Josephgflowers/DeepSeek-R1-Distill-Qwen-1.5B-LIMO, https://huggingface.co/GAIR/LIMO-v2, https://huggingface.co/Josephgflowers/TinyLlama-R1-LIMO, https://huggingface.co/BlackBeenie/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-LIMO, https://huggingface.co/t83714/llama-3.1-8b-instruct-limo, https://huggingface.co/t83714/llama-3.1-8b-instruct-limo-lora-adapter, https://huggingface.co/t83714/qwen2.5-32b-instruct-limo-lora-adapter, https://huggingface.co/loenma/conmemay, https://huggingface.co/prithivMLmods/Hatshepsut-Qwen3_QWQ-LCoT-4B, https://huggingface.co/Cbgcbg/limo-qwen3-8b-math, https://huggingface.co/Cbgcbg/limo-qwen3-8b-math-merged, https://huggingface.co/Cbgcbg/limo-qwen3-8b-math-full-precision_v2, https://huggingface.co/Cbgcbg/limo-qwen3-8b-math-full-precision_v3",
    "models_detailed": "[{\"name\": \"GAIR/LIMO\", \"link\": \"https://huggingface.co/GAIR/LIMO\", \"task\": \"\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"Feb 6\"}, {\"name\": \"Josephgflowers/DeepSeek-R1-Distill-Qwen-1.5B-LIMO\", \"link\": \"https://huggingface.co/Josephgflowers/DeepSeek-R1-Distill-Qwen-1.5B-LIMO\", \"task\": \"\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 14\"}, {\"name\": \"GAIR/LIMO-v2\", \"link\": \"https://huggingface.co/GAIR/LIMO-v2\", \"task\": \"\", \"likes\": \"172\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"Josephgflowers/TinyLlama-R1-LIMO\", \"link\": \"https://huggingface.co/Josephgflowers/TinyLlama-R1-LIMO\", \"task\": \"\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"BlackBeenie/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-LIMO\", \"link\": \"https://huggingface.co/BlackBeenie/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-LIMO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"t83714/llama-3.1-8b-instruct-limo\", \"link\": \"https://huggingface.co/t83714/llama-3.1-8b-instruct-limo\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 16\"}, {\"name\": \"t83714/llama-3.1-8b-instruct-limo-lora-adapter\", \"link\": \"https://huggingface.co/t83714/llama-3.1-8b-instruct-limo-lora-adapter\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 16\"}, {\"name\": \"t83714/qwen2.5-32b-instruct-limo-lora-adapter\", \"link\": \"https://huggingface.co/t83714/qwen2.5-32b-instruct-limo-lora-adapter\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"loenma/conmemay\", \"link\": \"https://huggingface.co/loenma/conmemay\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 10\"}, {\"name\": \"prithivMLmods/Hatshepsut-Qwen3_QWQ-LCoT-4B\", \"link\": \"https://huggingface.co/prithivMLmods/Hatshepsut-Qwen3_QWQ-LCoT-4B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 13\"}, {\"name\": \"Cbgcbg/limo-qwen3-8b-math\", \"link\": \"https://huggingface.co/Cbgcbg/limo-qwen3-8b-math\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"Cbgcbg/limo-qwen3-8b-math-merged\", \"link\": \"https://huggingface.co/Cbgcbg/limo-qwen3-8b-math-merged\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"Cbgcbg/limo-qwen3-8b-math-full-precision_v2\", \"link\": \"https://huggingface.co/Cbgcbg/limo-qwen3-8b-math-full-precision_v2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 16\"}, {\"name\": \"Cbgcbg/limo-qwen3-8b-math-full-precision_v3\", \"link\": \"https://huggingface.co/Cbgcbg/limo-qwen3-8b-math-full-precision_v3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 17\"}]",
    "num_datasets": 6,
    "datasets_list": "GAIR/LIMO, GAIR/LIMO-v2, lenML/limo-cod, junnei/ko-limo, Minami-su/LIMO_chinese, LLMTeamAkiyama/cleand_GAIR_LIMO",
    "datasets_links": "https://huggingface.co/datasets/GAIR/LIMO, https://huggingface.co/datasets/GAIR/LIMO-v2, https://huggingface.co/datasets/lenML/limo-cod, https://huggingface.co/datasets/junnei/ko-limo, https://huggingface.co/datasets/Minami-su/LIMO_chinese, https://huggingface.co/datasets/LLMTeamAkiyama/cleand_GAIR_LIMO",
    "datasets_detailed": "[{\"name\": \"GAIR/LIMO\", \"link\": \"https://huggingface.co/datasets/GAIR/LIMO\", \"task\": \"\", \"likes\": \"817\", \"downloads\": \"\", \"updated\": \"Feb 10\", \"size\": \"\"}, {\"name\": \"GAIR/LIMO-v2\", \"link\": \"https://huggingface.co/datasets/GAIR/LIMO-v2\", \"task\": \"\", \"likes\": \"800\", \"downloads\": \"\", \"updated\": \"Jul 30\", \"size\": \"\"}, {\"name\": \"lenML/limo-cod\", \"link\": \"https://huggingface.co/datasets/lenML/limo-cod\", \"task\": \"\", \"likes\": \"84\", \"downloads\": \"\", \"updated\": \"Apr 23\", \"size\": \"\"}, {\"name\": \"junnei/ko-limo\", \"link\": \"https://huggingface.co/datasets/junnei/ko-limo\", \"task\": \"\", \"likes\": \"817\", \"downloads\": \"\", \"updated\": \"Feb 12\", \"size\": \"\"}, {\"name\": \"Minami-su/LIMO_chinese\", \"link\": \"https://huggingface.co/datasets/Minami-su/LIMO_chinese\", \"task\": \"\", \"likes\": \"817\", \"downloads\": \"\", \"updated\": \"Feb 7\", \"size\": \"\"}, {\"name\": \"LLMTeamAkiyama/cleand_GAIR_LIMO\", \"link\": \"https://huggingface.co/datasets/LLMTeamAkiyama/cleand_GAIR_LIMO\", \"task\": \"\", \"likes\": \"741\", \"downloads\": \"\", \"updated\": \"Jul 28\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.01718",
    "first_seen_date": "2025-02-05",
    "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.01718ACECODER: Acing Coder RL via Automated Test-Case SynthesisPublished on Feb 3\u00b7Submitted byAKon Feb 5#2 Paper of the dayUpvote29+21Authors:Huaye Zeng,Dongfu Jiang,Haozhe Wang,Ping Nie,Xiaotong Chen,Wenhu ChenAbstractAutomated test-case synthesis improves reward models and reinforces coding models, demonstrating significant performance gains.AI-generated summaryMost progress in recent coder models has been driven by supervised\nfine-tuning (SFT), while the potential ofreinforcement learning(RL) remains\nlargely unexplored, primarily due to the lack of reliable reward data/model in\nthe code domain. In this paper, we address this challenge by leveragingautomated large-scale test-case synthesisto enhance code model training.\nSpecifically, we design a pipeline that generates extensive (question,\ntest-cases) pairs from existing code data. Using these test cases, we constructpreference pairsbased on pass rates over sampled programs to train reward\nmodels withBradley-Terry loss. It shows an average of 10-point improvement forLlama-3.1-8B-Insand 5-point improvement forQwen2.5-Coder-7B-Insthroughbest-of-32 sampling, making the 7B model on par with 236BDeepSeek-V2.5.\nFurthermore, we conductreinforcement learningwith bothreward modelsand\ntest-case pass rewards, leading to consistent improvements acrossHumanEval,MBPP,BigCodeBench, andLiveCodeBench(V4). Notably, we follow the R1-style\ntraining to start from Qwen2.5-Coder-base directly and show that our RL\ntraining can improve model onHumanEval-plusby over 25\\% andMBPP-plusby 6\\%\nfor merely 80 optimization steps. We believe our results highlight the huge\npotential ofreinforcement learningin coder models.View arXiv pageView PDFAdd to collectionCommunityakhaliqPaper submitterFeb 5https://tiger-ai-lab.github.io/AceCoder/See translation\ud83d\udd2544+Replylibrarian-botFeb 6This is an automated message from theLibrarian Bot. I found the following papers s",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2502.01718",
    "arxiv_url": "https://arxiv.org/abs/2502.01718",
    "num_models": 5,
    "models_list": "TIGER-Lab/AceCodeRM-32B, TIGER-Lab/AceCodeRM-7B, TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-Rule, TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-V1.1, TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-RM",
    "models_links": "https://huggingface.co/TIGER-Lab/AceCodeRM-32B, https://huggingface.co/TIGER-Lab/AceCodeRM-7B, https://huggingface.co/TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-Rule, https://huggingface.co/TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-V1.1, https://huggingface.co/TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-RM",
    "models_detailed": "[{\"name\": \"TIGER-Lab/AceCodeRM-32B\", \"link\": \"https://huggingface.co/TIGER-Lab/AceCodeRM-32B\", \"task\": \"\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"TIGER-Lab/AceCodeRM-7B\", \"link\": \"https://huggingface.co/TIGER-Lab/AceCodeRM-7B\", \"task\": \"\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-Rule\", \"link\": \"https://huggingface.co/TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-Rule\", \"task\": \"\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"Feb 12\"}, {\"name\": \"TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-V1.1\", \"link\": \"https://huggingface.co/TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-V1.1\", \"task\": \"\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"May 18\"}, {\"name\": \"TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-RM\", \"link\": \"https://huggingface.co/TIGER-Lab/AceCoder-Qwen2.5-Coder-7B-Ins-RM\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 12\"}]",
    "num_datasets": 9,
    "datasets_list": "allenai/Dolci-Think-RL-7B, TIGER-Lab/AceCode-87K, allenai/Dolci-Think-RL-32B, allenai/Dolci-Instruct-RL, TIGER-Lab/AceCodePair-300K, TIGER-Lab/AceCode-V1.1-69K, TIGER-Lab/AceCode-V2-122K, allenai/Dolci-Think-RL-7B-Completions-DPO, allenai/Dolci-Think-RL-7B-Completions-SFT",
    "datasets_links": "https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B, https://huggingface.co/datasets/TIGER-Lab/AceCode-87K, https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B, https://huggingface.co/datasets/allenai/Dolci-Instruct-RL, https://huggingface.co/datasets/TIGER-Lab/AceCodePair-300K, https://huggingface.co/datasets/TIGER-Lab/AceCode-V1.1-69K, https://huggingface.co/datasets/TIGER-Lab/AceCode-V2-122K, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO, https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT",
    "datasets_detailed": "[{\"name\": \"allenai/Dolci-Think-RL-7B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/AceCode-87K\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/AceCode-87K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 8\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-32B\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-32B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Instruct-RL\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Instruct-RL\", \"task\": \"\", \"likes\": \"774\", \"downloads\": \"\", \"updated\": \"12 days ago\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/AceCodePair-300K\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/AceCodePair-300K\", \"task\": \"\", \"likes\": \"321\", \"downloads\": \"\", \"updated\": \"Nov 7\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/AceCode-V1.1-69K\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/AceCode-V1.1-69K\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Nov 7\", \"size\": \"\"}, {\"name\": \"TIGER-Lab/AceCode-V2-122K\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/AceCode-V2-122K\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Aug 14\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-DPO\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-DPO\", \"task\": \"\", \"likes\": \"332\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}, {\"name\": \"allenai/Dolci-Think-RL-7B-Completions-SFT\", \"link\": \"https://huggingface.co/datasets/allenai/Dolci-Think-RL-7B-Completions-SFT\", \"task\": \"\", \"likes\": \"413\", \"downloads\": \"\", \"updated\": \"10 days ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2502.01456",
    "first_seen_date": "2025-02-04",
    "title": "Process Reinforcement through Implicit Rewards",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2502.01456Process Reinforcement through Implicit RewardsPublished on Feb 3\u00b7Submitted byHanbin Wangon Feb 4#3 Paper of the dayUpvote61+53Authors:Ganqu Cui,Lifan Yuan,Zefan Wang,Hanbin Wang,Wendi Li,Bingxiang He,Yuchen Fan,Tianyu Yu,Qixin Xu,Weize Chen,Jiarui Yuan,Huayu Chen,Kaiyan Zhang,Xingtai Lv,Shuo Wang,Yuan Yao,Xu Han,Hao Peng,Yu Cheng,Zhiyuan Liu,Maosong Sun,Bowen Zhou+1 authorsAbstractPRIME leverages implicit process rewards to improve the reinforcement learning of large language models, achieving better performance with less data compared to traditional methods.AI-generated summaryDense process rewardshave proven a more effective alternative to the sparse\noutcome-level rewards in the inference-time scaling oflarge language models(LLMs), particularly in tasks requiring complex multi-step reasoning. While\ndense rewards also offer an appealing choice for thereinforcement learning(RL) of LLMs since their fine-grained rewards have the potential to address\nsome inherent issues of outcome rewards, such as training efficiency and credit\nassignment, this potential remains largely unrealized. This can be primarily\nattributed to the challenges of trainingprocess reward models(PRMs) online,\nwhere collecting high-quality process labels is prohibitively expensive, making\nthem particularly vulnerable toreward hacking. To address these challenges, we\npropose PRIME (Process Reinforcement through IMplicit rEwards), which enables\nonline PRM updates using onlypolicy rolloutsand outcome labels through\nimplict process rewards. PRIME combines well with variousadvantage functionsand forgoes the dedicated reward model training phrase that existing approaches\nrequire, substantially reducing the development overhead. We demonstrate\nPRIME's effectiveness on competitional math and coding. Starting fromQwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several\nkeyreasoning benchmark",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2502,
    "github_repo": "https://github.com/PRIME-RL/PRIME",
    "hf_paper_url": "https://huggingface.co/papers/2502.01456",
    "arxiv_url": "https://arxiv.org/abs/2502.01456",
    "num_models": 9,
    "models_list": "Zyphra/ZR1-1.5B, PRIME-RL/Eurus-2-7B-PRIME, PRIME-RL/EurusPRM-Stage2, PRIME-RL/EurusPRM-Stage1, PRIME-RL/Eurus-2-7B-SFT, PRIME-RL/Eurus-2-7B-PRIME-Zero, lmstudio-community/ZR1-1.5B-GGUF, cgus/ZR1-1.5B-exl2, Mungert/ZR1-1.5B-GGUF",
    "models_links": "https://huggingface.co/Zyphra/ZR1-1.5B, https://huggingface.co/PRIME-RL/Eurus-2-7B-PRIME, https://huggingface.co/PRIME-RL/EurusPRM-Stage2, https://huggingface.co/PRIME-RL/EurusPRM-Stage1, https://huggingface.co/PRIME-RL/Eurus-2-7B-SFT, https://huggingface.co/PRIME-RL/Eurus-2-7B-PRIME-Zero, https://huggingface.co/lmstudio-community/ZR1-1.5B-GGUF, https://huggingface.co/cgus/ZR1-1.5B-exl2, https://huggingface.co/Mungert/ZR1-1.5B-GGUF",
    "models_detailed": "[{\"name\": \"Zyphra/ZR1-1.5B\", \"link\": \"https://huggingface.co/Zyphra/ZR1-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"PRIME-RL/Eurus-2-7B-PRIME\", \"link\": \"https://huggingface.co/PRIME-RL/Eurus-2-7B-PRIME\", \"task\": \"Text Generation\", \"likes\": \"731\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"PRIME-RL/EurusPRM-Stage2\", \"link\": \"https://huggingface.co/PRIME-RL/EurusPRM-Stage2\", \"task\": \"\", \"likes\": \"64\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"PRIME-RL/EurusPRM-Stage1\", \"link\": \"https://huggingface.co/PRIME-RL/EurusPRM-Stage1\", \"task\": \"\", \"likes\": \"27\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"PRIME-RL/Eurus-2-7B-SFT\", \"link\": \"https://huggingface.co/PRIME-RL/Eurus-2-7B-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"PRIME-RL/Eurus-2-7B-PRIME-Zero\", \"link\": \"https://huggingface.co/PRIME-RL/Eurus-2-7B-PRIME-Zero\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Mar 14\"}, {\"name\": \"lmstudio-community/ZR1-1.5B-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/ZR1-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Apr 11\"}, {\"name\": \"cgus/ZR1-1.5B-exl2\", \"link\": \"https://huggingface.co/cgus/ZR1-1.5B-exl2\", \"task\": \"Text Generation\", \"likes\": \"16\", \"downloads\": \"\", \"updated\": \"Apr 12\"}, {\"name\": \"Mungert/ZR1-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/ZR1-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"403\", \"downloads\": \"\", \"updated\": \"Sep 24\"}]",
    "num_datasets": 3,
    "datasets_list": "PRIME-RL/Eurus-2-RL-Data, PRIME-RL/Eurus-2-SFT-Data, PRIME-RL/EurusPRM-Stage1-Data",
    "datasets_links": "https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data, https://huggingface.co/datasets/PRIME-RL/Eurus-2-SFT-Data, https://huggingface.co/datasets/PRIME-RL/EurusPRM-Stage1-Data",
    "datasets_detailed": "[{\"name\": \"PRIME-RL/Eurus-2-RL-Data\", \"link\": \"https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\", \"size\": \"\"}, {\"name\": \"PRIME-RL/Eurus-2-SFT-Data\", \"link\": \"https://huggingface.co/datasets/PRIME-RL/Eurus-2-SFT-Data\", \"task\": \"\", \"likes\": \"187\", \"downloads\": \"\", \"updated\": \"Feb 19\", \"size\": \"\"}, {\"name\": \"PRIME-RL/EurusPRM-Stage1-Data\", \"link\": \"https://huggingface.co/datasets/PRIME-RL/EurusPRM-Stage1-Data\", \"task\": \"\", \"likes\": \"38\", \"downloads\": \"\", \"updated\": \"Feb 19\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.19339",
    "first_seen_date": "2025-02-03",
    "title": "PixelWorld: Towards Perceiving Everything as Pixels",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.19339PixelWorld: Towards Perceiving Everything as PixelsPublished on Jan 31\u00b7Submitted byWenhu Chenon Feb 3Upvote17+9Authors:Zhiheng Lyu,Xueguang Ma,Wenhu ChenAbstractA unified perception framework treating all modalities as pixel inputs (PEAP) outperforms token-based input in some tasks but degrades reasoning and coding capabilities, requiring enhancements in foundational models\u2019 perceptual abilities.AI-generated summaryExisting foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introducePixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models' performance. Our findings show\nthat (1) PEAP outperforms baseline withtoken-based inputin multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels'perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) theattention patternof PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting thespatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset wil",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/TIGER-AI-Lab/PixelWorld",
    "hf_paper_url": "https://huggingface.co/papers/2501.19339",
    "arxiv_url": "https://arxiv.org/abs/2501.19339",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/PixelWorld",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/PixelWorld",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/PixelWorld\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/PixelWorld\", \"task\": \"\", \"likes\": \"527\", \"downloads\": \"\", \"updated\": \"Feb 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.19393",
    "first_seen_date": "2025-02-03",
    "title": "s1: Simple test-time scaling",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.19393s1: Simple test-time scalingPublished on Jan 31\u00b7Submitted byAKon Feb 3#1 Paper of the dayUpvote124+116Authors:Niklas Muennighoff,Zitong Yang,Weijia Shi,Xiang Lisa Li,Li Fei-Fei,Hannaneh Hajishirzi,Luke Zettlemoyer,Percy Liang,Emmanuel Cand\u00e8s,Tatsunori HashimotoAbstractThe use of budget forcing during test time improves reasoning performance in language models, as demonstrated by the s1 model which outperforms the o1-preview on competition math questions.AI-generated summaryTest-time scalingis a promising new approach tolanguage modelingthat uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired withreasoning tracesrelying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelopbudget forcingto control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. Aftersupervised finetuningtheQwen2.5-32B-Instructlanguage model on s1K and\nequipping it withbudget forcing, our model s1 exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling s1\nwithbudget forcingallows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1.View arXiv pageView PDFGitHub6.62kautoAdd to collectionCommunityakhaliqPaper submitterFeb 3https://github.com/simplescaling/s1See translation\ud83d\udc4d11+ReplypengdanFeb 3@librar",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/simplescaling/s1",
    "hf_paper_url": "https://huggingface.co/papers/2501.19393",
    "arxiv_url": "https://arxiv.org/abs/2501.19393",
    "num_models": 57,
    "models_list": "google/medgemma-4b-it, google/medgemma-27b-text-it, simplescaling/s1-32B, google/medgemma-27b-it, google/medgemma-4b-pt, unsloth/medgemma-4b-it-GGUF, unsloth/medgemma-27b-text-it-GGUF, Intelligent-Internet/II-Medical-8B-1706, simplescaling/step-conditional-control-old, simplescaling/step-conditional-control, simplescaling/token-conditional-control, brittlewis12/s1-32B-GGUF, lemonilia/Mistral-Small-3-Reasoner-s1, cortexso/simplescaling-s1, mhdaw/s1-32B-awq, FallnAI/S1-Reasoning-32B, simplescaling/s1.1-32B, 2stacks/s1-0.5B, brittlewis12/s1.1-32B-GGUF, Valdemardi/s1.1-32B-AWQ, 2stacks/s1.1-0.5B, 2stacks/s1.1-1.5B, dankalin/ruadapt-s1, TikaToka/s1.1-1.5B-20k-bf16, II-Vietnam/II-Medical-8B-SFT, Intelligent-Internet/II-Medical-8B, unsloth/medgemma-27b-text-it, unsloth/medgemma-4b-it, unsloth/medgemma-4b-it-unsloth-bnb-4bit, unsloth/medgemma-4b-it-bnb-4bit, unsloth/medgemma-27b-text-it-unsloth-bnb-4bit, unsloth/medgemma-27b-text-it-bnb-4bit, unsloth/medgemma-4b-pt, vinuajeesh/medgemma-4b-pt, AnteriorAI/medgemma-4b-it, Mungert/medgemma-27b-text-it-GGUF, Mungert/medgemma-4b-it-GGUF, QuantFactory/II-Medical-8B-GGUF, Mungert/medgemma-4b-pt-GGUF, QuantFactory/II-Medical-8B-1706-GGUF, Mungert/II-Medical-8B-1706-GGUF, Prince-1/medgemma-4b-pt, Prince-1/Medgemma-4b-pt-Onnx, jamessatomgb/google-medgemma-4b-it-transformers, gabriellarson/medgemma-27b-it-GGUF, unsloth/medgemma-27b-it, unsloth/medgemma-27b-it-GGUF, Prince-1/medgemma-27b-it, onnx-community/MedGemma-27B-IT, FastFlowLM/medgemma-4b-it-NPU2, Muhammadidrees/my-medgamma, Muhammadidrees/Medgamma27B, OpenModels4all/mgemma, zarnow/medgemma-4b-it-bnb-4bit, pszemraj/medgemma-4b-it-heretic, pszemraj/medgemma-27b-text-heretic_med, tanujd/mg4b",
    "models_links": "https://huggingface.co/google/medgemma-4b-it, https://huggingface.co/google/medgemma-27b-text-it, https://huggingface.co/simplescaling/s1-32B, https://huggingface.co/google/medgemma-27b-it, https://huggingface.co/google/medgemma-4b-pt, https://huggingface.co/unsloth/medgemma-4b-it-GGUF, https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF, https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706, https://huggingface.co/simplescaling/step-conditional-control-old, https://huggingface.co/simplescaling/step-conditional-control, https://huggingface.co/simplescaling/token-conditional-control, https://huggingface.co/brittlewis12/s1-32B-GGUF, https://huggingface.co/lemonilia/Mistral-Small-3-Reasoner-s1, https://huggingface.co/cortexso/simplescaling-s1, https://huggingface.co/mhdaw/s1-32B-awq, https://huggingface.co/FallnAI/S1-Reasoning-32B, https://huggingface.co/simplescaling/s1.1-32B, https://huggingface.co/2stacks/s1-0.5B, https://huggingface.co/brittlewis12/s1.1-32B-GGUF, https://huggingface.co/Valdemardi/s1.1-32B-AWQ, https://huggingface.co/2stacks/s1.1-0.5B, https://huggingface.co/2stacks/s1.1-1.5B, https://huggingface.co/dankalin/ruadapt-s1, https://huggingface.co/TikaToka/s1.1-1.5B-20k-bf16, https://huggingface.co/II-Vietnam/II-Medical-8B-SFT, https://huggingface.co/Intelligent-Internet/II-Medical-8B, https://huggingface.co/unsloth/medgemma-27b-text-it, https://huggingface.co/unsloth/medgemma-4b-it, https://huggingface.co/unsloth/medgemma-4b-it-unsloth-bnb-4bit, https://huggingface.co/unsloth/medgemma-4b-it-bnb-4bit, https://huggingface.co/unsloth/medgemma-27b-text-it-unsloth-bnb-4bit, https://huggingface.co/unsloth/medgemma-27b-text-it-bnb-4bit, https://huggingface.co/unsloth/medgemma-4b-pt, https://huggingface.co/vinuajeesh/medgemma-4b-pt, https://huggingface.co/AnteriorAI/medgemma-4b-it, https://huggingface.co/Mungert/medgemma-27b-text-it-GGUF, https://huggingface.co/Mungert/medgemma-4b-it-GGUF, https://huggingface.co/QuantFactory/II-Medical-8B-GGUF, https://huggingface.co/Mungert/medgemma-4b-pt-GGUF, https://huggingface.co/QuantFactory/II-Medical-8B-1706-GGUF, https://huggingface.co/Mungert/II-Medical-8B-1706-GGUF, https://huggingface.co/Prince-1/medgemma-4b-pt, https://huggingface.co/Prince-1/Medgemma-4b-pt-Onnx, https://huggingface.co/jamessatomgb/google-medgemma-4b-it-transformers, https://huggingface.co/gabriellarson/medgemma-27b-it-GGUF, https://huggingface.co/unsloth/medgemma-27b-it, https://huggingface.co/unsloth/medgemma-27b-it-GGUF, https://huggingface.co/Prince-1/medgemma-27b-it, https://huggingface.co/onnx-community/MedGemma-27B-IT, https://huggingface.co/FastFlowLM/medgemma-4b-it-NPU2, https://huggingface.co/Muhammadidrees/my-medgamma, https://huggingface.co/Muhammadidrees/Medgamma27B, https://huggingface.co/OpenModels4all/mgemma, https://huggingface.co/zarnow/medgemma-4b-it-bnb-4bit, https://huggingface.co/pszemraj/medgemma-4b-it-heretic, https://huggingface.co/pszemraj/medgemma-27b-text-heretic_med, https://huggingface.co/tanujd/mg4b",
    "models_detailed": "[{\"name\": \"google/medgemma-4b-it\", \"link\": \"https://huggingface.co/google/medgemma-4b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"google/medgemma-27b-text-it\", \"link\": \"https://huggingface.co/google/medgemma-27b-text-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"simplescaling/s1-32B\", \"link\": \"https://huggingface.co/simplescaling/s1-32B\", \"task\": \"Text Generation\", \"likes\": \"854\", \"downloads\": \"\", \"updated\": \"Feb 26\"}, {\"name\": \"google/medgemma-27b-it\", \"link\": \"https://huggingface.co/google/medgemma-27b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"google/medgemma-4b-pt\", \"link\": \"https://huggingface.co/google/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"unsloth/medgemma-4b-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"unsloth/medgemma-27b-text-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"Intelligent-Internet/II-Medical-8B-1706\", \"link\": \"https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706\", \"task\": \"Text Generation\", \"likes\": \"265\", \"downloads\": \"\", \"updated\": \"Aug 12\"}, {\"name\": \"simplescaling/step-conditional-control-old\", \"link\": \"https://huggingface.co/simplescaling/step-conditional-control-old\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"simplescaling/step-conditional-control\", \"link\": \"https://huggingface.co/simplescaling/step-conditional-control\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 3\"}, {\"name\": \"simplescaling/token-conditional-control\", \"link\": \"https://huggingface.co/simplescaling/token-conditional-control\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 3\"}, {\"name\": \"brittlewis12/s1-32B-GGUF\", \"link\": \"https://huggingface.co/brittlewis12/s1-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 11\"}, {\"name\": \"lemonilia/Mistral-Small-3-Reasoner-s1\", \"link\": \"https://huggingface.co/lemonilia/Mistral-Small-3-Reasoner-s1\", \"task\": \"\", \"likes\": \"572\", \"downloads\": \"\", \"updated\": \"Feb 8\"}, {\"name\": \"cortexso/simplescaling-s1\", \"link\": \"https://huggingface.co/cortexso/simplescaling-s1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 3\"}, {\"name\": \"mhdaw/s1-32B-awq\", \"link\": \"https://huggingface.co/mhdaw/s1-32B-awq\", \"task\": \"Text Generation\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Feb 6\"}, {\"name\": \"FallnAI/S1-Reasoning-32B\", \"link\": \"https://huggingface.co/FallnAI/S1-Reasoning-32B\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"Feb 7\"}, {\"name\": \"simplescaling/s1.1-32B\", \"link\": \"https://huggingface.co/simplescaling/s1.1-32B\", \"task\": \"Text Generation\", \"likes\": \"641\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"2stacks/s1-0.5B\", \"link\": \"https://huggingface.co/2stacks/s1-0.5B\", \"task\": \"Text Generation\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Apr 28\"}, {\"name\": \"brittlewis12/s1.1-32B-GGUF\", \"link\": \"https://huggingface.co/brittlewis12/s1.1-32B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 12\"}, {\"name\": \"Valdemardi/s1.1-32B-AWQ\", \"link\": \"https://huggingface.co/Valdemardi/s1.1-32B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 12\"}, {\"name\": \"2stacks/s1.1-0.5B\", \"link\": \"https://huggingface.co/2stacks/s1.1-0.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 28\"}, {\"name\": \"2stacks/s1.1-1.5B\", \"link\": \"https://huggingface.co/2stacks/s1.1-1.5B\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Apr 28\"}, {\"name\": \"dankalin/ruadapt-s1\", \"link\": \"https://huggingface.co/dankalin/ruadapt-s1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 26\"}, {\"name\": \"TikaToka/s1.1-1.5B-20k-bf16\", \"link\": \"https://huggingface.co/TikaToka/s1.1-1.5B-20k-bf16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 5\"}, {\"name\": \"II-Vietnam/II-Medical-8B-SFT\", \"link\": \"https://huggingface.co/II-Vietnam/II-Medical-8B-SFT\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"Intelligent-Internet/II-Medical-8B\", \"link\": \"https://huggingface.co/Intelligent-Internet/II-Medical-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 12\"}, {\"name\": \"unsloth/medgemma-27b-text-it\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-text-it\", \"task\": \"\", \"likes\": \"969\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-4b-it\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"unsloth/medgemma-4b-it-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-4b-it-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it-bnb-4bit\", \"task\": \"\", \"likes\": \"587\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-27b-text-it-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-text-it-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-27b-text-it-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-text-it-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-4b-pt\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"93\", \"downloads\": \"\", \"updated\": \"Jun 15\"}, {\"name\": \"vinuajeesh/medgemma-4b-pt\", \"link\": \"https://huggingface.co/vinuajeesh/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"AnteriorAI/medgemma-4b-it\", \"link\": \"https://huggingface.co/AnteriorAI/medgemma-4b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 28\"}, {\"name\": \"Mungert/medgemma-27b-text-it-GGUF\", \"link\": \"https://huggingface.co/Mungert/medgemma-27b-text-it-GGUF\", \"task\": \"Text Generation\", \"likes\": \"385\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/medgemma-4b-it-GGUF\", \"link\": \"https://huggingface.co/Mungert/medgemma-4b-it-GGUF\", \"task\": \"\", \"likes\": \"421\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"QuantFactory/II-Medical-8B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/II-Medical-8B-GGUF\", \"task\": \"\", \"likes\": \"250\", \"downloads\": \"\", \"updated\": \"Jun 15\"}, {\"name\": \"Mungert/medgemma-4b-pt-GGUF\", \"link\": \"https://huggingface.co/Mungert/medgemma-4b-pt-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"QuantFactory/II-Medical-8B-1706-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/II-Medical-8B-1706-GGUF\", \"task\": \"\", \"likes\": \"79\", \"downloads\": \"\", \"updated\": \"Jun 21\"}, {\"name\": \"Mungert/II-Medical-8B-1706-GGUF\", \"link\": \"https://huggingface.co/Mungert/II-Medical-8B-1706-GGUF\", \"task\": \"\", \"likes\": \"376\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Prince-1/medgemma-4b-pt\", \"link\": \"https://huggingface.co/Prince-1/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 4\"}, {\"name\": \"Prince-1/Medgemma-4b-pt-Onnx\", \"link\": \"https://huggingface.co/Prince-1/Medgemma-4b-pt-Onnx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 4\"}, {\"name\": \"jamessatomgb/google-medgemma-4b-it-transformers\", \"link\": \"https://huggingface.co/jamessatomgb/google-medgemma-4b-it-transformers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"gabriellarson/medgemma-27b-it-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/medgemma-27b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"unsloth/medgemma-27b-it\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-it\", \"task\": \"\", \"likes\": \"183\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"unsloth/medgemma-27b-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"Prince-1/medgemma-27b-it\", \"link\": \"https://huggingface.co/Prince-1/medgemma-27b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"onnx-community/MedGemma-27B-IT\", \"link\": \"https://huggingface.co/onnx-community/MedGemma-27B-IT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"FastFlowLM/medgemma-4b-it-NPU2\", \"link\": \"https://huggingface.co/FastFlowLM/medgemma-4b-it-NPU2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 26\"}, {\"name\": \"Muhammadidrees/my-medgamma\", \"link\": \"https://huggingface.co/Muhammadidrees/my-medgamma\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\"}, {\"name\": \"Muhammadidrees/Medgamma27B\", \"link\": \"https://huggingface.co/Muhammadidrees/Medgamma27B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"OpenModels4all/mgemma\", \"link\": \"https://huggingface.co/OpenModels4all/mgemma\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"zarnow/medgemma-4b-it-bnb-4bit\", \"link\": \"https://huggingface.co/zarnow/medgemma-4b-it-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"pszemraj/medgemma-4b-it-heretic\", \"link\": \"https://huggingface.co/pszemraj/medgemma-4b-it-heretic\", \"task\": \"\", \"likes\": \"94\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"pszemraj/medgemma-27b-text-heretic_med\", \"link\": \"https://huggingface.co/pszemraj/medgemma-27b-text-heretic_med\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"29 days ago\"}, {\"name\": \"tanujd/mg4b\", \"link\": \"https://huggingface.co/tanujd/mg4b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 27,
    "datasets_list": "simplescaling/s1K-1.1, simplescaling/results, simplescaling/s1K, simplescaling/openaimath, Intelligent-Internet/II-Medical-Reasoning-SFT, simplescaling/s1-prob, simplescaling/s1-teasers, simplescaling/data_ablation_full59K, simplescaling/s1K-step-conditional-control-old, simplescaling/data_ablation_longest1K, simplescaling/data_ablation_random1K, simplescaling/data_ablation_diverse1K, simplescaling/aime_figures, simplescaling/aime24_figures, simplescaling/aime_nofigures, simplescaling/aime24_nofigures, simplescaling/s1K_tokenized, tarob0ba/s1K_think_tag, DONGJINAG/my-cool-dataset, DONGJINAG/s1k-cupy, Afnanh10/aime24_nofigures_copy, Afnanh10/openaimath_clone, Can111/m500, NewstaR/CoTton-64k-6725-Collective, NewstaR/CoTton-67k-6725-Collective, jdhwang/s1K-X, kabariap/II-Medical-Reasoning-SFT",
    "datasets_links": "https://huggingface.co/datasets/simplescaling/s1K-1.1, https://huggingface.co/datasets/simplescaling/results, https://huggingface.co/datasets/simplescaling/s1K, https://huggingface.co/datasets/simplescaling/openaimath, https://huggingface.co/datasets/Intelligent-Internet/II-Medical-Reasoning-SFT, https://huggingface.co/datasets/simplescaling/s1-prob, https://huggingface.co/datasets/simplescaling/s1-teasers, https://huggingface.co/datasets/simplescaling/data_ablation_full59K, https://huggingface.co/datasets/simplescaling/s1K-step-conditional-control-old, https://huggingface.co/datasets/simplescaling/data_ablation_longest1K, https://huggingface.co/datasets/simplescaling/data_ablation_random1K, https://huggingface.co/datasets/simplescaling/data_ablation_diverse1K, https://huggingface.co/datasets/simplescaling/aime_figures, https://huggingface.co/datasets/simplescaling/aime24_figures, https://huggingface.co/datasets/simplescaling/aime_nofigures, https://huggingface.co/datasets/simplescaling/aime24_nofigures, https://huggingface.co/datasets/simplescaling/s1K_tokenized, https://huggingface.co/datasets/tarob0ba/s1K_think_tag, https://huggingface.co/datasets/DONGJINAG/my-cool-dataset, https://huggingface.co/datasets/DONGJINAG/s1k-cupy, https://huggingface.co/datasets/Afnanh10/aime24_nofigures_copy, https://huggingface.co/datasets/Afnanh10/openaimath_clone, https://huggingface.co/datasets/Can111/m500, https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective, https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective, https://huggingface.co/datasets/jdhwang/s1K-X, https://huggingface.co/datasets/kabariap/II-Medical-Reasoning-SFT",
    "datasets_detailed": "[{\"name\": \"simplescaling/s1K-1.1\", \"link\": \"https://huggingface.co/datasets/simplescaling/s1K-1.1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 27\", \"size\": \"\"}, {\"name\": \"simplescaling/results\", \"link\": \"https://huggingface.co/datasets/simplescaling/results\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 9\", \"size\": \"\"}, {\"name\": \"simplescaling/s1K\", \"link\": \"https://huggingface.co/datasets/simplescaling/s1K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 11\", \"size\": \"\"}, {\"name\": \"simplescaling/openaimath\", \"link\": \"https://huggingface.co/datasets/simplescaling/openaimath\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 28\", \"size\": \"\"}, {\"name\": \"Intelligent-Internet/II-Medical-Reasoning-SFT\", \"link\": \"https://huggingface.co/datasets/Intelligent-Internet/II-Medical-Reasoning-SFT\", \"task\": \"\", \"likes\": \"722\", \"downloads\": \"\", \"updated\": \"Jul 3\", \"size\": \"\"}, {\"name\": \"simplescaling/s1-prob\", \"link\": \"https://huggingface.co/datasets/simplescaling/s1-prob\", \"task\": \"\", \"likes\": \"182\", \"downloads\": \"\", \"updated\": \"Jul 3\", \"size\": \"\"}, {\"name\": \"simplescaling/s1-teasers\", \"link\": \"https://huggingface.co/datasets/simplescaling/s1-teasers\", \"task\": \"\", \"likes\": \"24\", \"downloads\": \"\", \"updated\": \"Jul 3\", \"size\": \"\"}, {\"name\": \"simplescaling/data_ablation_full59K\", \"link\": \"https://huggingface.co/datasets/simplescaling/data_ablation_full59K\", \"task\": \"\", \"likes\": \"647\", \"downloads\": \"\", \"updated\": \"Feb 3\", \"size\": \"\"}, {\"name\": \"simplescaling/s1K-step-conditional-control-old\", \"link\": \"https://huggingface.co/datasets/simplescaling/s1K-step-conditional-control-old\", \"task\": \"\", \"likes\": \"35\", \"downloads\": \"\", \"updated\": \"Feb 3\", \"size\": \"\"}, {\"name\": \"simplescaling/data_ablation_longest1K\", \"link\": \"https://huggingface.co/datasets/simplescaling/data_ablation_longest1K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 3\", \"size\": \"\"}, {\"name\": \"simplescaling/data_ablation_random1K\", \"link\": \"https://huggingface.co/datasets/simplescaling/data_ablation_random1K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 3\", \"size\": \"\"}, {\"name\": \"simplescaling/data_ablation_diverse1K\", \"link\": \"https://huggingface.co/datasets/simplescaling/data_ablation_diverse1K\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 3\", \"size\": \"\"}, {\"name\": \"simplescaling/aime_figures\", \"link\": \"https://huggingface.co/datasets/simplescaling/aime_figures\", \"task\": \"\", \"likes\": \"90\", \"downloads\": \"\", \"updated\": \"Feb 3\", \"size\": \"\"}, {\"name\": \"simplescaling/aime24_figures\", \"link\": \"https://huggingface.co/datasets/simplescaling/aime24_figures\", \"task\": \"\", \"likes\": \"30\", \"downloads\": \"\", \"updated\": \"Jun 30\", \"size\": \"\"}, {\"name\": \"simplescaling/aime_nofigures\", \"link\": \"https://huggingface.co/datasets/simplescaling/aime_nofigures\", \"task\": \"\", \"likes\": \"90\", \"downloads\": \"\", \"updated\": \"Feb 3\", \"size\": \"\"}, {\"name\": \"simplescaling/aime24_nofigures\", \"link\": \"https://huggingface.co/datasets/simplescaling/aime24_nofigures\", \"task\": \"\", \"likes\": \"30\", \"downloads\": \"\", \"updated\": \"Jun 30\", \"size\": \"\"}, {\"name\": \"simplescaling/s1K_tokenized\", \"link\": \"https://huggingface.co/datasets/simplescaling/s1K_tokenized\", \"task\": \"\", \"likes\": \"297\", \"downloads\": \"\", \"updated\": \"Feb 6\", \"size\": \"\"}, {\"name\": \"tarob0ba/s1K_think_tag\", \"link\": \"https://huggingface.co/datasets/tarob0ba/s1K_think_tag\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 15\", \"size\": \"\"}, {\"name\": \"DONGJINAG/my-cool-dataset\", \"link\": \"https://huggingface.co/datasets/DONGJINAG/my-cool-dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 11\", \"size\": \"\"}, {\"name\": \"DONGJINAG/s1k-cupy\", \"link\": \"https://huggingface.co/datasets/DONGJINAG/s1k-cupy\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 13\", \"size\": \"\"}, {\"name\": \"Afnanh10/aime24_nofigures_copy\", \"link\": \"https://huggingface.co/datasets/Afnanh10/aime24_nofigures_copy\", \"task\": \"\", \"likes\": \"3\", \"downloads\": \"\", \"updated\": \"Mar 3\", \"size\": \"\"}, {\"name\": \"Afnanh10/openaimath_clone\", \"link\": \"https://huggingface.co/datasets/Afnanh10/openaimath_clone\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\", \"size\": \"\"}, {\"name\": \"Can111/m500\", \"link\": \"https://huggingface.co/datasets/Can111/m500\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Aug 20\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-64k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-64k-6725-Collective\", \"task\": \"\", \"likes\": \"95\", \"downloads\": \"\", \"updated\": \"Jun 8\", \"size\": \"\"}, {\"name\": \"NewstaR/CoTton-67k-6725-Collective\", \"link\": \"https://huggingface.co/datasets/NewstaR/CoTton-67k-6725-Collective\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Jun 7\", \"size\": \"\"}, {\"name\": \"jdhwang/s1K-X\", \"link\": \"https://huggingface.co/datasets/jdhwang/s1K-X\", \"task\": \"\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Sep 4\", \"size\": \"\"}, {\"name\": \"kabariap/II-Medical-Reasoning-SFT\", \"link\": \"https://huggingface.co/datasets/kabariap/II-Medical-Reasoning-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"1 day ago\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.19399",
    "first_seen_date": "2025-02-03",
    "title": "Scalable-Softmax Is Superior for Attention",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.19399Scalable-Softmax Is Superior for AttentionPublished on Jan 31\u00b7Submitted bySamuel Arcadinhoon Feb 3Upvote24+16Authors:Ken M. NakanishiAbstractScalable-Softmax (SSMax) enhances Transformer-based models by improving attention distribution and long-context performance, resolving issues posed by the Softmax function.AI-generated summaryThe maximum element of the vector output by theSoftmaxfunction approaches\nzero as the input vector size increases. Transformer-based language models rely\nonSoftmaxto computeattention scores, causing theattention distributionto\nflatten as thecontext sizegrows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we proposeScalable-Softmax (SSMax),\nwhich replacesSoftmaxin scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction duringpretrainingbut also significantly improve\nperformance inlong contextsandkey information retrieval. Furthermore, an\nanalysis ofattention scoresreveals that SSMax enables the model to focus\nattention on key information even inlong contexts. Additionally, although\nmodels that use SSMax from the beginning ofpretrainingachieve better length\ngeneralization, those that have already startedpretrainingcan still gain some\nof this ability by replacingSoftmaxin the attention layers with SSMax, either\nduring or afterpretraining.View arXiv pageView PDFGitHub17autoAdd to collectionCommunitySSamDavPaper submitterFeb 3Approach that fixes softmax squashing.See translation\ud83d\udd2511+ReplycnxhkFeb 4Similar idea is explained inhttps://arxiv.org/abs/2202.12172Section 5.3See translation\ud83d\udc4d22+Replylibrarian-botFeb 4This is an automated message from theLibrarian Bot. I found th",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/gdevos010/Scalable-Softmax",
    "hf_paper_url": "https://huggingface.co/papers/2501.19399",
    "arxiv_url": "https://arxiv.org/abs/2501.19399",
    "num_models": 7,
    "models_list": "mistralai/Devstral-Small-2-24B-Instruct-2512, unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF, unsloth/Devstral-Small-2-24B-Instruct-2512, cyankiwi/Devstral-Small-2-24B-Instruct-2512-AWQ-4bit, ExaltedSlayer/mistralai-devstral-small-2-24b-instruct-2512-mlx-mxfp4, AlexanderKyng/Devstral-Small-2-24B-Instruct-2512-exl3-4.5bpw-optimized, akoumpa/Devstral-Small-2-24B-Instruct-2512-BF16",
    "models_links": "https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512, https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF, https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512, https://huggingface.co/cyankiwi/Devstral-Small-2-24B-Instruct-2512-AWQ-4bit, https://huggingface.co/ExaltedSlayer/mistralai-devstral-small-2-24b-instruct-2512-mlx-mxfp4, https://huggingface.co/AlexanderKyng/Devstral-Small-2-24B-Instruct-2512-exl3-4.5bpw-optimized, https://huggingface.co/akoumpa/Devstral-Small-2-24B-Instruct-2512-BF16",
    "models_detailed": "[{\"name\": \"mistralai/Devstral-Small-2-24B-Instruct-2512\", \"link\": \"https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"2 days ago\"}, {\"name\": \"unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF\", \"link\": \"https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"unsloth/Devstral-Small-2-24B-Instruct-2512\", \"link\": \"https://huggingface.co/unsloth/Devstral-Small-2-24B-Instruct-2512\", \"task\": \"\", \"likes\": \"663\", \"downloads\": \"\", \"updated\": \"7 days ago\"}, {\"name\": \"cyankiwi/Devstral-Small-2-24B-Instruct-2512-AWQ-4bit\", \"link\": \"https://huggingface.co/cyankiwi/Devstral-Small-2-24B-Instruct-2512-AWQ-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"10 days ago\"}, {\"name\": \"ExaltedSlayer/mistralai-devstral-small-2-24b-instruct-2512-mlx-mxfp4\", \"link\": \"https://huggingface.co/ExaltedSlayer/mistralai-devstral-small-2-24b-instruct-2512-mlx-mxfp4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"AlexanderKyng/Devstral-Small-2-24B-Instruct-2512-exl3-4.5bpw-optimized\", \"link\": \"https://huggingface.co/AlexanderKyng/Devstral-Small-2-24B-Instruct-2512-exl3-4.5bpw-optimized\", \"task\": \"\", \"likes\": \"88\", \"downloads\": \"\", \"updated\": \"11 days ago\"}, {\"name\": \"akoumpa/Devstral-Small-2-24B-Instruct-2512-BF16\", \"link\": \"https://huggingface.co/akoumpa/Devstral-Small-2-24B-Instruct-2512-BF16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2501.18362",
    "first_seen_date": "2025-01-31",
    "title": "MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  Understanding",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.18362MedXpertQA: Benchmarking Expert-Level Medical Reasoning and\n  UnderstandingPublished on Jan 30\u00b7Submitted byShang (Lindsay) Quon Jan 31Upvote23+15Authors:Yuxin Zuo,Shang Qu,Yifei Li,Zhangren Chen,Xuekai Zhu,Ermo Hua,Kaiyan Zhang,Ning Ding,Bowen ZhouAbstractMedXpertQA is a benchmark for evaluating medical expertise and reasoning with a focus on advanced clinical knowledge and diverse data, including images and patient records.AI-generated summaryWe introduceMedXpertQA, a highly challenging and comprehensivebenchmarkto\nevaluate expert-level medical knowledge and advanced reasoning.MedXpertQAincludes 4,460 questions spanning 17 specialties and 11 body systems. It\nincludes two subsets, Text fortext evaluationand MM for multimodal\nevaluation. Notably, MM introduces expert-level exam questions with diverse\nimages and richclinical information, including patient records and examination\nresults, setting it apart from traditional medical multimodalbenchmarks with\nsimple QA pairs generated from image captions.MedXpertQAapplies rigorous\nfiltering and augmentation to address the insufficient difficulty of existingbenchmarks like MedQA, and incorporates specialty board questions to improve\nclinical relevance and comprehensiveness. We performdata synthesisto mitigate\ndata leakage risk and conduct multiple rounds ofexpert reviewsto ensure\naccuracy and reliability. We evaluate 16 leading models onMedXpertQA.\nMoreover, medicine is deeply connected to real-world decision-making, providing\na rich and representative setting for assessing reasoning abilities beyond\nmathematics and code. To this end, we develop areasoning-oriented subsetto\nfacilitate the assessment of o1-like models.View arXiv pageView PDFGitHub137Add to collectionCommunitylindsay-quPaper authorPaper submitterJan 31MedXpertQAis a highly challenging and comprehensive benchmark to evaluateexpert-level medical knowledge and advanc",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/TsinghuaC3I/MedXpertQA",
    "hf_paper_url": "https://huggingface.co/papers/2501.18362",
    "arxiv_url": "https://arxiv.org/abs/2501.18362",
    "num_models": 34,
    "models_list": "google/medgemma-4b-it, google/medgemma-27b-text-it, google/medgemma-27b-it, google/medgemma-4b-pt, unsloth/medgemma-4b-it-GGUF, unsloth/medgemma-27b-text-it-GGUF, unsloth/medgemma-27b-text-it, unsloth/medgemma-4b-it, unsloth/medgemma-4b-it-unsloth-bnb-4bit, unsloth/medgemma-4b-it-bnb-4bit, unsloth/medgemma-27b-text-it-unsloth-bnb-4bit, unsloth/medgemma-27b-text-it-bnb-4bit, unsloth/medgemma-4b-pt, vinuajeesh/medgemma-4b-pt, AnteriorAI/medgemma-4b-it, Mungert/medgemma-27b-text-it-GGUF, Mungert/medgemma-4b-it-GGUF, Mungert/medgemma-4b-pt-GGUF, Prince-1/medgemma-4b-pt, Prince-1/Medgemma-4b-pt-Onnx, jamessatomgb/google-medgemma-4b-it-transformers, gabriellarson/medgemma-27b-it-GGUF, unsloth/medgemma-27b-it, unsloth/medgemma-27b-it-GGUF, Prince-1/medgemma-27b-it, onnx-community/MedGemma-27B-IT, FastFlowLM/medgemma-4b-it-NPU2, Muhammadidrees/my-medgamma, Muhammadidrees/Medgamma27B, OpenModels4all/mgemma, zarnow/medgemma-4b-it-bnb-4bit, pszemraj/medgemma-4b-it-heretic, pszemraj/medgemma-27b-text-heretic_med, tanujd/mg4b",
    "models_links": "https://huggingface.co/google/medgemma-4b-it, https://huggingface.co/google/medgemma-27b-text-it, https://huggingface.co/google/medgemma-27b-it, https://huggingface.co/google/medgemma-4b-pt, https://huggingface.co/unsloth/medgemma-4b-it-GGUF, https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF, https://huggingface.co/unsloth/medgemma-27b-text-it, https://huggingface.co/unsloth/medgemma-4b-it, https://huggingface.co/unsloth/medgemma-4b-it-unsloth-bnb-4bit, https://huggingface.co/unsloth/medgemma-4b-it-bnb-4bit, https://huggingface.co/unsloth/medgemma-27b-text-it-unsloth-bnb-4bit, https://huggingface.co/unsloth/medgemma-27b-text-it-bnb-4bit, https://huggingface.co/unsloth/medgemma-4b-pt, https://huggingface.co/vinuajeesh/medgemma-4b-pt, https://huggingface.co/AnteriorAI/medgemma-4b-it, https://huggingface.co/Mungert/medgemma-27b-text-it-GGUF, https://huggingface.co/Mungert/medgemma-4b-it-GGUF, https://huggingface.co/Mungert/medgemma-4b-pt-GGUF, https://huggingface.co/Prince-1/medgemma-4b-pt, https://huggingface.co/Prince-1/Medgemma-4b-pt-Onnx, https://huggingface.co/jamessatomgb/google-medgemma-4b-it-transformers, https://huggingface.co/gabriellarson/medgemma-27b-it-GGUF, https://huggingface.co/unsloth/medgemma-27b-it, https://huggingface.co/unsloth/medgemma-27b-it-GGUF, https://huggingface.co/Prince-1/medgemma-27b-it, https://huggingface.co/onnx-community/MedGemma-27B-IT, https://huggingface.co/FastFlowLM/medgemma-4b-it-NPU2, https://huggingface.co/Muhammadidrees/my-medgamma, https://huggingface.co/Muhammadidrees/Medgamma27B, https://huggingface.co/OpenModels4all/mgemma, https://huggingface.co/zarnow/medgemma-4b-it-bnb-4bit, https://huggingface.co/pszemraj/medgemma-4b-it-heretic, https://huggingface.co/pszemraj/medgemma-27b-text-heretic_med, https://huggingface.co/tanujd/mg4b",
    "models_detailed": "[{\"name\": \"google/medgemma-4b-it\", \"link\": \"https://huggingface.co/google/medgemma-4b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 28\"}, {\"name\": \"google/medgemma-27b-text-it\", \"link\": \"https://huggingface.co/google/medgemma-27b-text-it\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"google/medgemma-27b-it\", \"link\": \"https://huggingface.co/google/medgemma-27b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"google/medgemma-4b-pt\", \"link\": \"https://huggingface.co/google/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"unsloth/medgemma-4b-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"unsloth/medgemma-27b-text-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-text-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-27b-text-it\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-text-it\", \"task\": \"\", \"likes\": \"969\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-4b-it\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 15\"}, {\"name\": \"unsloth/medgemma-4b-it-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-4b-it-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-it-bnb-4bit\", \"task\": \"\", \"likes\": \"587\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-27b-text-it-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-text-it-unsloth-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-27b-text-it-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-text-it-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"unsloth/medgemma-4b-pt\", \"link\": \"https://huggingface.co/unsloth/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"93\", \"downloads\": \"\", \"updated\": \"Jun 15\"}, {\"name\": \"vinuajeesh/medgemma-4b-pt\", \"link\": \"https://huggingface.co/vinuajeesh/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 24\"}, {\"name\": \"AnteriorAI/medgemma-4b-it\", \"link\": \"https://huggingface.co/AnteriorAI/medgemma-4b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 28\"}, {\"name\": \"Mungert/medgemma-27b-text-it-GGUF\", \"link\": \"https://huggingface.co/Mungert/medgemma-27b-text-it-GGUF\", \"task\": \"Text Generation\", \"likes\": \"385\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/medgemma-4b-it-GGUF\", \"link\": \"https://huggingface.co/Mungert/medgemma-4b-it-GGUF\", \"task\": \"\", \"likes\": \"421\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/medgemma-4b-pt-GGUF\", \"link\": \"https://huggingface.co/Mungert/medgemma-4b-pt-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Prince-1/medgemma-4b-pt\", \"link\": \"https://huggingface.co/Prince-1/medgemma-4b-pt\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 4\"}, {\"name\": \"Prince-1/Medgemma-4b-pt-Onnx\", \"link\": \"https://huggingface.co/Prince-1/Medgemma-4b-pt-Onnx\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 4\"}, {\"name\": \"jamessatomgb/google-medgemma-4b-it-transformers\", \"link\": \"https://huggingface.co/jamessatomgb/google-medgemma-4b-it-transformers\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"gabriellarson/medgemma-27b-it-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/medgemma-27b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"unsloth/medgemma-27b-it\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-it\", \"task\": \"\", \"likes\": \"183\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"unsloth/medgemma-27b-it-GGUF\", \"link\": \"https://huggingface.co/unsloth/medgemma-27b-it-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 10\"}, {\"name\": \"Prince-1/medgemma-27b-it\", \"link\": \"https://huggingface.co/Prince-1/medgemma-27b-it\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"onnx-community/MedGemma-27B-IT\", \"link\": \"https://huggingface.co/onnx-community/MedGemma-27B-IT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"FastFlowLM/medgemma-4b-it-NPU2\", \"link\": \"https://huggingface.co/FastFlowLM/medgemma-4b-it-NPU2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 26\"}, {\"name\": \"Muhammadidrees/my-medgamma\", \"link\": \"https://huggingface.co/Muhammadidrees/my-medgamma\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\"}, {\"name\": \"Muhammadidrees/Medgamma27B\", \"link\": \"https://huggingface.co/Muhammadidrees/Medgamma27B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 2\"}, {\"name\": \"OpenModels4all/mgemma\", \"link\": \"https://huggingface.co/OpenModels4all/mgemma\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"zarnow/medgemma-4b-it-bnb-4bit\", \"link\": \"https://huggingface.co/zarnow/medgemma-4b-it-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 14\"}, {\"name\": \"pszemraj/medgemma-4b-it-heretic\", \"link\": \"https://huggingface.co/pszemraj/medgemma-4b-it-heretic\", \"task\": \"\", \"likes\": \"94\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"pszemraj/medgemma-27b-text-heretic_med\", \"link\": \"https://huggingface.co/pszemraj/medgemma-27b-text-heretic_med\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"29 days ago\"}, {\"name\": \"tanujd/mg4b\", \"link\": \"https://huggingface.co/tanujd/mg4b\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}]",
    "num_datasets": 4,
    "datasets_list": "Voxel51/MedXpertQA, TsinghuaC3I/MedXpertQA, ChuGyouk/MedXpertQA, MedInjection-FR/Translated",
    "datasets_links": "https://huggingface.co/datasets/Voxel51/MedXpertQA, https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA, https://huggingface.co/datasets/ChuGyouk/MedXpertQA, https://huggingface.co/datasets/MedInjection-FR/Translated",
    "datasets_detailed": "[{\"name\": \"Voxel51/MedXpertQA\", \"link\": \"https://huggingface.co/datasets/Voxel51/MedXpertQA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 22\", \"size\": \"\"}, {\"name\": \"TsinghuaC3I/MedXpertQA\", \"link\": \"https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\", \"size\": \"\"}, {\"name\": \"ChuGyouk/MedXpertQA\", \"link\": \"https://huggingface.co/datasets/ChuGyouk/MedXpertQA\", \"task\": \"\", \"likes\": \"78\", \"downloads\": \"\", \"updated\": \"Jun 15\", \"size\": \"\"}, {\"name\": \"MedInjection-FR/Translated\", \"link\": \"https://huggingface.co/datasets/MedInjection-FR/Translated\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.17703",
    "first_seen_date": "2025-01-30",
    "title": "Critique Fine-Tuning: Learning to Critique is More Effective than\n  Learning to Imitate",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.17703Critique Fine-Tuning: Learning to Critique is More Effective than\n  Learning to ImitatePublished on Jan 29\u00b7Submitted byyuboon Jan 30#1 Paper of the dayUpvote59+51Authors:Yubo Wang,Xiang Yue,Wenhu ChenAbstractCritique Fine-Tuning (CFT) improves language model performance by training them to critique noisy responses, showing better reasoning ability than standard Supervised Fine-Tuning (SFT).AI-generated summarySupervised Fine-Tuning(SFT) is commonly used to train language models to\nimitate annotated responses for given instructions. In this paper, we challenge\nthis paradigm and proposeCritique Fine-Tuning(CFT), a strategy where models\nlearn to critique noisy responses rather than simply imitate correct ones.\nInspired by human learning processes that emphasize critical thinking,CFTencourages deeper analysis and nuanced understanding-traits often overlooked by\nstandardSFT. To validate the effectiveness ofCFT, we construct a 50K-sample\ndataset fromWebInstruct, usingGPT-4o as the teacher to generate critiques in\nthe form of (input=[query; noisy response], output=critique).CFTon this\ndataset yields a consistent 4-10% improvement overSFTon six math benchmarks\nwith different base models likeQwen2.5,Qwen2.5-MathandDeepSeek-Math. We\nfurther expand toMetaMathandNuminaMathdatasets and observe similar gains\noverSFT. Notably, ourQwen2.5-Math-CFTmodel-trained on just 50K\nsamples-matches or outperforms competitive models such asAceMathandQwen2.5-Math-Instructon most benchmarks, both of which use over 2M samples.\nAblation studies show thatCFTis robust to the source of noisy response and\nteacher critique model. Through these findings, we argue that critique-based\ntraining offers a more effective alternative to advance the reasoning of\nlanguage models.View arXiv pageView PDFGitHub180autoAdd to collectionCommunityubowangPaper authorPaper submitterJan 30Critique Fine-Tuning: Learning to Crit",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/TIGER-AI-Lab/CritiqueFineTuning",
    "hf_paper_url": "https://huggingface.co/papers/2501.17703",
    "arxiv_url": "https://arxiv.org/abs/2501.17703",
    "num_models": 2,
    "models_list": "TIGER-Lab/Qwen2.5-Math-7B-CFT, TIGER-Lab/Qwen2.5-32B-Instruct-CFT",
    "models_links": "https://huggingface.co/TIGER-Lab/Qwen2.5-Math-7B-CFT, https://huggingface.co/TIGER-Lab/Qwen2.5-32B-Instruct-CFT",
    "models_detailed": "[{\"name\": \"TIGER-Lab/Qwen2.5-Math-7B-CFT\", \"link\": \"https://huggingface.co/TIGER-Lab/Qwen2.5-Math-7B-CFT\", \"task\": \"Text Generation\", \"likes\": \"30\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"TIGER-Lab/Qwen2.5-32B-Instruct-CFT\", \"link\": \"https://huggingface.co/TIGER-Lab/Qwen2.5-32B-Instruct-CFT\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Feb 2\"}]",
    "num_datasets": 1,
    "datasets_list": "TIGER-Lab/WebInstruct-CFT",
    "datasets_links": "https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT",
    "datasets_detailed": "[{\"name\": \"TIGER-Lab/WebInstruct-CFT\", \"link\": \"https://huggingface.co/datasets/TIGER-Lab/WebInstruct-CFT\", \"task\": \"\", \"likes\": \"195\", \"downloads\": \"\", \"updated\": \"Feb 2\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.15383",
    "first_seen_date": "2025-01-28",
    "title": "Qwen2.5-1M Technical Report",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.15383Qwen2.5-1M Technical ReportPublished on Jan 26\u00b7Submitted byAKon Jan 28#1 Paper of the dayUpvote72+64Authors:An Yang,Bowen Yu,Chengyuan Li,Dayiheng Liu,Fei Huang,Haoyan Huang,Jiandong Jiang,Jianhong Tu,Jianwei Zhang,Jingren Zhou,Junyang Lin,Kai Dang,Kexin Yang,Le Yu,Mei Li,Minmin Sun,Qin Zhu,Rui Men,Tao He,Weijia Xu,Wenbiao Yin,Wenyuan Yu+6 authorsAbstractThe Qwen2.5-1M series models extend context length to 1 million tokens with enhanced long-context capabilities, employing techniques like long data synthesis and progressive pre-training, and are supported by an open-source inference framework with sparse attention and kernel optimizations.AI-generated summaryWe introduce Qwen2.5-1M, a series of models that extend the context length to\n1 million tokens. Compared to the previous 128K version, the Qwen2.5-1M series\nhave significantly enhanced long-context capabilities through long-context\npre-training and post-training. Key techniques such aslong data synthesis,progressive pre-training, andmulti-stage supervised fine-tuningare employed\nto effectively enhance long-context performance while reducing training costs.\n  To promote the use of long-context models among a broader user base, we\npresent and open-source our inference framework. This framework includes alength extrapolationmethod that can expand the model context lengths by at\nleast four times, or even more, without additional training. To reduce\ninference costs, we implement asparse attentionmethod along with chunked\nprefill optimization for deployment scenarios and asparsity refinementmethod\nto improve precision. Additionally, we detail our optimizations in the\ninference engine, includingkernel optimization,pipeline parallelism, andscheduling optimization, which significantly enhance overall inference\nperformance. By leveraging our inference framework, the Qwen2.5-1M models\nachieve a remarkable 3x to 7x prefill spee",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/QwenLM/vllm",
    "hf_paper_url": "https://huggingface.co/papers/2501.15383",
    "arxiv_url": "https://arxiv.org/abs/2501.15383",
    "num_models": 77,
    "models_list": "Qwen/Qwen3-Next-80B-A3B-Instruct, Qwen/Qwen3-235B-A22B-Instruct-2507, Qwen/Qwen3-30B-A3B-Instruct-2507, Qwen/Qwen3-Next-80B-A3B-Thinking, Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF, Qwen/Qwen3-30B-A3B-Thinking-2507, Qwen/Qwen3-Next-80B-A3B-Thinking-FP8, unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF, unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF, unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit, Qwen/Qwen3-Next-80B-A3B-Instruct-FP8, Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF, Qwen/Qwen3-235B-A22B-Thinking-2507, cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit, DavidAU/Qwen3-42B-A3B-Spock-Brutal-Recall-Instruct, Qwen/Qwen2.5-14B-Instruct-1M, Qwen/Qwen2.5-7B-Instruct-1M, async0x42/Qwen2.5-7B-Instruct-1M-exl2_4.65bpw, async0x42/Qwen2.5-14B-Instruct-1M-exl2_4.65bpw, ZeroXClem/Qwen2.5-7B-CelestialHarmony-1M, remymenard/Qwen2.5-7B-Instruct-1M-ct2-int8, QuantFactory/Qwen2.5-14B-Instruct-1M-GGUF, QuantFactory/Qwen2.5-7B-Instruct-1M-GGUF, professorf/Qwen2.5-7B-Instruct-1M-gguf, AightBits/Qwen2.5-14B-Instruct-1M-8.0bpw-h8-exl2, AightBits/Qwen2.5-7B-Instruct-1M-8.0bpw-h8-exl2, Mungert/Qwen2.5-14B-Instruct-1M-GGUF, Mungert/Qwen2.5-7B-Instruct-1M-GGUF, duyntnet/Qwen2.5-14B-Instruct-1M-imatrix-GGUF, RichardErkhov/Qwen_-_Qwen2.5-7B-Instruct-1M-4bits, RichardErkhov/Qwen_-_Qwen2.5-7B-Instruct-1M-8bits, mozilla-ai/Qwen2.5-7B-Instruct-1M-llamafile, mozilla-ai/Qwen2.5-14B-Instruct-1M-llamafile, limcheekin/Qwen2.5-7B-Instruct-1M-rk3588-1.1.4, limcheekin/Qwen2.5-14B-Instruct-1M-rk3588-1.1.4, thanhtantran/Qwen2.5-14B-Instruct-1M-rk3588-1.2.0, thanhtantran/Qwen2.5-7B-Instruct-1M-rk3588-1.2.0, AIDXteam/Qwen3-235B-A22B-Thinking-2507-AWQ, AmirHaz/Affine-yollloooo, Mungert/Qwen3-30B-A3B-Thinking-2507-GGUF, Mungert/Qwen3-30B-A3B-Instruct-2507-GGUF, Intellicia/Sullivan, chutesai/Qwen3-235B-A22B-Instruct-2507-1M, JunHowie/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int4, JunHowie/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8, JunHowie/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int4, CoderBak/Qwen3-30B-A3B-Instruct-2507-EnergyQA-Expansion, JunHowie/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int8, unsloth/Qwen3-Next-80B-A3B-Instruct, supermanaff/Affine-5FC1Dq1kdHAGmrEkSCLwEKeNM7i9YY6rXtZKaLM2q4qaAE6b, cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-4bit, ClashLuke/affine-qi, vonmises69/Affine-5CXQgGhdq1mJ1aYz4n9g9RTHhghuAr7LCPj8Ww2CyhR3YZM7_3, Cheeeeeeeeky/Affine-5C8JPiLuLKQAeBhZ6rXVJLtmY3PdAYxarRnSYvmyjJaQMZxe_4, parasail-ai/Qwen3-Next-80B-A3B-Thinking, vonmises69/Affine-5C8JPiLuLKQAeBhZ6rXVJLtmY3PdAYxarRnSYvmyjJaQMZxe_4, danikhan632/qwen3-1b, testmymodel112/Affine-new-model-155, cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-8bit, cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-8bit, bullerwins/Qwen3-30B-A3B-Instruct-2507-GGUF, Alphatao/Affine-3235227, xaura/affine-9575845-old-but-new, matt0xdev/Affine-lfg, matt0xdev/Affine-dark, PJMixers-Dev/Qwen3-30B-A3B-Instruct-2507-bnb-4bit, jackcloudman/Qwen3-Next-80B-A3B-Thinking-GGUF, hO61qjpwxu/Affine_qknt, iceberg0142/Affine-31B-All, Infatoshi/Qwen3-Next-80B-A3B-Thinking-EXL3-2.0bpw, amrothemich/Qwen3-30B-A3B-Thinking-2507, richardyoung/Qwen2.5-14B-Instruct-1M-heretic, unsloth/Qwen3-Next-80B-A3B-Thinking, Userb1az/Qwen3-Next-80B-A3B-Instruct-GGUF, ExaltedSlayer/qwen3-next-80b-a3b-thinking-3_4-mlx, x4ebup/Qwen3-235B-A22B-Instruct-2507-INT8, lovedheart/Qwen3-Next-80B-A3B-Instruct-fastllm-fp8-int4g128",
    "models_links": "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct, https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507, https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF, https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-FP8, https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF, https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF, https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct-FP8, https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF, https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507, https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit, https://huggingface.co/DavidAU/Qwen3-42B-A3B-Spock-Brutal-Recall-Instruct, https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M, https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M, https://huggingface.co/async0x42/Qwen2.5-7B-Instruct-1M-exl2_4.65bpw, https://huggingface.co/async0x42/Qwen2.5-14B-Instruct-1M-exl2_4.65bpw, https://huggingface.co/ZeroXClem/Qwen2.5-7B-CelestialHarmony-1M, https://huggingface.co/remymenard/Qwen2.5-7B-Instruct-1M-ct2-int8, https://huggingface.co/QuantFactory/Qwen2.5-14B-Instruct-1M-GGUF, https://huggingface.co/QuantFactory/Qwen2.5-7B-Instruct-1M-GGUF, https://huggingface.co/professorf/Qwen2.5-7B-Instruct-1M-gguf, https://huggingface.co/AightBits/Qwen2.5-14B-Instruct-1M-8.0bpw-h8-exl2, https://huggingface.co/AightBits/Qwen2.5-7B-Instruct-1M-8.0bpw-h8-exl2, https://huggingface.co/Mungert/Qwen2.5-14B-Instruct-1M-GGUF, https://huggingface.co/Mungert/Qwen2.5-7B-Instruct-1M-GGUF, https://huggingface.co/duyntnet/Qwen2.5-14B-Instruct-1M-imatrix-GGUF, https://huggingface.co/RichardErkhov/Qwen_-_Qwen2.5-7B-Instruct-1M-4bits, https://huggingface.co/RichardErkhov/Qwen_-_Qwen2.5-7B-Instruct-1M-8bits, https://huggingface.co/mozilla-ai/Qwen2.5-7B-Instruct-1M-llamafile, https://huggingface.co/mozilla-ai/Qwen2.5-14B-Instruct-1M-llamafile, https://huggingface.co/limcheekin/Qwen2.5-7B-Instruct-1M-rk3588-1.1.4, https://huggingface.co/limcheekin/Qwen2.5-14B-Instruct-1M-rk3588-1.1.4, https://huggingface.co/thanhtantran/Qwen2.5-14B-Instruct-1M-rk3588-1.2.0, https://huggingface.co/thanhtantran/Qwen2.5-7B-Instruct-1M-rk3588-1.2.0, https://huggingface.co/AIDXteam/Qwen3-235B-A22B-Thinking-2507-AWQ, https://huggingface.co/AmirHaz/Affine-yollloooo, https://huggingface.co/Mungert/Qwen3-30B-A3B-Thinking-2507-GGUF, https://huggingface.co/Mungert/Qwen3-30B-A3B-Instruct-2507-GGUF, https://huggingface.co/Intellicia/Sullivan, https://huggingface.co/chutesai/Qwen3-235B-A22B-Instruct-2507-1M, https://huggingface.co/JunHowie/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int4, https://huggingface.co/JunHowie/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8, https://huggingface.co/JunHowie/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int4, https://huggingface.co/CoderBak/Qwen3-30B-A3B-Instruct-2507-EnergyQA-Expansion, https://huggingface.co/JunHowie/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int8, https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct, https://huggingface.co/supermanaff/Affine-5FC1Dq1kdHAGmrEkSCLwEKeNM7i9YY6rXtZKaLM2q4qaAE6b, https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-4bit, https://huggingface.co/ClashLuke/affine-qi, https://huggingface.co/vonmises69/Affine-5CXQgGhdq1mJ1aYz4n9g9RTHhghuAr7LCPj8Ww2CyhR3YZM7_3, https://huggingface.co/Cheeeeeeeeky/Affine-5C8JPiLuLKQAeBhZ6rXVJLtmY3PdAYxarRnSYvmyjJaQMZxe_4, https://huggingface.co/parasail-ai/Qwen3-Next-80B-A3B-Thinking, https://huggingface.co/vonmises69/Affine-5C8JPiLuLKQAeBhZ6rXVJLtmY3PdAYxarRnSYvmyjJaQMZxe_4, https://huggingface.co/danikhan632/qwen3-1b, https://huggingface.co/testmymodel112/Affine-new-model-155, https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-8bit, https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-8bit, https://huggingface.co/bullerwins/Qwen3-30B-A3B-Instruct-2507-GGUF, https://huggingface.co/Alphatao/Affine-3235227, https://huggingface.co/xaura/affine-9575845-old-but-new, https://huggingface.co/matt0xdev/Affine-lfg, https://huggingface.co/matt0xdev/Affine-dark, https://huggingface.co/PJMixers-Dev/Qwen3-30B-A3B-Instruct-2507-bnb-4bit, https://huggingface.co/jackcloudman/Qwen3-Next-80B-A3B-Thinking-GGUF, https://huggingface.co/hO61qjpwxu/Affine_qknt, https://huggingface.co/iceberg0142/Affine-31B-All, https://huggingface.co/Infatoshi/Qwen3-Next-80B-A3B-Thinking-EXL3-2.0bpw, https://huggingface.co/amrothemich/Qwen3-30B-A3B-Thinking-2507, https://huggingface.co/richardyoung/Qwen2.5-14B-Instruct-1M-heretic, https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking, https://huggingface.co/Userb1az/Qwen3-Next-80B-A3B-Instruct-GGUF, https://huggingface.co/ExaltedSlayer/qwen3-next-80b-a3b-thinking-3_4-mlx, https://huggingface.co/x4ebup/Qwen3-235B-A22B-Instruct-2507-INT8, https://huggingface.co/lovedheart/Qwen3-Next-80B-A3B-Instruct-fastllm-fp8-int4g128",
    "models_detailed": "[{\"name\": \"Qwen/Qwen3-Next-80B-A3B-Instruct\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-Instruct-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-Instruct-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 17\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Thinking\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 15\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"Qwen/Qwen3-30B-A3B-Thinking-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 17\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Thinking-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\"}, {\"name\": \"unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"9 days ago\"}, {\"name\": \"unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 13\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Instruct-FP8\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 22\"}, {\"name\": \"Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"19 days ago\"}, {\"name\": \"Qwen/Qwen3-235B-A22B-Thinking-2507\", \"link\": \"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 17\"}, {\"name\": \"cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"DavidAU/Qwen3-42B-A3B-Spock-Brutal-Recall-Instruct\", \"link\": \"https://huggingface.co/DavidAU/Qwen3-42B-A3B-Spock-Brutal-Recall-Instruct\", \"task\": \"Text Generation\", \"likes\": \"44\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"Qwen/Qwen2.5-14B-Instruct-1M\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 29\"}, {\"name\": \"Qwen/Qwen2.5-7B-Instruct-1M\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-1M\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 29\"}, {\"name\": \"async0x42/Qwen2.5-7B-Instruct-1M-exl2_4.65bpw\", \"link\": \"https://huggingface.co/async0x42/Qwen2.5-7B-Instruct-1M-exl2_4.65bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 29\"}, {\"name\": \"async0x42/Qwen2.5-14B-Instruct-1M-exl2_4.65bpw\", \"link\": \"https://huggingface.co/async0x42/Qwen2.5-14B-Instruct-1M-exl2_4.65bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 29\"}, {\"name\": \"ZeroXClem/Qwen2.5-7B-CelestialHarmony-1M\", \"link\": \"https://huggingface.co/ZeroXClem/Qwen2.5-7B-CelestialHarmony-1M\", \"task\": \"Text Generation\", \"likes\": \"25\", \"downloads\": \"\", \"updated\": \"Feb 8\"}, {\"name\": \"remymenard/Qwen2.5-7B-Instruct-1M-ct2-int8\", \"link\": \"https://huggingface.co/remymenard/Qwen2.5-7B-Instruct-1M-ct2-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 3\"}, {\"name\": \"QuantFactory/Qwen2.5-14B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/Qwen2.5-14B-Instruct-1M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"846\", \"downloads\": \"\", \"updated\": \"Feb 8\"}, {\"name\": \"QuantFactory/Qwen2.5-7B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/Qwen2.5-7B-Instruct-1M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"280\", \"downloads\": \"\", \"updated\": \"Feb 9\"}, {\"name\": \"professorf/Qwen2.5-7B-Instruct-1M-gguf\", \"link\": \"https://huggingface.co/professorf/Qwen2.5-7B-Instruct-1M-gguf\", \"task\": \"Text Generation\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Feb 17\"}, {\"name\": \"AightBits/Qwen2.5-14B-Instruct-1M-8.0bpw-h8-exl2\", \"link\": \"https://huggingface.co/AightBits/Qwen2.5-14B-Instruct-1M-8.0bpw-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"AightBits/Qwen2.5-7B-Instruct-1M-8.0bpw-h8-exl2\", \"link\": \"https://huggingface.co/AightBits/Qwen2.5-7B-Instruct-1M-8.0bpw-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"Mungert/Qwen2.5-14B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen2.5-14B-Instruct-1M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/Qwen2.5-7B-Instruct-1M-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen2.5-7B-Instruct-1M-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"duyntnet/Qwen2.5-14B-Instruct-1M-imatrix-GGUF\", \"link\": \"https://huggingface.co/duyntnet/Qwen2.5-14B-Instruct-1M-imatrix-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 25\"}, {\"name\": \"RichardErkhov/Qwen_-_Qwen2.5-7B-Instruct-1M-4bits\", \"link\": \"https://huggingface.co/RichardErkhov/Qwen_-_Qwen2.5-7B-Instruct-1M-4bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"RichardErkhov/Qwen_-_Qwen2.5-7B-Instruct-1M-8bits\", \"link\": \"https://huggingface.co/RichardErkhov/Qwen_-_Qwen2.5-7B-Instruct-1M-8bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"mozilla-ai/Qwen2.5-7B-Instruct-1M-llamafile\", \"link\": \"https://huggingface.co/mozilla-ai/Qwen2.5-7B-Instruct-1M-llamafile\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"mozilla-ai/Qwen2.5-14B-Instruct-1M-llamafile\", \"link\": \"https://huggingface.co/mozilla-ai/Qwen2.5-14B-Instruct-1M-llamafile\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 30\"}, {\"name\": \"limcheekin/Qwen2.5-7B-Instruct-1M-rk3588-1.1.4\", \"link\": \"https://huggingface.co/limcheekin/Qwen2.5-7B-Instruct-1M-rk3588-1.1.4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 17\"}, {\"name\": \"limcheekin/Qwen2.5-14B-Instruct-1M-rk3588-1.1.4\", \"link\": \"https://huggingface.co/limcheekin/Qwen2.5-14B-Instruct-1M-rk3588-1.1.4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 18\"}, {\"name\": \"thanhtantran/Qwen2.5-14B-Instruct-1M-rk3588-1.2.0\", \"link\": \"https://huggingface.co/thanhtantran/Qwen2.5-14B-Instruct-1M-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"thanhtantran/Qwen2.5-7B-Instruct-1M-rk3588-1.2.0\", \"link\": \"https://huggingface.co/thanhtantran/Qwen2.5-7B-Instruct-1M-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"AIDXteam/Qwen3-235B-A22B-Thinking-2507-AWQ\", \"link\": \"https://huggingface.co/AIDXteam/Qwen3-235B-A22B-Thinking-2507-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 11\"}, {\"name\": \"AmirHaz/Affine-yollloooo\", \"link\": \"https://huggingface.co/AmirHaz/Affine-yollloooo\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 15\"}, {\"name\": \"Mungert/Qwen3-30B-A3B-Thinking-2507-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-30B-A3B-Thinking-2507-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/Qwen3-30B-A3B-Instruct-2507-GGUF\", \"link\": \"https://huggingface.co/Mungert/Qwen3-30B-A3B-Instruct-2507-GGUF\", \"task\": \"Text Generation\", \"likes\": \"253\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Intellicia/Sullivan\", \"link\": \"https://huggingface.co/Intellicia/Sullivan\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"chutesai/Qwen3-235B-A22B-Instruct-2507-1M\", \"link\": \"https://huggingface.co/chutesai/Qwen3-235B-A22B-Instruct-2507-1M\", \"task\": \"Text Generation\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Aug 19\"}, {\"name\": \"JunHowie/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"JunHowie/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"JunHowie/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int4\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"CoderBak/Qwen3-30B-A3B-Instruct-2507-EnergyQA-Expansion\", \"link\": \"https://huggingface.co/CoderBak/Qwen3-30B-A3B-Instruct-2507-EnergyQA-Expansion\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"JunHowie/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int8\", \"link\": \"https://huggingface.co/JunHowie/Qwen3-30B-A3B-Thinking-2507-GPTQ-Int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"unsloth/Qwen3-Next-80B-A3B-Instruct\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct\", \"task\": \"Text Generation\", \"likes\": \"687\", \"downloads\": \"\", \"updated\": \"15 days ago\"}, {\"name\": \"supermanaff/Affine-5FC1Dq1kdHAGmrEkSCLwEKeNM7i9YY6rXtZKaLM2q4qaAE6b\", \"link\": \"https://huggingface.co/supermanaff/Affine-5FC1Dq1kdHAGmrEkSCLwEKeNM7i9YY6rXtZKaLM2q4qaAE6b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-4bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\"}, {\"name\": \"ClashLuke/affine-qi\", \"link\": \"https://huggingface.co/ClashLuke/affine-qi\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 15\"}, {\"name\": \"vonmises69/Affine-5CXQgGhdq1mJ1aYz4n9g9RTHhghuAr7LCPj8Ww2CyhR3YZM7_3\", \"link\": \"https://huggingface.co/vonmises69/Affine-5CXQgGhdq1mJ1aYz4n9g9RTHhghuAr7LCPj8Ww2CyhR3YZM7_3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 15\"}, {\"name\": \"Cheeeeeeeeky/Affine-5C8JPiLuLKQAeBhZ6rXVJLtmY3PdAYxarRnSYvmyjJaQMZxe_4\", \"link\": \"https://huggingface.co/Cheeeeeeeeky/Affine-5C8JPiLuLKQAeBhZ6rXVJLtmY3PdAYxarRnSYvmyjJaQMZxe_4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"parasail-ai/Qwen3-Next-80B-A3B-Thinking\", \"link\": \"https://huggingface.co/parasail-ai/Qwen3-Next-80B-A3B-Thinking\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"vonmises69/Affine-5C8JPiLuLKQAeBhZ6rXVJLtmY3PdAYxarRnSYvmyjJaQMZxe_4\", \"link\": \"https://huggingface.co/vonmises69/Affine-5C8JPiLuLKQAeBhZ6rXVJLtmY3PdAYxarRnSYvmyjJaQMZxe_4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 16\"}, {\"name\": \"danikhan632/qwen3-1b\", \"link\": \"https://huggingface.co/danikhan632/qwen3-1b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 7\"}, {\"name\": \"testmymodel112/Affine-new-model-155\", \"link\": \"https://huggingface.co/testmymodel112/Affine-new-model-155\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 23\"}, {\"name\": \"cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Thinking-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"146\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-8bit\", \"link\": \"https://huggingface.co/cpatonn/Qwen3-Next-80B-A3B-Instruct-AWQ-8bit\", \"task\": \"Text Generation\", \"likes\": \"758\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"bullerwins/Qwen3-30B-A3B-Instruct-2507-GGUF\", \"link\": \"https://huggingface.co/bullerwins/Qwen3-30B-A3B-Instruct-2507-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 25\"}, {\"name\": \"Alphatao/Affine-3235227\", \"link\": \"https://huggingface.co/Alphatao/Affine-3235227\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 28\"}, {\"name\": \"xaura/affine-9575845-old-but-new\", \"link\": \"https://huggingface.co/xaura/affine-9575845-old-but-new\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 29\"}, {\"name\": \"matt0xdev/Affine-lfg\", \"link\": \"https://huggingface.co/matt0xdev/Affine-lfg\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 29\"}, {\"name\": \"matt0xdev/Affine-dark\", \"link\": \"https://huggingface.co/matt0xdev/Affine-dark\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\"}, {\"name\": \"PJMixers-Dev/Qwen3-30B-A3B-Instruct-2507-bnb-4bit\", \"link\": \"https://huggingface.co/PJMixers-Dev/Qwen3-30B-A3B-Instruct-2507-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 12\"}, {\"name\": \"jackcloudman/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"link\": \"https://huggingface.co/jackcloudman/Qwen3-Next-80B-A3B-Thinking-GGUF\", \"task\": \"Text Generation\", \"likes\": \"112\", \"downloads\": \"\", \"updated\": \"Oct 20\"}, {\"name\": \"hO61qjpwxu/Affine_qknt\", \"link\": \"https://huggingface.co/hO61qjpwxu/Affine_qknt\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}, {\"name\": \"iceberg0142/Affine-31B-All\", \"link\": \"https://huggingface.co/iceberg0142/Affine-31B-All\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 24\"}, {\"name\": \"Infatoshi/Qwen3-Next-80B-A3B-Thinking-EXL3-2.0bpw\", \"link\": \"https://huggingface.co/Infatoshi/Qwen3-Next-80B-A3B-Thinking-EXL3-2.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"amrothemich/Qwen3-30B-A3B-Thinking-2507\", \"link\": \"https://huggingface.co/amrothemich/Qwen3-30B-A3B-Thinking-2507\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 12\"}, {\"name\": \"richardyoung/Qwen2.5-14B-Instruct-1M-heretic\", \"link\": \"https://huggingface.co/richardyoung/Qwen2.5-14B-Instruct-1M-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"unsloth/Qwen3-Next-80B-A3B-Thinking\", \"link\": \"https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking\", \"task\": \"Text Generation\", \"likes\": \"449\", \"downloads\": \"\", \"updated\": \"23 days ago\"}, {\"name\": \"Userb1az/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"link\": \"https://huggingface.co/Userb1az/Qwen3-Next-80B-A3B-Instruct-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"17 days ago\"}, {\"name\": \"ExaltedSlayer/qwen3-next-80b-a3b-thinking-3_4-mlx\", \"link\": \"https://huggingface.co/ExaltedSlayer/qwen3-next-80b-a3b-thinking-3_4-mlx\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"6 days ago\"}, {\"name\": \"x4ebup/Qwen3-235B-A22B-Instruct-2507-INT8\", \"link\": \"https://huggingface.co/x4ebup/Qwen3-235B-A22B-Instruct-2507-INT8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"14 days ago\"}, {\"name\": \"lovedheart/Qwen3-Next-80B-A3B-Instruct-fastllm-fp8-int4g128\", \"link\": \"https://huggingface.co/lovedheart/Qwen3-Next-80B-A3B-Instruct-fastllm-fp8-int4g128\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2501.15907",
    "first_seen_date": "2025-01-28",
    "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\n  Speech Generation",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.15907Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for\n  Speech GenerationPublished on Jan 27\u00b7Submitted byHarryHe11on Jan 28Upvote17+9Authors:Haorui He,Zengqiang Shang,Chaoren Wang,Xuyuan Li,Yicheng Gu,Hua Hua,Liwei Liu,Chen Yang,Jiaqi Li,Peiyang Shi,Yuancheng Wang,Kai Chen,Pengyuan Zhang,Zhizheng WuAbstractEmilia-Pipe preprocessing pipeline creates Emilia and Emilia-Large datasets, enhancing speech generation with high-quality in-the-wild multilingual data, surpassing audiobook datasets.AI-generated summaryRecent advancements inspeech generationhave been driven by the large-scale\ntraining datasets. However, current models fall short of capturing the\nspontaneity and variability inherent in real-world human speech, due to their\nreliance on audiobook datasets limited to formal read-aloud speech styles. To\nbridge this gap, we introduceEmilia-Pipe, an open-source preprocessing\npipeline to extract high-quality training data from valuable yet underexploredin-the-wilddata that capture spontaneous human speech in real-world contexts.\nBy leveragingEmilia-Pipe, we constructEmilia, the first multilingual speech\ngeneration dataset derived fromin-the-wildspeech data. This dataset comprises\nover 101k hours of speech across six languages: English, Chinese, German,\nFrench, Japanese, and Korean. Besides, we expandEmiliatoEmilia-Large, a\ndataset exceeding 216k hours, making it the largest open-source speech\ngeneration dataset available. Extensive experiments demonstrate thatEmiliasignificantly outperforms traditional audiobook datasets in generating\nspontaneous and human-like speech, showcasing superior performance in capturing\ndiversespeaker timbreandspeaking stylesof real-world human speech.\nFurthermore, this work underscores the importance of scaling dataset size to\nadvancespeech generationresearch and validates the effectiveness ofEmiliafor both multilingual and cross",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2501.15907",
    "arxiv_url": "https://arxiv.org/abs/2501.15907",
    "num_models": 10,
    "models_list": "nineninesix/kani-tts-400m-en, nineninesix/kani-tts-400m-0.3-pt, nineninesix/kani-tts-400m-ko, nineninesix/kani-tts-400m-ar, nineninesix/kani-tts-400m-ky, nineninesix/kani-tts-400m-es, nineninesix/kani-tts-400m-de, nineninesix/kani-tts-400m-zh, nineninesix/kani-tts-400m-ky-kani, Mungert/kani-tts-400m-en-GGUF",
    "models_links": "https://huggingface.co/nineninesix/kani-tts-400m-en, https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt, https://huggingface.co/nineninesix/kani-tts-400m-ko, https://huggingface.co/nineninesix/kani-tts-400m-ar, https://huggingface.co/nineninesix/kani-tts-400m-ky, https://huggingface.co/nineninesix/kani-tts-400m-es, https://huggingface.co/nineninesix/kani-tts-400m-de, https://huggingface.co/nineninesix/kani-tts-400m-zh, https://huggingface.co/nineninesix/kani-tts-400m-ky-kani, https://huggingface.co/Mungert/kani-tts-400m-en-GGUF",
    "models_detailed": "[{\"name\": \"nineninesix/kani-tts-400m-en\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-en\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-0.3-pt\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-ko\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-ko\", \"task\": \"Text-to-Speech\", \"likes\": \"48\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-ar\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-ar\", \"task\": \"Text-to-Speech\", \"likes\": \"225\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-ky\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-ky\", \"task\": \"Text-to-Speech\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-es\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-es\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-de\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-de\", \"task\": \"Text-to-Speech\", \"likes\": \"322\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-zh\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-zh\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"nineninesix/kani-tts-400m-ky-kani\", \"link\": \"https://huggingface.co/nineninesix/kani-tts-400m-ky-kani\", \"task\": \"Text-to-Speech\", \"likes\": \"33\", \"downloads\": \"\", \"updated\": \"Nov 2\"}, {\"name\": \"Mungert/kani-tts-400m-en-GGUF\", \"link\": \"https://huggingface.co/Mungert/kani-tts-400m-en-GGUF\", \"task\": \"Text-to-Speech\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 3\"}]",
    "num_datasets": 4,
    "datasets_list": "amphion/Emilia-Dataset, laion/Emolia, laion/Emilia-with-Emotion-Annotations, nytopop/emilia-en-snac",
    "datasets_links": "https://huggingface.co/datasets/amphion/Emilia-Dataset, https://huggingface.co/datasets/laion/Emolia, https://huggingface.co/datasets/laion/Emilia-with-Emotion-Annotations, https://huggingface.co/datasets/nytopop/emilia-en-snac",
    "datasets_detailed": "[{\"name\": \"amphion/Emilia-Dataset\", \"link\": \"https://huggingface.co/datasets/amphion/Emilia-Dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 28\", \"size\": \"\"}, {\"name\": \"laion/Emolia\", \"link\": \"https://huggingface.co/datasets/laion/Emolia\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 20\", \"size\": \"\"}, {\"name\": \"laion/Emilia-with-Emotion-Annotations\", \"link\": \"https://huggingface.co/datasets/laion/Emilia-with-Emotion-Annotations\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 20\", \"size\": \"\"}, {\"name\": \"nytopop/emilia-en-snac\", \"link\": \"https://huggingface.co/datasets/nytopop/emilia-en-snac\", \"task\": \"\", \"likes\": \"275\", \"downloads\": \"\", \"updated\": \"Jul 14\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.12948",
    "first_seen_date": "2025-01-23",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement Learning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.12948DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement LearningPublished on Jan 22\u00b7Submitted byAKon Jan 23#1 Paper of the day\u00b7DeepSeekUpvote430+422Authors:DeepSeek-AI,Daya Guo,Dejian Yang,Haowei Zhang,Junxiao Song,Ruoyu Zhang,Runxin Xu,Qihao Zhu,Shirong Ma,Peiyi Wang,Xiao Bi,Xiaokang Zhang,Xingkai Yu,Yu Wu,Z. F. Wu,Zhibin Gou,Zhihong Shao,Zhuoshu Li,Ziyi Gao,Aixin Liu,Bing Xue,Bingxuan Wang+178 authorsAbstractDeepSeek-R1-Zero and DeepSeek-R1 utilize reinforcement learning and multi-stage training to enhance reasoning capabilities, with DeepSeek-R1 achieving performance comparable to OpenAI-o1-1217.AI-generated summaryWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporatesmulti-stage trainingand cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based onQwenandLlama.View arXiv pageView PDFGitHub91.6kautoAdd to collectionCommunityakhaliqPaper submitterJan 23https://github.com/deepseek-ai/DeepSeek-R1See translation\ud83d\udd251616+Replylibrarian-botJan 24This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIImproving Multi-Step Reason",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/deepseek-ai/deepseek-r1",
    "hf_paper_url": "https://huggingface.co/papers/2501.12948",
    "arxiv_url": "https://arxiv.org/abs/2501.12948",
    "num_models": 294,
    "models_list": "deepseek-ai/DeepSeek-R1, deepseek-ai/DeepSeek-R1-0528, deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, deepseek-ai/DeepSeek-R1-0528-Qwen3-8B, deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, deepseek-ai/DeepSeek-R1-Zero, deepseek-ai/DeepSeek-R1-Distill-Llama-8B, deepseek-ai/DeepSeek-R1-Distill-Qwen-14B, deepseek-ai/DeepSeek-R1-Distill-Llama-70B, unsloth/DeepSeek-R1-GGUF, unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF, unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF, unsloth/DeepSeek-R1-0528-GGUF, unsloth/DeepSeek-R1-0528-BF16, unsloth/DeepSeek-R1, unsloth/DeepSeek-R1-BF16, unsloth/DeepSeek-R1-Distill-Qwen-1.5B, unsloth/DeepSeek-R1-Distill-Llama-70B, unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF, unsloth/DeepSeek-R1-Zero-GGUF, unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF, unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF, unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF, unsloth/DeepSeek-R1-Distill-Qwen-32B-unsloth-bnb-4bit, fivenightatfreddyLTD/ChatGPTrival, QuantFactory/DeepSeek-R1-Distill-Qwen-1.5B-GGUF, QuantFactory/DeepSeek-R1-Distill-Qwen-14B-GGUF, jbower/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.2, PedroPareja/DeepSeek-R1-Distill-Qwen-32B-6.5bpw-h6-exl2, PedroPareja/DeepSeek-R1-Distill-Qwen-32B-7bpw-h8-exl2, Spestly/Atlas-Flash-1.5B-Preview, Spestly/Atlas-Flash-7B-Preview, calcuis/deepseek-r1, jbower/DeepSeek-R1-Distill-Llama-8B-rk3588-1.1.2, cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese, cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese, jobs-git/DeepSeek-R1, FallnAI/2.5-R1-Distill, Deepdive404/Deepseek-fork, slomkarafa/15-00, BuckalewFinancial/DeepSeek-R1, unsloth/DeepSeek-R1-Distill-Llama-70B-unsloth-bnb-4bit, TachyHealth/DeepSeek-R1, FlorianJc/DeepSeek-R1-Distill-Llama-8B-vllm-fp8, GPT4All-Community/DeepSeek-R1-Distill-Llama-8B-GGUF, Ihor/Text2Graph-R1-Qwen2.5-0.5b, Corianas/DeepSeek-R1-Distill-Qwen-14B-AWQ, Omartificial-Intelligence-Space/Arabic-DeepSeek-R1-Distill-8B, suayptalha/Maestro-R1-Llama-8B, afdsas/myseek671b, aeertrd/model2, stelterlab/DeepSeek-R1-Distill-Qwen-14B-AWQ, LoneStriker/DeepSeek-R1-Distill-Llama-70B-6.0bpw-h6-exl2, LoneStriker/DeepSeek-R1-Distill-Llama-70B-8.0bpw-h8-exl2, patrickbdevaney/deepseek-r1-qwen-7b-q6-exl2, Tech-Awesome-Hub/deepseek-distilled-qwen-1.5B, dwetzel/DeepSeek-R1-Distill-Qwen-32B-GPTQ-INT4, QuantFactory/DeepSeek-R1-Distill-Qwen-7B-GGUF, Bojun-Feng/DeepSeek-R1-Distill-Qwen-1.5B-GGUF-llamafile, Bojun-Feng/DeepSeek-R1-Distill-Qwen-7B-GGUF-llamafile, Bojun-Feng/DeepSeek-R1-Distill-Llama-8B-GGUF-llamafile, Bojun-Feng/DeepSeek-R1-Distill-Qwen-14B-GGUF-llamafile, anshul6273/Qwen2.5-7B-Atcoder-Reasoning-v1-GGUF, Bojun-Feng/DeepSeek-R1-Distill-Qwen-32B-GGUF-llamafile, rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b, rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-awq, rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf, rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gptq-int4, rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gptq-int8, EasierAI/DeepSeek-R1-Distill-Llama-8B, EasierAI/DeepSeek-R1-Distill-Qwen-7B, EasierAI/DeepSeek-R1-Distill-Qwen-14B, Josephgflowers/DeepSeek-R1-Distill-Qwen-1.5B-LIMO, dhruv070/labonweb, PsiPi/deepseek-r1-14b-gguf, baseten/r1-nextn-head0, SGLang/DeepSeek-R1-NextN, professorf/DeepSeek-R1-Distill-Qwen-1.5B-gguf, professorf/DeepSeek-R1-Distill-Qwen-7B-gguf, BlackBeenie/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-LIMO, jhall0310/chat, perplexity-ai/r1-1776-distill-llama-70b, yuanzu/DeepSeek-R1-INT8, matatonic/r1-1776-distill-llama-70b-6.5bpw-h8-exl2, unsloth/r1-1776-distill-llama-70b-GGUF, unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit, thellumi/LLuMi_Think_70B, thisnick/DeepSeek-R1-Distill-Llama-70B-abliterated, meituan/DeepSeek-R1-Block-INT8, Guygir/DeepSeek-ZipNN, thellumi/LLuMi_Think_3B, thellumi/LLuMi_Think_8B, KnutJaegersberg/r1-1776-distill-llama-70b-exl2-4.65bpw, Apel-sin/r1-1776-distill-llama-70b-exl2, meituan/DeepSeek-R1-Channel-INT8, datakurre/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.4, Orion-zhen/DeepSeek-R1-Distill-Llama-70B-4.4bpw-h6-exl2, VarunBudhani/deepseek-r1, gilrm92/deep-seek-copy, AnteriorAI/deepseek-r1-distilled-qwen-32b, minpeter/DeepSeek-R1, Gaaiwing/DeepSeek-R1-Distill-Qwen-1.5B, spuliz/Ahilab, braindao/DeepSeek-R1-Distill-Qwen-14B, braindao/DeepSeek-R1-Distill-Qwen-7B, ahz-r3v/DeepSeek-R1-Distill-Qwen-7B-rk3588-rkllm-1.1.4, ajaykumar1973/my-ai-model, ajaykumar1973/ai-model-1.5b, Amyww/case001, birvii/DeepSeek-R1-Distill-Qwen-7B, real-jiakai/DeepSeek-R1-Distill-Qwen-7B-News-Classifier, rinna/qwq-bakeneko-32b, rinna/qwq-bakeneko-32b-awq, rinna/qwq-bakeneko-32b-gguf, rinna/qwq-bakeneko-32b-gptq-int8, rinna/qwq-bakeneko-32b-gptq-int4, realYinkaIyiola/DistillR1-1.5B-Pruned-1.1B, zipnn/DeepSeek-R1-Distill-Qwen-32B-ZipNN, LLMJapan/qwq-bakeneko-32b_exl2_8.0bpw, rinna/qwen2.5-bakeneko-32b-instruct-v2-awq, rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf, rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int8, rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int4, takanori39/DeepSeek-R1-Distill-Qwen-1.5B-awq, Victor1o/deepseek-r1-model, medmekk/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit, Mungert/DeepSeek-R1-Distill-Qwen-1.5B-GGUF, Mungert/DeepSeek-R1-Distill-Qwen-7B-GGUF, Mungert/DeepSeek-R1-Distill-Llama-8B-GGUF, bnb-community/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit, bnb-community/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit, Mungert/DeepSeek-R1-Distill-Qwen-14B-GGUF, Compumacy/R1, PursuitOfDataScience/Qwen2.5-1.5B-Instruct-Lora-Deepseek-R1, benchang1110/Qwen2.5-Taiwan-3B-Reason-SFT, yin80871901/DeepSeek-R1-MF, RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Qwen-7B-awq, chutesai/DeepSeek-R1-NextN, chutesai/DeepSeek-R1-Zero-NextN, HandH1998/DeepSeek-R1-QQQ-g128, HandH1998/DeepSeek-R1-QQQ-channel, RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-4bits, RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-8bits, RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-awq, liuda1/DeepSeek-R1-Distill-Qwen-32B-bnb-8bit, DimensionSTP/DeepSeek-R1-Distill-Llama-70B-Ko-Reasoning, otomatis911/ModelTuning_1.5, mozilla-ai/DeepSeek-R1-Distill-Qwen-1.5B-llamafile, mozilla-ai/DeepSeek-R1-Distill-Qwen-7B-llamafile, mozilla-ai/DeepSeek-R1-Distill-Qwen-14B-llamafile, mozilla-ai/DeepSeek-R1-Distill-Llama-8B-llamafile, dbaeka/DeepSeek-R1-Distill-Qwen-1.5B, Mungert/DeepSeek-R1-Distill-Qwen-32B-GGUF, DimensionSTP/DeepSeek-R1-Distill-Llama-8B-Ko-Reasoning, Alhdrawi/R-Ray-Ai-model, Mungert/DeepSeek-R1-Distill-Llama-70B-GGUF, dbaeka/DeepSeek-R1-Distill-Llama-8B, datakurre/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.2.0, meituan/DeepSeek-R1-Channel-FP8, AlashOk/deepseek-r1-distill-qwen-7b, imkebe/DeepSeek-R1-Distill-Qwen-14B-rk3588-1.2.0, imkebe/DeepSeek-R1-Distill-Qwen-7B-rk3588-1.2.0, imkebe/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.2.0, TeeZee/DeepSeek-R1-Distill-Qwen-32B-bpw4.0-h8-exl2, unsloth/DeepSeek-R1-GGUF-UD, naiweizi/test, yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview, Ham3141/Deepseek-14b, benchang1110/Qwen2.5-Taiwan-3B-Reason-GRPO, m8than/DeepSeek-R1-ChatFix, n0morehhh/deepseek-14b, recursechat/DeepSeek-R1-Distill-Qwen-7B-GGUF, jiabinbin/model, unsloth/DeepSeek-R1-0528, bullerwins/DeepSeek-R1-0528-bf16, iconer-321/DeepSeek-R1-Distill-Llama-8B-rk3588-1.1.2, bullerwins/DeepSeek-R1-0528-GGUF, iconer-321/DeepSeek-R1-Distill-Qwen-7B-rk3588-1.1.2, unsloth/DeepSeek-R1-0528-Qwen3-8B, unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF, bullerwins/DeepSeek-R1-0528-Qwen3-8B-GGUF, quantized4all/DeepSeek-R1-0528-Qwen3-8B-GGUF, bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-8.0bpw, bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-6.0bpw, matatonic/DeepSeek-R1-0528-Qwen3-8B-5.0bpw-exl2, matatonic/DeepSeek-R1-0528-Qwen3-8B-4.25bpw-exl2, matatonic/DeepSeek-R1-0528-Qwen3-8B-6.5bpw-h8-exl2, unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit, unsloth/DeepSeek-R1-0528-Qwen3-8B-bnb-4bit, bullerwins/DeepSeek-R1-0528-Qwen3-8B-fp8, bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-5.0bpw, xabicasa/DeepSeek-R1-0528-Qwen3-8B-MLX-4bit, dulimov/DeepSeek-R1-0528-Qwen3-8B-rk3588-1.2.1, Jackmin108/qwen-7b-rl-step-1, Jackmin108/qwen-7b-rl-step-2, Jackmin108/qwen-7b-rl-step-3, Jackmin108/qwen-7b-rl-step-4, Jackmin108/qwen-7b-rl-step-8, Jackmin108/qwen-7b-rl-step-16, Jackmin108/qwen-7b-rl-step-31, Jackmin108/qwen-7b-rl-step-32, QuantTrio/DeepSeek-R1-0528-Qwen3-8B-GPTQ-Int4-Int8Mix, QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int8-W8A16, QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int4-W4A16, iconer-321/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.2, QuixiAI/DeepSeek-R1-0528-bf16, samsja/DeepSeek-R1-Distill-Qwen-1.5B-math-only-async-4, samsja/DeepSeek-R1-Distill-Qwen-1.5B-math-only-async-2, Disya/DeepSeek-R1-0528-Qwen3-8B-exl2-8bpw-h8, adamo1139/DeepSeek-R1-0528-AWQ, QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Lite, DOFOFFICIAL/DeepSeek-R1-0528-Qwen3-8B-Correct-GGUF, QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Compact, Mungert/DeepSeek-R1-0528-Qwen3-8B-GGUF, Antigma/DeepSeek-R1-0528-Qwen3-8B-GGUF, Sci-fi-vy/DeepSeek-R1-0528-Qwen3-8B-GGUF, QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Medium, stelterlab/DeepSeek-R1-0528-Qwen3-8B-AWQ, async0x42/DeepSeek-R1-0528-Qwen3-8B-exl3_4.0bpw, async0x42/DeepSeek-R1-0528-Qwen3-8B-exl3_4.5bpw, QuantFactory/DeepSeek-R1-0528-Qwen3-8B-GGUF, mikaylagawarecki/foo_bar, muzerai/qwen3-8b-aijoah-magic8, openentry/qwen3-8b-merge-openentry, openentry/qwen3-8b-merge-openentry-GGUF, gabriellarson/DeepSeek-R1-0528-Qwen3-8B-GGUF, Supergene/DeepSeek-R1-Distill-Qwen-32B-Japanese, aarnphm/deepseek-r1-distill-llama-70b-sharded-tp4, Conexis/DeepSeek-R1-0528-NextN, Conexis/DeepSeek-R1-0528-NextN-BF16, Conexis/DeepSeek-R1-0528-NextN-Channel-INT8, Conexis/DeepSeek-R1-0528-Channel-INT8, NVFP4/DeepSeek-R1-0528-Qwen3-8B-FP4, muranAI/DeepSeek-R1-0528-Qwen3-8B-GGUF, dyadya-sam/DeepSeek-R1-0528-GGUF-Q4_K_XL, FastFlowLM/Deepseek-R1-Distill-Llama-8B-NPU2, backups/DeepSeek-R1-0528, taronaeo/DeepSeek-R1-Distill-Llama-8B-BE-GGUF, NotDivYt/Deepseek_R1_DIVV, DheyoAI/DeepSeek-R1-Distill-Qwen-1.5B-GGUF, CTCT-CT2/changeway_guardrails, TMElyralab/DeepSeek-R1-AWQ-W4AFP8, taronaeo/DeepSeek-R1-0528-Qwen3-8B-BE-GGUF, nickosn/ds-qwen7b, ChangyuLiu/DeepSeek-R1-Distill-Qwen-1.5B-GPTQ_W8A8_G128, ChangyuLiu/DeepSeek-R1-Distill-Qwen-1.5B-GPTQ_FP8_DYNAMIC_G128, ChangyuLiu/DeepSeek-R1-Distill-Qwen-7B-GPTQ_W8A8_G128, ChangyuLiu/DeepSeek-R1-Distill-Llama-8B-GPTQ_W8A8_G128, neko-llm/DeepSeek-R1-0528-bf16, agreeupon/agreeupon-deepseek-model, TMElyralab/DeepSeek-R1-0528-AWQ-W4AFP8, recursechat/DeepSeek-R1-Distill-Llama-8B-GGUF, Trisert/DeepSeek-R1-0528-Qwen3-8B-exl2, pavimaheshwari/deepseek-8b-ict-finetuned-genesis, weblab-llm-competition-2025-bridge/oNo-1-DeepSeek-R1-0528-QLoRA-MedMCQA, NeoChen1024/DeepSeek-R1-0528-Qwen3-8B-FP8_DYNAMIC, FastFlowLM/DeepSeek-R1-0528-Qwen3-8B-NPU2, ronx-labs/affine-deepseek-r1-1.5b, wanchoi/DeepSeek-R1-Reasoning-Agent, AlekseyCalvin/Lyrical_MT_ru2en_2a_DeepSeekR1_Qwen3_8b, ChuGyouk/DeepSeek-R1-Distill-Qwen-1.5B-tokenizer-fixed, areddydev/r1-7B, weblab-llm-competition-2025-bridge/oNo-1-DeepSeek-R1-0528-QLoRA-MedMCQA-bf16, WenxinChen66/DeepSeek-R1-0528-Channel-INT8, WenxinChen66/DeepSeek-R1-0528-NextN-Channel-INT8, aiqwen/DeepSeek-R1, aiqwen/DeepSeek-R1-Distill-Qwen-32B, aiqwen/DeepSeek-R1-Distill-Qwen-1.5B, areddydev/r1-8B, pawlalowe/DeepSeek-R1-Distill-Qwen-14B-heretic, federicomarcuzzi/DeepSeek-R1-Distill-Llama-8B, federicomarcuzzi/DeepSeek-R1-Distill-Llama-70B, federicomarcuzzi/DeepSeek-R1-Distill-Qwen-14B, federicomarcuzzi/DeepSeek-R1-Distill-Qwen-32B, Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env16-kl-coef0.01, Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env16-kl-coef0.005, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-800, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-400, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-400, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-200, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-100, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-200, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-200, Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-256-400, Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-256-300, Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-rand16_1-400, Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-rand16_2-400, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-300, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-100, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-300, Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-100, llmware/deepseek-r1-distill-qwen-7b-onnx-qnn, llmware/deepseek-r1-distill-qwen-14b-onnx-qnn",
    "models_links": "https://huggingface.co/deepseek-ai/DeepSeek-R1, https://huggingface.co/deepseek-ai/DeepSeek-R1-0528, https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B, https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero, https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B, https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B, https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B, https://huggingface.co/unsloth/DeepSeek-R1-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-0528-BF16, https://huggingface.co/unsloth/DeepSeek-R1, https://huggingface.co/unsloth/DeepSeek-R1-BF16, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-Zero-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-unsloth-bnb-4bit, https://huggingface.co/fivenightatfreddyLTD/ChatGPTrival, https://huggingface.co/QuantFactory/DeepSeek-R1-Distill-Qwen-1.5B-GGUF, https://huggingface.co/QuantFactory/DeepSeek-R1-Distill-Qwen-14B-GGUF, https://huggingface.co/jbower/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.2, https://huggingface.co/PedroPareja/DeepSeek-R1-Distill-Qwen-32B-6.5bpw-h6-exl2, https://huggingface.co/PedroPareja/DeepSeek-R1-Distill-Qwen-32B-7bpw-h8-exl2, https://huggingface.co/Spestly/Atlas-Flash-1.5B-Preview, https://huggingface.co/Spestly/Atlas-Flash-7B-Preview, https://huggingface.co/calcuis/deepseek-r1, https://huggingface.co/jbower/DeepSeek-R1-Distill-Llama-8B-rk3588-1.1.2, https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese, https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese, https://huggingface.co/jobs-git/DeepSeek-R1, https://huggingface.co/FallnAI/2.5-R1-Distill, https://huggingface.co/Deepdive404/Deepseek-fork, https://huggingface.co/slomkarafa/15-00, https://huggingface.co/BuckalewFinancial/DeepSeek-R1, https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-unsloth-bnb-4bit, https://huggingface.co/TachyHealth/DeepSeek-R1, https://huggingface.co/FlorianJc/DeepSeek-R1-Distill-Llama-8B-vllm-fp8, https://huggingface.co/GPT4All-Community/DeepSeek-R1-Distill-Llama-8B-GGUF, https://huggingface.co/Ihor/Text2Graph-R1-Qwen2.5-0.5b, https://huggingface.co/Corianas/DeepSeek-R1-Distill-Qwen-14B-AWQ, https://huggingface.co/Omartificial-Intelligence-Space/Arabic-DeepSeek-R1-Distill-8B, https://huggingface.co/suayptalha/Maestro-R1-Llama-8B, https://huggingface.co/afdsas/myseek671b, https://huggingface.co/aeertrd/model2, https://huggingface.co/stelterlab/DeepSeek-R1-Distill-Qwen-14B-AWQ, https://huggingface.co/LoneStriker/DeepSeek-R1-Distill-Llama-70B-6.0bpw-h6-exl2, https://huggingface.co/LoneStriker/DeepSeek-R1-Distill-Llama-70B-8.0bpw-h8-exl2, https://huggingface.co/patrickbdevaney/deepseek-r1-qwen-7b-q6-exl2, https://huggingface.co/Tech-Awesome-Hub/deepseek-distilled-qwen-1.5B, https://huggingface.co/dwetzel/DeepSeek-R1-Distill-Qwen-32B-GPTQ-INT4, https://huggingface.co/QuantFactory/DeepSeek-R1-Distill-Qwen-7B-GGUF, https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Qwen-1.5B-GGUF-llamafile, https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Qwen-7B-GGUF-llamafile, https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Llama-8B-GGUF-llamafile, https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Qwen-14B-GGUF-llamafile, https://huggingface.co/anshul6273/Qwen2.5-7B-Atcoder-Reasoning-v1-GGUF, https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Qwen-32B-GGUF-llamafile, https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b, https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-awq, https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf, https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gptq-int4, https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gptq-int8, https://huggingface.co/EasierAI/DeepSeek-R1-Distill-Llama-8B, https://huggingface.co/EasierAI/DeepSeek-R1-Distill-Qwen-7B, https://huggingface.co/EasierAI/DeepSeek-R1-Distill-Qwen-14B, https://huggingface.co/Josephgflowers/DeepSeek-R1-Distill-Qwen-1.5B-LIMO, https://huggingface.co/dhruv070/labonweb, https://huggingface.co/PsiPi/deepseek-r1-14b-gguf, https://huggingface.co/baseten/r1-nextn-head0, https://huggingface.co/SGLang/DeepSeek-R1-NextN, https://huggingface.co/professorf/DeepSeek-R1-Distill-Qwen-1.5B-gguf, https://huggingface.co/professorf/DeepSeek-R1-Distill-Qwen-7B-gguf, https://huggingface.co/BlackBeenie/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-LIMO, https://huggingface.co/jhall0310/chat, https://huggingface.co/perplexity-ai/r1-1776-distill-llama-70b, https://huggingface.co/yuanzu/DeepSeek-R1-INT8, https://huggingface.co/matatonic/r1-1776-distill-llama-70b-6.5bpw-h8-exl2, https://huggingface.co/unsloth/r1-1776-distill-llama-70b-GGUF, https://huggingface.co/unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit, https://huggingface.co/thellumi/LLuMi_Think_70B, https://huggingface.co/thisnick/DeepSeek-R1-Distill-Llama-70B-abliterated, https://huggingface.co/meituan/DeepSeek-R1-Block-INT8, https://huggingface.co/Guygir/DeepSeek-ZipNN, https://huggingface.co/thellumi/LLuMi_Think_3B, https://huggingface.co/thellumi/LLuMi_Think_8B, https://huggingface.co/KnutJaegersberg/r1-1776-distill-llama-70b-exl2-4.65bpw, https://huggingface.co/Apel-sin/r1-1776-distill-llama-70b-exl2, https://huggingface.co/meituan/DeepSeek-R1-Channel-INT8, https://huggingface.co/datakurre/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.4, https://huggingface.co/Orion-zhen/DeepSeek-R1-Distill-Llama-70B-4.4bpw-h6-exl2, https://huggingface.co/VarunBudhani/deepseek-r1, https://huggingface.co/gilrm92/deep-seek-copy, https://huggingface.co/AnteriorAI/deepseek-r1-distilled-qwen-32b, https://huggingface.co/minpeter/DeepSeek-R1, https://huggingface.co/Gaaiwing/DeepSeek-R1-Distill-Qwen-1.5B, https://huggingface.co/spuliz/Ahilab, https://huggingface.co/braindao/DeepSeek-R1-Distill-Qwen-14B, https://huggingface.co/braindao/DeepSeek-R1-Distill-Qwen-7B, https://huggingface.co/ahz-r3v/DeepSeek-R1-Distill-Qwen-7B-rk3588-rkllm-1.1.4, https://huggingface.co/ajaykumar1973/my-ai-model, https://huggingface.co/ajaykumar1973/ai-model-1.5b, https://huggingface.co/Amyww/case001, https://huggingface.co/birvii/DeepSeek-R1-Distill-Qwen-7B, https://huggingface.co/real-jiakai/DeepSeek-R1-Distill-Qwen-7B-News-Classifier, https://huggingface.co/rinna/qwq-bakeneko-32b, https://huggingface.co/rinna/qwq-bakeneko-32b-awq, https://huggingface.co/rinna/qwq-bakeneko-32b-gguf, https://huggingface.co/rinna/qwq-bakeneko-32b-gptq-int8, https://huggingface.co/rinna/qwq-bakeneko-32b-gptq-int4, https://huggingface.co/realYinkaIyiola/DistillR1-1.5B-Pruned-1.1B, https://huggingface.co/zipnn/DeepSeek-R1-Distill-Qwen-32B-ZipNN, https://huggingface.co/LLMJapan/qwq-bakeneko-32b_exl2_8.0bpw, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-awq, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int8, https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int4, https://huggingface.co/takanori39/DeepSeek-R1-Distill-Qwen-1.5B-awq, https://huggingface.co/Victor1o/deepseek-r1-model, https://huggingface.co/medmekk/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit, https://huggingface.co/Mungert/DeepSeek-R1-Distill-Qwen-1.5B-GGUF, https://huggingface.co/Mungert/DeepSeek-R1-Distill-Qwen-7B-GGUF, https://huggingface.co/Mungert/DeepSeek-R1-Distill-Llama-8B-GGUF, https://huggingface.co/bnb-community/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit, https://huggingface.co/bnb-community/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit, https://huggingface.co/Mungert/DeepSeek-R1-Distill-Qwen-14B-GGUF, https://huggingface.co/Compumacy/R1, https://huggingface.co/PursuitOfDataScience/Qwen2.5-1.5B-Instruct-Lora-Deepseek-R1, https://huggingface.co/benchang1110/Qwen2.5-Taiwan-3B-Reason-SFT, https://huggingface.co/yin80871901/DeepSeek-R1-MF, https://huggingface.co/RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Qwen-7B-awq, https://huggingface.co/chutesai/DeepSeek-R1-NextN, https://huggingface.co/chutesai/DeepSeek-R1-Zero-NextN, https://huggingface.co/HandH1998/DeepSeek-R1-QQQ-g128, https://huggingface.co/HandH1998/DeepSeek-R1-QQQ-channel, https://huggingface.co/RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-4bits, https://huggingface.co/RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-8bits, https://huggingface.co/RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-awq, https://huggingface.co/liuda1/DeepSeek-R1-Distill-Qwen-32B-bnb-8bit, https://huggingface.co/DimensionSTP/DeepSeek-R1-Distill-Llama-70B-Ko-Reasoning, https://huggingface.co/otomatis911/ModelTuning_1.5, https://huggingface.co/mozilla-ai/DeepSeek-R1-Distill-Qwen-1.5B-llamafile, https://huggingface.co/mozilla-ai/DeepSeek-R1-Distill-Qwen-7B-llamafile, https://huggingface.co/mozilla-ai/DeepSeek-R1-Distill-Qwen-14B-llamafile, https://huggingface.co/mozilla-ai/DeepSeek-R1-Distill-Llama-8B-llamafile, https://huggingface.co/dbaeka/DeepSeek-R1-Distill-Qwen-1.5B, https://huggingface.co/Mungert/DeepSeek-R1-Distill-Qwen-32B-GGUF, https://huggingface.co/DimensionSTP/DeepSeek-R1-Distill-Llama-8B-Ko-Reasoning, https://huggingface.co/Alhdrawi/R-Ray-Ai-model, https://huggingface.co/Mungert/DeepSeek-R1-Distill-Llama-70B-GGUF, https://huggingface.co/dbaeka/DeepSeek-R1-Distill-Llama-8B, https://huggingface.co/datakurre/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.2.0, https://huggingface.co/meituan/DeepSeek-R1-Channel-FP8, https://huggingface.co/AlashOk/deepseek-r1-distill-qwen-7b, https://huggingface.co/imkebe/DeepSeek-R1-Distill-Qwen-14B-rk3588-1.2.0, https://huggingface.co/imkebe/DeepSeek-R1-Distill-Qwen-7B-rk3588-1.2.0, https://huggingface.co/imkebe/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.2.0, https://huggingface.co/TeeZee/DeepSeek-R1-Distill-Qwen-32B-bpw4.0-h8-exl2, https://huggingface.co/unsloth/DeepSeek-R1-GGUF-UD, https://huggingface.co/naiweizi/test, https://huggingface.co/yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview, https://huggingface.co/Ham3141/Deepseek-14b, https://huggingface.co/benchang1110/Qwen2.5-Taiwan-3B-Reason-GRPO, https://huggingface.co/m8than/DeepSeek-R1-ChatFix, https://huggingface.co/n0morehhh/deepseek-14b, https://huggingface.co/recursechat/DeepSeek-R1-Distill-Qwen-7B-GGUF, https://huggingface.co/jiabinbin/model, https://huggingface.co/unsloth/DeepSeek-R1-0528, https://huggingface.co/bullerwins/DeepSeek-R1-0528-bf16, https://huggingface.co/iconer-321/DeepSeek-R1-Distill-Llama-8B-rk3588-1.1.2, https://huggingface.co/bullerwins/DeepSeek-R1-0528-GGUF, https://huggingface.co/iconer-321/DeepSeek-R1-Distill-Qwen-7B-rk3588-1.1.2, https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B, https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/quantized4all/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-8.0bpw, https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-6.0bpw, https://huggingface.co/matatonic/DeepSeek-R1-0528-Qwen3-8B-5.0bpw-exl2, https://huggingface.co/matatonic/DeepSeek-R1-0528-Qwen3-8B-4.25bpw-exl2, https://huggingface.co/matatonic/DeepSeek-R1-0528-Qwen3-8B-6.5bpw-h8-exl2, https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit, https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-bnb-4bit, https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-fp8, https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-5.0bpw, https://huggingface.co/xabicasa/DeepSeek-R1-0528-Qwen3-8B-MLX-4bit, https://huggingface.co/dulimov/DeepSeek-R1-0528-Qwen3-8B-rk3588-1.2.1, https://huggingface.co/Jackmin108/qwen-7b-rl-step-1, https://huggingface.co/Jackmin108/qwen-7b-rl-step-2, https://huggingface.co/Jackmin108/qwen-7b-rl-step-3, https://huggingface.co/Jackmin108/qwen-7b-rl-step-4, https://huggingface.co/Jackmin108/qwen-7b-rl-step-8, https://huggingface.co/Jackmin108/qwen-7b-rl-step-16, https://huggingface.co/Jackmin108/qwen-7b-rl-step-31, https://huggingface.co/Jackmin108/qwen-7b-rl-step-32, https://huggingface.co/QuantTrio/DeepSeek-R1-0528-Qwen3-8B-GPTQ-Int4-Int8Mix, https://huggingface.co/QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int8-W8A16, https://huggingface.co/QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int4-W4A16, https://huggingface.co/iconer-321/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.2, https://huggingface.co/QuixiAI/DeepSeek-R1-0528-bf16, https://huggingface.co/samsja/DeepSeek-R1-Distill-Qwen-1.5B-math-only-async-4, https://huggingface.co/samsja/DeepSeek-R1-Distill-Qwen-1.5B-math-only-async-2, https://huggingface.co/Disya/DeepSeek-R1-0528-Qwen3-8B-exl2-8bpw-h8, https://huggingface.co/adamo1139/DeepSeek-R1-0528-AWQ, https://huggingface.co/QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Lite, https://huggingface.co/DOFOFFICIAL/DeepSeek-R1-0528-Qwen3-8B-Correct-GGUF, https://huggingface.co/QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Compact, https://huggingface.co/Mungert/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/Antigma/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/Sci-fi-vy/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Medium, https://huggingface.co/stelterlab/DeepSeek-R1-0528-Qwen3-8B-AWQ, https://huggingface.co/async0x42/DeepSeek-R1-0528-Qwen3-8B-exl3_4.0bpw, https://huggingface.co/async0x42/DeepSeek-R1-0528-Qwen3-8B-exl3_4.5bpw, https://huggingface.co/QuantFactory/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/mikaylagawarecki/foo_bar, https://huggingface.co/muzerai/qwen3-8b-aijoah-magic8, https://huggingface.co/openentry/qwen3-8b-merge-openentry, https://huggingface.co/openentry/qwen3-8b-merge-openentry-GGUF, https://huggingface.co/gabriellarson/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/Supergene/DeepSeek-R1-Distill-Qwen-32B-Japanese, https://huggingface.co/aarnphm/deepseek-r1-distill-llama-70b-sharded-tp4, https://huggingface.co/Conexis/DeepSeek-R1-0528-NextN, https://huggingface.co/Conexis/DeepSeek-R1-0528-NextN-BF16, https://huggingface.co/Conexis/DeepSeek-R1-0528-NextN-Channel-INT8, https://huggingface.co/Conexis/DeepSeek-R1-0528-Channel-INT8, https://huggingface.co/NVFP4/DeepSeek-R1-0528-Qwen3-8B-FP4, https://huggingface.co/muranAI/DeepSeek-R1-0528-Qwen3-8B-GGUF, https://huggingface.co/dyadya-sam/DeepSeek-R1-0528-GGUF-Q4_K_XL, https://huggingface.co/FastFlowLM/Deepseek-R1-Distill-Llama-8B-NPU2, https://huggingface.co/backups/DeepSeek-R1-0528, https://huggingface.co/taronaeo/DeepSeek-R1-Distill-Llama-8B-BE-GGUF, https://huggingface.co/NotDivYt/Deepseek_R1_DIVV, https://huggingface.co/DheyoAI/DeepSeek-R1-Distill-Qwen-1.5B-GGUF, https://huggingface.co/CTCT-CT2/changeway_guardrails, https://huggingface.co/TMElyralab/DeepSeek-R1-AWQ-W4AFP8, https://huggingface.co/taronaeo/DeepSeek-R1-0528-Qwen3-8B-BE-GGUF, https://huggingface.co/nickosn/ds-qwen7b, https://huggingface.co/ChangyuLiu/DeepSeek-R1-Distill-Qwen-1.5B-GPTQ_W8A8_G128, https://huggingface.co/ChangyuLiu/DeepSeek-R1-Distill-Qwen-1.5B-GPTQ_FP8_DYNAMIC_G128, https://huggingface.co/ChangyuLiu/DeepSeek-R1-Distill-Qwen-7B-GPTQ_W8A8_G128, https://huggingface.co/ChangyuLiu/DeepSeek-R1-Distill-Llama-8B-GPTQ_W8A8_G128, https://huggingface.co/neko-llm/DeepSeek-R1-0528-bf16, https://huggingface.co/agreeupon/agreeupon-deepseek-model, https://huggingface.co/TMElyralab/DeepSeek-R1-0528-AWQ-W4AFP8, https://huggingface.co/recursechat/DeepSeek-R1-Distill-Llama-8B-GGUF, https://huggingface.co/Trisert/DeepSeek-R1-0528-Qwen3-8B-exl2, https://huggingface.co/pavimaheshwari/deepseek-8b-ict-finetuned-genesis, https://huggingface.co/weblab-llm-competition-2025-bridge/oNo-1-DeepSeek-R1-0528-QLoRA-MedMCQA, https://huggingface.co/NeoChen1024/DeepSeek-R1-0528-Qwen3-8B-FP8_DYNAMIC, https://huggingface.co/FastFlowLM/DeepSeek-R1-0528-Qwen3-8B-NPU2, https://huggingface.co/ronx-labs/affine-deepseek-r1-1.5b, https://huggingface.co/wanchoi/DeepSeek-R1-Reasoning-Agent, https://huggingface.co/AlekseyCalvin/Lyrical_MT_ru2en_2a_DeepSeekR1_Qwen3_8b, https://huggingface.co/ChuGyouk/DeepSeek-R1-Distill-Qwen-1.5B-tokenizer-fixed, https://huggingface.co/areddydev/r1-7B, https://huggingface.co/weblab-llm-competition-2025-bridge/oNo-1-DeepSeek-R1-0528-QLoRA-MedMCQA-bf16, https://huggingface.co/WenxinChen66/DeepSeek-R1-0528-Channel-INT8, https://huggingface.co/WenxinChen66/DeepSeek-R1-0528-NextN-Channel-INT8, https://huggingface.co/aiqwen/DeepSeek-R1, https://huggingface.co/aiqwen/DeepSeek-R1-Distill-Qwen-32B, https://huggingface.co/aiqwen/DeepSeek-R1-Distill-Qwen-1.5B, https://huggingface.co/areddydev/r1-8B, https://huggingface.co/pawlalowe/DeepSeek-R1-Distill-Qwen-14B-heretic, https://huggingface.co/federicomarcuzzi/DeepSeek-R1-Distill-Llama-8B, https://huggingface.co/federicomarcuzzi/DeepSeek-R1-Distill-Llama-70B, https://huggingface.co/federicomarcuzzi/DeepSeek-R1-Distill-Qwen-14B, https://huggingface.co/federicomarcuzzi/DeepSeek-R1-Distill-Qwen-32B, https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env16-kl-coef0.01, https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env16-kl-coef0.005, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-800, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-400, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-400, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-200, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-100, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-200, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-200, https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-256-400, https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-256-300, https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-rand16_1-400, https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-rand16_2-400, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-300, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-100, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-300, https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-100, https://huggingface.co/llmware/deepseek-r1-distill-qwen-7b-onnx-qnn, https://huggingface.co/llmware/deepseek-r1-distill-qwen-14b-onnx-qnn",
    "models_detailed": "[{\"name\": \"deepseek-ai/DeepSeek-R1\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-0528\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-Zero\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\", \"link\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"unsloth/DeepSeek-R1-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 19\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"unsloth/DeepSeek-R1-0528-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 15\"}, {\"name\": \"unsloth/DeepSeek-R1-0528-BF16\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-0528-BF16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 14\"}, {\"name\": \"unsloth/DeepSeek-R1\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1\", \"task\": \"Text Generation\", \"likes\": \"247\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"unsloth/DeepSeek-R1-BF16\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-BF16\", \"task\": \"Text Generation\", \"likes\": \"112\", \"downloads\": \"\", \"updated\": \"Apr 19\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 19\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Llama-70B\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B\", \"task\": \"Text Generation\", \"likes\": \"119\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 10\"}, {\"name\": \"unsloth/DeepSeek-R1-Zero-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Zero-GGUF\", \"task\": \"\", \"likes\": \"393\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Qwen-32B-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"610\", \"downloads\": \"\", \"updated\": \"Feb 14\"}, {\"name\": \"fivenightatfreddyLTD/ChatGPTrival\", \"link\": \"https://huggingface.co/fivenightatfreddyLTD/ChatGPTrival\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 23\"}, {\"name\": \"QuantFactory/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\", \"task\": \"\", \"likes\": \"211\", \"downloads\": \"\", \"updated\": \"Jan 23\"}, {\"name\": \"QuantFactory/DeepSeek-R1-Distill-Qwen-14B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/DeepSeek-R1-Distill-Qwen-14B-GGUF\", \"task\": \"\", \"likes\": \"225\", \"downloads\": \"\", \"updated\": \"Jan 24\"}, {\"name\": \"jbower/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.2\", \"link\": \"https://huggingface.co/jbower/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.2\", \"task\": \"Text Generation\", \"likes\": \"4\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"PedroPareja/DeepSeek-R1-Distill-Qwen-32B-6.5bpw-h6-exl2\", \"link\": \"https://huggingface.co/PedroPareja/DeepSeek-R1-Distill-Qwen-32B-6.5bpw-h6-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"PedroPareja/DeepSeek-R1-Distill-Qwen-32B-7bpw-h8-exl2\", \"link\": \"https://huggingface.co/PedroPareja/DeepSeek-R1-Distill-Qwen-32B-7bpw-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"Spestly/Atlas-Flash-1.5B-Preview\", \"link\": \"https://huggingface.co/Spestly/Atlas-Flash-1.5B-Preview\", \"task\": \"Text Generation\", \"likes\": \"47\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"Spestly/Atlas-Flash-7B-Preview\", \"link\": \"https://huggingface.co/Spestly/Atlas-Flash-7B-Preview\", \"task\": \"Text Generation\", \"likes\": \"80\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"calcuis/deepseek-r1\", \"link\": \"https://huggingface.co/calcuis/deepseek-r1\", \"task\": \"Text Generation\", \"likes\": \"524\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"jbower/DeepSeek-R1-Distill-Llama-8B-rk3588-1.1.2\", \"link\": \"https://huggingface.co/jbower/DeepSeek-R1-Distill-Llama-8B-rk3588-1.1.2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 26\"}, {\"name\": \"cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese\", \"link\": \"https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese\", \"task\": \"Text Generation\", \"likes\": \"784\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese\", \"link\": \"https://huggingface.co/cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese\", \"task\": \"Text Generation\", \"likes\": \"439\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"jobs-git/DeepSeek-R1\", \"link\": \"https://huggingface.co/jobs-git/DeepSeek-R1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"FallnAI/2.5-R1-Distill\", \"link\": \"https://huggingface.co/FallnAI/2.5-R1-Distill\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 27\"}, {\"name\": \"Deepdive404/Deepseek-fork\", \"link\": \"https://huggingface.co/Deepdive404/Deepseek-fork\", \"task\": \"Text Generation\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Jan 28\"}, {\"name\": \"slomkarafa/15-00\", \"link\": \"https://huggingface.co/slomkarafa/15-00\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 29\"}, {\"name\": \"BuckalewFinancial/DeepSeek-R1\", \"link\": \"https://huggingface.co/BuckalewFinancial/DeepSeek-R1\", \"task\": \"Text Generation\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Jan 29\"}, {\"name\": \"unsloth/DeepSeek-R1-Distill-Llama-70B-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"98\", \"downloads\": \"\", \"updated\": \"Feb 14\"}, {\"name\": \"TachyHealth/DeepSeek-R1\", \"link\": \"https://huggingface.co/TachyHealth/DeepSeek-R1\", \"task\": \"Text Generation\", \"likes\": \"126\", \"downloads\": \"\", \"updated\": \"Jan 30\"}, {\"name\": \"FlorianJc/DeepSeek-R1-Distill-Llama-8B-vllm-fp8\", \"link\": \"https://huggingface.co/FlorianJc/DeepSeek-R1-Distill-Llama-8B-vllm-fp8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 30\"}, {\"name\": \"GPT4All-Community/DeepSeek-R1-Distill-Llama-8B-GGUF\", \"link\": \"https://huggingface.co/GPT4All-Community/DeepSeek-R1-Distill-Llama-8B-GGUF\", \"task\": \"\", \"likes\": \"794\", \"downloads\": \"\", \"updated\": \"Jan 30\"}, {\"name\": \"Ihor/Text2Graph-R1-Qwen2.5-0.5b\", \"link\": \"https://huggingface.co/Ihor/Text2Graph-R1-Qwen2.5-0.5b\", \"task\": \"Text Generation\", \"likes\": \"122\", \"downloads\": \"\", \"updated\": \"Aug 18\"}, {\"name\": \"Corianas/DeepSeek-R1-Distill-Qwen-14B-AWQ\", \"link\": \"https://huggingface.co/Corianas/DeepSeek-R1-Distill-Qwen-14B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 31\"}, {\"name\": \"Omartificial-Intelligence-Space/Arabic-DeepSeek-R1-Distill-8B\", \"link\": \"https://huggingface.co/Omartificial-Intelligence-Space/Arabic-DeepSeek-R1-Distill-8B\", \"task\": \"Text Generation\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"suayptalha/Maestro-R1-Llama-8B\", \"link\": \"https://huggingface.co/suayptalha/Maestro-R1-Llama-8B\", \"task\": \"Text Generation\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Feb 1\"}, {\"name\": \"afdsas/myseek671b\", \"link\": \"https://huggingface.co/afdsas/myseek671b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"aeertrd/model2\", \"link\": \"https://huggingface.co/aeertrd/model2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"stelterlab/DeepSeek-R1-Distill-Qwen-14B-AWQ\", \"link\": \"https://huggingface.co/stelterlab/DeepSeek-R1-Distill-Qwen-14B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"LoneStriker/DeepSeek-R1-Distill-Llama-70B-6.0bpw-h6-exl2\", \"link\": \"https://huggingface.co/LoneStriker/DeepSeek-R1-Distill-Llama-70B-6.0bpw-h6-exl2\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"LoneStriker/DeepSeek-R1-Distill-Llama-70B-8.0bpw-h8-exl2\", \"link\": \"https://huggingface.co/LoneStriker/DeepSeek-R1-Distill-Llama-70B-8.0bpw-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"patrickbdevaney/deepseek-r1-qwen-7b-q6-exl2\", \"link\": \"https://huggingface.co/patrickbdevaney/deepseek-r1-qwen-7b-q6-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 3\"}, {\"name\": \"Tech-Awesome-Hub/deepseek-distilled-qwen-1.5B\", \"link\": \"https://huggingface.co/Tech-Awesome-Hub/deepseek-distilled-qwen-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 5\"}, {\"name\": \"dwetzel/DeepSeek-R1-Distill-Qwen-32B-GPTQ-INT4\", \"link\": \"https://huggingface.co/dwetzel/DeepSeek-R1-Distill-Qwen-32B-GPTQ-INT4\", \"task\": \"Text Generation\", \"likes\": \"52\", \"downloads\": \"\", \"updated\": \"Feb 4\"}, {\"name\": \"QuantFactory/DeepSeek-R1-Distill-Qwen-7B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/DeepSeek-R1-Distill-Qwen-7B-GGUF\", \"task\": \"\", \"likes\": \"206\", \"downloads\": \"\", \"updated\": \"Mar 9\"}, {\"name\": \"Bojun-Feng/DeepSeek-R1-Distill-Qwen-1.5B-GGUF-llamafile\", \"link\": \"https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Qwen-1.5B-GGUF-llamafile\", \"task\": \"\", \"likes\": \"63\", \"downloads\": \"\", \"updated\": \"Feb 5\"}, {\"name\": \"Bojun-Feng/DeepSeek-R1-Distill-Qwen-7B-GGUF-llamafile\", \"link\": \"https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Qwen-7B-GGUF-llamafile\", \"task\": \"\", \"likes\": \"261\", \"downloads\": \"\", \"updated\": \"Feb 6\"}, {\"name\": \"Bojun-Feng/DeepSeek-R1-Distill-Llama-8B-GGUF-llamafile\", \"link\": \"https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Llama-8B-GGUF-llamafile\", \"task\": \"\", \"likes\": \"143\", \"downloads\": \"\", \"updated\": \"Feb 6\"}, {\"name\": \"Bojun-Feng/DeepSeek-R1-Distill-Qwen-14B-GGUF-llamafile\", \"link\": \"https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Qwen-14B-GGUF-llamafile\", \"task\": \"\", \"likes\": \"111\", \"downloads\": \"\", \"updated\": \"Feb 6\"}, {\"name\": \"anshul6273/Qwen2.5-7B-Atcoder-Reasoning-v1-GGUF\", \"link\": \"https://huggingface.co/anshul6273/Qwen2.5-7B-Atcoder-Reasoning-v1-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 10\"}, {\"name\": \"Bojun-Feng/DeepSeek-R1-Distill-Qwen-32B-GGUF-llamafile\", \"link\": \"https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Qwen-32B-GGUF-llamafile\", \"task\": \"\", \"likes\": \"116\", \"downloads\": \"\", \"updated\": \"Feb 11\"}, {\"name\": \"rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b\", \"link\": \"https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b\", \"task\": \"Text Generation\", \"likes\": \"84\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-awq\", \"link\": \"https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-awq\", \"task\": \"Text Generation\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf\", \"link\": \"https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf\", \"task\": \"Text Generation\", \"likes\": \"197\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gptq-int4\", \"link\": \"https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gptq-int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gptq-int8\", \"link\": \"https://huggingface.co/rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gptq-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"EasierAI/DeepSeek-R1-Distill-Llama-8B\", \"link\": \"https://huggingface.co/EasierAI/DeepSeek-R1-Distill-Llama-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 12\"}, {\"name\": \"EasierAI/DeepSeek-R1-Distill-Qwen-7B\", \"link\": \"https://huggingface.co/EasierAI/DeepSeek-R1-Distill-Qwen-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 12\"}, {\"name\": \"EasierAI/DeepSeek-R1-Distill-Qwen-14B\", \"link\": \"https://huggingface.co/EasierAI/DeepSeek-R1-Distill-Qwen-14B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 12\"}, {\"name\": \"Josephgflowers/DeepSeek-R1-Distill-Qwen-1.5B-LIMO\", \"link\": \"https://huggingface.co/Josephgflowers/DeepSeek-R1-Distill-Qwen-1.5B-LIMO\", \"task\": \"\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 14\"}, {\"name\": \"dhruv070/labonweb\", \"link\": \"https://huggingface.co/dhruv070/labonweb\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 14\"}, {\"name\": \"PsiPi/deepseek-r1-14b-gguf\", \"link\": \"https://huggingface.co/PsiPi/deepseek-r1-14b-gguf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 21\"}, {\"name\": \"baseten/r1-nextn-head0\", \"link\": \"https://huggingface.co/baseten/r1-nextn-head0\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 14\"}, {\"name\": \"SGLang/DeepSeek-R1-NextN\", \"link\": \"https://huggingface.co/SGLang/DeepSeek-R1-NextN\", \"task\": \"\", \"likes\": \"150\", \"downloads\": \"\", \"updated\": \"Feb 15\"}, {\"name\": \"professorf/DeepSeek-R1-Distill-Qwen-1.5B-gguf\", \"link\": \"https://huggingface.co/professorf/DeepSeek-R1-Distill-Qwen-1.5B-gguf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 16\"}, {\"name\": \"professorf/DeepSeek-R1-Distill-Qwen-7B-gguf\", \"link\": \"https://huggingface.co/professorf/DeepSeek-R1-Distill-Qwen-7B-gguf\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 16\"}, {\"name\": \"BlackBeenie/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-LIMO\", \"link\": \"https://huggingface.co/BlackBeenie/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-LIMO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 19\"}, {\"name\": \"jhall0310/chat\", \"link\": \"https://huggingface.co/jhall0310/chat\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 15\"}, {\"name\": \"perplexity-ai/r1-1776-distill-llama-70b\", \"link\": \"https://huggingface.co/perplexity-ai/r1-1776-distill-llama-70b\", \"task\": \"Text Generation\", \"likes\": \"346\", \"downloads\": \"\", \"updated\": \"Feb 26\"}, {\"name\": \"yuanzu/DeepSeek-R1-INT8\", \"link\": \"https://huggingface.co/yuanzu/DeepSeek-R1-INT8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"matatonic/r1-1776-distill-llama-70b-6.5bpw-h8-exl2\", \"link\": \"https://huggingface.co/matatonic/r1-1776-distill-llama-70b-6.5bpw-h8-exl2\", \"task\": \"\", \"likes\": \"5\", \"downloads\": \"\", \"updated\": \"Feb 22\"}, {\"name\": \"unsloth/r1-1776-distill-llama-70b-GGUF\", \"link\": \"https://huggingface.co/unsloth/r1-1776-distill-llama-70b-GGUF\", \"task\": \"Text Generation\", \"likes\": \"616\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/r1-1776-distill-llama-70b-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 22\"}, {\"name\": \"thellumi/LLuMi_Think_70B\", \"link\": \"https://huggingface.co/thellumi/LLuMi_Think_70B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 26\"}, {\"name\": \"thisnick/DeepSeek-R1-Distill-Llama-70B-abliterated\", \"link\": \"https://huggingface.co/thisnick/DeepSeek-R1-Distill-Llama-70B-abliterated\", \"task\": \"Text Generation\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"meituan/DeepSeek-R1-Block-INT8\", \"link\": \"https://huggingface.co/meituan/DeepSeek-R1-Block-INT8\", \"task\": \"Text Generation\", \"likes\": \"63\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"Guygir/DeepSeek-ZipNN\", \"link\": \"https://huggingface.co/Guygir/DeepSeek-ZipNN\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"thellumi/LLuMi_Think_3B\", \"link\": \"https://huggingface.co/thellumi/LLuMi_Think_3B\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Feb 26\"}, {\"name\": \"thellumi/LLuMi_Think_8B\", \"link\": \"https://huggingface.co/thellumi/LLuMi_Think_8B\", \"task\": \"Text Generation\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Feb 26\"}, {\"name\": \"KnutJaegersberg/r1-1776-distill-llama-70b-exl2-4.65bpw\", \"link\": \"https://huggingface.co/KnutJaegersberg/r1-1776-distill-llama-70b-exl2-4.65bpw\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 25\"}, {\"name\": \"Apel-sin/r1-1776-distill-llama-70b-exl2\", \"link\": \"https://huggingface.co/Apel-sin/r1-1776-distill-llama-70b-exl2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 26\"}, {\"name\": \"meituan/DeepSeek-R1-Channel-INT8\", \"link\": \"https://huggingface.co/meituan/DeepSeek-R1-Channel-INT8\", \"task\": \"Text Generation\", \"likes\": \"258\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"datakurre/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.4\", \"link\": \"https://huggingface.co/datakurre/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 27\"}, {\"name\": \"Orion-zhen/DeepSeek-R1-Distill-Llama-70B-4.4bpw-h6-exl2\", \"link\": \"https://huggingface.co/Orion-zhen/DeepSeek-R1-Distill-Llama-70B-4.4bpw-h6-exl2\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Mar 1\"}, {\"name\": \"VarunBudhani/deepseek-r1\", \"link\": \"https://huggingface.co/VarunBudhani/deepseek-r1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 1\"}, {\"name\": \"gilrm92/deep-seek-copy\", \"link\": \"https://huggingface.co/gilrm92/deep-seek-copy\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 3\"}, {\"name\": \"AnteriorAI/deepseek-r1-distilled-qwen-32b\", \"link\": \"https://huggingface.co/AnteriorAI/deepseek-r1-distilled-qwen-32b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 3\"}, {\"name\": \"minpeter/DeepSeek-R1\", \"link\": \"https://huggingface.co/minpeter/DeepSeek-R1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\"}, {\"name\": \"Gaaiwing/DeepSeek-R1-Distill-Qwen-1.5B\", \"link\": \"https://huggingface.co/Gaaiwing/DeepSeek-R1-Distill-Qwen-1.5B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 6\"}, {\"name\": \"spuliz/Ahilab\", \"link\": \"https://huggingface.co/spuliz/Ahilab\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 5\"}, {\"name\": \"braindao/DeepSeek-R1-Distill-Qwen-14B\", \"link\": \"https://huggingface.co/braindao/DeepSeek-R1-Distill-Qwen-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\"}, {\"name\": \"braindao/DeepSeek-R1-Distill-Qwen-7B\", \"link\": \"https://huggingface.co/braindao/DeepSeek-R1-Distill-Qwen-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 4\"}, {\"name\": \"ahz-r3v/DeepSeek-R1-Distill-Qwen-7B-rk3588-rkllm-1.1.4\", \"link\": \"https://huggingface.co/ahz-r3v/DeepSeek-R1-Distill-Qwen-7B-rk3588-rkllm-1.1.4\", \"task\": \"Text Generation\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Mar 4\"}, {\"name\": \"ajaykumar1973/my-ai-model\", \"link\": \"https://huggingface.co/ajaykumar1973/my-ai-model\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 5\"}, {\"name\": \"ajaykumar1973/ai-model-1.5b\", \"link\": \"https://huggingface.co/ajaykumar1973/ai-model-1.5b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 5\"}, {\"name\": \"Amyww/case001\", \"link\": \"https://huggingface.co/Amyww/case001\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 6\"}, {\"name\": \"birvii/DeepSeek-R1-Distill-Qwen-7B\", \"link\": \"https://huggingface.co/birvii/DeepSeek-R1-Distill-Qwen-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 6\"}, {\"name\": \"real-jiakai/DeepSeek-R1-Distill-Qwen-7B-News-Classifier\", \"link\": \"https://huggingface.co/real-jiakai/DeepSeek-R1-Distill-Qwen-7B-News-Classifier\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 7\"}, {\"name\": \"rinna/qwq-bakeneko-32b\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b\", \"task\": \"Text Generation\", \"likes\": \"31\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwq-bakeneko-32b-awq\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b-awq\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwq-bakeneko-32b-gguf\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b-gguf\", \"task\": \"Text Generation\", \"likes\": \"192\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwq-bakeneko-32b-gptq-int8\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b-gptq-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwq-bakeneko-32b-gptq-int4\", \"link\": \"https://huggingface.co/rinna/qwq-bakeneko-32b-gptq-int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"realYinkaIyiola/DistillR1-1.5B-Pruned-1.1B\", \"link\": \"https://huggingface.co/realYinkaIyiola/DistillR1-1.5B-Pruned-1.1B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 13\"}, {\"name\": \"zipnn/DeepSeek-R1-Distill-Qwen-32B-ZipNN\", \"link\": \"https://huggingface.co/zipnn/DeepSeek-R1-Distill-Qwen-32B-ZipNN\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 16\"}, {\"name\": \"LLMJapan/qwq-bakeneko-32b_exl2_8.0bpw\", \"link\": \"https://huggingface.co/LLMJapan/qwq-bakeneko-32b_exl2_8.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 15\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct-v2-awq\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-awq\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gguf\", \"task\": \"Text Generation\", \"likes\": \"472\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int8\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int4\", \"link\": \"https://huggingface.co/rinna/qwen2.5-bakeneko-32b-instruct-v2-gptq-int4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"takanori39/DeepSeek-R1-Distill-Qwen-1.5B-awq\", \"link\": \"https://huggingface.co/takanori39/DeepSeek-R1-Distill-Qwen-1.5B-awq\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 17\"}, {\"name\": \"Victor1o/deepseek-r1-model\", \"link\": \"https://huggingface.co/Victor1o/deepseek-r1-model\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 12\"}, {\"name\": \"medmekk/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\", \"link\": \"https://huggingface.co/medmekk/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 19\"}, {\"name\": \"Mungert/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\", \"link\": \"https://huggingface.co/Mungert/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\", \"task\": \"\", \"likes\": \"274\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/DeepSeek-R1-Distill-Qwen-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/DeepSeek-R1-Distill-Qwen-7B-GGUF\", \"task\": \"\", \"likes\": \"256\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Mungert/DeepSeek-R1-Distill-Llama-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/DeepSeek-R1-Distill-Llama-8B-GGUF\", \"task\": \"\", \"likes\": \"352\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"bnb-community/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\", \"link\": \"https://huggingface.co/bnb-community/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"bnb-community/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit\", \"link\": \"https://huggingface.co/bnb-community/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"Mungert/DeepSeek-R1-Distill-Qwen-14B-GGUF\", \"link\": \"https://huggingface.co/Mungert/DeepSeek-R1-Distill-Qwen-14B-GGUF\", \"task\": \"\", \"likes\": \"212\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Compumacy/R1\", \"link\": \"https://huggingface.co/Compumacy/R1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"PursuitOfDataScience/Qwen2.5-1.5B-Instruct-Lora-Deepseek-R1\", \"link\": \"https://huggingface.co/PursuitOfDataScience/Qwen2.5-1.5B-Instruct-Lora-Deepseek-R1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 23\"}, {\"name\": \"benchang1110/Qwen2.5-Taiwan-3B-Reason-SFT\", \"link\": \"https://huggingface.co/benchang1110/Qwen2.5-Taiwan-3B-Reason-SFT\", \"task\": \"Text Generation\", \"likes\": \"14\", \"downloads\": \"\", \"updated\": \"Apr 27\"}, {\"name\": \"yin80871901/DeepSeek-R1-MF\", \"link\": \"https://huggingface.co/yin80871901/DeepSeek-R1-MF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\"}, {\"name\": \"RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Qwen-7B-awq\", \"link\": \"https://huggingface.co/RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Qwen-7B-awq\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\"}, {\"name\": \"chutesai/DeepSeek-R1-NextN\", \"link\": \"https://huggingface.co/chutesai/DeepSeek-R1-NextN\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\"}, {\"name\": \"chutesai/DeepSeek-R1-Zero-NextN\", \"link\": \"https://huggingface.co/chutesai/DeepSeek-R1-Zero-NextN\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\"}, {\"name\": \"HandH1998/DeepSeek-R1-QQQ-g128\", \"link\": \"https://huggingface.co/HandH1998/DeepSeek-R1-QQQ-g128\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"HandH1998/DeepSeek-R1-QQQ-channel\", \"link\": \"https://huggingface.co/HandH1998/DeepSeek-R1-QQQ-channel\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-4bits\", \"link\": \"https://huggingface.co/RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-4bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-8bits\", \"link\": \"https://huggingface.co/RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-8bits\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-awq\", \"link\": \"https://huggingface.co/RichardErkhov/deepseek-ai_-_DeepSeek-R1-Distill-Llama-8B-awq\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 27\"}, {\"name\": \"liuda1/DeepSeek-R1-Distill-Qwen-32B-bnb-8bit\", \"link\": \"https://huggingface.co/liuda1/DeepSeek-R1-Distill-Qwen-32B-bnb-8bit\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Mar 28\"}, {\"name\": \"DimensionSTP/DeepSeek-R1-Distill-Llama-70B-Ko-Reasoning\", \"link\": \"https://huggingface.co/DimensionSTP/DeepSeek-R1-Distill-Llama-70B-Ko-Reasoning\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"otomatis911/ModelTuning_1.5\", \"link\": \"https://huggingface.co/otomatis911/ModelTuning_1.5\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 29\"}, {\"name\": \"mozilla-ai/DeepSeek-R1-Distill-Qwen-1.5B-llamafile\", \"link\": \"https://huggingface.co/mozilla-ai/DeepSeek-R1-Distill-Qwen-1.5B-llamafile\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 31\"}, {\"name\": \"mozilla-ai/DeepSeek-R1-Distill-Qwen-7B-llamafile\", \"link\": \"https://huggingface.co/mozilla-ai/DeepSeek-R1-Distill-Qwen-7B-llamafile\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 31\"}, {\"name\": \"mozilla-ai/DeepSeek-R1-Distill-Qwen-14B-llamafile\", \"link\": \"https://huggingface.co/mozilla-ai/DeepSeek-R1-Distill-Qwen-14B-llamafile\", \"task\": \"Text Generation\", \"likes\": \"84\", \"downloads\": \"\", \"updated\": \"Apr 3\"}, {\"name\": \"mozilla-ai/DeepSeek-R1-Distill-Llama-8B-llamafile\", \"link\": \"https://huggingface.co/mozilla-ai/DeepSeek-R1-Distill-Llama-8B-llamafile\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 31\"}, {\"name\": \"dbaeka/DeepSeek-R1-Distill-Qwen-1.5B\", \"link\": \"https://huggingface.co/dbaeka/DeepSeek-R1-Distill-Qwen-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 2\"}, {\"name\": \"Mungert/DeepSeek-R1-Distill-Qwen-32B-GGUF\", \"link\": \"https://huggingface.co/Mungert/DeepSeek-R1-Distill-Qwen-32B-GGUF\", \"task\": \"\", \"likes\": \"391\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"DimensionSTP/DeepSeek-R1-Distill-Llama-8B-Ko-Reasoning\", \"link\": \"https://huggingface.co/DimensionSTP/DeepSeek-R1-Distill-Llama-8B-Ko-Reasoning\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Jul 24\"}, {\"name\": \"Alhdrawi/R-Ray-Ai-model\", \"link\": \"https://huggingface.co/Alhdrawi/R-Ray-Ai-model\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"Mungert/DeepSeek-R1-Distill-Llama-70B-GGUF\", \"link\": \"https://huggingface.co/Mungert/DeepSeek-R1-Distill-Llama-70B-GGUF\", \"task\": \"\", \"likes\": \"453\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"dbaeka/DeepSeek-R1-Distill-Llama-8B\", \"link\": \"https://huggingface.co/dbaeka/DeepSeek-R1-Distill-Llama-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"datakurre/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.2.0\", \"link\": \"https://huggingface.co/datakurre/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"meituan/DeepSeek-R1-Channel-FP8\", \"link\": \"https://huggingface.co/meituan/DeepSeek-R1-Channel-FP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"AlashOk/deepseek-r1-distill-qwen-7b\", \"link\": \"https://huggingface.co/AlashOk/deepseek-r1-distill-qwen-7b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"imkebe/DeepSeek-R1-Distill-Qwen-14B-rk3588-1.2.0\", \"link\": \"https://huggingface.co/imkebe/DeepSeek-R1-Distill-Qwen-14B-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 16\"}, {\"name\": \"imkebe/DeepSeek-R1-Distill-Qwen-7B-rk3588-1.2.0\", \"link\": \"https://huggingface.co/imkebe/DeepSeek-R1-Distill-Qwen-7B-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"imkebe/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.2.0\", \"link\": \"https://huggingface.co/imkebe/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.2.0\", \"task\": \"Text Generation\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"Apr 16\"}, {\"name\": \"TeeZee/DeepSeek-R1-Distill-Qwen-32B-bpw4.0-h8-exl2\", \"link\": \"https://huggingface.co/TeeZee/DeepSeek-R1-Distill-Qwen-32B-bpw4.0-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"unsloth/DeepSeek-R1-GGUF-UD\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-GGUF-UD\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 28\"}, {\"name\": \"naiweizi/test\", \"link\": \"https://huggingface.co/naiweizi/test\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 21\"}, {\"name\": \"yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview\", \"link\": \"https://huggingface.co/yanolja/YanoljaNEXT-EEVE-Instruct-7B-v2-Preview\", \"task\": \"\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Aug 29\"}, {\"name\": \"Ham3141/Deepseek-14b\", \"link\": \"https://huggingface.co/Ham3141/Deepseek-14b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 24\"}, {\"name\": \"benchang1110/Qwen2.5-Taiwan-3B-Reason-GRPO\", \"link\": \"https://huggingface.co/benchang1110/Qwen2.5-Taiwan-3B-Reason-GRPO\", \"task\": \"Text Generation\", \"likes\": \"10\", \"downloads\": \"\", \"updated\": \"Apr 27\"}, {\"name\": \"m8than/DeepSeek-R1-ChatFix\", \"link\": \"https://huggingface.co/m8than/DeepSeek-R1-ChatFix\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 8\"}, {\"name\": \"n0morehhh/deepseek-14b\", \"link\": \"https://huggingface.co/n0morehhh/deepseek-14b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 12\"}, {\"name\": \"recursechat/DeepSeek-R1-Distill-Qwen-7B-GGUF\", \"link\": \"https://huggingface.co/recursechat/DeepSeek-R1-Distill-Qwen-7B-GGUF\", \"task\": \"\", \"likes\": \"13\", \"downloads\": \"\", \"updated\": \"May 15\"}, {\"name\": \"jiabinbin/model\", \"link\": \"https://huggingface.co/jiabinbin/model\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"unsloth/DeepSeek-R1-0528\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-0528\", \"task\": \"Text Generation\", \"likes\": \"55\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"bullerwins/DeepSeek-R1-0528-bf16\", \"link\": \"https://huggingface.co/bullerwins/DeepSeek-R1-0528-bf16\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"iconer-321/DeepSeek-R1-Distill-Llama-8B-rk3588-1.1.2\", \"link\": \"https://huggingface.co/iconer-321/DeepSeek-R1-Distill-Llama-8B-rk3588-1.1.2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"bullerwins/DeepSeek-R1-0528-GGUF\", \"link\": \"https://huggingface.co/bullerwins/DeepSeek-R1-0528-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"iconer-321/DeepSeek-R1-Distill-Qwen-7B-rk3588-1.1.2\", \"link\": \"https://huggingface.co/iconer-321/DeepSeek-R1-Distill-Qwen-7B-rk3588-1.1.2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"unsloth/DeepSeek-R1-0528-Qwen3-8B\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 16\"}, {\"name\": \"bullerwins/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"\", \"likes\": \"76\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"quantized4all/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/quantized4all/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"\", \"likes\": \"444\", \"downloads\": \"\", \"updated\": \"Jun 17\"}, {\"name\": \"bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-8.0bpw\", \"link\": \"https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-8.0bpw\", \"task\": \"Text Generation\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-6.0bpw\", \"link\": \"https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-6.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"matatonic/DeepSeek-R1-0528-Qwen3-8B-5.0bpw-exl2\", \"link\": \"https://huggingface.co/matatonic/DeepSeek-R1-0528-Qwen3-8B-5.0bpw-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"matatonic/DeepSeek-R1-0528-Qwen3-8B-4.25bpw-exl2\", \"link\": \"https://huggingface.co/matatonic/DeepSeek-R1-0528-Qwen3-8B-4.25bpw-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"matatonic/DeepSeek-R1-0528-Qwen3-8B-6.5bpw-h8-exl2\", \"link\": \"https://huggingface.co/matatonic/DeepSeek-R1-0528-Qwen3-8B-6.5bpw-h8-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"unsloth/DeepSeek-R1-0528-Qwen3-8B-bnb-4bit\", \"link\": \"https://huggingface.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-bnb-4bit\", \"task\": \"Text Generation\", \"likes\": \"570\", \"downloads\": \"\", \"updated\": \"Jun 10\"}, {\"name\": \"bullerwins/DeepSeek-R1-0528-Qwen3-8B-fp8\", \"link\": \"https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-fp8\", \"task\": \"Text Generation\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-5.0bpw\", \"link\": \"https://huggingface.co/bullerwins/DeepSeek-R1-0528-Qwen3-8B-exl3-5.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"xabicasa/DeepSeek-R1-0528-Qwen3-8B-MLX-4bit\", \"link\": \"https://huggingface.co/xabicasa/DeepSeek-R1-0528-Qwen3-8B-MLX-4bit\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 29\"}, {\"name\": \"dulimov/DeepSeek-R1-0528-Qwen3-8B-rk3588-1.2.1\", \"link\": \"https://huggingface.co/dulimov/DeepSeek-R1-0528-Qwen3-8B-rk3588-1.2.1\", \"task\": \"Text Generation\", \"likes\": \"12\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"Jackmin108/qwen-7b-rl-step-1\", \"link\": \"https://huggingface.co/Jackmin108/qwen-7b-rl-step-1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"Jackmin108/qwen-7b-rl-step-2\", \"link\": \"https://huggingface.co/Jackmin108/qwen-7b-rl-step-2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"Jackmin108/qwen-7b-rl-step-3\", \"link\": \"https://huggingface.co/Jackmin108/qwen-7b-rl-step-3\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"Jackmin108/qwen-7b-rl-step-4\", \"link\": \"https://huggingface.co/Jackmin108/qwen-7b-rl-step-4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"Jackmin108/qwen-7b-rl-step-8\", \"link\": \"https://huggingface.co/Jackmin108/qwen-7b-rl-step-8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"Jackmin108/qwen-7b-rl-step-16\", \"link\": \"https://huggingface.co/Jackmin108/qwen-7b-rl-step-16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"Jackmin108/qwen-7b-rl-step-31\", \"link\": \"https://huggingface.co/Jackmin108/qwen-7b-rl-step-31\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"Jackmin108/qwen-7b-rl-step-32\", \"link\": \"https://huggingface.co/Jackmin108/qwen-7b-rl-step-32\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"QuantTrio/DeepSeek-R1-0528-Qwen3-8B-GPTQ-Int4-Int8Mix\", \"link\": \"https://huggingface.co/QuantTrio/DeepSeek-R1-0528-Qwen3-8B-GPTQ-Int4-Int8Mix\", \"task\": \"Text Generation\", \"likes\": \"140\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int8-W8A16\", \"link\": \"https://huggingface.co/QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int8-W8A16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int4-W4A16\", \"link\": \"https://huggingface.co/QuantTrio/DeepSeek-R1-0528-Qwen3-8B-Int4-W4A16\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 5\"}, {\"name\": \"iconer-321/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.2\", \"link\": \"https://huggingface.co/iconer-321/DeepSeek-R1-Distill-Qwen-1.5B-rk3588-1.1.2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 30\"}, {\"name\": \"QuixiAI/DeepSeek-R1-0528-bf16\", \"link\": \"https://huggingface.co/QuixiAI/DeepSeek-R1-0528-bf16\", \"task\": \"Text Generation\", \"likes\": \"23\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"samsja/DeepSeek-R1-Distill-Qwen-1.5B-math-only-async-4\", \"link\": \"https://huggingface.co/samsja/DeepSeek-R1-Distill-Qwen-1.5B-math-only-async-4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 31\"}, {\"name\": \"samsja/DeepSeek-R1-Distill-Qwen-1.5B-math-only-async-2\", \"link\": \"https://huggingface.co/samsja/DeepSeek-R1-Distill-Qwen-1.5B-math-only-async-2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 31\"}, {\"name\": \"Disya/DeepSeek-R1-0528-Qwen3-8B-exl2-8bpw-h8\", \"link\": \"https://huggingface.co/Disya/DeepSeek-R1-0528-Qwen3-8B-exl2-8bpw-h8\", \"task\": \"Text Generation\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"May 31\"}, {\"name\": \"adamo1139/DeepSeek-R1-0528-AWQ\", \"link\": \"https://huggingface.co/adamo1139/DeepSeek-R1-0528-AWQ\", \"task\": \"Text Generation\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Sep 18\"}, {\"name\": \"QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Lite\", \"link\": \"https://huggingface.co/QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Lite\", \"task\": \"Text Generation\", \"likes\": \"20\", \"downloads\": \"\", \"updated\": \"Aug 30\"}, {\"name\": \"DOFOFFICIAL/DeepSeek-R1-0528-Qwen3-8B-Correct-GGUF\", \"link\": \"https://huggingface.co/DOFOFFICIAL/DeepSeek-R1-0528-Qwen3-8B-Correct-GGUF\", \"task\": \"\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"Jun 3\"}, {\"name\": \"QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Compact\", \"link\": \"https://huggingface.co/QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Compact\", \"task\": \"Text Generation\", \"likes\": \"27\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"Mungert/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/Mungert/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"\", \"likes\": \"467\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"Antigma/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/Antigma/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 2\"}, {\"name\": \"Sci-fi-vy/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/Sci-fi-vy/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"359\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Medium\", \"link\": \"https://huggingface.co/QuantTrio/DeepSeek-R1-0528-GPTQ-Int4-Int8Mix-Medium\", \"task\": \"Text Generation\", \"likes\": \"134\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"stelterlab/DeepSeek-R1-0528-Qwen3-8B-AWQ\", \"link\": \"https://huggingface.co/stelterlab/DeepSeek-R1-0528-Qwen3-8B-AWQ\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 4\"}, {\"name\": \"async0x42/DeepSeek-R1-0528-Qwen3-8B-exl3_4.0bpw\", \"link\": \"https://huggingface.co/async0x42/DeepSeek-R1-0528-Qwen3-8B-exl3_4.0bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"async0x42/DeepSeek-R1-0528-Qwen3-8B-exl3_4.5bpw\", \"link\": \"https://huggingface.co/async0x42/DeepSeek-R1-0528-Qwen3-8B-exl3_4.5bpw\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 9\"}, {\"name\": \"QuantFactory/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/QuantFactory/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"\", \"likes\": \"122\", \"downloads\": \"\", \"updated\": \"Jun 11\"}, {\"name\": \"mikaylagawarecki/foo_bar\", \"link\": \"https://huggingface.co/mikaylagawarecki/foo_bar\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 11\"}, {\"name\": \"muzerai/qwen3-8b-aijoah-magic8\", \"link\": \"https://huggingface.co/muzerai/qwen3-8b-aijoah-magic8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"openentry/qwen3-8b-merge-openentry\", \"link\": \"https://huggingface.co/openentry/qwen3-8b-merge-openentry\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"openentry/qwen3-8b-merge-openentry-GGUF\", \"link\": \"https://huggingface.co/openentry/qwen3-8b-merge-openentry-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 14\"}, {\"name\": \"gabriellarson/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/gabriellarson/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 18\"}, {\"name\": \"Supergene/DeepSeek-R1-Distill-Qwen-32B-Japanese\", \"link\": \"https://huggingface.co/Supergene/DeepSeek-R1-Distill-Qwen-32B-Japanese\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 23\"}, {\"name\": \"aarnphm/deepseek-r1-distill-llama-70b-sharded-tp4\", \"link\": \"https://huggingface.co/aarnphm/deepseek-r1-distill-llama-70b-sharded-tp4\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 23\"}, {\"name\": \"Conexis/DeepSeek-R1-0528-NextN\", \"link\": \"https://huggingface.co/Conexis/DeepSeek-R1-0528-NextN\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"Conexis/DeepSeek-R1-0528-NextN-BF16\", \"link\": \"https://huggingface.co/Conexis/DeepSeek-R1-0528-NextN-BF16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"Conexis/DeepSeek-R1-0528-NextN-Channel-INT8\", \"link\": \"https://huggingface.co/Conexis/DeepSeek-R1-0528-NextN-Channel-INT8\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"Conexis/DeepSeek-R1-0528-Channel-INT8\", \"link\": \"https://huggingface.co/Conexis/DeepSeek-R1-0528-Channel-INT8\", \"task\": \"Text Generation\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Jun 24\"}, {\"name\": \"NVFP4/DeepSeek-R1-0528-Qwen3-8B-FP4\", \"link\": \"https://huggingface.co/NVFP4/DeepSeek-R1-0528-Qwen3-8B-FP4\", \"task\": \"\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Jul 23\"}, {\"name\": \"muranAI/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"link\": \"https://huggingface.co/muranAI/DeepSeek-R1-0528-Qwen3-8B-GGUF\", \"task\": \"\", \"likes\": \"141\", \"downloads\": \"\", \"updated\": \"Jun 28\"}, {\"name\": \"dyadya-sam/DeepSeek-R1-0528-GGUF-Q4_K_XL\", \"link\": \"https://huggingface.co/dyadya-sam/DeepSeek-R1-0528-GGUF-Q4_K_XL\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 27\"}, {\"name\": \"FastFlowLM/Deepseek-R1-Distill-Llama-8B-NPU2\", \"link\": \"https://huggingface.co/FastFlowLM/Deepseek-R1-Distill-Llama-8B-NPU2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 11\"}, {\"name\": \"backups/DeepSeek-R1-0528\", \"link\": \"https://huggingface.co/backups/DeepSeek-R1-0528\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 1\"}, {\"name\": \"taronaeo/DeepSeek-R1-Distill-Llama-8B-BE-GGUF\", \"link\": \"https://huggingface.co/taronaeo/DeepSeek-R1-Distill-Llama-8B-BE-GGUF\", \"task\": \"\", \"likes\": \"106\", \"downloads\": \"\", \"updated\": \"Jul 12\"}, {\"name\": \"NotDivYt/Deepseek_R1_DIVV\", \"link\": \"https://huggingface.co/NotDivYt/Deepseek_R1_DIVV\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 13\"}, {\"name\": \"DheyoAI/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\", \"link\": \"https://huggingface.co/DheyoAI/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"49\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"CTCT-CT2/changeway_guardrails\", \"link\": \"https://huggingface.co/CTCT-CT2/changeway_guardrails\", \"task\": \"\", \"likes\": \"11\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"TMElyralab/DeepSeek-R1-AWQ-W4AFP8\", \"link\": \"https://huggingface.co/TMElyralab/DeepSeek-R1-AWQ-W4AFP8\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"taronaeo/DeepSeek-R1-0528-Qwen3-8B-BE-GGUF\", \"link\": \"https://huggingface.co/taronaeo/DeepSeek-R1-0528-Qwen3-8B-BE-GGUF\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 26\"}, {\"name\": \"nickosn/ds-qwen7b\", \"link\": \"https://huggingface.co/nickosn/ds-qwen7b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"ChangyuLiu/DeepSeek-R1-Distill-Qwen-1.5B-GPTQ_W8A8_G128\", \"link\": \"https://huggingface.co/ChangyuLiu/DeepSeek-R1-Distill-Qwen-1.5B-GPTQ_W8A8_G128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"ChangyuLiu/DeepSeek-R1-Distill-Qwen-1.5B-GPTQ_FP8_DYNAMIC_G128\", \"link\": \"https://huggingface.co/ChangyuLiu/DeepSeek-R1-Distill-Qwen-1.5B-GPTQ_FP8_DYNAMIC_G128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"ChangyuLiu/DeepSeek-R1-Distill-Qwen-7B-GPTQ_W8A8_G128\", \"link\": \"https://huggingface.co/ChangyuLiu/DeepSeek-R1-Distill-Qwen-7B-GPTQ_W8A8_G128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"ChangyuLiu/DeepSeek-R1-Distill-Llama-8B-GPTQ_W8A8_G128\", \"link\": \"https://huggingface.co/ChangyuLiu/DeepSeek-R1-Distill-Llama-8B-GPTQ_W8A8_G128\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"neko-llm/DeepSeek-R1-0528-bf16\", \"link\": \"https://huggingface.co/neko-llm/DeepSeek-R1-0528-bf16\", \"task\": \"Text Generation\", \"likes\": \"15\", \"downloads\": \"\", \"updated\": \"Jul 28\"}, {\"name\": \"agreeupon/agreeupon-deepseek-model\", \"link\": \"https://huggingface.co/agreeupon/agreeupon-deepseek-model\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 29\"}, {\"name\": \"TMElyralab/DeepSeek-R1-0528-AWQ-W4AFP8\", \"link\": \"https://huggingface.co/TMElyralab/DeepSeek-R1-0528-AWQ-W4AFP8\", \"task\": \"Text Generation\", \"likes\": \"18\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"recursechat/DeepSeek-R1-Distill-Llama-8B-GGUF\", \"link\": \"https://huggingface.co/recursechat/DeepSeek-R1-Distill-Llama-8B-GGUF\", \"task\": \"Text Generation\", \"likes\": \"386\", \"downloads\": \"\", \"updated\": \"Aug 6\"}, {\"name\": \"Trisert/DeepSeek-R1-0528-Qwen3-8B-exl2\", \"link\": \"https://huggingface.co/Trisert/DeepSeek-R1-0528-Qwen3-8B-exl2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 12\"}, {\"name\": \"pavimaheshwari/deepseek-8b-ict-finetuned-genesis\", \"link\": \"https://huggingface.co/pavimaheshwari/deepseek-8b-ict-finetuned-genesis\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 14\"}, {\"name\": \"weblab-llm-competition-2025-bridge/oNo-1-DeepSeek-R1-0528-QLoRA-MedMCQA\", \"link\": \"https://huggingface.co/weblab-llm-competition-2025-bridge/oNo-1-DeepSeek-R1-0528-QLoRA-MedMCQA\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 25\"}, {\"name\": \"NeoChen1024/DeepSeek-R1-0528-Qwen3-8B-FP8_DYNAMIC\", \"link\": \"https://huggingface.co/NeoChen1024/DeepSeek-R1-0528-Qwen3-8B-FP8_DYNAMIC\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 26\"}, {\"name\": \"FastFlowLM/DeepSeek-R1-0528-Qwen3-8B-NPU2\", \"link\": \"https://huggingface.co/FastFlowLM/DeepSeek-R1-0528-Qwen3-8B-NPU2\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"ronx-labs/affine-deepseek-r1-1.5b\", \"link\": \"https://huggingface.co/ronx-labs/affine-deepseek-r1-1.5b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 27\"}, {\"name\": \"wanchoi/DeepSeek-R1-Reasoning-Agent\", \"link\": \"https://huggingface.co/wanchoi/DeepSeek-R1-Reasoning-Agent\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Aug 28\"}, {\"name\": \"AlekseyCalvin/Lyrical_MT_ru2en_2a_DeepSeekR1_Qwen3_8b\", \"link\": \"https://huggingface.co/AlekseyCalvin/Lyrical_MT_ru2en_2a_DeepSeekR1_Qwen3_8b\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 7\"}, {\"name\": \"ChuGyouk/DeepSeek-R1-Distill-Qwen-1.5B-tokenizer-fixed\", \"link\": \"https://huggingface.co/ChuGyouk/DeepSeek-R1-Distill-Qwen-1.5B-tokenizer-fixed\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 13\"}, {\"name\": \"areddydev/r1-7B\", \"link\": \"https://huggingface.co/areddydev/r1-7B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 8\"}, {\"name\": \"weblab-llm-competition-2025-bridge/oNo-1-DeepSeek-R1-0528-QLoRA-MedMCQA-bf16\", \"link\": \"https://huggingface.co/weblab-llm-competition-2025-bridge/oNo-1-DeepSeek-R1-0528-QLoRA-MedMCQA-bf16\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 10\"}, {\"name\": \"WenxinChen66/DeepSeek-R1-0528-Channel-INT8\", \"link\": \"https://huggingface.co/WenxinChen66/DeepSeek-R1-0528-Channel-INT8\", \"task\": \"Text Generation\", \"likes\": \"5\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"WenxinChen66/DeepSeek-R1-0528-NextN-Channel-INT8\", \"link\": \"https://huggingface.co/WenxinChen66/DeepSeek-R1-0528-NextN-Channel-INT8\", \"task\": \"\", \"likes\": \"6\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"aiqwen/DeepSeek-R1\", \"link\": \"https://huggingface.co/aiqwen/DeepSeek-R1\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"aiqwen/DeepSeek-R1-Distill-Qwen-32B\", \"link\": \"https://huggingface.co/aiqwen/DeepSeek-R1-Distill-Qwen-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"aiqwen/DeepSeek-R1-Distill-Qwen-1.5B\", \"link\": \"https://huggingface.co/aiqwen/DeepSeek-R1-Distill-Qwen-1.5B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\"}, {\"name\": \"areddydev/r1-8B\", \"link\": \"https://huggingface.co/areddydev/r1-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 27\"}, {\"name\": \"pawlalowe/DeepSeek-R1-Distill-Qwen-14B-heretic\", \"link\": \"https://huggingface.co/pawlalowe/DeepSeek-R1-Distill-Qwen-14B-heretic\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"28 days ago\"}, {\"name\": \"federicomarcuzzi/DeepSeek-R1-Distill-Llama-8B\", \"link\": \"https://huggingface.co/federicomarcuzzi/DeepSeek-R1-Distill-Llama-8B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"federicomarcuzzi/DeepSeek-R1-Distill-Llama-70B\", \"link\": \"https://huggingface.co/federicomarcuzzi/DeepSeek-R1-Distill-Llama-70B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"federicomarcuzzi/DeepSeek-R1-Distill-Qwen-14B\", \"link\": \"https://huggingface.co/federicomarcuzzi/DeepSeek-R1-Distill-Qwen-14B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"federicomarcuzzi/DeepSeek-R1-Distill-Qwen-32B\", \"link\": \"https://huggingface.co/federicomarcuzzi/DeepSeek-R1-Distill-Qwen-32B\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env16-kl-coef0.01\", \"link\": \"https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env16-kl-coef0.01\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env16-kl-coef0.005\", \"link\": \"https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env16-kl-coef0.005\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-800\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-800\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-400\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-400\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-400\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-400\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-200\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-200\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-100\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-256-100\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"5 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-200\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-200\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-200\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-200\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"4 days ago\"}, {\"name\": \"Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-256-400\", \"link\": \"https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-256-400\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-256-300\", \"link\": \"https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-256-300\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-rand16_1-400\", \"link\": \"https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-rand16_1-400\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-rand16_2-400\", \"link\": \"https://huggingface.co/Huawei-RLVE/DeepSeek-R1-Distill-Qwen-1.5B-env-rand16_2-400\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-300\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-300\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-100\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-16-100\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-300\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-300\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-100\", \"link\": \"https://huggingface.co/Huawei-RLVE/Deepseek-R1-Distill-Qwen-1.5B-env-4-100\", \"task\": \"Text Generation\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"3 days ago\"}, {\"name\": \"llmware/deepseek-r1-distill-qwen-7b-onnx-qnn\", \"link\": \"https://huggingface.co/llmware/deepseek-r1-distill-qwen-7b-onnx-qnn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"1 day ago\"}, {\"name\": \"llmware/deepseek-r1-distill-qwen-14b-onnx-qnn\", \"link\": \"https://huggingface.co/llmware/deepseek-r1-distill-qwen-14b-onnx-qnn\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"1 day ago\"}]",
    "num_datasets": 9,
    "datasets_list": "hkuzxc/scaf-grpo-dataset, TheBlueScrubs/TheBlueScrubs-v1, Can111/m500, davanstrien/hub-tldr-model-summaries-llama, attn-signs/gromov-0, Multiverse4FM/Multiverse-1K, noepsl/H-LLMC2, CTCT-CT2/ChangeMore-prompt-injection-eval, amishor/reinforce-learning-grpo",
    "datasets_links": "https://huggingface.co/datasets/hkuzxc/scaf-grpo-dataset, https://huggingface.co/datasets/TheBlueScrubs/TheBlueScrubs-v1, https://huggingface.co/datasets/Can111/m500, https://huggingface.co/datasets/davanstrien/hub-tldr-model-summaries-llama, https://huggingface.co/datasets/attn-signs/gromov-0, https://huggingface.co/datasets/Multiverse4FM/Multiverse-1K, https://huggingface.co/datasets/noepsl/H-LLMC2, https://huggingface.co/datasets/CTCT-CT2/ChangeMore-prompt-injection-eval, https://huggingface.co/datasets/amishor/reinforce-learning-grpo",
    "datasets_detailed": "[{\"name\": \"hkuzxc/scaf-grpo-dataset\", \"link\": \"https://huggingface.co/datasets/hkuzxc/scaf-grpo-dataset\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 25\", \"size\": \"\"}, {\"name\": \"TheBlueScrubs/TheBlueScrubs-v1\", \"link\": \"https://huggingface.co/datasets/TheBlueScrubs/TheBlueScrubs-v1\", \"task\": \"\", \"likes\": \"143\", \"downloads\": \"\", \"updated\": \"Feb 24\", \"size\": \"\"}, {\"name\": \"Can111/m500\", \"link\": \"https://huggingface.co/datasets/Can111/m500\", \"task\": \"\", \"likes\": \"500\", \"downloads\": \"\", \"updated\": \"Aug 20\", \"size\": \"\"}, {\"name\": \"davanstrien/hub-tldr-model-summaries-llama\", \"link\": \"https://huggingface.co/datasets/davanstrien/hub-tldr-model-summaries-llama\", \"task\": \"\", \"likes\": \"75\", \"downloads\": \"\", \"updated\": \"Feb 17\", \"size\": \"\"}, {\"name\": \"attn-signs/gromov-0\", \"link\": \"https://huggingface.co/datasets/attn-signs/gromov-0\", \"task\": \"\", \"likes\": \"29\", \"downloads\": \"\", \"updated\": \"Apr 12\", \"size\": \"\"}, {\"name\": \"Multiverse4FM/Multiverse-1K\", \"link\": \"https://huggingface.co/datasets/Multiverse4FM/Multiverse-1K\", \"task\": \"\", \"likes\": \"64\", \"downloads\": \"\", \"updated\": \"Jun 13\", \"size\": \"\"}, {\"name\": \"noepsl/H-LLMC2\", \"link\": \"https://huggingface.co/datasets/noepsl/H-LLMC2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"27 days ago\", \"size\": \"\"}, {\"name\": \"CTCT-CT2/ChangeMore-prompt-injection-eval\", \"link\": \"https://huggingface.co/datasets/CTCT-CT2/ChangeMore-prompt-injection-eval\", \"task\": \"\", \"likes\": \"72\", \"downloads\": \"\", \"updated\": \"Jul 25\", \"size\": \"\"}, {\"name\": \"amishor/reinforce-learning-grpo\", \"link\": \"https://huggingface.co/datasets/amishor/reinforce-learning-grpo\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 19\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.13106",
    "first_seen_date": "2025-01-23",
    "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.13106VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  UnderstandingPublished on Jan 22\u00b7Submitted byAKon Jan 23#3 Paper of the dayUpvote90+82Authors:Boqiang Zhang,Kehan Li,Zesen Cheng,Zhiqiang Hu,Yuqian Yuan,Guanzheng Chen,Sicong Leng,Yuming Jiang,Hang Zhang,Xin Li,Peng Jin,Wenqi Zhang,Fan Wang,Lidong Bing,Deli ZhaoAbstractA vision-centric multimodal foundation model, VideoLLaMA3, improves image and video understanding through strategic training stages and adaptive vision token encoding.AI-generated summaryIn this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: thevision-centric training paradigmandvision-centric framework design. The key\ninsight of ourvision-centric training paradigmis that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)vision-centric alignment stage, which warms up the vision encoder and\nprojector; 2)vision-language pretraining stage, which jointly tunes the vision\nencoder, projector, and LLM with large-scale image-text data covering multiple\ntypes (including scene images, documents, charts) as well as text-only data. 3)multi-task fine-tuning stage, which incorporates image-text SFT data for\ndownstream tasks and video-text data to establish a foundation for video\nunderstanding. 4)video-centric fine-tuning, which further improves the model's\ncapability in video understanding. As for the framework design, to better\ncapture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes intovision tokenswith corresponding\nnumbers, rath",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/DAMO-NLP-SG/VideoLLaMA3",
    "hf_paper_url": "https://huggingface.co/papers/2501.13106",
    "arxiv_url": "https://arxiv.org/abs/2501.13106",
    "num_models": 9,
    "models_list": "DAMO-NLP-SG/VideoLLaMA3-7B, DAMO-NLP-SG/VideoLLaMA3-2B, DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B, DAMO-NLP-SG/VideoLLaMA3-7B-Image, DAMO-NLP-SG/VideoLLaMA3-2B-Image, DAMO-NLP-SG/VL3-SigLIP-NaViT, DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B, MMR1/MMR1-32B-SFT, cbipok/VideoLLaMA3-2B-fork",
    "models_links": "https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-7B, https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-2B, https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B, https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-7B-Image, https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-2B-Image, https://huggingface.co/DAMO-NLP-SG/VL3-SigLIP-NaViT, https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B, https://huggingface.co/MMR1/MMR1-32B-SFT, https://huggingface.co/cbipok/VideoLLaMA3-2B-fork",
    "models_detailed": "[{\"name\": \"DAMO-NLP-SG/VideoLLaMA3-7B\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 2\"}, {\"name\": \"DAMO-NLP-SG/VideoLLaMA3-2B\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-2B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 3\"}, {\"name\": \"DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B\", \"task\": \"\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"DAMO-NLP-SG/VideoLLaMA3-7B-Image\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-7B-Image\", \"task\": \"Question Answering\", \"likes\": \"391\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"DAMO-NLP-SG/VideoLLaMA3-2B-Image\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoLLaMA3-2B-Image\", \"task\": \"Question Answering\", \"likes\": \"118\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"DAMO-NLP-SG/VL3-SigLIP-NaViT\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VL3-SigLIP-NaViT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 20\"}, {\"name\": \"DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B\", \"task\": \"\", \"likes\": \"54\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"MMR1/MMR1-32B-SFT\", \"link\": \"https://huggingface.co/MMR1/MMR1-32B-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\"}, {\"name\": \"cbipok/VideoLLaMA3-2B-fork\", \"link\": \"https://huggingface.co/cbipok/VideoLLaMA3-2B-fork\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Nov 19\"}]",
    "num_datasets": 1,
    "datasets_list": "DAMO-NLP-SG/VL3-Syn7M",
    "datasets_links": "https://huggingface.co/datasets/DAMO-NLP-SG/VL3-Syn7M",
    "datasets_detailed": "[{\"name\": \"DAMO-NLP-SG/VL3-Syn7M\", \"link\": \"https://huggingface.co/datasets/DAMO-NLP-SG/VL3-Syn7M\", \"task\": \"\", \"likes\": \"56\", \"downloads\": \"\", \"updated\": \"Feb 7\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.12273",
    "first_seen_date": "2025-01-22",
    "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  Refinement",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.12273Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  RefinementPublished on Jan 21\u00b7Submitted bySongyang Zhangon Jan 22Upvote14+6Authors:Maosong Cao,Taolin Zhang,Mo Li,Chuyu Zhang,Yunxin Liu,Haodong Duan,Songyang Zhang,Kai ChenAbstractCondor, a two-stage synthetic data generation framework, enhances the performance of large language models through World Knowledge Tree and Self-Reflection Refinement, generating high-quality supervised fine-tuning data.AI-generated summaryThe quality ofSupervised Fine-Tuning(SFT) data plays a critical role in\nenhancing the conversational capabilities ofLarge Language Models(LLMs).\nHowever, as LLMs become more advanced, the availability of high-quality\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\ngreater reliance on synthetic training data. In this work, we introduceCondor,\na novel two-stage synthetic data generation framework that incorporates World\nKnowledge Tree andSelf-Reflection Refinementto produce high-quality SFT data\nat scale. Our experimental results demonstrate that a base model fine-tuned on\nonly 20KCondor-generated samples achieves superior performance compared to\ncounterparts. The additional refinement stage inCondorfurther enables\niterative self-improvement for LLMs at various scales (up to 72B), validating\nthe effectiveness of our approach. Furthermore, our investigation into the\nscaling for synthetic data in post-training reveals substantial unexplored\npotential for performance improvements, opening promising avenues for future\nresearch.View arXiv pageView PDFGitHub38autoAdd to collectionCommunityzsytonyPaper authorPaper submitterJan 22Data:https://huggingface.co/datasets/internlm/Condor-SFT-20KSee translationReplylibrarian-botJan 24This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended b",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/internlm/condor",
    "hf_paper_url": "https://huggingface.co/papers/2501.12273",
    "arxiv_url": "https://arxiv.org/abs/2501.12273",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "internlm/Condor-SFT-20K",
    "datasets_links": "https://huggingface.co/datasets/internlm/Condor-SFT-20K",
    "datasets_detailed": "[{\"name\": \"internlm/Condor-SFT-20K\", \"link\": \"https://huggingface.co/datasets/internlm/Condor-SFT-20K\", \"task\": \"\", \"likes\": \"80\", \"downloads\": \"\", \"updated\": \"Jan 23\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.12326",
    "first_seen_date": "2025-01-22",
    "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.12326UI-TARS: Pioneering Automated GUI Interaction with Native AgentsPublished on Jan 21\u00b7Submitted byAKon Jan 22Upvote65+57Authors:Yujia Qin,Yining Ye,Junjie Fang,Haoming Wang,Shihao Liang,Shizuo Tian,Junda Zhang,Jiahao Li,Yunxin Li,Shijue Huang,Wanjun Zhong,Kuanye Li,Jiale Yang,Yu Miao,Woyu Lin,Longxiang Liu,Xu Jiang,Qianli Ma,Jingyu Li,Xiaojun Xiao,Kai Cai,Chuang Li+13 authorsAbstractUI-TARS, a native GUI agent model using screenshots as input, outperforms commercial models in various benchmarks through enhanced perception, unified action modeling, system-2 reasoning, and iterative training with reflective online traces.AI-generated summaryThis paper introduces UI-TARS, anative GUI agent modelthat solely perceives\nthe screenshots as input and performs human-like interactions (e.g., keyboard\nand mouse operations). Unlike prevailing agent frameworks that depend on\nheavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts\nand workflows, UI-TARS is an end-to-end model that outperforms these\nsophisticated frameworks. Experiments demonstrate its superior performance:\nUI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating\nperception, grounding, and GUI task execution. Notably, in the OSWorld\nbenchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15\nsteps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld,\nUI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several\nkey innovations: (1) Enhanced Perception: leveraging a large-scale dataset of\nGUI screenshots forcontext-aware understandingof UI elements and precise\ncaptioning; (2)Unified Action Modeling, which standardizes actions into a\nunified space across platforms and achieves precise grounding and interaction\nthrough large-scale action traces; (3)System-2 Reasoning, which incorporates\ndeliberate reasoning into multi-step decision making, invo",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/bytedance/UI-TARS",
    "hf_paper_url": "https://huggingface.co/papers/2501.12326",
    "arxiv_url": "https://arxiv.org/abs/2501.12326",
    "num_models": 21,
    "models_list": "ByteDance-Seed/UI-TARS-1.5-7B, ByteDance-Seed/UI-TARS-7B-DPO, ByteDance-Seed/UI-TARS-7B-SFT, ByteDance-Seed/UI-TARS-72B-DPO, ByteDance-Seed/UI-TARS-2B-SFT, ByteDance-Seed/UI-TARS-72B-SFT, lmstudio-community/UI-TARS-7B-DPO-GGUF, lmstudio-community/UI-TARS-2B-SFT-GGUF, lmstudio-community/UI-TARS-72B-DPO-GGUF, pauljmorris/UI-TARS-7B-DPO, parasail-ai/UI-TARS-7B-DPO, parasail-ai/UI-TARS-72B-DPO, tomaxe/UI-TARS-7B-DPO, tomaxe/UI-TARS-72B-DPO, Mungert/UI-TARS-1.5-7B-GGUF, ahmed-masry/UI-TARS-2B-SFT, swamysriv/uitars, swamysriv/UI-TARS-72B-SFT, what2up/UI-TARS-1.5-7B, chakra-labs/GLADOS-1, vocaela/Vocaela-500M",
    "models_links": "https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B, https://huggingface.co/ByteDance-Seed/UI-TARS-7B-DPO, https://huggingface.co/ByteDance-Seed/UI-TARS-7B-SFT, https://huggingface.co/ByteDance-Seed/UI-TARS-72B-DPO, https://huggingface.co/ByteDance-Seed/UI-TARS-2B-SFT, https://huggingface.co/ByteDance-Seed/UI-TARS-72B-SFT, https://huggingface.co/lmstudio-community/UI-TARS-7B-DPO-GGUF, https://huggingface.co/lmstudio-community/UI-TARS-2B-SFT-GGUF, https://huggingface.co/lmstudio-community/UI-TARS-72B-DPO-GGUF, https://huggingface.co/pauljmorris/UI-TARS-7B-DPO, https://huggingface.co/parasail-ai/UI-TARS-7B-DPO, https://huggingface.co/parasail-ai/UI-TARS-72B-DPO, https://huggingface.co/tomaxe/UI-TARS-7B-DPO, https://huggingface.co/tomaxe/UI-TARS-72B-DPO, https://huggingface.co/Mungert/UI-TARS-1.5-7B-GGUF, https://huggingface.co/ahmed-masry/UI-TARS-2B-SFT, https://huggingface.co/swamysriv/uitars, https://huggingface.co/swamysriv/UI-TARS-72B-SFT, https://huggingface.co/what2up/UI-TARS-1.5-7B, https://huggingface.co/chakra-labs/GLADOS-1, https://huggingface.co/vocaela/Vocaela-500M",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/UI-TARS-1.5-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 18\"}, {\"name\": \"ByteDance-Seed/UI-TARS-7B-DPO\", \"link\": \"https://huggingface.co/ByteDance-Seed/UI-TARS-7B-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"ByteDance-Seed/UI-TARS-7B-SFT\", \"link\": \"https://huggingface.co/ByteDance-Seed/UI-TARS-7B-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"ByteDance-Seed/UI-TARS-72B-DPO\", \"link\": \"https://huggingface.co/ByteDance-Seed/UI-TARS-72B-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"ByteDance-Seed/UI-TARS-2B-SFT\", \"link\": \"https://huggingface.co/ByteDance-Seed/UI-TARS-2B-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"ByteDance-Seed/UI-TARS-72B-SFT\", \"link\": \"https://huggingface.co/ByteDance-Seed/UI-TARS-72B-SFT\", \"task\": \"\", \"likes\": \"71\", \"downloads\": \"\", \"updated\": \"Jan 25\"}, {\"name\": \"lmstudio-community/UI-TARS-7B-DPO-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/UI-TARS-7B-DPO-GGUF\", \"task\": \"\", \"likes\": \"271\", \"downloads\": \"\", \"updated\": \"Jan 23\"}, {\"name\": \"lmstudio-community/UI-TARS-2B-SFT-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/UI-TARS-2B-SFT-GGUF\", \"task\": \"\", \"likes\": \"145\", \"downloads\": \"\", \"updated\": \"Jan 23\"}, {\"name\": \"lmstudio-community/UI-TARS-72B-DPO-GGUF\", \"link\": \"https://huggingface.co/lmstudio-community/UI-TARS-72B-DPO-GGUF\", \"task\": \"\", \"likes\": \"80\", \"downloads\": \"\", \"updated\": \"Jan 23\"}, {\"name\": \"pauljmorris/UI-TARS-7B-DPO\", \"link\": \"https://huggingface.co/pauljmorris/UI-TARS-7B-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"parasail-ai/UI-TARS-7B-DPO\", \"link\": \"https://huggingface.co/parasail-ai/UI-TARS-7B-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 3\"}, {\"name\": \"parasail-ai/UI-TARS-72B-DPO\", \"link\": \"https://huggingface.co/parasail-ai/UI-TARS-72B-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 3\"}, {\"name\": \"tomaxe/UI-TARS-7B-DPO\", \"link\": \"https://huggingface.co/tomaxe/UI-TARS-7B-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 14\"}, {\"name\": \"tomaxe/UI-TARS-72B-DPO\", \"link\": \"https://huggingface.co/tomaxe/UI-TARS-72B-DPO\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 15\"}, {\"name\": \"Mungert/UI-TARS-1.5-7B-GGUF\", \"link\": \"https://huggingface.co/Mungert/UI-TARS-1.5-7B-GGUF\", \"task\": \"\", \"likes\": \"403\", \"downloads\": \"\", \"updated\": \"Sep 24\"}, {\"name\": \"ahmed-masry/UI-TARS-2B-SFT\", \"link\": \"https://huggingface.co/ahmed-masry/UI-TARS-2B-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 9\"}, {\"name\": \"swamysriv/uitars\", \"link\": \"https://huggingface.co/swamysriv/uitars\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 30\"}, {\"name\": \"swamysriv/UI-TARS-72B-SFT\", \"link\": \"https://huggingface.co/swamysriv/UI-TARS-72B-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jul 27\"}, {\"name\": \"what2up/UI-TARS-1.5-7B\", \"link\": \"https://huggingface.co/what2up/UI-TARS-1.5-7B\", \"task\": \"\", \"likes\": \"7\", \"downloads\": \"\", \"updated\": \"Aug 7\"}, {\"name\": \"chakra-labs/GLADOS-1\", \"link\": \"https://huggingface.co/chakra-labs/GLADOS-1\", \"task\": \"\", \"likes\": \"17\", \"downloads\": \"\", \"updated\": \"Sep 23\"}, {\"name\": \"vocaela/Vocaela-500M\", \"link\": \"https://huggingface.co/vocaela/Vocaela-500M\", \"task\": \"\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Oct 19\"}]",
    "num_datasets": 1,
    "datasets_list": "Hcompany/WebClick",
    "datasets_links": "https://huggingface.co/datasets/Hcompany/WebClick",
    "datasets_detailed": "[{\"name\": \"Hcompany/WebClick\", \"link\": \"https://huggingface.co/datasets/Hcompany/WebClick\", \"task\": \"\", \"likes\": \"969\", \"downloads\": \"\", \"updated\": \"Jun 9\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.12368",
    "first_seen_date": "2025-01-22",
    "title": "InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward\n  Model",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.12368InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward\n  ModelPublished on Jan 21\u00b7Submitted byJiaqi Wangon Jan 22Upvote45+37Authors:Yuhang Zang,Xiaoyi Dong,Pan Zhang,Yuhang Cao,Ziyu Liu,Shengyuan Ding,Shenxi Wu,Yubo Ma,Haodong Duan,Wenwei Zhang,Kai Chen,Dahua Lin,Jiaqi WangAbstractIXC-2.5-Reward is a multi-modal reward model that enhances Large Vision Language Models by aligning them with human preferences, improving performance across various tasks and applications.AI-generated summaryDespite the promising performance ofLarge Vision Language Models(LVLMs) in\nvisual understanding, they occasionally generate incorrect outputs. Whilereward models(RMs) withreinforcement learningortest-time scalingoffer the\npotential for improving generation quality, a critical gap remains: publicly\navailable multi-modalRMsforLVLMsare scarce, and the implementation details\nof proprietary models are often unclear. We bridge this gap with\nInternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effectivemulti-modal reward modelthat alignsLVLMswith human preferences. To ensure\nthe robustness and versatility of IXC-2.5-Reward, we set up a high-quality\nmulti-modalpreference corpusspanningtext,image, andvideoinputs across\ndiverse domains, such asinstruction following,general understanding,text-rich documents,mathematical reasoning, andvideo understanding.\nIXC-2.5-Reward achieves excellent results on the latest multi-modal reward\nmodel benchmark and shows competitive performance ontext-only reward model\nbenchmarks. We further demonstrate three key applications of IXC-2.5-Reward:\n(1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward\nwithProximal Policy Optimization(PPO) yieldsIXC-2.5-Chat, which shows\nconsistent improvements ininstruction followingand multi-modal open-ended\ndialogue; (2) Selecting the best response fromcandidate responsesfortest-time sca",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/internlm/internlm-xcomposer",
    "hf_paper_url": "https://huggingface.co/papers/2501.12368",
    "arxiv_url": "https://arxiv.org/abs/2501.12368",
    "num_models": 2,
    "models_list": "internlm/internlm-xcomposer2d5-7b-reward, internlm/internlm-xcomposer2d5-7b-chat",
    "models_links": "https://huggingface.co/internlm/internlm-xcomposer2d5-7b-reward, https://huggingface.co/internlm/internlm-xcomposer2d5-7b-chat",
    "models_detailed": "[{\"name\": \"internlm/internlm-xcomposer2d5-7b-reward\", \"link\": \"https://huggingface.co/internlm/internlm-xcomposer2d5-7b-reward\", \"task\": \"\", \"likes\": \"94\", \"downloads\": \"\", \"updated\": \"May 20\"}, {\"name\": \"internlm/internlm-xcomposer2d5-7b-chat\", \"link\": \"https://huggingface.co/internlm/internlm-xcomposer2d5-7b-chat\", \"task\": \"Question Answering\", \"likes\": \"66\", \"downloads\": \"\", \"updated\": \"Jan 23\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2501.07783",
    "first_seen_date": "2025-01-16",
    "title": "Parameter-Inverted Image Pyramid Networks for Visual Perception and\n  Multimodal Understanding",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.07783Parameter-Inverted Image Pyramid Networks for Visual Perception and\n  Multimodal UnderstandingPublished on Jan 14\u00b7Submitted byZhaokai Wangon Jan 16Upvote8Authors:Zhaokai Wang,Xizhou Zhu,Xue Yang,Gen Luo,Hao Li,Changyao Tian,Wenhan Dou,Junqi Ge,Lewei Lu,Yu Qiao,Jifeng DaiAbstractParameter-Inverted Image Pyramid Networks (PIIP) use pretrained models to efficiently process multi-scale images, reducing computational cost while improving performance across various perception and multimodal understanding tasks.AI-generated summaryImage pyramids are widely adopted in top-performing methods to obtain\nmulti-scale features for precise visual perception and understanding. However,\ncurrent image pyramids use the same large-scale model to process multiple\nresolutions of images, leading to significant computational cost. To address\nthis challenge, we propose a novel network architecture, calledParameter-Inverted Image Pyramid Networks(PIIP). Specifically,PIIPuses\npretrained models (ViTsorCNNs) as branches to process multi-scale images,\nwhere images of higher resolutions are processed by smaller network branches to\nbalance computational cost and performance. To integrate information from\ndifferent spatial scales, we further propose a novel cross-branch feature\ninteraction mechanism. To validatePIIP, we apply it to various perception\nmodels and a representative multimodal large language model called LLaVA, and\nconduct extensive experiments on various tasks such as object detection,\nsegmentation, image classification and multimodal understanding.PIIPachieves\nsuperior performance compared to single-branch and existing multi-resolution\napproaches with lower computational cost. When applied toInternViT-6B, a\nlarge-scale vision foundation model,PIIPcan improve its performance by 1%-2%\non detection and segmentation with only 40%-60% of the original computation,\nfinally achieving 60.0box APonM",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/opengvlab/piip",
    "hf_paper_url": "https://huggingface.co/papers/2501.07783",
    "arxiv_url": "https://arxiv.org/abs/2501.07783",
    "num_models": 11,
    "models_list": "OpenGVLab/PIIP, OpenGVLab/PIIP-LLaVA_CLIP-BL_512-448_13B, OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_1024-336_13B, OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_640-224_7B, OpenGVLab/PIIP-LLaVA-Plus_ConvNeXt-L_CLIP-L_1024-336_7B, OpenGVLab/PIIP-LLaVA_ConvNeXt-L_CLIP-L_1024-336_13B, OpenGVLab/PIIP-LLaVA_CLIP-BL_512-448_7B, OpenGVLab/clip-vit-large-patch14to16-336, OpenGVLab/PIIP-LLaVA_ConvNeXt-L_CLIP-L_1024-336_7B, OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_1024-336_7B, OpenGVLab/PIIP-LLaVA_CLIP-BL_512-256_7B",
    "models_links": "https://huggingface.co/OpenGVLab/PIIP, https://huggingface.co/OpenGVLab/PIIP-LLaVA_CLIP-BL_512-448_13B, https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_1024-336_13B, https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_640-224_7B, https://huggingface.co/OpenGVLab/PIIP-LLaVA-Plus_ConvNeXt-L_CLIP-L_1024-336_7B, https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-L_CLIP-L_1024-336_13B, https://huggingface.co/OpenGVLab/PIIP-LLaVA_CLIP-BL_512-448_7B, https://huggingface.co/OpenGVLab/clip-vit-large-patch14to16-336, https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-L_CLIP-L_1024-336_7B, https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_1024-336_7B, https://huggingface.co/OpenGVLab/PIIP-LLaVA_CLIP-BL_512-256_7B",
    "models_detailed": "[{\"name\": \"OpenGVLab/PIIP\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 16\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA_CLIP-BL_512-448_13B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA_CLIP-BL_512-448_13B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_1024-336_13B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_1024-336_13B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_640-224_7B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_640-224_7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA-Plus_ConvNeXt-L_CLIP-L_1024-336_7B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA-Plus_ConvNeXt-L_CLIP-L_1024-336_7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA_ConvNeXt-L_CLIP-L_1024-336_13B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-L_CLIP-L_1024-336_13B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA_CLIP-BL_512-448_7B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA_CLIP-BL_512-448_7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/clip-vit-large-patch14to16-336\", \"link\": \"https://huggingface.co/OpenGVLab/clip-vit-large-patch14to16-336\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA_ConvNeXt-L_CLIP-L_1024-336_7B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-L_CLIP-L_1024-336_7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_1024-336_7B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA_ConvNeXt-B_CLIP-L_1024-336_7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}, {\"name\": \"OpenGVLab/PIIP-LLaVA_CLIP-BL_512-256_7B\", \"link\": \"https://huggingface.co/OpenGVLab/PIIP-LLaVA_CLIP-BL_512-256_7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 20\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2501.07301",
    "first_seen_date": "2025-01-14",
    "title": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.07301The Lessons of Developing Process Reward Models in Mathematical\n  ReasoningPublished on Jan 13\u00b7Submitted byChujie Zhengon Jan 14#1 Paper of the dayUpvote99+91Authors:Zhenru Zhang,Chujie Zheng,Yangzhen Wu,Beichen Zhang,Runji Lin,Bowen Yu,Dayiheng Liu,Jingren Zhou,Junyang LinAbstractProcess Reward Models improve performance in mathematical reasoning by integrating Monte Carlo estimation with LLM-as-a-judge, addressing biases in Best-of-N evaluation and step-wise error identification.AI-generated summaryProcess Reward Models(PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared toLLM-as-a-judgeand human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives ofprocess verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2501.07301",
    "arxiv_url": "https://arxiv.org/abs/2501.07301",
    "num_models": 3,
    "models_list": "Qwen/Qwen2.5-Math-PRM-7B, Qwen/Qwen2.5-Math-PRM-72B, Qwen/Qwen2.5-Math-7B-PRM800K",
    "models_links": "https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B, https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B, https://huggingface.co/Qwen/Qwen2.5-Math-7B-PRM800K",
    "models_detailed": "[{\"name\": \"Qwen/Qwen2.5-Math-PRM-7B\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 17\"}, {\"name\": \"Qwen/Qwen2.5-Math-PRM-72B\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-Math-PRM-72B\", \"task\": \"\", \"likes\": \"314\", \"downloads\": \"\", \"updated\": \"Jan 17\"}, {\"name\": \"Qwen/Qwen2.5-Math-7B-PRM800K\", \"link\": \"https://huggingface.co/Qwen/Qwen2.5-Math-7B-PRM800K\", \"task\": \"\", \"likes\": \"399\", \"downloads\": \"\", \"updated\": \"Jan 17\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2501.07574",
    "first_seen_date": "2025-01-14",
    "title": "UnCommon Objects in 3D",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.07574UnCommon Objects in 3DPublished on Jan 13\u00b7Submitted byAKon Jan 14Upvote13+5Authors:Xingchen Liu,Piyush Tayal,Jianyuan Wang,Jesus Zarzar,Tom Monnier,Konstantinos Tertikas,Jiali Duan,Antoine Toisoul,Jason Y. Zhang,Natalia Neverova,Andrea Vedaldi,Roman Shapovalov,David NovotnyAbstractThe introduction of uCO3D, a comprehensive 3D dataset with high-resolution videos and detailed annotations, enhances the performance of 3D deep learning and generative AI models.AI-generated summaryWe introduce Uncommon Objects in 3D (uCO3D), a newobject-centric datasetfor\n3D deep learning and 3D generative AI. uCO3D is the largest publicly-available\ncollection of high-resolution videos of objects with3D annotationsthat\nensures full-360^{circ} coverage. uCO3D is significantly more diverse than\nMVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of\nhigher quality, due to extensive quality checks of both the collected videos\nand the3D annotations. Similar to analogous datasets, uCO3D contains\nannotations for3D camera poses,depth mapsandsparse point clouds. In\naddition, each object is equipped with a caption and a 3D Gaussian Splat\nreconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D\nand obtain superior results using the latter, showing that uCO3D is better for\nlearning applications.View arXiv pageView PDFProject pageGitHub1.31kAdd to collectionCommunityakhaliqPaper submitterJan 14https://uco3d.github.io/See translationReplylibrarian-botJan 15This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APIHOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos(2024)MVImgNet2.0: A Larger-scale Dataset of Multi-view Images(2024)Street Gaussians without 3D Object Tracker(2024)Holistic Understanding of 3D Scenes as Universal Scene De",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/facebookresearch/uco3d",
    "hf_paper_url": "https://huggingface.co/papers/2501.07574",
    "arxiv_url": "https://arxiv.org/abs/2501.07574",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 2,
    "datasets_list": "facebook/uco3d, Voxel51/uco3d",
    "datasets_links": "https://huggingface.co/datasets/facebook/uco3d, https://huggingface.co/datasets/Voxel51/uco3d",
    "datasets_detailed": "[{\"name\": \"facebook/uco3d\", \"link\": \"https://huggingface.co/datasets/facebook/uco3d\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 17\", \"size\": \"\"}, {\"name\": \"Voxel51/uco3d\", \"link\": \"https://huggingface.co/datasets/Voxel51/uco3d\", \"task\": \"\", \"likes\": \"104\", \"downloads\": \"\", \"updated\": \"Aug 21\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.06186",
    "first_seen_date": "2025-01-13",
    "title": "LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.06186LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMsPublished on Jan 10\u00b7Submitted byAhmed Heaklon Jan 13#3 Paper of the dayUpvote65+57Authors:Omkar Thawakar,Dinura Dissanayake,Ketan More,Ritesh Thawkar,Ahmed Heakl,Noor Ahsan,Yuhao Li,Mohammed Zumri,Jean Lahoud,Rao Muhammad Anwer,Hisham Cholakkal,Ivan Laptev,Mubarak Shah,Fahad Shahbaz Khan,Salman KhanAbstractA framework for evaluating and improving step-by-step visual reasoning in large language models using a specialized benchmark and a novel multimodal model trained with curriculum learning.AI-generated summaryReasoning is a fundamental capability for solving complex multi-step\nproblems, particularly in visual contexts where sequential step-wise\nunderstanding is essential. Existing approaches lack a comprehensive framework\nfor evaluatingvisual reasoningand do not emphasize step-wiseproblem-solving.\nTo this end, we propose a comprehensive framework for advancing step-by-stepvisual reasoninginlarge language models(LMMs) through three key\ncontributions. First, we introduce avisual reasoning benchmarkspecifically\ndesigned to evaluatemulti-step reasoning tasks. The benchmark presents a\ndiverse set of challenges with eight different categories ranging from complexvisual perceptiontoscientific reasoningwith over 4kreasoning stepsin\ntotal, enabling robust evaluation of LLMs' abilities to perform accurate and\ninterpretablevisual reasoningacross multiple steps. Second, we propose a\nnovel metric that assessesvisual reasoning qualityat the granularity of\nindividual steps, emphasizing both correctness and logical coherence. The\nproposed metric offers deeper insights into reasoning performance compared to\ntraditionalend-task accuracy metrics. Third, we present a new multimodalvisual reasoningmodel, namedLlamaV-o1, trained using a multi-step curriculum\nlearning approach, where tasks are progressively organized to facilitateincr",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/mbzuai-oryx/llamav-o1",
    "hf_paper_url": "https://huggingface.co/papers/2501.06186",
    "arxiv_url": "https://arxiv.org/abs/2501.06186",
    "num_models": 1,
    "models_list": "omkarthawakar/LlamaV-o1",
    "models_links": "https://huggingface.co/omkarthawakar/LlamaV-o1",
    "models_detailed": "[{\"name\": \"omkarthawakar/LlamaV-o1\", \"link\": \"https://huggingface.co/omkarthawakar/LlamaV-o1\", \"task\": \"Question Answering\", \"likes\": \"62\", \"downloads\": \"\", \"updated\": \"Jan 13\"}]",
    "num_datasets": 3,
    "datasets_list": "ahmedheakl/llamav-o1-instruct-stage1, omkarthawakar/VRC-Bench, ahmedheakl/llamav-o1-instruct-stage2",
    "datasets_links": "https://huggingface.co/datasets/ahmedheakl/llamav-o1-instruct-stage1, https://huggingface.co/datasets/omkarthawakar/VRC-Bench, https://huggingface.co/datasets/ahmedheakl/llamav-o1-instruct-stage2",
    "datasets_detailed": "[{\"name\": \"ahmedheakl/llamav-o1-instruct-stage1\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/llamav-o1-instruct-stage1\", \"task\": \"\", \"likes\": \"285\", \"downloads\": \"\", \"updated\": \"Jan 14\", \"size\": \"\"}, {\"name\": \"omkarthawakar/VRC-Bench\", \"link\": \"https://huggingface.co/datasets/omkarthawakar/VRC-Bench\", \"task\": \"\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"Jan 13\", \"size\": \"\"}, {\"name\": \"ahmedheakl/llamav-o1-instruct-stage2\", \"link\": \"https://huggingface.co/datasets/ahmedheakl/llamav-o1-instruct-stage2\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jan 14\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.05040",
    "first_seen_date": "2025-01-10",
    "title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue Resolution",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.05040SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub\n  Issue ResolutionPublished on Jan 9\u00b7Submitted bychengxing xieon Jan 10Upvote15+7Authors:Chengxing Xie,Bowen Li,Chang Gao,He Du,Wai Lam,Difan Zou,Kai ChenAbstractSWE-Fixer is an open-source Large Language Model designed to resolve GitHub issues using two modules: a code file retrieval module that uses BM25 and a lightweight LLM, and a code editing module that generates patches for identified files, achieving state-of-the-art performance on the SWE-Bench Lite and Verified benchmarks.AI-generated summaryLarge Language Models(LLMs) have demonstrated remarkable proficiency across\na variety of complex tasks. One significant application ofLLMsis in tackling\nsoftware engineering challenges, particularly in resolving real-world tasks on\nGitHub by fixing code based on the issues reported by the users. However, many\ncurrent approaches rely on proprietaryLLMs, which limits reproducibility,\naccessibility, and transparency. The critical components ofLLMsfor addressing\nsoftware engineering issues and how their capabilities can be effectively\nenhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a\nnovel open-source LLM designed to effectively and efficiently resolve GitHub\nissues. SWE-Fixer comprises two essential modules: acode file retrieval moduleand acode editing module. The retrieval module employsBM25along with a\nlightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently,\nthecode editing moduleutilizes the other LLM model to generate patches for\nthe identified files. Then, to mitigate the lack of publicly available\ndatasets, we compile an extensive dataset that includes 110KGitHub issuesalong with their corresponding patches, and train the two modules of SWE-Fixer\nseparately. We assess our approach on theSWE-Bench Liteand Verified\nbenchmarks, achieving state-of-the-ar",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/internlm/swe-fixer",
    "hf_paper_url": "https://huggingface.co/papers/2501.05040",
    "arxiv_url": "https://arxiv.org/abs/2501.05040",
    "num_models": 2,
    "models_list": "internlm/SWE-Fixer-Editor-72B, internlm/SWE-Fixer-Retriever-7B",
    "models_links": "https://huggingface.co/internlm/SWE-Fixer-Editor-72B, https://huggingface.co/internlm/SWE-Fixer-Retriever-7B",
    "models_detailed": "[{\"name\": \"internlm/SWE-Fixer-Editor-72B\", \"link\": \"https://huggingface.co/internlm/SWE-Fixer-Editor-72B\", \"task\": \"Text Generation\", \"likes\": \"65\", \"downloads\": \"\", \"updated\": \"Jan 10\"}, {\"name\": \"internlm/SWE-Fixer-Retriever-7B\", \"link\": \"https://huggingface.co/internlm/SWE-Fixer-Retriever-7B\", \"task\": \"Text Generation\", \"likes\": \"79\", \"downloads\": \"\", \"updated\": \"Jan 10\"}]",
    "num_datasets": 1,
    "datasets_list": "internlm/SWE-Fixer-Train-110K",
    "datasets_links": "https://huggingface.co/datasets/internlm/SWE-Fixer-Train-110K",
    "datasets_detailed": "[{\"name\": \"internlm/SWE-Fixer-Train-110K\", \"link\": \"https://huggingface.co/datasets/internlm/SWE-Fixer-Train-110K\", \"task\": \"\", \"likes\": \"98\", \"downloads\": \"\", \"updated\": \"Mar 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.03575",
    "first_seen_date": "2025-01-08",
    "title": "Cosmos World Foundation Model Platform for Physical AI",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.03575Cosmos World Foundation Model Platform for Physical AIPublished on Jan 7\u00b7Submitted byAKon Jan 8#2 Paper of the dayUpvote81+73Authors:NVIDIA,Niket Agarwal,Arslan Ali,Maciej Bala,Yogesh Balaji,Erik Barker,Tiffany Cai,Prithvijit Chattopadhyay,Yongxin Chen,Yin Cui,Yifan Ding,Daniel Dworakowski,Jiaojiao Fan,Michele Fenzi,Francesco Ferroni,Sanja Fidler,Dieter Fox,Songwei Ge,Yunhao Ge,Jinwei Gu,Siddharth Gururani,Ethan He+56 authorsAbstractThe Cosmos World Foundation Model Platform provides a framework for developing customized world models for Physical AI by leveraging video curation, pre-trained models, and tokenizers.AI-generated summaryPhysical AI needs to be trained digitally first. It needs adigital twinof\nitself, thepolicy model, and adigital twinof the world, theworld model. In\nthis paper, we present the CosmosWorld Foundation ModelPlatform to help\ndevelopers build customizedworld models for their Physical AI setups. We\nposition aworld foundation modelas a general-purposeworld modelthat can be\nfine-tuned into customizedworld models for downstream applications. Our\nplatform covers avideo curationpipeline,pre-trained world foundation models,\nexamples ofpost-trainingofpre-trained world foundation models, and video\ntokenizers. To help Physical AI builders solve the most critical problems of\nour society, we make our platform open-source and our models open-weight with\npermissive licenses available via https://github.com/NVIDIA/Cosmos.View arXiv pageView PDFProject pageGitHub388autoAdd to collectionCommunityakhaliqPaper submitterJan 8https://github.com/NVIDIA/CosmosSee translation\ud83d\udc4d11+Replylibrarian-botJan 9This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar APISynthetic Vision: Training Vision-Language Models to Understand Physics(2024)Predictive Inverse Dynamics M",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/nvidia-cosmos/cosmos-predict1",
    "hf_paper_url": "https://huggingface.co/papers/2501.03575",
    "arxiv_url": "https://arxiv.org/abs/2501.03575",
    "num_models": 48,
    "models_list": "nvidia/Cosmos-1.0-Diffusion-7B-Text2World, nvidia/Cosmos-1.0-Diffusion-14B-Text2World, nvidia/Cosmos-1.0-Guardrail, nvidia/Cosmos-1.0-Diffusion-14B-Video2World, nvidia/Cosmos-Transfer1-7B-Sample-AV, nvidia/Cosmos-Guardrail1, nvidia/Cosmos-1.0-Tokenizer-CV8x8x8, nvidia/Cosmos-1.0-Tokenizer-DV8x16x16, nvidia/Cosmos-1.0-Prompt-Upsampler-12B-Text2World, nvidia/Cosmos-1.0-Diffusion-7B-Video2World, nvidia/Cosmos-1.0-Autoregressive-13B-Video2World, nvidia/Cosmos-1.0-Autoregressive-12B, nvidia/Cosmos-1.0-Autoregressive-5B-Video2World, nvidia/Cosmos-1.0-Diffusion-7B-Decoder-DV8x16x16ToCV8x8x8, nvidia/Cosmos-1.0-Autoregressive-4B, EthanZyh/DiffusionText2WorldGeneration, Nvidia-CMU25/DiffusionVideo2WorldGeneration, NeverMore0123/AutoregressiveVideo2WorldGeneration, Nvidia-CMU25/AutoregressiveBase, Nvidia-CMU25/DiffusionText2WorldGeneration, Nvidia-CMU25/ARVideo2WorldGeneration, Nvidia-CMU25/AutoregressiveFutureWorld, appmana/Cosmos-1.0-Prompt-Upsampler-12B-Text2World-hf, nvidia/Cosmos-Predict1-7B-Text2World-Sample-AV-Multiview, nvidia/Cosmos-Predict1-7B-Video2World-Sample-AV-Multiview, nvidia/Cosmos-Predict1-7B-Text2World, nvidia/Cosmos-Predict1-7B-Video2World, nvidia/Cosmos-Predict1-14B-Text2World, nvidia/Cosmos-Predict1-14B-Video2World, nvidia/Cosmos-Predict1-4B, nvidia/Cosmos-Predict1-12B, nvidia/Cosmos-Predict1-13B-Video2World, nvidia/Cosmos-Predict1-5B-Video2World, nvidia/Cosmos-Tokenize1-CI8x8-360p, nvidia/Cosmos-Tokenize1-CI16x16-360p, nvidia/Cosmos-Tokenize1-CV4x8x8-360p, nvidia/Cosmos-Tokenize1-DI8x8-360p, nvidia/Cosmos-Tokenize1-DI16x16-360p, nvidia/Cosmos-Tokenize1-DV4x8x8-360p, nvidia/Cosmos-Tokenize1-CV8x8x8-720p, nvidia/Cosmos-Tokenize1-DV8x16x16-720p, nvidia/Cosmos-UpsamplePrompt1-12B-Text2World, nvidia/Cosmos-Predict1-7B-Decoder-DV8x16x16ToCV8x8x8-720p, nvidia/Cosmos-UpsamplePrompt1-12B-Transfer, nvidia/Cosmos-Predict1-7B-WorldInterpolator, nvidia/Cosmos-Predict1-7B-Video2World-Sample-AV-Single2MultiView, nvidia/Cosmos-Transfer1-7B-Sample-AV-Single2MultiView, hiskiv/cosmos-models",
    "models_links": "https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-7B-Text2World, https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Text2World, https://huggingface.co/nvidia/Cosmos-1.0-Guardrail, https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Video2World, https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV, https://huggingface.co/nvidia/Cosmos-Guardrail1, https://huggingface.co/nvidia/Cosmos-1.0-Tokenizer-CV8x8x8, https://huggingface.co/nvidia/Cosmos-1.0-Tokenizer-DV8x16x16, https://huggingface.co/nvidia/Cosmos-1.0-Prompt-Upsampler-12B-Text2World, https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-7B-Video2World, https://huggingface.co/nvidia/Cosmos-1.0-Autoregressive-13B-Video2World, https://huggingface.co/nvidia/Cosmos-1.0-Autoregressive-12B, https://huggingface.co/nvidia/Cosmos-1.0-Autoregressive-5B-Video2World, https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-7B-Decoder-DV8x16x16ToCV8x8x8, https://huggingface.co/nvidia/Cosmos-1.0-Autoregressive-4B, https://huggingface.co/EthanZyh/DiffusionText2WorldGeneration, https://huggingface.co/Nvidia-CMU25/DiffusionVideo2WorldGeneration, https://huggingface.co/NeverMore0123/AutoregressiveVideo2WorldGeneration, https://huggingface.co/Nvidia-CMU25/AutoregressiveBase, https://huggingface.co/Nvidia-CMU25/DiffusionText2WorldGeneration, https://huggingface.co/Nvidia-CMU25/ARVideo2WorldGeneration, https://huggingface.co/Nvidia-CMU25/AutoregressiveFutureWorld, https://huggingface.co/appmana/Cosmos-1.0-Prompt-Upsampler-12B-Text2World-hf, https://huggingface.co/nvidia/Cosmos-Predict1-7B-Text2World-Sample-AV-Multiview, https://huggingface.co/nvidia/Cosmos-Predict1-7B-Video2World-Sample-AV-Multiview, https://huggingface.co/nvidia/Cosmos-Predict1-7B-Text2World, https://huggingface.co/nvidia/Cosmos-Predict1-7B-Video2World, https://huggingface.co/nvidia/Cosmos-Predict1-14B-Text2World, https://huggingface.co/nvidia/Cosmos-Predict1-14B-Video2World, https://huggingface.co/nvidia/Cosmos-Predict1-4B, https://huggingface.co/nvidia/Cosmos-Predict1-12B, https://huggingface.co/nvidia/Cosmos-Predict1-13B-Video2World, https://huggingface.co/nvidia/Cosmos-Predict1-5B-Video2World, https://huggingface.co/nvidia/Cosmos-Tokenize1-CI8x8-360p, https://huggingface.co/nvidia/Cosmos-Tokenize1-CI16x16-360p, https://huggingface.co/nvidia/Cosmos-Tokenize1-CV4x8x8-360p, https://huggingface.co/nvidia/Cosmos-Tokenize1-DI8x8-360p, https://huggingface.co/nvidia/Cosmos-Tokenize1-DI16x16-360p, https://huggingface.co/nvidia/Cosmos-Tokenize1-DV4x8x8-360p, https://huggingface.co/nvidia/Cosmos-Tokenize1-CV8x8x8-720p, https://huggingface.co/nvidia/Cosmos-Tokenize1-DV8x16x16-720p, https://huggingface.co/nvidia/Cosmos-UpsamplePrompt1-12B-Text2World, https://huggingface.co/nvidia/Cosmos-Predict1-7B-Decoder-DV8x16x16ToCV8x8x8-720p, https://huggingface.co/nvidia/Cosmos-UpsamplePrompt1-12B-Transfer, https://huggingface.co/nvidia/Cosmos-Predict1-7B-WorldInterpolator, https://huggingface.co/nvidia/Cosmos-Predict1-7B-Video2World-Sample-AV-Single2MultiView, https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV-Single2MultiView, https://huggingface.co/hiskiv/cosmos-models",
    "models_detailed": "[{\"name\": \"nvidia/Cosmos-1.0-Diffusion-7B-Text2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-7B-Text2World\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/Cosmos-1.0-Diffusion-14B-Text2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Text2World\", \"task\": \"\", \"likes\": \"381\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/Cosmos-1.0-Guardrail\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Guardrail\", \"task\": \"\", \"likes\": \"257\", \"downloads\": \"\", \"updated\": \"Jun 11\"}, {\"name\": \"nvidia/Cosmos-1.0-Diffusion-14B-Video2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Video2World\", \"task\": \"\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/Cosmos-Transfer1-7B-Sample-AV\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 9\"}, {\"name\": \"nvidia/Cosmos-Guardrail1\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Guardrail1\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 1\"}, {\"name\": \"nvidia/Cosmos-1.0-Tokenizer-CV8x8x8\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Tokenizer-CV8x8x8\", \"task\": \"\", \"likes\": \"223\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/Cosmos-1.0-Tokenizer-DV8x16x16\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Tokenizer-DV8x16x16\", \"task\": \"\", \"likes\": \"109\", \"downloads\": \"\", \"updated\": \"Jan 12\"}, {\"name\": \"nvidia/Cosmos-1.0-Prompt-Upsampler-12B-Text2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Prompt-Upsampler-12B-Text2World\", \"task\": \"\", \"likes\": \"79\", \"downloads\": \"\", \"updated\": \"Jan 10\"}, {\"name\": \"nvidia/Cosmos-1.0-Diffusion-7B-Video2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-7B-Video2World\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"May 7\"}, {\"name\": \"nvidia/Cosmos-1.0-Autoregressive-13B-Video2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Autoregressive-13B-Video2World\", \"task\": \"\", \"likes\": \"44\", \"downloads\": \"\", \"updated\": \"Feb 8\"}, {\"name\": \"nvidia/Cosmos-1.0-Autoregressive-12B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Autoregressive-12B\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Feb 11\"}, {\"name\": \"nvidia/Cosmos-1.0-Autoregressive-5B-Video2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Autoregressive-5B-Video2World\", \"task\": \"\", \"likes\": \"48\", \"downloads\": \"\", \"updated\": \"Feb 8\"}, {\"name\": \"nvidia/Cosmos-1.0-Diffusion-7B-Decoder-DV8x16x16ToCV8x8x8\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-7B-Decoder-DV8x16x16ToCV8x8x8\", \"task\": \"\", \"likes\": \"34\", \"downloads\": \"\", \"updated\": \"Jan 10\"}, {\"name\": \"nvidia/Cosmos-1.0-Autoregressive-4B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-1.0-Autoregressive-4B\", \"task\": \"\", \"likes\": \"82\", \"downloads\": \"\", \"updated\": \"Feb 11\"}, {\"name\": \"EthanZyh/DiffusionText2WorldGeneration\", \"link\": \"https://huggingface.co/EthanZyh/DiffusionText2WorldGeneration\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 2\"}, {\"name\": \"Nvidia-CMU25/DiffusionVideo2WorldGeneration\", \"link\": \"https://huggingface.co/Nvidia-CMU25/DiffusionVideo2WorldGeneration\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 12\"}, {\"name\": \"NeverMore0123/AutoregressiveVideo2WorldGeneration\", \"link\": \"https://huggingface.co/NeverMore0123/AutoregressiveVideo2WorldGeneration\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 4\"}, {\"name\": \"Nvidia-CMU25/AutoregressiveBase\", \"link\": \"https://huggingface.co/Nvidia-CMU25/AutoregressiveBase\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 4\"}, {\"name\": \"Nvidia-CMU25/DiffusionText2WorldGeneration\", \"link\": \"https://huggingface.co/Nvidia-CMU25/DiffusionText2WorldGeneration\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 4\"}, {\"name\": \"Nvidia-CMU25/ARVideo2WorldGeneration\", \"link\": \"https://huggingface.co/Nvidia-CMU25/ARVideo2WorldGeneration\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 4\"}, {\"name\": \"Nvidia-CMU25/AutoregressiveFutureWorld\", \"link\": \"https://huggingface.co/Nvidia-CMU25/AutoregressiveFutureWorld\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Feb 11\"}, {\"name\": \"appmana/Cosmos-1.0-Prompt-Upsampler-12B-Text2World-hf\", \"link\": \"https://huggingface.co/appmana/Cosmos-1.0-Prompt-Upsampler-12B-Text2World-hf\", \"task\": \"\", \"likes\": \"9\", \"downloads\": \"\", \"updated\": \"Feb 24\"}, {\"name\": \"nvidia/Cosmos-Predict1-7B-Text2World-Sample-AV-Multiview\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-7B-Text2World-Sample-AV-Multiview\", \"task\": \"\", \"likes\": \"89\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-7B-Video2World-Sample-AV-Multiview\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-7B-Video2World-Sample-AV-Multiview\", \"task\": \"\", \"likes\": \"36\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-7B-Text2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-7B-Text2World\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-7B-Video2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-7B-Video2World\", \"task\": \"\", \"likes\": \"70\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-14B-Text2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-14B-Text2World\", \"task\": \"\", \"likes\": \"53\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-14B-Video2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-14B-Video2World\", \"task\": \"\", \"likes\": \"58\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-4B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-4B\", \"task\": \"\", \"likes\": \"48\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-12B\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-12B\", \"task\": \"\", \"likes\": \"42\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-13B-Video2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-13B-Video2World\", \"task\": \"\", \"likes\": \"41\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-5B-Video2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-5B-Video2World\", \"task\": \"\", \"likes\": \"46\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Tokenize1-CI8x8-360p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Tokenize1-CI8x8-360p\", \"task\": \"\", \"likes\": \"69\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"nvidia/Cosmos-Tokenize1-CI16x16-360p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Tokenize1-CI16x16-360p\", \"task\": \"\", \"likes\": \"55\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"nvidia/Cosmos-Tokenize1-CV4x8x8-360p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Tokenize1-CV4x8x8-360p\", \"task\": \"\", \"likes\": \"75\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"nvidia/Cosmos-Tokenize1-DI8x8-360p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Tokenize1-DI8x8-360p\", \"task\": \"\", \"likes\": \"60\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"nvidia/Cosmos-Tokenize1-DI16x16-360p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Tokenize1-DI16x16-360p\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"nvidia/Cosmos-Tokenize1-DV4x8x8-360p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Tokenize1-DV4x8x8-360p\", \"task\": \"\", \"likes\": \"81\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"nvidia/Cosmos-Tokenize1-CV8x8x8-720p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Tokenize1-CV8x8x8-720p\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Apr 23\"}, {\"name\": \"nvidia/Cosmos-Tokenize1-DV8x16x16-720p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Tokenize1-DV8x16x16-720p\", \"task\": \"\", \"likes\": \"101\", \"downloads\": \"\", \"updated\": \"Mar 18\"}, {\"name\": \"nvidia/Cosmos-UpsamplePrompt1-12B-Text2World\", \"link\": \"https://huggingface.co/nvidia/Cosmos-UpsamplePrompt1-12B-Text2World\", \"task\": \"\", \"likes\": \"296\", \"downloads\": \"\", \"updated\": \"Apr 1\"}, {\"name\": \"nvidia/Cosmos-Predict1-7B-Decoder-DV8x16x16ToCV8x8x8-720p\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-7B-Decoder-DV8x16x16ToCV8x8x8-720p\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 16\"}, {\"name\": \"nvidia/Cosmos-UpsamplePrompt1-12B-Transfer\", \"link\": \"https://huggingface.co/nvidia/Cosmos-UpsamplePrompt1-12B-Transfer\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 26\"}, {\"name\": \"nvidia/Cosmos-Predict1-7B-WorldInterpolator\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-7B-WorldInterpolator\", \"task\": \"\", \"likes\": \"82\", \"downloads\": \"\", \"updated\": \"Apr 8\"}, {\"name\": \"nvidia/Cosmos-Predict1-7B-Video2World-Sample-AV-Single2MultiView\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Predict1-7B-Video2World-Sample-AV-Single2MultiView\", \"task\": \"\", \"likes\": \"8\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"nvidia/Cosmos-Transfer1-7B-Sample-AV-Single2MultiView\", \"link\": \"https://huggingface.co/nvidia/Cosmos-Transfer1-7B-Sample-AV-Single2MultiView\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Jun 5\"}, {\"name\": \"hiskiv/cosmos-models\", \"link\": \"https://huggingface.co/hiskiv/cosmos-models\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 17\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2501.04001",
    "first_seen_date": "2025-01-08",
    "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.04001Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and VideosPublished on Jan 7\u00b7Submitted byXiangtai Lion Jan 8\u00b7ByteDanceUpvote47+39Authors:Haobo Yuan,Xiangtai Li,Tao Zhang,Zilong Huang,Shilin Xu,Shunping Ji,Yunhai Tong,Lu Qi,Jiashi Feng,Ming-Hsuan YangAbstractSa2VA unifies image and video understanding by integrating SAM-2 and LLaVA into a shared token space, achieving state-of-the-art performance across multiple tasks.AI-generated summaryThis work presentsSa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,Sa2VAsupports a wide range of image and video tasks, including referring\nsegmentation andconversation, with minimal one-shot instruction tuning.Sa2VAcombinesSAM-2, a foundation video segmentation model, withLLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM,Sa2VAgenerates instruction tokens that guideSAM-2in producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduceRef-SAV, anauto-labeled datasetcontaining over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in theRef-SAVdatasets to benchmark referring video object\nsegmentation in complex environments. Experiments show thatSa2VAachieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.View arXiv pageView PDFGitHub1.46kAdd to collectionCommunityLXTPaper authorPaper submitterJan 8\u2022edited Jan 8We present Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/magic-research/Sa2VA",
    "hf_paper_url": "https://huggingface.co/papers/2501.04001",
    "arxiv_url": "https://arxiv.org/abs/2501.04001",
    "num_models": 14,
    "models_list": "ByteDance/Sa2VA-4B, ByteDance/Sa2VA-8B, ByteDance/Sa2VA-26B, ByteDance/Sa2VA-1B, ByteDance/Sa2VA-Qwen3-VL-2B, kumuji/Sa2VA-i-4B, kumuji/Sa2VA-i-1B, kumuji/Sa2VA-i-8B, ByteDance/Sa2VA-InternVL3-2B, ByteDance/Sa2VA-InternVL3-8B, ByteDance/Sa2VA-InternVL3-14B, ByteDance/Sa2VA-Qwen2_5-VL-3B, ByteDance/Sa2VA-Qwen2_5-VL-7B, ByteDance/Sa2VA-Qwen3-VL-4B",
    "models_links": "https://huggingface.co/ByteDance/Sa2VA-4B, https://huggingface.co/ByteDance/Sa2VA-8B, https://huggingface.co/ByteDance/Sa2VA-26B, https://huggingface.co/ByteDance/Sa2VA-1B, https://huggingface.co/ByteDance/Sa2VA-Qwen3-VL-2B, https://huggingface.co/kumuji/Sa2VA-i-4B, https://huggingface.co/kumuji/Sa2VA-i-1B, https://huggingface.co/kumuji/Sa2VA-i-8B, https://huggingface.co/ByteDance/Sa2VA-InternVL3-2B, https://huggingface.co/ByteDance/Sa2VA-InternVL3-8B, https://huggingface.co/ByteDance/Sa2VA-InternVL3-14B, https://huggingface.co/ByteDance/Sa2VA-Qwen2_5-VL-3B, https://huggingface.co/ByteDance/Sa2VA-Qwen2_5-VL-7B, https://huggingface.co/ByteDance/Sa2VA-Qwen3-VL-4B",
    "models_detailed": "[{\"name\": \"ByteDance/Sa2VA-4B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"ByteDance/Sa2VA-8B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-8B\", \"task\": \"\", \"likes\": \"357\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"ByteDance/Sa2VA-26B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-26B\", \"task\": \"\", \"likes\": \"39\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"ByteDance/Sa2VA-1B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-1B\", \"task\": \"\", \"likes\": \"293\", \"downloads\": \"\", \"updated\": \"Sep 8\"}, {\"name\": \"ByteDance/Sa2VA-Qwen3-VL-2B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-Qwen3-VL-2B\", \"task\": \"\", \"likes\": \"155\", \"downloads\": \"\", \"updated\": \"24 days ago\"}, {\"name\": \"kumuji/Sa2VA-i-4B\", \"link\": \"https://huggingface.co/kumuji/Sa2VA-i-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"kumuji/Sa2VA-i-1B\", \"link\": \"https://huggingface.co/kumuji/Sa2VA-i-1B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"kumuji/Sa2VA-i-8B\", \"link\": \"https://huggingface.co/kumuji/Sa2VA-i-8B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"26 days ago\"}, {\"name\": \"ByteDance/Sa2VA-InternVL3-2B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-InternVL3-2B\", \"task\": \"\", \"likes\": \"152\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"ByteDance/Sa2VA-InternVL3-8B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-InternVL3-8B\", \"task\": \"\", \"likes\": \"48\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"ByteDance/Sa2VA-InternVL3-14B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-InternVL3-14B\", \"task\": \"\", \"likes\": \"140\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"ByteDance/Sa2VA-Qwen2_5-VL-3B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-Qwen2_5-VL-3B\", \"task\": \"\", \"likes\": \"50\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"ByteDance/Sa2VA-Qwen2_5-VL-7B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-Qwen2_5-VL-7B\", \"task\": \"\", \"likes\": \"65\", \"downloads\": \"\", \"updated\": \"Oct 16\"}, {\"name\": \"ByteDance/Sa2VA-Qwen3-VL-4B\", \"link\": \"https://huggingface.co/ByteDance/Sa2VA-Qwen3-VL-4B\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 21\"}]",
    "num_datasets": 3,
    "datasets_list": "Dense-World/Sa2VA-Training, tlzhang96/Sa2VA-Training, QuanzhuNiu/MeViS-Qframe",
    "datasets_links": "https://huggingface.co/datasets/Dense-World/Sa2VA-Training, https://huggingface.co/datasets/tlzhang96/Sa2VA-Training, https://huggingface.co/datasets/QuanzhuNiu/MeViS-Qframe",
    "datasets_detailed": "[{\"name\": \"Dense-World/Sa2VA-Training\", \"link\": \"https://huggingface.co/datasets/Dense-World/Sa2VA-Training\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Sep 8\", \"size\": \"\"}, {\"name\": \"tlzhang96/Sa2VA-Training\", \"link\": \"https://huggingface.co/datasets/tlzhang96/Sa2VA-Training\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"13 days ago\", \"size\": \"\"}, {\"name\": \"QuanzhuNiu/MeViS-Qframe\", \"link\": \"https://huggingface.co/datasets/QuanzhuNiu/MeViS-Qframe\", \"task\": \"\", \"likes\": \"22\", \"downloads\": \"\", \"updated\": \"Oct 3\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.00599",
    "first_seen_date": "2025-01-03",
    "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLM",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.00599VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLMPublished on Dec 31, 2024\u00b7Submitted byYuqianYuanon Jan 3Upvote47+39Authors:Yuqian Yuan,Hang Zhang,Wentong Li,Zesen Cheng,Boqiang Zhang,Long Li,Xin Li,Deli Zhao,Wenqiao Zhang,Yueting Zhuang,Jianke Zhu,Lidong BingAbstractThe VideoRefer Suite enhances Video LLMs for finer spatial-temporal understanding through a comprehensive dataset, model, and benchmark.AI-generated summaryVideo Large Language Models (Video LLMs) have recently exhibited remarkable\ncapabilities in general video understanding. However, they mainly focus on\nholistic comprehension and struggle with capturing fine-grained spatial and\ntemporal details. Besides, the lack of high-quality object-level video\ninstruction data and a comprehensive benchmark further hinders their\nadvancements. To tackle these challenges, we introduce theVideoRefer Suiteto\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\nenabling perception and reasoning on any objects throughout the video.\nSpecially, we thoroughly developVideoRefer Suiteacross three essential\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\ndata engine to meticulously curate a large-scale, high-quality object-level\nvideo instruction dataset, termedVideoRefer-700K. Next, we present the\nVideoRefer model, which equips a versatilespatial-temporal object encoderto\ncapture precise regional and sequential representations. Finally, we\nmeticulously create aVideoRefer-Benchto comprehensively assess the\nspatial-temporal understanding capability of a Video LLM, evaluating it across\nvarious aspects. Extensive experiments and analyses demonstrate that our\nVideoRefer model not only achieves promising performance on video referring\nbenchmarks but also facilitates general video understanding capabilities.View arXiv pageView PDFGitHub332autoAdd to col",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/damo-nlp-sg/videorefer",
    "hf_paper_url": "https://huggingface.co/papers/2501.00599",
    "arxiv_url": "https://arxiv.org/abs/2501.00599",
    "num_models": 3,
    "models_list": "DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B, DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B, MMR1/MMR1-32B-SFT",
    "models_links": "https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B, https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B, https://huggingface.co/MMR1/MMR1-32B-SFT",
    "models_detailed": "[{\"name\": \"DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B\", \"task\": \"\", \"likes\": \"59\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B\", \"link\": \"https://huggingface.co/DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B\", \"task\": \"\", \"likes\": \"54\", \"downloads\": \"\", \"updated\": \"Jun 19\"}, {\"name\": \"MMR1/MMR1-32B-SFT\", \"link\": \"https://huggingface.co/MMR1/MMR1-32B-SFT\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Oct 1\"}]",
    "num_datasets": 0,
    "datasets_list": "",
    "datasets_links": "",
    "datasets_detailed": "[]"
  },
  {
    "arxiv_id": "2501.00958",
    "first_seen_date": "2025-01-03",
    "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.009582.5 Years in Class: A Multimodal Textbook for Vision-Language\n  PretrainingPublished on Jan 1\u00b7Submitted byWenqi Zhangon Jan 3#1 Paper of the dayUpvote107+99Authors:Wenqi Zhang,Hang Zhang,Xin Li,Jiashuo Sun,Yongliang Shen,Weiming Lu,Deli Zhao,Yueting Zhuang,Lidong BingAbstractA new high-quality instructional video-based corpus improves VLM pretraining through more coherent context and richer knowledge, enhancing performance in knowledge-intensive tasks.AI-generated summaryCompared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vastinstructional videos(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality multimodal\ntextbook corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years ofinstructional videos, totaling 22,000 class hours.\nWe first use anLLM-proposed taxonomyto systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as animage-text interleaved corpusbased on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks likeScienceQAandMathVista. Moreover,VLMspre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual c",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/damo-nlp-sg/multimodal_textbook",
    "hf_paper_url": "https://huggingface.co/papers/2501.00958",
    "arxiv_url": "https://arxiv.org/abs/2501.00958",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "DAMO-NLP-SG/multimodal_textbook",
    "datasets_links": "https://huggingface.co/datasets/DAMO-NLP-SG/multimodal_textbook",
    "datasets_detailed": "[{\"name\": \"DAMO-NLP-SG/multimodal_textbook\", \"link\": \"https://huggingface.co/datasets/DAMO-NLP-SG/multimodal_textbook\", \"task\": \"\", \"likes\": \"\", \"downloads\": \"\", \"updated\": \"Mar 17\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.01257",
    "first_seen_date": "2025-01-03",
    "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.01257CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo RatingsPublished on Jan 2\u00b7Submitted byAKon Jan 3#3 Paper of the dayUpvote52+44Authors:Shanghaoran Quan,Jiaxi Yang,Bowen Yu,Bo Zheng,Dayiheng Liu,An Yang,Xuancheng Ren,Bofei Gao,Yibo Miao,Yunlong Feng,Zekun Wang,Jian Yang,Zeyu Cui,Yang Fan,Yichang Zhang,Binyuan Hui,Junyang LinAbstractCodeElo is a standardized code generation benchmark based on CodeForces contests, providing comprehensive evaluation of LLMs' coding abilities with reliable Elo ratings.AI-generated summaryWith the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models likeOpenAI o1and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, likeLiveCodeBenchandUSACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduceCodeElo, a standardizedcompetition-level code generationbenchmark that\neffectively addresses all these challenges for the first time.CodeElobenchmark is mainly based on the officialCodeForcesplatform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems onCodeForceswith detailed information such as contest\ndivisions, problem difficulty ratings, and problemalgorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliableElo ratingcalculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on ourCodeElo, we provide theElo ratings of 30 existing popular\nopen-source and 3 proprietaryLLMsfor the first time. The results show",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "",
    "hf_paper_url": "https://huggingface.co/papers/2501.01257",
    "arxiv_url": "https://arxiv.org/abs/2501.01257",
    "num_models": 0,
    "models_list": "",
    "models_links": "",
    "models_detailed": "[]",
    "num_datasets": 1,
    "datasets_list": "Qwen/CodeElo",
    "datasets_links": "https://huggingface.co/datasets/Qwen/CodeElo",
    "datasets_detailed": "[{\"name\": \"Qwen/CodeElo\", \"link\": \"https://huggingface.co/datasets/Qwen/CodeElo\", \"task\": \"\", \"likes\": \"408\", \"downloads\": \"\", \"updated\": \"Jan 5\", \"size\": \"\"}]"
  },
  {
    "arxiv_id": "2501.01320",
    "first_seen_date": "2025-01-03",
    "title": "SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video\n  Restoration",
    "summary": "Hugging FaceModelsDatasetsSpacesCommunityDocsEnterprisePricingLog InSign UpPapersarxiv:2501.01320SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video\n  RestorationPublished on Jan 2\u00b7Submitted byJIANYI WANGon Jan 3\u00b7ByteDance SeedUpvote12+4Authors:Jianyi Wang,Zhijie Lin,Meng Wei,Yang Zhao,Ceyuan Yang,Chen Change Loy,Lu JiangAbstractSeedVR, a diffusion transformer with shifted window attention, effectively handles real-world video restoration with high performance on synthetic and real-world benchmarks.AI-generated summaryVideo restorationposes non-trivial challenges in maintaining fidelity while\nrecovering temporally consistent details from unknown degradations in the wild.\nDespite recent advances in diffusion-based restoration, these methods often\nface limitations in generation capability and sampling efficiency. In this\nwork, we present SeedVR, adiffusion transformerdesigned to handle real-worldvideo restorationwith arbitrary length and resolution. The core design of\nSeedVR lies in theshifted window attentionthat facilitates effective\nrestoration on long video sequences. SeedVR further supports variable-sized\nwindows near the boundary of both spatial and temporal dimensions, overcoming\nthe resolution constraints of traditional window attention. Equipped with\ncontemporary practices, includingcausal video autoencoder, mixed image and\nvideo training, andprogressive training, SeedVR achieves highly-competitive\nperformance on both synthetic and real-world benchmarks, as well as\nAI-generated videos. Extensive experiments demonstrate SeedVR's superiority\nover existing methods for genericvideo restoration.View arXiv pageView PDFProject pageGitHub105Add to collectionCommunityIceclearPaper authorPaper submitterJan 3https://iceclear.github.io/projects/seedvr/See translationReplylibrarian-botJan 4This is an automated message from theLibrarian Bot. I found the following papers similar to this paper.The following papers were recommended by the Semantic Scholar ",
    "authors": "",
    "num_authors": 0,
    "upvotes": 2501,
    "github_repo": "https://github.com/IceClear/SeedVR",
    "hf_paper_url": "https://huggingface.co/papers/2501.01320",
    "arxiv_url": "https://arxiv.org/abs/2501.01320",
    "num_models": 2,
    "models_list": "ByteDance-Seed/SeedVR-7B, ByteDance-Seed/SeedVR-3B",
    "models_links": "https://huggingface.co/ByteDance-Seed/SeedVR-7B, https://huggingface.co/ByteDance-Seed/SeedVR-3B",
    "models_detailed": "[{\"name\": \"ByteDance-Seed/SeedVR-7B\", \"link\": \"https://huggingface.co/ByteDance-Seed/SeedVR-7B\", \"task\": \"\", \"likes\": \"128\", \"downloads\": \"\", \"updated\": \"Jun 20\"}, {\"name\": \"ByteDance-Seed/SeedVR-3B\", \"link\": \"https://huggingface.co/ByteDance-Seed/SeedVR-3B\", \"task\": \"\", \"likes\": \"78\", \"downloads\": \"\", \"updated\": \"Jun 20\"}]",
    "num_datasets": 1,
    "datasets_list": "Iceclear/SeedVR_VideoDemos",
    "datasets_links": "https://huggingface.co/datasets/Iceclear/SeedVR_VideoDemos",
    "datasets_detailed": "[{\"name\": \"Iceclear/SeedVR_VideoDemos\", \"link\": \"https://huggingface.co/datasets/Iceclear/SeedVR_VideoDemos\", \"task\": \"\", \"likes\": \"104\", \"downloads\": \"\", \"updated\": \"Jun 19\", \"size\": \"\"}]"
  }
]